15:46:26.654130 [2] proc begin: <DistEnv 2/4 nccl>
16:08:59.885533 [2] proc begin: <DistEnv 2/4 nccl>
16:10:30.559279 [2] proc begin: <DistEnv 2/4 nccl>
17:06:50.099182 [2] proc begin: <DistEnv 2/4 nccl>
17:06:56.690367 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
17:06:56.694401 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53351 KB |  217029 KB |  301754 KB |  248402 KB |
|       from large pool |   53351 KB |  217029 KB |  279213 KB |  225862 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:07:03.305582 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.020929 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:04.564174 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.110831 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:05.651855 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.193911 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:06.733479 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.278050 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:07.819540 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.366279 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:08.914850 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:09.511238 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.062370 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:10.612908 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.158726 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:11.703023 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.247501 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:12.789170 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.335042 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:13.882885 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:14.428924 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.027790 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:15.572466 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.120503 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:16.664376 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.210765 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:17.755062 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.303638 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:18.845018 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.392335 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:19.937498 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:20.536815 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.081974 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:21.625535 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.171196 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:22.710353 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.254249 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:23.796642 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.339435 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:24.883919 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:25.426547 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.023823 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:26.566649 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.108675 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:27.646335 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.185437 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:28.723392 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.264407 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:29.800934 [2] Warning: no training nodes in this partition! Backward fake loss.
17:07:30.341139 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.136053 [2] proc begin: <DistEnv 2/4 nccl>
17:17:48.916822 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:17:48.917959 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:17:50.337953 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.507991 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.582819 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.643613 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.709040 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.771887 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.831488 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.893270 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.956459 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.021664 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.084874 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.157587 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.228489 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.299372 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.382082 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.447807 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.507368 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.577571 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.643959 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.713073 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.780044 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.856787 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.923737 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:51.990961 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.056467 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.129582 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.210807 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.284917 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.350060 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.414872 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.491842 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.559951 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.627293 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.697350 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.784369 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.886786 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.992336 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.068846 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.147782 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.224108 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.298136 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.369860 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.445394 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.519516 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.590030 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.683369 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.788448 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.895246 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:53.996227 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.106449 [2] Warning: no training nodes in this partition! Backward fake loss.
13:29:51.791190 [2] proc begin: <DistEnv 2/4 nccl>
13:30:15.394968 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
13:30:15.397543 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  367117 KB |  369001 KB |  372785 KB |    5668 KB |
|       from large pool |  365521 KB |  367404 KB |  371169 KB |    5647 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  367117 KB |  369001 KB |  372785 KB |    5668 KB |
|       from large pool |  365521 KB |  367404 KB |  371169 KB |    5647 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  393216 KB |  393216 KB |  393216 KB |       0 B  |
|       from large pool |  391168 KB |  391168 KB |  391168 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   26098 KB |   26098 KB |   33134 KB |    7036 KB |
|       from large pool |   25646 KB |   25646 KB |   31294 KB |    5647 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:33:33.363220 [2] proc begin: <DistEnv 2/4 nccl>
13:33:42.117168 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
13:33:42.121148 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  367117 KB |  369001 KB |  372785 KB |    5668 KB |
|       from large pool |  365521 KB |  367404 KB |  371169 KB |    5647 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| Active memory         |  367117 KB |  369001 KB |  372785 KB |    5668 KB |
|       from large pool |  365521 KB |  367404 KB |  371169 KB |    5647 KB |
|       from small pool |    1596 KB |    1598 KB |    1616 KB |      20 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  393216 KB |  393216 KB |  393216 KB |       0 B  |
|       from large pool |  391168 KB |  391168 KB |  391168 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   26098 KB |   26098 KB |   33134 KB |    7036 KB |
|       from large pool |   25646 KB |   25646 KB |   31294 KB |    5647 KB |
|       from small pool |     452 KB |    1820 KB |    1840 KB |    1388 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      53    |      36    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      40    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:26.578259 [2] proc begin: <DistEnv 2/4 nccl>
13:35:43.035448 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
13:35:43.038023 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53351 KB |  217029 KB |  301754 KB |  248402 KB |
|       from large pool |   53351 KB |  217029 KB |  279213 KB |  225862 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:35:57.958009 [2] Warning: no training nodes in this partition! Backward fake loss.
13:35:58.674467 [2] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.219538 [2] Warning: no training nodes in this partition! Backward fake loss.
13:35:59.760514 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.303176 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:00.846214 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.390881 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:01.965616 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:02.536531 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.101944 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:03.645736 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.247368 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:04.794918 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.342617 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:05.889616 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.438034 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:06.987269 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:07.537421 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.090482 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:08.636957 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.186276 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:09.782983 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.327780 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:10.875387 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.421548 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:11.970909 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:12.517605 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.067818 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:13.616220 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.169685 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:14.717198 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.315483 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:15.860005 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.403477 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:16.942886 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:17.483015 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.026372 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:18.562985 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.108079 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:19.648192 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.191468 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:20.784711 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.332955 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:21.876285 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.416956 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:22.962805 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:23.502941 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.042698 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:24.583232 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.126980 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:25.667284 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.260465 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:26.800060 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.345738 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:27.888300 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.431971 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:28.973003 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:29.517255 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.058135 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:30.598117 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.142156 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:31.734629 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.277342 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:32.819485 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.358412 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:33.903166 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.447417 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:34.995003 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:35.542552 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.096503 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:36.644078 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.252940 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:37.797339 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.345412 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:38.891980 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.435929 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:39.979086 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:40.520584 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.060049 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:41.596904 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.135751 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:42.728135 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.267176 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:43.804691 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.344231 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:44.883502 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.425607 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:45.963629 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:46.501427 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.042848 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:47.581434 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.177100 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:48.714055 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.254018 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:49.790492 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.328521 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:50.865768 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.408544 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:51.951583 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:52.492080 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.035549 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:53.629686 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.171159 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:54.711190 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.254640 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:55.793991 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.336361 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:56.873736 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.414195 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:57.958544 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:58.498788 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.095195 [2] Warning: no training nodes in this partition! Backward fake loss.
13:36:59.633759 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.178130 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:00.717424 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.262270 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:01.802895 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.367099 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:02.939783 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:03.506748 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.051717 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:04.648655 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.198609 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:05.744432 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.298650 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:06.849685 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.396830 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:07.942923 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:08.488516 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.038913 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:09.582549 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.179923 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:10.718263 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.259705 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:11.795823 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.334305 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:12.872882 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.415573 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:13.960581 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:14.501933 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.043555 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:15.638155 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.182652 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:16.722900 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.266295 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:17.804444 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.345243 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:18.886049 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.425042 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:19.967142 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:20.508765 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.104406 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:21.644863 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.191141 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:22.730408 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.274708 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:23.815245 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.357853 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:24.900305 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.441810 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:25.984774 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:26.578189 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.123867 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:27.663941 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.204996 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:28.747403 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.294467 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:29.840404 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.387184 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:30.936680 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:31.485834 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.086990 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:32.630504 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.178110 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:33.715614 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.254973 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:34.792974 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.331317 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:35.868169 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.405648 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:36.946191 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:37.542964 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.084891 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:38.625533 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.165415 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:39.703247 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.243575 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:40.780036 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.319244 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:41.859789 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.396484 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:42.986194 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:43.527502 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.066628 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:44.605903 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.146729 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:45.681994 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.221484 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:46.763096 [2] Warning: no training nodes in this partition! Backward fake loss.
13:37:47.302912 [2] Warning: no training nodes in this partition! Backward fake loss.
13:39:46.274456 [2] proc begin: <DistEnv 2/4 nccl>
13:39:48.871912 [2] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 2, |V|: 42336, |E|: 333013>
13:39:48.875476 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   26252 KB |   44155 KB |   51509 KB |   25256 KB |
|       from large pool |   22491 KB |   43659 KB |   47739 KB |   25248 KB |
|       from small pool |    3761 KB |    3763 KB |    3770 KB |       8 KB |
|---------------------------------------------------------------------------|
| Active memory         |   26252 KB |   44155 KB |   51509 KB |   25256 KB |
|       from large pool |   22491 KB |   43659 KB |   47739 KB |   25248 KB |
|       from small pool |    3761 KB |    3763 KB |    3770 KB |       8 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   69632 KB |   69632 KB |   69632 KB |       0 B  |
|       from large pool |   65536 KB |   65536 KB |   65536 KB |       0 B  |
|       from small pool |    4096 KB |    4096 KB |    4096 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   20851 KB |   23428 KB |   29446 KB |    8594 KB |
|       from large pool |   20517 KB |   21877 KB |   25957 KB |    5440 KB |
|       from small pool |     334 KB |    1882 KB |    3489 KB |    3154 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      38    |      21    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      38    |      21    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      32    |      17    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:32.313772 [2] proc begin: <DistEnv 2/4 nccl>
13:55:38.560252 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
13:55:38.563746 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53351 KB |  217029 KB |  301754 KB |  248402 KB |
|       from large pool |   53351 KB |  217029 KB |  279213 KB |  225862 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:55:43.805164 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:44.513970 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.050620 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:45.591913 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.131806 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:46.674433 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.214555 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:47.759539 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.297734 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:48.841705 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.382921 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:49.976476 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:50.518308 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.056703 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:51.595342 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.135539 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:52.673200 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.208311 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:53.744475 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.279521 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:54.817404 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.407156 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:55.944517 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:56.481420 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.021792 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:57.558834 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.094608 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:58.635213 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.171587 [2] Warning: no training nodes in this partition! Backward fake loss.
13:55:59.710768 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.242467 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:00.835885 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.375710 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:01.941766 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:02.507211 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.064464 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:03.604431 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.144731 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:04.683881 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.221793 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:05.761159 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.350674 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:06.891284 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.427743 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:07.966393 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:08.506270 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.042159 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:09.581408 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.117776 [2] Warning: no training nodes in this partition! Backward fake loss.
13:56:10.658360 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:09.578435 [2] proc begin: <DistEnv 2/4 nccl>
13:59:11.880275 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
13:59:11.882311 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

13:59:13.629901 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.852451 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:13.928944 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.032641 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.104175 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.182153 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.254843 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.330962 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.412671 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.516078 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.624055 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.727053 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.830906 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.918483 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:14.999255 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.098450 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.195617 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.303286 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.397202 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.502196 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.608331 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.717476 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.822218 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:15.924708 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.023724 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.121560 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.221455 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.322115 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.421568 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.526306 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.635155 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.718030 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.800146 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.873695 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:16.941705 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.007230 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.080199 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.160005 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.255734 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.352832 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.447555 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.535323 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.605116 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.704223 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.811667 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:17.912934 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.018081 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.088490 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.159065 [2] Warning: no training nodes in this partition! Backward fake loss.
13:59:18.251888 [2] Warning: no training nodes in this partition! Backward fake loss.
15:17:46.794405 [2] proc begin: <DistEnv 2/4 nccl>
15:17:48.277330 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:17:48.278450 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:13.984977 [2] proc begin: <DistEnv 2/4 nccl>
15:18:15.468178 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:18:15.469354 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:16.860316 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.001406 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.018025 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.037338 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.056639 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.073486 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.088045 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.103614 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.116503 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.131493 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.144209 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.159418 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.172282 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.183375 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.198549 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.212472 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.225483 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.240526 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.252362 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.267218 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.282150 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.297631 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.313388 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.330175 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.348528 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.364618 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.377935 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.393440 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.406674 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.421665 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.439720 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.460077 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.476777 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.494738 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.513352 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.534804 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.550421 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.563047 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.577844 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.593031 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.605141 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.623855 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.647217 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.665656 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.684953 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.705952 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.726056 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.745994 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.767173 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:17.785882 [2] Warning: no training nodes in this partition! Backward fake loss.
16:04:46.951301 [2] proc begin: <DistEnv 2/4 nccl>
16:04:51.313216 [2] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 2, |V|: 42336, |E|: 333013>
16:04:51.320863 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26250 KiB |  44155 KiB |  51387 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3759 KiB |   3759 KiB |   3759 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20851 KiB |  23428 KiB |  29442 KiB |   8590 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    334 KiB |   1882 KiB |   3485 KiB |   3150 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:51.688883 [2] proc begin: <DistEnv 2/4 nccl>
16:13:52.316770 [2] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 2, |V|: 42336, |E|: 333013>
16:13:52.331932 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26250 KiB |  44155 KiB |  51387 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3759 KiB |   3759 KiB |   3759 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20851 KiB |  23428 KiB |  29442 KiB |   8590 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    334 KiB |   1882 KiB |   3485 KiB |   3150 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:34:42.554025 [2] proc begin: <DistEnv 2/4 nccl>
16:34:43.211584 [2] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 2, |V|: 42336, |E|: 333013>
16:34:43.222179 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26250 KiB |  44155 KiB |  51387 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3759 KiB |   3759 KiB |   3759 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20851 KiB |  23428 KiB |  29442 KiB |   8590 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    334 KiB |   1882 KiB |   3485 KiB |   3150 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:21.966587 [2] proc begin: <DistEnv 2/4 nccl>
15:59:23.642475 [2] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 2, |V|: 42336, |E|: 333013>
15:59:23.684107 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         |  26252 KiB |  44155 KiB |  51505 KiB |  25252 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47739 KiB |  25248 KiB |
|       from small pool |   3761 KiB |   3763 KiB |   3766 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |  26250 KiB |  44155 KiB |  51387 KiB |  25137 KiB |
|       from large pool |  22491 KiB |  43659 KiB |  47628 KiB |  25137 KiB |
|       from small pool |   3759 KiB |   3759 KiB |   3759 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  69632 KiB |  69632 KiB |  69632 KiB |      0 B   |
|       from large pool |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |
|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  20851 KiB |  23428 KiB |  29442 KiB |   8590 KiB |
|       from large pool |  20517 KiB |  21877 KiB |  25957 KiB |   5440 KiB |
|       from small pool |    334 KiB |   1882 KiB |   3485 KiB |   3150 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |      15    |      18    |      24    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       8    |       4    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:28.013445 [2] proc begin: <DistEnv 2/4 nccl>
16:00:28.182691 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:00:28.193942 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:29.612884 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.340402 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.348368 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.356406 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.367161 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.373107 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.377495 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.381786 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.388372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.394392 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.398730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.403949 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.407940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.411765 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.416042 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.421497 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.425133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.428844 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.432563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.436290 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.440115 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.445032 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.449003 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.452879 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.456712 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.460188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.463944 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.467921 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.473943 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.479150 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.482782 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.487646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.491513 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.495207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.499169 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.503363 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.507571 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.511659 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.517807 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.525413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.531430 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.536914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.543208 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.549578 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.554597 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.560351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.566591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.574079 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.580294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.587680 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.594945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.599920 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.604084 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.608115 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.614124 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.622069 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.627049 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.631054 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.635096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.640251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.644629 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.653044 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.658375 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.664105 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.670895 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.677145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.682000 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.686728 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.691589 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.696397 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.701019 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.708374 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.713486 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.718484 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.723299 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.727526 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.733816 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.738297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.742413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.746339 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.750973 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.756358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.760529 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.766112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.773376 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.779158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.783993 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.790401 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.795016 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.799265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.803431 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.810899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.818229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.822463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.829587 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.834902 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.838584 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.842718 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.846788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.850571 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.854466 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.861173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.865506 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.869442 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.873073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.878843 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.883252 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.887148 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.891368 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.895248 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.900618 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.907878 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.913366 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.917563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.922220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.928341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.935604 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.940071 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.943998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.948187 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.952687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.957860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.961749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.965770 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.969635 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.973997 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.978188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.982049 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.985802 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.989681 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.993828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:30.999261 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.003254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.007228 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.011148 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.018249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.022611 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.027221 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.031325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.035700 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.039579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.044303 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.048666 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.052689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.056498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.060241 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.064119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.069431 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.075351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.079261 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.083277 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.088756 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.094941 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.099254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.104673 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.109825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.123064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.129205 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.134423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.138789 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.151628 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.159900 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.164212 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.168566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.174727 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.179621 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.183955 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.187910 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.191714 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.195590 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.199464 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.206170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.211332 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.217672 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.223860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.228624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.232966 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.236894 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.240608 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.244307 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.248189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.253115 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.257235 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.261136 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.265341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.269345 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.273387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.277475 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.281577 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.285602 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.289661 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.294785 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.298788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.304632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.311587 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.315790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.319869 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.323917 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.327671 [2] Warning: no training nodes in this partition! Backward fake loss.
16:00:31.331544 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:22.755666 [2] proc begin: <DistEnv 2/4 nccl>
16:05:22.806084 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:05:22.815259 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:05:24.916245 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.608830 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.615890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.622840 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.628809 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.633006 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.636995 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.643278 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.647091 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.650874 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.654500 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.661058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.666833 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.671399 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.676966 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.681086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.684946 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.688945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.692593 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.696098 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.700606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.707484 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.712776 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.716687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.720708 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.726933 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.733293 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.737717 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.743718 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.748353 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.752087 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.757167 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.761106 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.764563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.768477 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.772736 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.778426 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.783989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.788414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.792386 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.796089 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.800658 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.805832 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.811170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.815289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.819931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.824092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.828133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.833594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.839102 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.844133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.848971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.854615 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.863548 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.868028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.872186 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.877361 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.883615 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.888114 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.892250 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.896419 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.901524 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.905397 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.909476 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.913428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.917313 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.921204 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.925267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.929471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.933806 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.937657 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.942341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.946174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.950118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.953948 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.960100 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.965775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.970904 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.977795 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.983653 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.987750 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.992664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:25.996611 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.000554 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.006655 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.013018 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.019896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.026812 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.032061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.037144 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.041887 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.046716 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.052169 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.057918 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.062467 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.066540 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.070614 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.074418 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.078389 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.082446 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.087221 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.094101 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.098364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.102285 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.105751 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.111886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.116070 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.121047 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.125780 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.129913 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.133711 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.144751 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.148793 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.152723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.156822 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.160804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.164327 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.168349 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.173696 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.180994 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.186634 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.192479 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.196183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.200145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.203977 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.208011 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.211959 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.215785 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.219645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.223423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.227427 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.232211 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.236048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.239790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.243605 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.247500 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.251288 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.255272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.259374 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.263227 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.266852 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.271757 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.275794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.279595 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.283547 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.287206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.290771 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.294697 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.298873 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.303305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.306940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.311305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.315360 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.319267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.322887 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.327606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.332794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.336707 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.340249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.344013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.348025 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.352810 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.356328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.360065 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.364976 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.368721 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.374567 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.380597 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.386536 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.390647 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.394329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.399143 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.403005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.406750 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.412748 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.416647 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.420530 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.424300 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.428293 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.432154 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.437515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.445768 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.450254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.454140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.458008 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.461773 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.465346 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.469046 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.472745 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.478177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.484112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.490436 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.494871 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.498986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.503274 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.509058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.514694 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.521347 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.526870 [2] Warning: no training nodes in this partition! Backward fake loss.
16:05:26.531063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:11:45.406805 [2] proc begin: <DistEnv 2/4 nccl>
16:12:00.902831 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:12:00.923322 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:57.944217 [2] proc begin: <DistEnv 2/4 nccl>
16:46:58.004075 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:46:58.013692 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:46:59.282497 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.949618 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.958066 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.963496 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.967549 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.973331 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.978255 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.982256 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.986125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.990226 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.994495 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.999927 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.004406 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.008966 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.013250 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.017640 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.024878 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.030177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.036208 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.040698 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.044546 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.049827 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.054012 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.059528 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.065218 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.069585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.073535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.077336 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.081290 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.085162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.089059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.093787 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.097909 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.101684 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.107431 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.111481 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.115544 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.120713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.126086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.131178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.138070 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.143338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.147388 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.151009 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.155357 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.159482 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.164281 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.169234 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.175612 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.181362 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.187158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.192286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.196415 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.200606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.204760 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.208458 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.212307 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.216557 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.220623 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.226367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.231896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.239675 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.244458 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.250439 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.255830 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.259657 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.265220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.270618 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.275575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.279565 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.283453 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.289953 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.296521 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.303244 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.307155 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.311029 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.315059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.319206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.323356 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.329283 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.333564 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.338705 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.342608 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.349142 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.353947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.357982 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.363575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.367294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.372916 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.377997 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.382327 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.388532 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.392591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.396476 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.402024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.406234 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.410164 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.414058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.417666 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.421364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.427386 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.435904 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.441143 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.446310 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.450753 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.454650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.458426 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.464594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.470829 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.474649 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.478743 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.484002 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.487726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.491323 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.494889 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.498914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.503041 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.506945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.510886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.514456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.518695 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.523694 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.527520 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.531330 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.535092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.538982 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.542695 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.546600 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.550267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.553957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.557755 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.562978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.567202 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.571088 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.575163 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.579199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.584855 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.591304 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.595202 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.599170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.602825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.607726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.613573 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.619620 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.623950 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.628090 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.631677 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.635631 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.639484 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.643236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.646827 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.651454 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.655263 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.658726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.662265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.666017 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.669776 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.673850 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.677593 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.681247 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.684861 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.689603 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.696041 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.700367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.704260 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.708064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.711578 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.715330 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.719173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.722986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.726772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.731531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.735239 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.738875 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.742669 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.746181 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.749914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.753701 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.759309 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.764289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.768335 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.773896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.778174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.782414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.788659 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.793085 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.797351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.801413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.805693 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.810082 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.814184 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.819261 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.823019 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.826744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.830733 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.834470 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.838315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.842331 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.846454 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.850343 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:45.973363 [2] proc begin: <DistEnv 2/4 nccl>
16:47:46.024057 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:47:46.033406 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:48.110534 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.734107 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.741891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.748445 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.753289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.758014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.762507 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.766790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.774508 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.779158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.784372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.792259 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.797997 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.802166 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.806125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.810327 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.814268 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.818249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.822238 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.826227 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.830501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.835791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.840579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.844734 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.848764 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.854612 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.862796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.866720 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.871089 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.876975 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.882722 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.891231 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.899203 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.905053 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.910053 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.913970 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.917924 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.921808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.925422 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.929206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.933238 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.939767 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.945682 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.950292 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.954574 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.959300 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.965400 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.969408 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.976078 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.982494 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.986798 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.992038 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.996271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.002239 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.006091 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.009922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.014846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.020574 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.026793 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.031952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.036393 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.042160 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.045885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.049472 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.053342 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.057667 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.062623 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.066427 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.070386 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.083759 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.091510 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.099624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.103695 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.109294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.114739 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.118868 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.123123 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.127520 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.131692 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.135723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.141479 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.148675 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.153292 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.157498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.161685 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.165603 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.171577 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.177159 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.180988 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.184591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.189081 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.197530 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.203334 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.208616 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.213245 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.218151 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.222301 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.226506 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.230300 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.234425 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.238471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.245711 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.251214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.255060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.258880 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.262952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.266913 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.270833 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.274749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.278583 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.282340 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.288607 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.293424 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.297286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.301219 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.305098 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.308867 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.312779 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.317879 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.322321 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.326403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.331836 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.337183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.343100 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.349312 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.356088 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.375687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.381455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.386098 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.390648 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.395861 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.402473 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.408658 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.413371 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.419367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.428846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.438015 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.443537 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.455120 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.462430 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.467132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.476315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.482499 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.487056 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.498801 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.537303 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.558305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.563272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.568560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.576925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.581597 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.588225 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.593149 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.597842 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.602444 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.607764 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.613027 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.617839 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.622546 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.628048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.632423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.638433 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.643217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.649337 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.654017 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.658510 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.662935 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.667541 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.672024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.676101 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.680210 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.685886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.690728 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.695254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.699825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.703557 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.707690 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.711482 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.715427 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.719185 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.723724 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.729349 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.733768 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.737958 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.741911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.745952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.751808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.756438 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.760903 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.765158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.773364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.779611 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.784276 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.789201 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.792909 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.796708 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.800599 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.808898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.815311 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.821585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:24.357895 [2] proc begin: <DistEnv 2/4 nccl>
16:49:24.387275 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:49:24.397297 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:49:26.294974 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.164502 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.173178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.180291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.184670 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.188691 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.192949 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.196714 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.200460 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.204316 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.208203 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.213212 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.217535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.221904 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.228353 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.234129 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.238797 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.243235 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.247035 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.252480 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.257281 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.262344 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.266064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.269682 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.274074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.280561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.286437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.290630 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.294680 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.298387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.302514 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.309641 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.316923 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.321651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.328689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.334040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.339070 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.344971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.350709 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.354778 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.360462 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.366064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.370300 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.376685 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.381909 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.386231 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.390612 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.396642 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.400686 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.404543 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.408377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.413815 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.418013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.423634 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.428077 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.432619 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.437421 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.441820 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.446679 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.452586 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.457177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.462464 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.466456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.470788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.477266 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.481782 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.485393 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.489580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.493754 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.497859 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.501696 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.508224 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.513568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.517132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.520576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.525744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.529941 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.534062 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.538055 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.541768 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.545382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.552786 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.558907 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.564251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.568821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.573336 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.577198 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.583146 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.589444 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.593813 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.598326 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.603665 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.607927 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.611896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.615791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.621037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.626222 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.630161 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.634298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.639851 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.645173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.650039 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.658463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.663770 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.668083 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.673382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.678087 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.682037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.687894 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.692194 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.696626 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.703006 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.709516 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.714787 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.720278 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.725116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.729796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.734683 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.739447 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.744364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.749316 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.755462 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.760381 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.765268 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.769461 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.775255 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.781864 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.786606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.790416 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.794052 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.798891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.805595 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.809742 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.815998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.822911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.827763 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.832348 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.836637 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.842295 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.848020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.852296 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.857463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.861382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.865283 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.869358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.873294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.876943 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.880598 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.884456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.888575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.892246 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.897235 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.901661 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.905565 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.909329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.912946 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.917444 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.923762 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.932440 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.940818 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.945985 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.953254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.957607 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.961898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.966393 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.970500 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.975307 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.979347 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.983297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.987501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.991411 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:27.996293 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.001139 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.005339 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.009220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.012990 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.016978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.020723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.025465 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.029246 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.033093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.041601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.046243 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.050178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.053801 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.058170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.064508 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.070918 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.076311 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.080195 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.083996 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.089236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.093191 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.096824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.108083 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.122930 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.128428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.132830 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.137119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.141074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.363948 [2] proc begin: <DistEnv 2/4 nccl>
16:52:30.476658 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:52:30.488856 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:31.745566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.435792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.443387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.449646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.453808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.459486 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.463915 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.468554 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.472994 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.478792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.483563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.489095 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.493178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.497414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.501630 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.505676 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.509601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.513600 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.517369 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.522588 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.527657 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.533053 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.536964 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.541101 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.544886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.548620 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.552121 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.555711 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.559471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.563583 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.567433 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.572723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.576561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.580626 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.584848 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.588812 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.593338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.597540 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.601384 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.605367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.609410 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.614585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.620151 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.624438 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.628254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.633978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.640111 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.645013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.649410 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.653037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.657261 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.662863 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.667181 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.670822 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.674552 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.678458 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.682377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.686298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.690346 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.694170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.699751 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.707022 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.711472 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.715568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.719765 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.725720 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.729773 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.733667 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.737285 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.740922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.744721 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.750064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.754066 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.761531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.767045 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.771286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.775672 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.779403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.783324 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.787497 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.793199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.800259 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.805200 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.809684 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.813637 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.817401 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.821303 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.825119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.829977 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.833823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.837737 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.843396 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.847504 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.851585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.855713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.859870 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.864008 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.868249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.872514 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.876694 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.881046 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.886645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.890861 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.894896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.899156 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.903403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.907661 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.911546 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.915252 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.918930 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.922935 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.928391 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.932557 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.936440 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.940192 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.944265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.948112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.952073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.955880 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.959517 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.963099 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.968573 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.973369 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.977289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.981240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.986177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.992383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:32.997217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.001413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.005411 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.009216 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.014601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.018650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.022364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.028035 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.031946 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.035755 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.039606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.043568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.047403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.051119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.056338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.062328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.067606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.071775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.075949 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.080037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.083922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.087823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.091700 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.097213 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.102836 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.107092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.110975 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.114841 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.118927 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.122605 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.130287 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.137011 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.142728 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.146990 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.151925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.158264 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.163411 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.167615 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.172119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.176148 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.180248 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.184208 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.189422 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.195249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.200631 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.204506 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.208535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.212825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.217015 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.221085 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.227400 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.231867 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.236182 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.241512 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.246645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.250592 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.254835 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.265219 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.269880 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.274132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.282294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.287423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.291957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.297809 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.303886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.307576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.311706 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.315736 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.319771 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.323752 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.327430 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.331118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.334989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:42.149952 [2] proc begin: <DistEnv 2/4 nccl>
16:52:42.202415 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:52:42.211781 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:52:44.401391 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.184233 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.190775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.195914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.201586 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.207644 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.211614 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.215693 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.219855 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.223716 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.227645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.232933 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.239524 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.247120 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.253542 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.258730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.263823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.268819 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.273968 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.282922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.290905 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.299326 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.304293 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.309436 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.314594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.319326 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.324073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.329082 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.334797 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.340757 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.347302 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.356971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.363950 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.369795 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.376251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.380851 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.385183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.389109 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.393197 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.397271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.401259 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.406760 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.411124 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.415183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.419051 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.423085 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.426928 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.430901 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.434832 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.439201 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.443947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.449395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.455113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.459240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.463002 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.468972 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.475582 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.481272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.485294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.489240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.493666 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.499298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.503177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.507602 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.513272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.520001 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.525977 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.530791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.536063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.541138 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.546234 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.552634 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.557358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.562320 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.567048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.573655 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.579860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.585151 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.590122 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.595177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.601595 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.612224 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.617195 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.621987 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.626560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.631786 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.635650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.639859 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.644234 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.648445 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.652416 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.657464 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.661519 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.665376 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.669290 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.673241 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.677366 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.681251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.685001 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.688914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.692627 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.698703 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.703035 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.707134 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.711305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.720218 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.724826 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.731244 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.735437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.741383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.746788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.754337 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.760211 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.764007 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.767906 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.772437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.776882 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.781040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.785136 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.789051 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.792735 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.798273 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.802743 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.814506 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.820439 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.825037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.829414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.834187 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.840509 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.847881 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.852306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.857720 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.861787 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.865819 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.869864 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.873568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.879674 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.885991 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.890940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.895220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.901316 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.908954 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.918730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.923471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.929040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.934975 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.939752 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.944284 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.948356 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.953744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.957915 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.964523 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.968325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.972713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.977149 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.981608 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.985860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.989931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.994226 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:45.998337 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.002236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.007665 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.012198 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.016796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.020817 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.024531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.028832 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.032811 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.036624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.040804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.044772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.049991 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.054116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.059543 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.065830 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.072158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.076080 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.079763 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.084185 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.088579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.093201 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.106086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.115730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.121846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.126971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.131793 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.135932 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.140004 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.143924 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.148061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.152101 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.165951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.174192 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.179957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.190096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.195329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.200045 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.206260 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.211452 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.215503 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:29.606036 [2] proc begin: <DistEnv 2/4 nccl>
16:53:29.674215 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:53:29.683512 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:53:30.817161 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.545691 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.553899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.560113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.565918 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.572138 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.577965 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.584125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.590650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.597047 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.603200 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.610132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.616319 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.622319 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.634140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.642887 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.650231 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.655813 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.661492 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.668358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.674059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.683775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.690363 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.696127 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.704559 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.710805 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.716583 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.723717 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.729351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.736012 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.742651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.748902 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.754654 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.760168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.765421 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.771008 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.776113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.781154 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.786182 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.792944 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.798174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.803857 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.809140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.817498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.823263 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.829005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.834826 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.841986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.847781 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.853168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.858680 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.864007 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.869414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.874712 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.879809 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.884812 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.890565 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.896085 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.901975 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.907849 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.913419 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.918806 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.924345 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.930338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.937381 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.942893 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.948197 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.954926 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.960513 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.965891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.972904 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.979811 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.985777 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.991432 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:31.998351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.004019 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.009550 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.015200 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.020589 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.025978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.031395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.036818 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.042524 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.048061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.054806 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.061515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.067050 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.072795 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.078521 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.083740 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.088857 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.094003 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.104417 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.110943 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.116384 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.121951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.127140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.134575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.141223 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.146595 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.151948 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.157399 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.162681 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.167947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.177308 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.183457 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.190339 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.195753 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.204355 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.213491 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.221441 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.227672 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.233455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.239289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.245151 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.251304 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.260150 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.266146 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.272062 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.280714 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.286707 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.293420 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.299579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.308315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.313879 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.319622 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.325110 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.330449 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.336024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.343792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.349575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.355599 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.361264 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.366825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.374139 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.381161 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.386781 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.395753 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.404306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.412341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.420013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.425588 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.431057 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.436803 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.442539 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.449530 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.455817 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.461832 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.467283 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.473076 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.478872 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.484664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.490566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.496166 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.502533 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.508624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.519005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.525905 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.532516 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.537454 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.542823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.548162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.553132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.558134 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.563187 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.568291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.580899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.589286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.597103 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.603788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.631158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.638391 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.644925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.649906 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.655566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.660931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.665931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.670999 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.676093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.681048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.686093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.691162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.697054 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.706655 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.713032 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.719131 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.724940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.730654 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.737306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.743784 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.749632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.755735 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.761081 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.766555 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.775399 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.781745 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.787175 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.792831 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.798029 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.806635 [2] Warning: no training nodes in this partition! Backward fake loss.
16:55:22.431394 [2] proc begin: <DistEnv 2/4 nccl>
16:55:29.042683 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:55:29.060805 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:57.832667 [2] proc begin: <DistEnv 2/4 nccl>
19:17:57.968557 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
19:17:57.979738 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:17:59.393083 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.193768 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.202773 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.210147 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.215924 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.222337 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.228585 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.234022 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.239821 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.245740 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.251219 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.256741 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.262548 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.268207 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.273355 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.278866 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.288107 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.294223 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.301258 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.306843 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.312723 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.318343 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.325033 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.331808 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.338040 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.343601 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.349608 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.356514 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.362940 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.368628 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.376849 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.383163 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.389257 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.396256 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.405139 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.412467 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.420639 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.427542 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.434707 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.441992 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.448220 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.455502 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.461499 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.469556 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.475663 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.481717 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.487577 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.494783 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.502289 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.509427 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.518473 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.525430 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.531426 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.536937 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.543875 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.551234 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.556857 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.562153 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.567896 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.574169 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.579666 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.585178 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.593325 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.598849 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.604239 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.613123 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.619595 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.629040 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.638548 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.646184 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.654381 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.662425 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.672659 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.684881 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.701525 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.717516 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.727949 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.736196 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.743487 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.750463 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.760162 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.767118 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.777884 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.785086 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.794705 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.801641 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.813639 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.820235 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.826322 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.836155 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.844856 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.852408 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.859463 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.866337 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.873436 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.882566 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.890647 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.898180 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.906449 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.914579 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.922551 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.928968 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.935012 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.942733 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.948798 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.955315 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.961808 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.967870 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.973649 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.980344 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.985964 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.993074 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:00.999240 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.005291 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.012607 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.018980 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.024983 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.030917 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.036976 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.051508 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.058462 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.065123 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.071950 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.077878 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.084461 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.090252 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.097851 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.105285 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.113676 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.119875 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.125724 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.131381 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.137314 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.143569 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.149335 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.155500 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.163326 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.168671 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.174921 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.181132 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.186467 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.194210 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.201381 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.207050 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.212848 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.218467 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.227708 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.233594 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.241660 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.248848 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.255857 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.261591 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.267098 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.272607 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.278086 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.287395 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.293352 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.299549 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.306096 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.311631 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.317132 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.322604 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.328014 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.333206 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.338622 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.343779 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.349257 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.354595 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.364980 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.371820 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.377757 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.383424 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.388925 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.394488 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.400436 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.406595 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.411961 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.421591 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.428656 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.435199 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.441836 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.447730 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.453617 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.459062 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.464866 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.471088 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.476945 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.483101 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.489108 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.496273 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.503347 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.509342 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.514783 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.520806 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.527013 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.533762 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.539233 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.545415 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.556399 [2] Warning: no training nodes in this partition! Backward fake loss.
19:18:01.565998 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:18.002847 [2] proc begin: <DistEnv 2/4 nccl>
19:23:18.072515 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
19:23:18.082470 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:23:19.177691 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.881878 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.890869 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.896388 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.903755 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.909743 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.919275 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.925768 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.932020 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.937041 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.947618 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.953720 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.959367 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.964559 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.970332 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.976204 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.981361 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.990592 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:19.996465 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.004201 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.009982 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.016681 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.023589 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.030707 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.037453 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.043109 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.050529 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.057889 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.064881 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.070827 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.080004 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.087377 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.093041 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.099135 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.104787 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.110247 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.115826 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.122798 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.127976 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.133173 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.138488 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.143865 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.149109 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.154231 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.159848 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.167817 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.176548 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.182582 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.188407 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.193671 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.199483 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.205758 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.211498 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.217095 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.223068 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.230019 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.235402 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.243292 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.248770 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.253987 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.259807 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.265188 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.270621 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.276041 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.281429 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.286919 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.292448 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.302323 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.308586 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.314302 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.320127 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.325798 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.331349 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.336719 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.341780 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.351070 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.359095 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.366154 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.371980 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.377593 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.384114 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.389981 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.396911 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.403780 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.409359 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.414732 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.421516 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.431015 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.436571 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.442223 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.447984 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.453848 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.459233 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.464886 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.470754 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.476463 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.482085 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.487743 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.492913 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.498154 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.504368 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.509878 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.515177 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.522241 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.529076 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.534687 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.541480 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.548582 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.554143 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.559336 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.564582 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.573287 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.580630 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.587583 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.595809 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.603169 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.609737 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.616323 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.621737 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.630570 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.640255 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.646775 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.653329 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.659269 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.664880 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.670335 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.677648 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.684226 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.690923 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.696764 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.702584 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.707966 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.715656 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.721377 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.726889 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.736022 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.741910 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.747622 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.753154 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.758593 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.768807 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.774924 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.780299 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.785807 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.791150 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.799402 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.808885 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.821282 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.831158 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.836675 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.842008 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.847456 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.853029 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.858847 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.863972 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.869304 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.876098 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.882819 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.887837 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.893223 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.898388 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.904286 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.909823 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.915128 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.920384 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.926508 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.932313 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.937269 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.946210 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.952472 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.958017 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.967216 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.977757 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.986715 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.993261 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:20.999291 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.016918 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.025618 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.034954 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.043995 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.058470 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.065670 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.071876 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.077276 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.082842 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.089722 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.096556 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.105962 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.111895 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.117440 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.122757 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.128115 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.133692 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.140936 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.149737 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.155591 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.160893 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.166164 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.174778 [2] Warning: no training nodes in this partition! Backward fake loss.
19:23:21.180236 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:45.869672 [2] proc begin: <DistEnv 2/4 nccl>
20:00:45.961114 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:00:45.970739 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:47.257528 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.952512 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.961395 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.972005 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.977891 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.983922 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.989527 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:47.995341 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.001056 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.006391 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.012111 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.018015 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.023316 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.028885 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.034775 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.040640 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.046093 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.051593 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.057138 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.062525 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.067900 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.073411 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.083564 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.089645 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.096679 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.103483 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.112084 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.117326 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.122874 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.128554 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.133977 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.139869 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.145604 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.150874 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.157406 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.163203 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.168741 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.174277 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.180543 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.186766 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.192338 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.198623 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.204840 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.210772 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.217351 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.223479 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.229593 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.235305 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.242042 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.248123 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.253686 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.259468 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.266415 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.273529 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.280175 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.285542 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.291271 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.296682 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.302079 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.307629 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.312932 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.318053 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.323131 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.328283 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.333335 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.338778 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.344793 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.351718 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.357185 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.362298 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.367763 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.378191 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.386304 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.392481 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.398615 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.404434 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.414197 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.421919 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.428121 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.434002 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.439539 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.444932 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.450559 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.456051 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.464304 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.471699 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.478036 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.483158 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.488641 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.495303 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.502087 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.507938 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.513828 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.519315 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.524971 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.530692 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.536017 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.541984 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.552635 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.558476 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.565085 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.570948 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.576872 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.582279 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.589713 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.597201 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.604082 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.611165 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.616918 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.622858 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.630467 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.637571 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.642937 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.649259 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.655707 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.661861 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.668946 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.676417 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.682791 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.689656 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.696472 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.703194 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.709110 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.714900 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.720892 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.729154 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.735533 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.741556 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.747992 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.753808 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.759882 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.765305 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.770917 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.778982 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.784635 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.790001 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.795950 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.804752 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.810232 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.819992 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.827229 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.833160 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.838970 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.844550 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.850371 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.859375 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.865207 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.871094 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.876270 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.881728 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.887046 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.895039 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.909322 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.916425 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.923030 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.928792 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.934332 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.943985 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.949691 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.957753 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.963714 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.970804 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.976338 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.981575 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.988321 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:48.997014 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.006121 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.011883 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.017497 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.026952 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.032993 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.040527 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.049112 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.056662 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.062970 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.068354 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.075335 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.080873 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.086534 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.092178 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.097753 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.103380 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.108824 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.114559 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.119781 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.126211 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.132566 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.144078 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.150489 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.157092 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.163510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.171093 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.178520 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.185177 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.191376 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.198526 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.205170 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.211776 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.218578 [2] Warning: no training nodes in this partition! Backward fake loss.
20:00:49.225331 [2] Warning: no training nodes in this partition! Backward fake loss.
20:48:25.074481 [2] proc begin: <DistEnv 2/4 nccl>
20:48:42.903283 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:48:42.921139 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:26:54.584743 [2] proc begin: <DistEnv 2/4 nccl>
20:27:06.607258 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:27:06.624005 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:27.779989 [2] proc begin: <DistEnv 2/4 nccl>
17:01:28.055592 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:01:28.083146 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:01:29.993845 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.257273 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.338717 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.431076 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.589552 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.722681 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.890135 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:31.955480 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.065477 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.233410 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.359643 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.494733 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.578309 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.747854 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:32.863186 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.027778 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.090929 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.228349 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.377115 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.498166 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.637155 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.721092 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.890213 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:33.966737 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.118599 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.184102 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.258865 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.333503 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.375431 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.382596 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.389552 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.397602 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.405848 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.418808 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.428627 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.435512 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.442628 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.449194 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.460042 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.487155 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.501709 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.509355 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.517264 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.525166 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.533228 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.546016 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.558809 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.573366 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.585055 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.593833 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.604342 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.618164 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.637638 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.693505 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:34.911791 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.113476 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.272567 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.488602 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.652294 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.863979 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:35.998358 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.221446 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.429904 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.673422 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:36.835186 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.099132 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.308261 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.497813 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.661484 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:37.892440 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.057667 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.236628 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.430534 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.608874 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.755886 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:38.952079 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.148868 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.425097 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.630486 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.845410 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:39.987362 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.232973 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.416531 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.672852 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:40.852235 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.072339 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.220351 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.391941 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.518548 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.756588 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:41.918160 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.094120 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.237896 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.418281 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.575686 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.739007 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:42.917246 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.126332 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.233042 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.416758 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.564346 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.740564 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:43.897271 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.030695 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.177852 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.368948 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.545466 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.608263 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.740167 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:44.901510 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.025618 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.160021 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.247215 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.430588 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.576338 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.728515 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.808074 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:45.977235 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.089273 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.258777 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.323761 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.494656 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.633614 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.810745 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:46.871665 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.036291 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.175018 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.264476 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.403151 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.460331 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.543740 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.620760 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.698343 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.875252 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:47.948528 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.126267 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.208856 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.350805 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.505570 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.623336 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.769614 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:48.851892 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.026792 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.163259 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.309074 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.373613 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.450584 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.527284 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.606400 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.700880 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.792314 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:49.888147 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.044542 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.189206 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.363274 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.430975 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.595898 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.752964 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:50.928091 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.005654 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.105797 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.262015 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.396458 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.543140 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.605911 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.782194 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.900653 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.912036 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:51.994864 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.073411 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.089775 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.097916 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.108201 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.116735 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.136360 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.150731 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.165616 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.207240 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.335579 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.343632 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.452953 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.615294 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.799818 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:52.949783 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.153213 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.282203 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.489598 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.607863 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.757391 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.813893 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:53.979308 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.126933 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.255530 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.376751 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.486883 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.653537 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.794963 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:54.941543 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.022572 [2] Warning: no training nodes in this partition! Backward fake loss.
17:01:55.155064 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.012828 [2] proc begin: <DistEnv 2/4 nccl>
17:16:41.127591 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:16:41.139626 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:43.573320 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.646918 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.738242 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.758473 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.773520 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.797070 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.814646 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.828614 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.840819 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.853461 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.863477 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.916665 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.187177 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.394469 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.637603 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:45.896315 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.089294 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.321456 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.554487 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.783275 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.994320 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.180674 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.386959 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.508852 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.734096 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:47.940534 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.136048 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.358400 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.496368 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.769809 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.960607 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.185251 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.334772 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.528570 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.668958 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:49.918881 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.106406 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.348687 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.537726 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.607855 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.625472 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.650483 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.703686 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.884088 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.164407 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.423145 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.703174 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:51.930939 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.223861 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.520016 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.769319 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.114230 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.413197 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.646816 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.970672 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.166133 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.468758 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.718023 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:54.984601 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.256829 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.584029 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.843294 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.093997 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.382049 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.571604 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.760331 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:56.920610 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.094275 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.240252 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.451537 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.685733 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.835176 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.027096 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.240058 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.392199 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.637152 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.862400 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:58.996516 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.247545 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.434226 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.588112 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.791024 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.983418 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.119746 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.342835 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.549620 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.691204 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:00.930004 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.163837 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.264544 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.463257 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.642551 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.833300 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.012087 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.257942 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.443588 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.543403 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.642833 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:02.816553 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.098058 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.258420 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.345933 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.364685 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.386140 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.413291 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.436785 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.461361 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.621302 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.656259 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.778566 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.815797 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.836980 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.859136 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.875413 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.891246 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.914605 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.963002 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.155646 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.429158 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.634263 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.870941 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.074694 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.291314 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.383378 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.618584 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.802301 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:05.975515 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.109719 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.371561 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.544862 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.734875 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.968119 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.178334 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.197526 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.214229 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.243202 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.266035 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.405848 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.662400 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:07.872135 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.108586 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.331038 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.543174 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.785283 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.991135 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.106076 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.214888 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.292271 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.369452 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.486095 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.685888 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:09.920808 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.126520 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.338667 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.566770 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.746654 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.901945 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.131586 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.332446 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.496995 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.679757 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:11.929324 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.010489 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.160917 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.386831 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.600601 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.650539 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.680771 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.728381 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.848181 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.878952 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.907028 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.133972 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.409633 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.682681 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:13.870626 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.185549 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.517963 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.782998 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.090800 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.393413 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.629433 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.879031 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.153970 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.438773 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.637293 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:16.879678 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.154821 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.414030 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.692112 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.937933 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.159852 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.350545 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.575537 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:18.805554 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.003853 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.190789 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.438796 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.560127 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.580938 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:46.971582 [2] proc begin: <DistEnv 2/4 nccl>
22:21:47.480768 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
22:21:47.504237 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:21:49.147206 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.049277 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.063506 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.072709 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.082472 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.091941 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.104754 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.114009 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.123338 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.134387 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.143919 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.162849 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.173873 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.185553 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.195259 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.205345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.218285 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.232501 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.244771 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.253942 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.264204 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.274561 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.285566 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.299748 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.313844 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.324709 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.334986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.344270 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.353848 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.363089 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.372618 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.382076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.391297 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.400910 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.414390 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.433399 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.446283 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.455644 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.470433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.482730 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.492341 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.502190 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.511762 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.520936 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.530249 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.539735 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.549487 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.566388 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.577532 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.588319 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.601660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.612690 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.624520 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.636108 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.648588 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.658674 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.669090 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.679934 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.690060 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.700628 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.711008 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.722226 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.732804 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.743151 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.754072 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.764410 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.774792 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.785146 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.795630 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.806127 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.816379 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.829234 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.844267 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.854186 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.866693 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.877709 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.889398 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.899787 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.911068 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.921334 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.933813 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.945389 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.956240 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.972271 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:50.994507 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.009588 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.023275 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.032488 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.041840 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.059530 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.070628 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.079864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.089482 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.098910 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.108402 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.121009 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.131039 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.144439 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.164472 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.178821 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.187808 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.197939 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.207412 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.216945 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.228444 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.237387 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.246715 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.258251 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.267380 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.276757 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.286378 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.296305 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.306816 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.316163 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.325853 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.334748 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.345820 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.358648 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.368635 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.378062 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.387366 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.398061 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.408941 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.418173 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.427447 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.436502 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.446397 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.455761 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.465306 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.474531 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.487367 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.498857 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.509782 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.519712 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.528740 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.538009 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.546945 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.558665 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.570201 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.579232 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.588514 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.598168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.607386 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.616547 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.627883 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.637970 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.647220 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.656758 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.665979 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.675496 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.684838 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.694189 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.703862 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.712999 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.722209 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.731483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.740661 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.750212 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.759439 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.768754 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.777971 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.787318 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.796581 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.805945 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.815373 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.824437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.833773 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.843053 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.852209 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.861485 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.870726 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.880118 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.889856 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.899486 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.908564 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.917777 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.927128 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.936430 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.945835 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.955594 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.965220 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:51.995603 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.014139 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.027562 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.036943 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.048082 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.060330 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.070397 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.080496 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.090874 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.100931 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.111923 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.122398 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.132303 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.144268 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.154667 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.164483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.175066 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.184964 [2] Warning: no training nodes in this partition! Backward fake loss.
22:21:52.194901 [2] Warning: no training nodes in this partition! Backward fake loss.
22:22:57.566238 [2] proc begin: <DistEnv 2/4 nccl>
22:22:57.660812 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
22:22:57.673653 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:22:59.077098 [2] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.966824 [2] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.982878 [2] Warning: no training nodes in this partition! Backward fake loss.
22:22:59.996485 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.007922 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.017963 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.027609 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.036886 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.049845 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.061808 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.070868 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.080188 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.089780 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.102050 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.111495 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.120745 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.130130 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.139283 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.148873 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.158586 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.168276 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.180089 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.190830 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.200508 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.209924 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.219361 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.229767 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.239213 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.248427 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.257876 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.267298 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.276704 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.288325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.301127 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.311134 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.320437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.329984 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.339317 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.348501 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.357853 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.367774 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.377258 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.389021 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.402019 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.417040 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.428299 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.441786 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.453951 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.467051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.477769 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.490766 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.503270 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.513199 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.525852 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.539478 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.550853 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.568246 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.580878 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.596884 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.609274 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.621930 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.638476 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.651171 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.660113 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.673876 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.684353 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.693116 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.702112 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.712003 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.721058 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.731092 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.739935 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.748980 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.759135 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.768559 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.777856 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.786659 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.796423 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.808430 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.818381 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.827431 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.836373 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.845472 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.854429 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.863320 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.872182 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.883881 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.894582 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.903281 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.918233 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.929293 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.938950 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.947950 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.957058 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.966227 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.976330 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.988351 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:00.998578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.007461 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.016509 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.025577 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.034629 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.043974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.053062 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.061894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.070996 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.080136 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.089181 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.098991 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.108825 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.117963 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.126903 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.135870 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.145117 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.154152 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.164816 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.174079 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.183552 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.192417 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.201909 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.210738 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.219673 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.228863 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.239369 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.250792 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.263149 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.272441 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.282174 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.291385 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.300994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.312136 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.321342 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.330966 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.340204 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.349779 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.359051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.368824 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.378024 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.388328 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.399871 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.411438 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.422140 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.437428 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.446967 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.456380 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.465888 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.475194 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.484673 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.496790 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.505902 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.515443 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.524841 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.534431 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.543496 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.552974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.574268 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.586055 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.602590 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.614746 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.624243 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.633996 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.649104 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.660948 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.670194 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.681038 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.700819 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.713219 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.725466 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.734566 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.743792 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.753325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.763127 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.772585 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.781796 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.794435 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.803469 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.815395 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.824587 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.834006 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.845707 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.854642 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.863763 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.873060 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.882175 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.891207 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.900421 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.909670 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.918769 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.931189 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.942998 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.952158 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.963829 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.975664 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.985936 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:01.997437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.007707 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.017547 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.027629 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.036729 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:02.046479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:49.970011 [2] proc begin: <DistEnv 2/4 nccl>
22:23:50.068147 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
22:23:50.081204 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:23:51.487437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.223989 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.237618 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.247343 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.256703 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.269892 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.282823 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.292037 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.301460 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.310942 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.322525 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.331556 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.340510 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.349578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.358762 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.374808 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.384291 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.393062 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.402053 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.412870 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.421904 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.432511 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.443430 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.452737 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.461796 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.470859 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.479818 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.488960 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.500565 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.509373 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.526064 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.536229 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.545294 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.555660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.566674 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.575912 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.585109 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.594039 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.602891 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.611816 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.621012 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.633318 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.644026 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.654086 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.664459 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.673572 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.682719 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.691662 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.705056 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.715149 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.728710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.737864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.747398 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.756791 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.767092 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.776511 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.791571 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.802372 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.811094 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.820484 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.829824 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.838762 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.847900 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.857179 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.873613 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.885336 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.895406 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.904538 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.915122 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.924325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.938878 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.948419 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.957849 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.972423 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:52.995551 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.007530 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.017706 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.029200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.039165 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.048253 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.057511 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.066778 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.076023 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.085167 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.094653 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.104291 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.115986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.127378 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.141231 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.156229 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.168863 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.180028 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.192419 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.202578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.211995 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.221120 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.230212 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.239298 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.250860 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.260335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.269783 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.278815 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.287986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.297239 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.306297 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.315295 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.324397 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.333721 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.345931 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.355437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.368318 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.380482 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.389899 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.399075 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.411854 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.420586 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.432940 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.444226 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.453291 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.463340 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.477988 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.489424 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.502243 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.516774 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.526424 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.535651 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.545113 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.555119 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.564528 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.576200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.585797 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.595185 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.604477 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.613852 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.623051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.632350 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.641807 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.651094 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.660188 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.669905 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.680158 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.693881 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.706601 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.715660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.725371 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.734804 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.744249 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.756376 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.769544 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.779531 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.789257 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.798855 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.808208 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.818008 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.828653 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.838576 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.850637 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.860206 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.872823 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.882367 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.893110 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.905743 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.915258 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.924708 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.934360 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.947045 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.956320 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.967011 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:53.992504 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.001861 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.011602 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.020809 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.030069 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.040098 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.050131 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.060603 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.071115 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.080838 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.092103 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.102395 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.113054 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.123725 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.134214 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.144064 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.152826 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.162988 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.171663 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.180837 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.190152 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.199125 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.209947 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.220052 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.232828 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.241821 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.251390 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.263731 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.275335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.288421 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.297019 [2] Warning: no training nodes in this partition! Backward fake loss.
22:23:54.306206 [2] Warning: no training nodes in this partition! Backward fake loss.
22:24:49.284876 [2] proc begin: <DistEnv 2/4 nccl>
22:25:05.982360 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:25:05.998796 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:26:05.969993 [2] proc begin: <DistEnv 2/4 nccl>
22:26:11.971102 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:26:11.986170 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:27:25.077282 [2] proc begin: <DistEnv 2/4 nccl>
22:27:31.922689 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:27:31.938814 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:28:35.152612 [2] proc begin: <DistEnv 2/4 nccl>
22:28:41.828111 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:28:41.845991 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:09.604413 [2] proc begin: <DistEnv 2/4 nccl>
22:33:25.011483 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:33:25.017187 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:33:36.337826 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.354081 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.535942 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.716875 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:37.897595 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.078921 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.259127 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.440408 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.620084 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.799997 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:38.980587 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.160312 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.339597 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.522685 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.702888 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:39.882873 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.062596 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.242981 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.423582 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.603468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.784895 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:40.964981 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.145377 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.330078 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.511572 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.692061 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:41.872233 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.052626 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.232623 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.415035 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.595183 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.775934 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:42.956603 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.137529 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.318383 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.500541 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.684584 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:43.865324 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.046369 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.226795 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.407059 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.587903 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.779776 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:44.966094 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.146407 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.327170 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.507644 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.687793 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:45.868289 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.049974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.230319 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.410769 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.592622 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.774552 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:46.956666 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.136877 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.316707 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.496744 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.678355 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:47.858719 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.039076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.219741 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.399399 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.579689 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.760706 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:48.942064 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.121615 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.302221 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.482749 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.663397 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:49.844027 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.025686 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.207044 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.387544 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.570125 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.750156 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:50.930614 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.111596 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.292365 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.473388 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.653706 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:51.834523 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.014266 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.194866 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.375384 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.556641 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.737467 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:52.918355 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.101016 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.280628 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.461244 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.642057 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:53.822697 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.003217 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.183223 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.363500 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.544179 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.723920 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:54.903752 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.084608 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.265047 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.445500 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.627763 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.807895 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:55.988699 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.169245 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.349736 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.529784 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.710163 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:56.891762 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.072660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.252652 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.433357 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.613474 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.793288 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:57.973820 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.154191 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.334117 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.515507 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.695857 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:58.875540 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.055633 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.235537 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.416151 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.596194 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.776730 [2] Warning: no training nodes in this partition! Backward fake loss.
22:33:59.956908 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.137036 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.316590 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.497672 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.677333 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:00.857010 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.036930 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.216965 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.398799 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.579255 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.760178 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:01.950096 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.138877 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.328786 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.517546 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.705521 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:02.893452 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.082571 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.268675 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.450194 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.630678 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.811052 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:03.991938 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.172713 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.353592 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.534529 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.715399 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:04.898335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.079180 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.259995 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.441656 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.622361 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.802812 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:05.983918 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.164425 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.347343 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.528497 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.709479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:06.890894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.071866 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.253153 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.434361 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.615632 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.797051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:07.978387 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.159986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.341203 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.522048 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.704681 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:08.887018 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.068253 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.250243 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.431557 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.613516 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.794333 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:09.975019 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.156179 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.337475 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.518197 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.698796 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:10.879136 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.060866 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.241948 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.422912 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.604242 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.785825 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:11.966280 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.148345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.329371 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.509991 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.689975 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:12.870311 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.050102 [2] Warning: no training nodes in this partition! Backward fake loss.
22:34:13.230263 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:03.054944 [2] proc begin: <DistEnv 2/4 nccl>
22:35:08.796864 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:35:08.806798 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:35:13.869251 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:15.433948 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.202627 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:16.972041 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:17.741250 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:18.510520 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:19.278297 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.053003 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:20.823218 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:21.592702 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:22.359431 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.128122 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:23.895791 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:24.664692 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:25.435979 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.206037 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:26.978388 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:27.748456 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:28.518816 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:29.289124 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.059381 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:30.831448 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:31.600986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:32.373087 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.141828 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:33.911021 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:34.678815 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:35.448777 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.217259 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:36.986222 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:37.755330 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:38.524134 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:39.293335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.062282 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:40.832827 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:41.603581 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:42.374421 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.143728 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:43.913323 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:44.683154 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:45.453589 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.225475 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:46.995935 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:47.766496 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:48.535289 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:49.304178 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.074264 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:50.844193 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:51.615417 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:52.385256 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.155291 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:53.926415 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:54.696606 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:55.466882 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:56.237226 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.006986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:57.776766 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:58.548347 [2] Warning: no training nodes in this partition! Backward fake loss.
22:35:59.317410 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.086793 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:00.857799 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:01.628554 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:02.428059 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:03.233105 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.003476 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:04.773816 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:05.544723 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:06.315705 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.086675 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:07.856565 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:08.627523 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:09.397720 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.169079 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:10.941069 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:11.711655 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:12.482095 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:13.251325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.020463 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:14.790595 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:15.562221 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:16.332385 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.102066 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:17.872601 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:18.643479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:19.414572 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.186097 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:20.957272 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:21.726566 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:22.494634 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:23.266879 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.031124 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:24.798791 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:25.569747 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:26.337562 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.106349 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:27.875938 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:28.645465 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:29.413576 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.183638 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:30.968798 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:31.740168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:32.512014 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:33.282387 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.051312 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:34.819480 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:35.588353 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:36.357202 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.125655 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:37.894479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:38.664407 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:39.434484 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.204135 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:40.974157 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:41.742297 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:42.511111 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:43.278410 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.048278 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:44.816729 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:45.585572 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:46.354400 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.122478 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:47.891591 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:48.660432 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:49.429219 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.196928 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:50.966048 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:51.733507 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:52.500034 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:53.266379 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.033938 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:54.802874 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:55.576579 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:56.344448 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.111034 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:57.877923 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:58.644974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:36:59.412228 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.180489 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:00.951064 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:01.737888 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:02.543187 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:03.327146 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.097236 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:04.866513 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:05.635995 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:06.406011 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.175416 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:07.946828 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:08.715279 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:09.484589 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:10.253582 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.024452 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:11.794168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:12.564080 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:13.334034 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.104259 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:14.874024 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:15.643872 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:16.414519 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.183591 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:17.953971 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:18.724817 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:19.494522 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:20.264156 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.034460 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:21.804103 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:22.572746 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:23.342214 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.109580 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:24.879213 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:25.647818 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:26.415704 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.184317 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:27.952592 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:28.720310 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:29.487523 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:30.255612 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.022992 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:31.790778 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:32.560175 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:33.327608 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.094774 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:34.862234 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:35.630345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:36.398723 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.166924 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:37.934500 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:38.704304 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:39.474506 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:40.242515 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.011108 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:41.778063 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:42.546541 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:43.314280 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.081794 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:44.849266 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:45.617516 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:46.384579 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.153441 [2] Warning: no training nodes in this partition! Backward fake loss.
22:37:47.919486 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:43.746880 [2] proc begin: <DistEnv 2/4 nccl>
22:38:49.011985 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:38:49.030506 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:38:54.370979 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:55.892013 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:56.659318 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:57.426076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.195106 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:58.965303 [2] Warning: no training nodes in this partition! Backward fake loss.
22:38:59.733355 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:00.501912 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:01.270932 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.068682 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:02.872228 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:03.640114 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:04.406560 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.174459 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:05.943655 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:06.712800 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:07.481729 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:08.252320 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.022617 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:09.792199 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:10.564212 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:11.334461 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.105100 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:12.874842 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:13.645737 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:14.415345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.185203 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:15.954468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:16.723573 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:17.490544 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:18.257803 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.025033 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:19.792488 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:20.561606 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:21.329710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.096373 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:22.863912 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:23.631483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:24.399479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.168253 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:25.937898 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:26.707754 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:27.477174 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:28.245855 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.015722 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:29.784970 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:30.556049 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:31.324267 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.093882 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:32.863579 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:33.634361 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:34.403978 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.174629 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:35.946010 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:36.715365 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:37.484345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:38.254346 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.024113 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:39.793696 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:40.563235 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:41.332354 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.102259 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:42.873306 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:43.642838 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:44.412711 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.182529 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:45.952509 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:46.723336 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:47.493264 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:48.262606 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.031901 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:49.802335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:50.572611 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:51.342013 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.111449 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:52.881907 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:53.650254 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:54.419852 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.189997 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:55.960200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:56.730629 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:57.500202 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:58.270114 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.039482 [2] Warning: no training nodes in this partition! Backward fake loss.
22:39:59.809074 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:00.578572 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:01.348690 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.125431 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:02.926538 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:03.723939 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:04.491595 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:05.262231 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.031618 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:06.801341 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:07.571601 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:08.342907 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.112204 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:09.882684 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:10.652725 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:11.422440 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.192452 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:12.962569 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:13.732200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:14.502082 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:15.271761 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.043182 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:16.812421 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:17.581569 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:18.351012 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.120985 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:19.890470 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:20.658625 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:21.426471 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.193953 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:22.960488 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:23.729524 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:24.497228 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:25.265453 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.033109 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:26.801730 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:27.569088 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:28.336707 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.105032 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:29.873169 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:30.642621 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:31.410399 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.178605 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:32.947768 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:33.717994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:34.486958 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:35.256756 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.025338 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:36.795426 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:37.566501 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:38.335900 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.105542 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:39.875628 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:40.646351 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:41.416220 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.184524 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:42.952488 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:43.719221 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:44.486444 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:45.254308 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.022209 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:46.790221 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:47.558105 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:48.325824 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.094272 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:49.863078 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:50.632319 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:51.401122 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.169256 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:52.936937 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:53.703558 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:54.471266 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:55.238394 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.006473 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:56.773619 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:57.541564 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:58.308791 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.073698 [2] Warning: no training nodes in this partition! Backward fake loss.
22:40:59.840201 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:00.606296 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:01.372981 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.166059 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:02.968581 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:03.739641 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:04.509578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:05.277120 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.045840 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:06.814178 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:07.582803 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:08.350864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.119609 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:09.888286 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:10.657103 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:11.426500 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.194230 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:12.963182 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:13.731026 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:14.499305 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:15.266468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.035028 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:16.803499 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:17.571975 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:18.339806 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.107578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:19.878273 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:20.647489 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:21.415027 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.182227 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:22.949932 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:23.717883 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:24.484127 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:25.252566 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.021093 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:26.787316 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:27.553840 [2] Warning: no training nodes in this partition! Backward fake loss.
22:41:28.320110 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:23.313152 [2] proc begin: <DistEnv 2/4 nccl>
22:42:28.498640 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:42:28.506074 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:42:32.429128 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:33.867103 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:34.635015 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:35.402923 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.169018 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:36.935554 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:37.703106 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:38.469633 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:39.240705 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.006076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:40.774367 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:41.541198 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:42.308954 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.077109 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:43.845650 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:44.613903 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:45.382116 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.149709 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:46.917531 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:47.684510 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:48.451758 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.218992 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:49.986179 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:50.753607 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:51.521803 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:52.289569 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.057404 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:53.825459 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:54.594815 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:55.362468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.129736 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:56.898021 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:57.665388 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:58.431979 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.199573 [2] Warning: no training nodes in this partition! Backward fake loss.
22:42:59.968417 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:00.736501 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:01.504473 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:02.300200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.103133 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:03.875958 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:04.645715 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:05.414738 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.185366 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:06.953246 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:07.722960 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:08.493386 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:09.262672 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.031889 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:10.801046 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:11.570835 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:12.340207 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.109797 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:13.879230 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:14.648309 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:15.418473 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.187071 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:16.955620 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:17.723436 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:18.491995 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:19.259822 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.027986 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:20.796729 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:21.564075 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:22.332432 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.100871 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:23.869498 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:24.638535 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:25.407578 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.176313 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:26.945359 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:27.714665 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:28.484337 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:29.253146 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.023060 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:30.792958 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:31.560926 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:32.344348 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.112892 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:33.882588 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:34.650594 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:35.418447 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.185495 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:36.953697 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:37.721310 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:38.488757 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:39.256892 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.025817 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:40.793750 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:41.564629 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:42.329645 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.098370 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:43.865660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:44.632528 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:45.399686 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.166879 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:46.934104 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:47.699848 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:48.466858 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:49.234196 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.000652 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:50.766820 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:51.533422 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:52.299988 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.067144 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:53.833506 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:54.599720 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:55.366963 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.135469 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:56.903540 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:57.670999 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:58.436810 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.203339 [2] Warning: no training nodes in this partition! Backward fake loss.
22:43:59.969962 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:00.737269 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:01.512768 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:02.313945 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.103849 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:03.876181 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:04.645294 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:05.413918 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.183001 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:06.952263 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:07.720668 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:08.488980 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:09.257512 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.026856 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:10.795609 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:11.565443 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:12.333894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.104371 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:13.872379 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:14.640914 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:15.409798 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.177277 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:16.945603 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:17.713080 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:18.482461 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:19.250783 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.020588 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:20.789676 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:21.556696 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:22.324070 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.091914 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:23.859945 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:24.626664 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:25.393125 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.160894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:26.927996 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:27.695821 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:28.464329 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:29.231236 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:29.999745 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:30.766506 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:31.534026 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:32.302008 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.069497 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:33.840691 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:34.608974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:35.377039 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.144952 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:36.914035 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:37.681716 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:38.450371 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.218268 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:39.986338 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:40.755019 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:41.522483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:42.290651 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.059162 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:43.828342 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:44.597225 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:45.365827 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.134229 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:46.903069 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:47.671414 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:48.439094 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.205863 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:49.973433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:50.740457 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:51.508553 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:52.275616 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.043128 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:53.811241 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:54.579207 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:55.347555 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.117271 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:56.885810 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:57.655439 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:58.423904 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.192713 [2] Warning: no training nodes in this partition! Backward fake loss.
22:44:59.960573 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:00.730238 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:01.499085 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:02.284858 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.088490 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:03.868020 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:04.638803 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:05.409642 [2] Warning: no training nodes in this partition! Backward fake loss.
22:45:06.180653 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:05.755932 [2] proc begin: <DistEnv 2/4 nccl>
22:47:11.342734 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:47:11.355796 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:47:16.644575 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:18.276665 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.045449 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:19.815083 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:20.585517 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:21.356904 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.127453 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:22.897305 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:23.667710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:24.438078 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.207378 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:25.976546 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:26.747731 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:27.518456 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:28.290226 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.059710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:29.829216 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:30.598610 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:31.369234 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.138994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:32.908746 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:33.679220 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:34.451125 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.222122 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:35.992067 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:36.761036 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:37.528853 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:38.296684 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.064338 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:39.832474 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:40.599617 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:41.365983 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.134049 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:42.901293 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:43.667636 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:44.434427 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.200792 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:45.969070 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:46.735289 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:47.503183 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:48.268706 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.036472 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:49.803153 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:50.573378 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:51.340894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.109325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:52.877236 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:53.645912 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:54.413265 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.180413 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:55.949758 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:56.716834 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:57.484894 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:58.251977 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.018812 [2] Warning: no training nodes in this partition! Backward fake loss.
22:47:59.787245 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:00.556169 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:01.324489 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.122574 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:02.925179 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:03.694423 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:04.464314 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:05.235076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.005454 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:06.775043 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:07.545401 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:08.315097 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.086311 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:09.856583 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:10.627773 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:11.396081 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.165378 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:12.937624 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:13.707954 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:14.478022 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:15.244833 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.013098 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:16.780640 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:17.548964 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:18.317076 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.084765 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:19.852740 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:20.622901 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:21.389774 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.157359 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:22.926301 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:23.694049 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:24.462214 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:25.229765 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.003355 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:26.769018 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:27.536763 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:28.304260 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.072154 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:29.839668 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:30.607363 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:31.375280 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.143669 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:32.910442 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:33.679380 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:34.449149 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.217483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:35.985454 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:36.753365 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:37.520936 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:38.289266 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.057452 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:39.823988 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:40.593433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:41.360146 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.127494 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:42.893903 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:43.661341 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:44.428393 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.195172 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:45.962116 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:46.729349 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:47.496843 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:48.264246 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.032742 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:49.801275 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:50.570080 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:51.337188 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.106006 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:52.874590 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:53.642765 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:54.411187 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.179184 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:55.947700 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:56.715508 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:57.483822 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:58.252659 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.021875 [2] Warning: no training nodes in this partition! Backward fake loss.
22:48:59.789801 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:00.558828 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:01.327337 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.100096 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:02.901950 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:03.694713 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:04.464128 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:05.231493 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:05.999498 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:06.768859 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:07.536783 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:08.304877 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.073487 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:09.841519 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:10.610421 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:11.378269 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.146018 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:12.914006 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:13.682209 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:14.450617 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.218864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:15.986627 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:16.754330 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:17.522024 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:18.292216 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.060168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:19.827752 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:20.595582 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:21.363093 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.131538 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:22.900122 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:23.668036 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:24.435237 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.202731 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:25.970393 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:26.738463 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:27.506363 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:28.273676 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.041135 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:29.809476 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:30.576911 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:31.345038 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.112161 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:32.879879 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:33.646050 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:34.414282 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.182204 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:35.950305 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:36.717541 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:37.484708 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:38.250858 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.018296 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:39.784975 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:40.552039 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:41.319041 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.085862 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:42.852132 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:43.618664 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:44.387656 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.155698 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:45.923819 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:46.692335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:47.461204 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.229880 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:48.997872 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:49.764961 [2] Warning: no training nodes in this partition! Backward fake loss.
22:49:50.533084 [2] Warning: no training nodes in this partition! Backward fake loss.
00:51:54.829814 [2] proc begin: <DistEnv 2/4 nccl>
00:52:03.204755 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
00:52:03.235338 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:53:37.796324 [2] proc begin: <DistEnv 2/4 nccl>
00:53:43.785917 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
00:53:43.800085 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

00:55:14.162861 [2] proc begin: <DistEnv 2/4 nccl>
00:55:21.595066 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
00:55:21.610897 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:54.108193 [2] proc begin: <DistEnv 2/4 nccl>
02:06:54.161263 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:06:54.177383 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:06:55.751122 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.543192 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.562970 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.578455 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.590505 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.599784 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.609880 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.619242 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.629073 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.638223 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.647528 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.657036 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.666361 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.675816 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.685615 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.695041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.704832 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.714453 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.723798 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.733283 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.742801 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.752482 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.763780 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.772896 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.782669 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.792382 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.802369 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.811742 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.821369 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.832014 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.841062 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.852457 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.861940 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.875013 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.887605 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.900059 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.909999 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.922910 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.933486 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.943960 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.953227 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.963314 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.972833 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.982437 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:56.991583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.001052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.010430 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.019664 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.029152 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.041287 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.050085 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.059676 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.069692 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.078758 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.087865 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.097303 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.111443 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.124104 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.136941 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.150426 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.159570 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.181539 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.194082 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.206746 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.217197 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.226756 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.236185 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.250999 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.260530 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.270123 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.280041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.291811 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.304246 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.313411 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.322717 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.332385 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.341927 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.351201 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.360615 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.371893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.380706 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.390019 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.402030 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.411636 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.420907 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.430085 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.439520 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.448664 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.457972 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.475077 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.490022 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.502058 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.512947 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.526032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.536886 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.548578 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.563942 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.577552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.587503 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.599308 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.609351 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.618846 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.628282 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.637552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.646755 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.655977 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.665477 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.674812 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.684136 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.695998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.705315 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.714836 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.724000 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.733173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.742472 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.751680 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.760872 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.770110 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.779190 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.788463 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.797642 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.806678 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.815749 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.824792 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.837183 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.853454 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.863599 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.872893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.882266 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.894953 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.904091 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.915449 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.928897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.940854 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.950186 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.959536 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.971858 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.980743 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:57.989930 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.001909 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.010967 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.020111 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.029539 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.038574 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.047554 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.059772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.069292 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.078488 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.087654 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.096912 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.106152 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.129865 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.148567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.160257 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.177528 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.187113 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.196313 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.205559 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.214583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.223834 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.233207 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.249756 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.267241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.280861 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.293372 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.305329 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.314590 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.324010 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.334998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.344819 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.354472 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.363516 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.372566 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.382068 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.391133 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.400426 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.409602 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.418784 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.427992 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.437167 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.446336 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.455439 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.464442 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.473792 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.484964 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.494425 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.503344 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.513888 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.531032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.543091 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.553353 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.562706 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.572039 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.581454 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.590635 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.599882 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.608871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.621294 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.630318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:06:58.639553 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:42.937509 [2] proc begin: <DistEnv 2/4 nccl>
02:07:42.993495 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:07:43.006311 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:07:44.589514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.408640 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.427078 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.437298 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.447284 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.457004 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.468909 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.478253 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.488714 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.500502 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.510089 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.519636 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.528860 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.541095 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.550414 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.560707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.571792 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.582389 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.591943 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.603590 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.612839 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.622725 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.637553 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.647171 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.656948 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.668674 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.677961 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.687377 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.696616 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.705877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.715365 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.724695 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.734845 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.744575 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.754305 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.763995 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.773793 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.783483 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.794285 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.803806 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.815797 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.826623 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.836136 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.855168 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.868337 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.877848 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.890861 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.900047 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.909334 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.918763 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.929353 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.944145 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.958989 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.970277 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.979476 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:45.988707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.000212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.016439 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.028347 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.038574 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.048860 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.060638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.074311 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.088872 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.100836 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.112562 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.122098 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.131401 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.143784 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.154072 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.165443 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.175757 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.186239 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.197067 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.207598 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.218063 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.228819 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.239466 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.249323 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.259163 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.269933 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.279715 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.290547 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.301532 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.312998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.324286 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.333716 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.342941 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.355488 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.375113 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.396413 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.406847 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.416532 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.426060 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.435657 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.445452 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.455722 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.465048 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.474668 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.483905 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.493305 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.502911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.515687 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.525107 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.534951 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.545955 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.555173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.564356 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.573763 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.585509 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.596527 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.609526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.620724 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.629854 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.639207 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.648921 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.660143 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.669295 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.681211 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.690627 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.699551 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.709134 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.718682 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.730921 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.740400 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.754087 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.767421 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.778264 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.789567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.801645 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.813668 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.824638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.840515 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.854356 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.866035 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.877436 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.891265 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.902049 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.913275 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.932190 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.952302 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.974202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.984198 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:46.993205 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.002187 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.025851 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.038417 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.047660 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.056889 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.066182 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.081533 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.091638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.103762 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.112759 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.121772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.130694 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.139616 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.151639 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.163052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.171949 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.187366 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.200421 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.210965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.220618 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.231436 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.241312 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.250704 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.260003 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.269211 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.278255 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.287619 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.296718 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.306038 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.315026 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.326562 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.338213 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.351022 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.362959 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.372115 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.381766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.391051 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.400202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.409685 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.418968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.428264 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.437840 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.447143 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.456293 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.465928 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.475370 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.484785 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.495056 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.504222 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.513500 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.522965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.532349 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.541930 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.551191 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.560736 [2] Warning: no training nodes in this partition! Backward fake loss.
02:07:47.570579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:51.738340 [2] proc begin: <DistEnv 2/4 nccl>
02:08:51.790257 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:08:51.802849 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:08:53.406617 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.200676 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.215186 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.226800 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.236379 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.249288 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.258640 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.268328 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.280773 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.290249 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.299676 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.309477 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.319128 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.328888 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.342192 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.351071 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.360726 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.370390 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.381081 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.390715 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.402538 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.411970 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.421544 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.431293 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.440965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.450198 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.459673 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.469170 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.478618 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.489024 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.498780 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.510191 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.519538 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.528972 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.539520 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.548973 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.564524 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.582002 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.594592 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.605277 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.616165 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.625454 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.634620 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.643961 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.653393 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.662913 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.678902 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.688727 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.698492 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.707963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.717618 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.727043 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.741795 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.753719 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.775420 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.789292 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.801997 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.812552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.822181 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.831455 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.840823 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.850135 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.859258 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.868413 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.877580 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.886756 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.899867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.911469 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.921281 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.930595 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.939925 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.950045 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.959614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.971234 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.981483 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.990804 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:54.999869 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.009377 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.018479 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.027662 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.037164 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.046808 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.056010 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.065601 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.074761 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.083870 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.093105 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.102226 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.111607 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.122105 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.131772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.141278 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.151511 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.162114 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.171521 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.180712 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.189926 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.198918 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.208128 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.217647 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.226780 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.235989 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.245097 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.258885 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.275082 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.284218 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.294078 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.303670 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.314938 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.324106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.337982 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.350817 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.360384 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.369582 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.378791 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.388098 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.397490 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.406541 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.415850 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.425412 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.434618 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.443989 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.453881 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.463334 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.473104 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.485446 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.498274 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.507239 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.516666 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.525967 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.535328 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.544965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.554977 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.564843 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.577256 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.590274 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.599587 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.610589 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.620088 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.629987 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.639303 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.648605 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.657864 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.671649 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.682046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.691514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.701153 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.712917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.725325 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.734625 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.753923 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.772242 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.793744 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.805481 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.816508 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.826692 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.835700 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.845504 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.855615 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.865906 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.875124 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.884449 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.894110 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.909450 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.920566 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.929938 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.939583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.948968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.962509 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.973471 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.984567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:55.997086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.006328 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.016395 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.029266 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.039728 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.049226 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.062099 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.073915 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.086394 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.099459 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.108569 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.118197 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.127372 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.136597 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.146246 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.155753 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.165383 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.174735 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.184224 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.193993 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.203220 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.212639 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.221996 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.231701 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.241221 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.250414 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.259868 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.269707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:08:56.279308 [2] Warning: no training nodes in this partition! Backward fake loss.
02:18:49.278716 [2] proc begin: <DistEnv 2/4 nccl>
02:18:53.844356 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
02:18:53.852055 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:18:59.118336 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.202937 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.486105 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:00.767362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.046884 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.327872 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.620787 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:01.913221 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.205776 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.498514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:02.788896 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.072867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.354192 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.635067 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:03.914917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.196197 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.477022 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:04.757899 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.038373 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.319675 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.602210 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:05.883849 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.165752 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.446665 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:06.729382 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.011035 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.292031 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.573759 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:07.854785 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.135824 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.417991 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.699169 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:08.980906 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.262569 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.545803 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:09.827859 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.109577 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.392411 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.672711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:10.954216 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.235663 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.516058 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:11.797910 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.079178 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.359995 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.641427 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:12.923131 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.203877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.484871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:13.766183 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.047253 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.328531 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.610633 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:14.892299 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.174372 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.456292 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:15.737741 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.019208 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.300374 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.581911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:16.862488 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.144230 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.424478 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.705422 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:17.986383 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.267047 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.548670 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:18.831854 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.112978 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.394656 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.675949 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:19.957621 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.238087 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.519770 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:20.801218 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.082552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.363183 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.644154 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:21.925162 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.207155 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.488775 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:22.769812 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.050824 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.331931 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.613578 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:23.894497 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.175973 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.457376 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:24.738559 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.019594 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.300222 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.581337 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:25.862903 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.144587 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.426057 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.707059 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:26.989700 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.271555 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.552322 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:27.833103 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.114595 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.396456 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.677358 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:28.959202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.240612 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.521707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:29.803381 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.084394 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.366464 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.647688 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:30.930302 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.211226 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.491767 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:31.773468 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.054746 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.337194 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.618344 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:32.898933 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.181450 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.462674 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:33.743775 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.024994 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.305842 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.587117 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:34.868632 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.149614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.430194 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.711098 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:35.992340 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.273233 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.555405 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:36.836217 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.117858 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.399919 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.680511 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:37.961795 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.244259 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.525677 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:38.807650 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.089486 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.371362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.652549 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:39.933647 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.215925 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.498113 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:40.779722 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.062181 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.343345 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.623966 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:41.904567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.185300 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.465802 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:42.746216 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.026579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.308330 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.589538 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:43.870312 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.150950 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.431902 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.713251 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:44.994478 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.276279 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.556804 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:45.838049 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.119101 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.401230 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.683401 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:46.965298 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.247214 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.528671 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:47.809896 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.090407 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.371429 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.652931 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:48.933268 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.214839 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.496554 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:49.777982 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.058911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.340722 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.621905 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:50.903283 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.184614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.466234 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:51.746901 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.028117 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.309291 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.589959 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:52.870052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.151138 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.432244 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.713048 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:53.993349 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.274382 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.555638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:54.837778 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.118854 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.400283 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.680598 [2] Warning: no training nodes in this partition! Backward fake loss.
02:19:55.963685 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:06.368934 [2] proc begin: <DistEnv 2/4 nccl>
02:22:12.153519 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
02:22:12.164474 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:22:17.552137 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:18.852374 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.334291 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:19.814082 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.293514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:20.774412 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.256046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:21.738174 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.219842 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:22.699407 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.180155 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:23.661034 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.142801 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:24.626893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.109957 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:25.591195 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.073119 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:26.555296 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.036749 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.517300 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:27.996310 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.477374 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:28.958838 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.441040 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:29.922748 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.404716 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:30.887856 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.369787 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:31.853197 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.334897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:32.817437 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.299590 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:33.783719 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.266893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:34.749532 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.231237 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:35.714564 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.196418 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:36.677369 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.159962 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:37.641830 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.123278 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:38.605731 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.088897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:39.570603 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.051101 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:40.529579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.009510 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.488698 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:41.968215 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.447827 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:42.926658 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.405572 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:43.884783 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.363594 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:44.842304 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.320623 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:45.799206 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.277085 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:46.756281 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.234365 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:47.713917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.192175 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:48.670166 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.148564 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:49.627669 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.106345 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:50.585709 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.064814 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:51.543178 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.021568 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.500573 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:52.979163 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.457620 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:53.936531 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.415766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:54.893993 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.373005 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:55.853111 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.332067 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:56.810848 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.289595 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:57.767939 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.246738 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:58.725546 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.204838 [2] Warning: no training nodes in this partition! Backward fake loss.
02:22:59.683324 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.162510 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:00.641461 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.121277 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:01.615059 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.116867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:02.618848 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.104569 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:03.584809 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.064128 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:04.543252 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.022148 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.501106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:05.980365 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.459401 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:06.938418 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.418111 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:07.897223 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.377259 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:08.855579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.334006 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:09.813314 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.292462 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:10.771298 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.250933 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:11.729766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.208350 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:12.687582 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.167820 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:13.646917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.127923 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:14.607557 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.086891 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:15.566994 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.046751 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:16.526194 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.003724 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.480685 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:17.958347 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.435968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:18.913514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.390558 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:19.868370 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.346032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:20.822979 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.301714 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:21.779596 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.257553 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:22.735608 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.212650 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:23.689965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.167933 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:24.646123 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.125173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:25.604425 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.083457 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:26.562447 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.040680 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.518483 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:27.997361 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.476129 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:28.955255 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.434260 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:29.912633 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.391232 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:30.869910 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.348676 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:31.826816 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.305751 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:32.783843 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.262004 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:33.743212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.222853 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:34.702184 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.182351 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:35.661492 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.141523 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:36.621029 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.100871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:37.580339 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.059323 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:38.538585 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.018565 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.496205 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:39.974982 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.452609 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:40.931053 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.409668 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:41.887137 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.364537 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:42.842295 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.320910 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:43.800071 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.279567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:44.759339 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.238894 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:45.717626 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.197158 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:46.676427 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.155642 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:47.635866 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.115970 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:48.598580 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.079069 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:49.560653 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.042270 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:50.522393 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.002792 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.485065 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:51.966498 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.448008 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:52.929093 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.410635 [2] Warning: no training nodes in this partition! Backward fake loss.
02:23:53.892663 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:12.794831 [2] proc begin: <DistEnv 2/4 nccl>
02:25:17.639360 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
02:25:17.646922 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:25:23.316395 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:24.798323 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:25.688934 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:26.577765 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:27.464222 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:28.352301 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:29.239423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:30.125713 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.010417 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:31.895736 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:32.781041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:33.664361 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:34.550275 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:35.434008 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:36.318096 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:37.200307 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.082477 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:38.963934 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:39.848950 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:40.731990 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:41.615075 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:42.498221 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:43.379313 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:44.263013 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:45.144838 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.028629 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:46.910357 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:47.794448 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:48.676556 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:49.557712 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:50.440607 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:51.322643 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:52.205743 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.089768 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:53.970196 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:54.852329 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:55.732527 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:56.612526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:57.493530 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:58.376671 [2] Warning: no training nodes in this partition! Backward fake loss.
02:25:59.259579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:00.142764 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.025836 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:01.917075 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:02.837553 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:03.733086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:04.617046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:05.501886 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:06.385336 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:07.270515 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:08.156178 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.042284 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:09.928748 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:10.814366 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:11.701907 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:12.587705 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:13.473760 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:14.359871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:15.243596 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:16.128990 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.014250 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:17.901748 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:18.787126 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:19.674128 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:20.558546 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:21.444738 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:22.331653 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:23.218009 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.104015 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:24.989662 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:25.874754 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:26.758811 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:27.643697 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:28.526940 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:29.413801 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:30.299027 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:31.185462 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.069124 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:32.955237 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:33.839018 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:34.723944 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:35.608506 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:36.491126 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:37.375780 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:38.259171 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:39.141698 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.024581 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:40.908474 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:41.792751 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:42.677053 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:43.561871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:44.444897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:45.328179 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:46.212120 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.096816 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:47.981236 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:48.866301 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:49.750288 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:50.633649 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:51.518808 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:52.402634 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:53.286845 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:54.171494 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.055557 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:55.939312 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:56.823371 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:57.709273 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:58.594041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:26:59.478060 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:00.360770 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:01.241085 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:02.159845 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.069567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:03.954235 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:04.836152 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:05.719435 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:06.605925 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:07.490381 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:08.375079 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:09.260867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:10.145806 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.028288 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:11.912106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:12.795146 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:13.679770 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:14.563174 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:15.448098 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:16.332587 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:17.217631 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.101629 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:18.986968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:19.871804 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:20.756598 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:21.640584 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:22.525855 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:23.410307 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:24.295520 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:25.178096 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.062459 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:26.946476 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:27.830491 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:28.713174 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:29.596005 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:30.479964 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:31.362681 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:32.245233 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:33.128172 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.011108 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:34.895483 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:35.778237 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:36.660438 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:37.545071 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:38.428134 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:39.311429 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:40.195173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.077771 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:41.961513 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:42.841921 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:43.722906 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:44.607367 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:45.489707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:46.372134 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:47.256041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:48.139972 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.020714 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:49.903847 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:50.789035 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:51.670202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:52.552257 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:53.434975 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:54.318810 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:55.200829 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.080723 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:56.961173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:57.842840 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:58.725516 [2] Warning: no training nodes in this partition! Backward fake loss.
02:27:59.607175 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:00.488970 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:01.372062 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:02.272864 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:03.192610 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.079846 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:04.965516 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:05.850629 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:06.736762 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:07.622797 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:08.508460 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:09.394014 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:10.281568 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:11.167782 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.052607 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:12.936877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:13.822558 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:14.707551 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:15.592135 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:16.478232 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:17.362353 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:18.247326 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:19.131884 [2] Warning: no training nodes in this partition! Backward fake loss.
02:28:20.017857 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:32.473009 [2] proc begin: <DistEnv 2/4 nccl>
02:29:32.532677 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:29:32.545299 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:29:34.069052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.838870 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.855430 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.869179 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.879748 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.890604 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.901065 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.914200 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.925116 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.937567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.954373 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.970104 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.982608 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:34.991734 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.001208 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.010720 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.019635 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.032822 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.043121 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.053380 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.063470 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.073106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.083344 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.094390 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.104672 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.115142 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.126046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.136668 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.147076 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.157552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.167791 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.178237 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.188406 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.198856 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.210500 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.222904 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.233257 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.244410 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.253508 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.262830 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.271988 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.282670 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.292021 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.301417 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.310681 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.326155 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.335282 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.345012 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.354464 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.370540 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.382039 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.390973 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.405407 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.414552 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.423834 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.434911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.450809 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.462529 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.477273 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.489917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.499241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.508396 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.523638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.533893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.549227 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.565770 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.580883 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.590789 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.603027 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.611684 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.621849 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.630493 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.639386 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.648023 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.657664 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.667796 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.679176 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.688735 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.700591 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.709555 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.718640 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.727690 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.738401 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.746881 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.755589 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.766869 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.776413 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.785542 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.797282 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.812032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.821318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.830342 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.839324 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.849115 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.859095 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.868545 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.878331 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.887696 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.896891 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.908516 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.922280 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.935023 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.943993 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.953439 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.963465 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.974257 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.983018 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:35.992300 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.001470 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.011761 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.024080 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.033012 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.041995 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.051055 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.061318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.073132 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.085868 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.097790 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.110704 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.123627 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.133016 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.144270 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.155370 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.164766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.174146 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.183693 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.196991 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.206635 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.215829 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.225460 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.236302 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.245481 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.254635 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.263649 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.272848 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.282124 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.291564 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.301316 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.311598 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.324675 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.335963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.347981 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.362122 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.371614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.380759 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.390307 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.399383 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.408913 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.418509 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.427684 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.436771 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.445996 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.457965 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.468873 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.484563 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.496277 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.506408 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.515489 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.534630 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.545284 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.569446 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.581014 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.593003 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.602526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.611574 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.620767 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.631953 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.641032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.650178 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.659472 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.668641 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.677576 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.686501 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.695896 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.705425 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.714412 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.723382 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.732350 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.743645 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.754089 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.763278 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.776601 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.791880 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.804597 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.814441 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.823458 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.835086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.849771 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.860390 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.869960 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.878989 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.888365 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.899951 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.911850 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.920583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.929845 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.938940 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.948240 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.958919 [2] Warning: no training nodes in this partition! Backward fake loss.
02:29:36.968507 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:11.170500 [2] proc begin: <DistEnv 2/4 nccl>
02:30:11.212299 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:30:11.225228 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:30:12.804049 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.570300 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.586292 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.598967 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.611022 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.620307 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.633152 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.646201 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.659406 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.668653 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.678058 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.687554 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.699391 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.710126 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.719595 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.729062 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.738505 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.747702 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.757524 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.770422 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.781094 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.795250 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.805838 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.818151 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.827689 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.837256 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.846867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.856174 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.865936 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.875547 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.885016 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.897405 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.906992 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.916445 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.926793 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.936408 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.946265 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.958618 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.971092 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.984190 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:13.996893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.009652 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.021148 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.030776 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.040168 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.050683 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.060441 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.072519 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.082659 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.092652 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.102487 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.112139 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.121814 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.131201 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.140782 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.150646 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.167215 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.184019 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.196086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.206534 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.216282 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.225924 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.239458 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.249465 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.258959 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.274469 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.285518 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.296017 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.305649 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.316519 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.325760 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.335253 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.344813 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.359225 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.371005 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.380326 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.389853 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.401538 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.411873 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.421156 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.430789 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.441212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.452950 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.463684 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.474451 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.485071 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.494881 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.504575 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.514111 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.535091 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.548441 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.558229 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.567792 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.577189 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.586462 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.595467 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.605270 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.619497 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.629943 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.639475 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.649296 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.660478 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.670235 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.679818 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.689097 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.701010 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.710190 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.720711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.730163 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.739436 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.752356 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.761843 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.773742 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.782914 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.792498 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.802079 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.811348 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.820999 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.830258 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.839625 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.850474 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.867432 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.879858 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.889508 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.899431 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.908938 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.918480 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.927648 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.937060 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.946512 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.955884 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.965742 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.980203 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:14.991724 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.001597 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.011179 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.020723 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.030346 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.039615 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.048821 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.058682 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.068061 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.077686 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.086863 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.098345 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.107284 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.116750 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.128572 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.140398 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.150083 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.159477 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.181958 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.192375 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.204762 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.214536 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.223949 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.233190 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.257107 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.279643 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.291561 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.301150 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.310505 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.319723 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.329093 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.338367 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.350306 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.359387 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.369001 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.378196 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.387433 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.396602 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.406180 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.415286 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.426688 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.435702 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.445248 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.457416 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.469663 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.478886 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.494202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.509834 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.523145 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.536289 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.546490 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.555897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.565336 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.574911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.585465 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.597191 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.608933 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.618285 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.627452 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.638158 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.648643 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.658133 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.667255 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.678120 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.687108 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.696469 [2] Warning: no training nodes in this partition! Backward fake loss.
02:30:15.705888 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:49.108567 [2] proc begin: <DistEnv 2/4 nccl>
02:31:49.172901 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:31:49.185739 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:31:50.793178 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.571715 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.587135 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.596883 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.608791 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.625614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.636246 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.646550 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.656106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.665736 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.675091 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.687624 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.696808 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.709176 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.718343 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.727768 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.737224 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.746592 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.756019 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.765722 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.777539 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.787076 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.796835 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.806320 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.816733 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.827241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.836400 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.846531 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.855820 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.867550 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.876601 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.885989 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.899526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.910537 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.923918 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.933601 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.944561 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.957052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.967173 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.977472 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.986912 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:51.996834 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.008649 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.019691 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.028962 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.039605 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.048975 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.058435 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.067739 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.076970 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.086375 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.095728 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.105488 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.122525 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.134670 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.143826 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.153393 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.165703 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.178676 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.190328 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.199697 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.209535 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.221614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.231077 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.240176 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.249391 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.258563 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.267704 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.281126 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.291046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.300898 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.310499 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.319567 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.328949 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.344071 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.353020 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.362273 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.378014 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.387638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.397014 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.406422 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.415734 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.424968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.434475 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.445343 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.457086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.467621 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.477215 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.490821 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.518905 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.537729 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.551023 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.560242 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.570469 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.579869 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.589318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.605154 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.615732 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.627338 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.637032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.649429 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.658931 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.668220 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.677575 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.686994 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.699083 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.708522 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.717801 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.732993 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.742066 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.751106 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.763152 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.776020 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.785511 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.794699 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.804267 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.813937 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.823290 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.832439 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.841877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.850703 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.866731 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.877423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.888403 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.899186 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.909052 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.919448 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.929445 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.939720 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.950389 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.961087 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.974031 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.983426 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:52.992307 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.001367 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.010420 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.019407 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.028752 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.038165 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.047116 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.058064 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.070711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.079761 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.091449 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.104082 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.120574 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.130007 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.139176 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.148274 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.157479 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.184045 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.196056 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.209537 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.222401 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.234836 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.243728 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.255371 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.264532 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.274043 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.289176 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.299124 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.308683 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.319726 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.330367 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.342210 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.351163 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.360514 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.369558 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.382880 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.395030 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.406529 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.415362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.427423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.436482 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.445752 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.454872 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.463997 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.473701 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.482868 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.492126 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.504837 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.513850 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.523044 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.532053 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.541324 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.554027 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.566859 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.583243 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.595744 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.605980 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.615213 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.624372 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.633709 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.642911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.652077 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.661212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.670675 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.681496 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.690947 [2] Warning: no training nodes in this partition! Backward fake loss.
02:31:53.700265 [2] Warning: no training nodes in this partition! Backward fake loss.
02:32:59.893633 [2] proc begin: <DistEnv 2/4 nccl>
02:33:06.791444 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
02:33:06.808590 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:33:51.473321 [2] proc begin: <DistEnv 2/4 nccl>
02:33:57.635173 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
02:33:57.653095 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:35:06.650617 [2] proc begin: <DistEnv 2/4 nccl>
02:35:13.597655 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
02:35:13.614367 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:39.151138 [2] proc begin: <DistEnv 2/4 nccl>
02:36:39.230213 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:36:39.242516 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:36:40.795848 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.690428 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.726199 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.762151 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.790745 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.820456 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.848689 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.877656 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.906968 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.937147 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.962297 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:41.998526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.025435 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.058425 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.079473 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.111314 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.135624 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.167625 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.196993 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.225061 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.254888 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.275124 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.305621 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.336305 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.355256 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.389231 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.412032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.440840 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.474120 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.500478 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.529074 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.557443 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.586583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.615329 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.642580 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.670299 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.701131 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.733769 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.767647 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.799262 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.830423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.862597 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.895061 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.924278 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.958121 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:42.985518 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.019960 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.050700 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.081558 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.111231 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.154800 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.198768 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.228333 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.257774 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.292111 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.321727 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.343039 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.373461 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.413387 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.441948 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.469834 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.498213 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.528203 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.553999 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.582913 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.613760 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.643561 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.674395 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.704395 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.733212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.757573 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.791891 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.812697 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.841033 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.871104 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.904800 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.935735 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.963515 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:43.985394 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.016018 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.045921 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.076246 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.101802 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.136826 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.188431 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.217471 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.241127 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.271774 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.305418 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.331250 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.361186 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.384725 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.427289 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.460526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.489602 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.520367 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.543255 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.572602 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.601711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.628819 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.658257 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.688370 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.716576 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.744916 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.777120 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.808636 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.838165 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.861547 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.891811 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.921094 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.950309 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:44.978617 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.013017 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.036076 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.065040 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.094318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.124391 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.162978 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.210398 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.240563 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.268849 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.299587 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.328995 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.359241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.388695 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.423365 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.452658 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.480808 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.509241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.538472 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.568400 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.598238 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.628424 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.657501 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.680948 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.711208 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.740085 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.767754 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.797037 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.824537 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.850346 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.877019 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.897827 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.926196 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.956629 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:45.980955 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.011584 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.038877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.068232 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.097337 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.127978 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.168911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.203990 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.231453 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.257084 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.287698 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.321572 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.348852 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.375919 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.405342 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.443003 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.463596 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.493963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.523858 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.551408 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.579312 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.611195 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.640712 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.664205 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.695010 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.724044 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.750854 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.781377 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.812444 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.846432 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.866642 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.900909 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.932287 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.961506 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:46.991912 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.019081 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.047432 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.074681 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.103819 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.133218 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.164752 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.218759 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.249672 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.277524 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.305248 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.334032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.365464 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.394266 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.433116 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.458123 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.488706 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.517895 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.546690 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.577907 [2] Warning: no training nodes in this partition! Backward fake loss.
02:36:47.606905 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:14.172973 [2] proc begin: <DistEnv 2/4 nccl>
02:37:14.230449 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:37:14.242782 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:37:15.672228 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.436536 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.463506 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.487219 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.508165 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.529781 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.555131 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.580269 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.604505 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.629821 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.658822 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.686434 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.711179 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.735434 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.758817 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.778839 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.800747 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.827313 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.850269 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.874570 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.901312 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.926715 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.949615 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.972108 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:16.989096 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.001897 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.016073 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.029725 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.051879 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.074678 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.097638 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.124143 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.144922 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.167318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.195963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.217443 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.243249 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.259753 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.283570 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.306713 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.325723 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.350398 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.371391 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.387656 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.410501 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.428046 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.450570 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.469791 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.494419 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.515636 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.538362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.561034 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.587029 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.609959 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.632535 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.654335 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.675718 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.698711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.718087 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.739846 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.762419 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.786362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.810998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.832724 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.854006 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.877617 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.898605 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.924282 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.947342 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.963859 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:17.981331 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.005710 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.030035 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.054240 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.077467 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.099406 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.122482 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.142839 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.163475 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.185622 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.212018 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.240423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.259011 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.279616 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.301219 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.322389 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.338639 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.359081 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.378488 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.396177 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.419572 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.444646 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.470348 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.494680 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.518781 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.537891 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.562424 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.586361 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.611344 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.635738 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.658579 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.676381 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.698611 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.716021 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.738772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.759188 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.782919 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.802778 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.825166 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.846707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.870318 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.891461 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.914998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.938022 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.962216 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:18.981617 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.006245 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.027060 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.047296 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.068210 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.089171 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.111037 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.135129 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.157724 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.182314 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.209971 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.241728 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.264460 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.281105 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.300026 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.313646 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.325259 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.338528 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.360698 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.383292 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.398837 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.410405 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.424534 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.448440 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.470215 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.491315 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.515721 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.537707 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.562417 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.586280 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.607962 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.630972 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.652694 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.677461 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.697780 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.715695 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.738383 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.762244 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.781360 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.804258 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.827026 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.850884 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.870977 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.888762 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.910622 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.934827 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.954830 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.979094 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:19.999779 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.022250 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.041791 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.066341 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.087708 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.111493 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.134177 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.152763 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.178086 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.202943 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.225732 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.255458 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.278281 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.302930 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.326039 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.349499 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.378202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.406170 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.430800 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.457867 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.486041 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.509760 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.534187 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.554189 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.579260 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.601714 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.624871 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.650060 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.674916 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.698007 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.721201 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.736042 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.753695 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.771334 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.794289 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.814806 [2] Warning: no training nodes in this partition! Backward fake loss.
02:37:20.838748 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:16.506183 [2] proc begin: <DistEnv 2/4 nccl>
02:38:16.584560 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
02:38:16.597642 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

02:38:18.128691 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.007453 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.027963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.043916 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.061452 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.073227 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.083606 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.093723 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.103444 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.113492 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.123577 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.133768 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.145413 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.155480 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.169554 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.186858 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.206477 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.223911 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.247846 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.266217 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.286883 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.303654 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.322963 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.341700 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.360314 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.373298 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.383928 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.394194 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.403577 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.414189 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.424102 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.437437 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.456267 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.476118 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.500245 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.522134 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.539772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.560267 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.579585 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.598313 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.615597 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.627288 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.637529 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.647050 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.656363 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.666375 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.676058 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.687349 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.697037 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.707557 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.718006 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.728883 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.742695 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.752228 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.762998 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.774265 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.788130 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.797892 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.808241 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.819303 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.835003 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.854202 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.871450 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.890849 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.911157 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.930105 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.947182 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.958381 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.977076 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:19.997706 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.008994 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.022685 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.032343 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.042209 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.052422 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.062691 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.072703 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.082523 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.097089 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.115094 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.136346 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.155378 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.174299 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.192663 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.216775 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.232263 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.241929 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.252020 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.263362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.273368 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.283403 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.293377 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.304866 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.316442 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.327685 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.336992 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.355495 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.367236 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.377386 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.387334 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.397526 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.411504 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.421100 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.431194 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.448588 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.461991 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.472558 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.483095 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.499457 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.517900 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.536729 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.556732 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.575375 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.588661 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.608042 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.628179 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.648525 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.668135 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.692641 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.710951 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.730226 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.747906 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.768914 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.787350 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.806223 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.833132 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.852721 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.871877 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.890212 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.910979 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.922185 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.931521 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.941839 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.954665 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.968772 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:20.987883 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.009133 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.026815 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.046382 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.063917 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.083538 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.102743 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.122516 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.138306 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.148044 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.162090 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.172553 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.185172 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.198058 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.208529 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.218322 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.227902 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.238023 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.247766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.258224 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.271180 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.290419 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.307165 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.318048 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.327819 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.337621 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.347517 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.357922 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.368369 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.378614 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.389181 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.399644 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.411032 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.421981 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.447591 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.467259 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.485821 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.507181 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.528016 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.540959 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.560403 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.574304 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.584575 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.594699 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.605051 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.616429 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.631362 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.641076 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.651406 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.665273 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.674766 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.684583 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.702873 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.716275 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.725962 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.736711 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.748281 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.757764 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.769901 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.779464 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.792893 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.802423 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.812182 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.827096 [2] Warning: no training nodes in this partition! Backward fake loss.
02:38:21.837331 [2] Warning: no training nodes in this partition! Backward fake loss.
09:41:28.423770 [2] proc begin: <DistEnv 2/4 nccl>
09:42:00.232978 [2] proc begin: <DistEnv 2/4 nccl>
09:42:07.445697 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:42:07.463791 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:43:32.989497 [2] proc begin: <DistEnv 2/4 nccl>
09:43:39.654835 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:43:39.675187 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:45:34.990605 [2] proc begin: <DistEnv 2/4 nccl>
09:45:41.800121 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:45:41.819524 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:48:52.122159 [2] proc begin: <DistEnv 2/4 nccl>
09:48:57.653030 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:48:57.669856 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:49:45.632334 [2] proc begin: <DistEnv 2/4 nccl>
09:49:52.374005 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:49:52.390871 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:50:38.595610 [2] proc begin: <DistEnv 2/4 nccl>
09:50:44.235227 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:50:44.251725 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:42.355186 [2] proc begin: <DistEnv 2/4 nccl>
09:52:47.011031 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:52:47.019430 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:52.636911 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.664012 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:53.943048 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.223097 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.502034 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:54.780467 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.058440 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.340692 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.619642 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:55.898182 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.176587 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.455675 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:56.735137 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.013551 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.291838 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.571275 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:57.850158 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.128387 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.407749 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.686229 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:58.964727 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.244078 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.523525 [2] Warning: no training nodes in this partition! Backward fake loss.
09:52:59.802190 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.080609 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.359372 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.639301 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:00.918704 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.197827 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.476546 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:01.761788 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.052721 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.344964 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.637398 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:02.927758 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.215153 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.495168 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:03.774657 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.055114 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.334771 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.614186 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:04.893685 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.173317 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.451521 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:05.729669 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.008187 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.288580 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.569259 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:06.850289 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.130997 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.412282 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.692963 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:07.973749 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.253886 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.534225 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:08.814689 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.095835 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.376931 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.657213 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:09.937658 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.218560 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.498642 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:10.779008 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.060249 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.340445 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.620172 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:11.901542 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.183190 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.465009 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:12.745349 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.025702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.306027 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.585870 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:13.866294 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.146679 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.427023 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.707928 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:14.988725 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.269296 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.549291 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:15.830738 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.111569 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.392475 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.672496 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:16.952843 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.233957 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.514780 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:17.796413 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.076641 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.357680 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.637622 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:18.917406 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.198635 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.479357 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:19.759828 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.040251 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.320680 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.601012 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:20.882134 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.162000 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.441740 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:21.721847 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.001859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.282675 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.563066 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:22.843653 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.124183 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.404300 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.684563 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:23.964594 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.244435 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.524557 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:24.804520 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.084261 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.364213 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.645121 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:25.925298 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.205382 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.485553 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:26.767008 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.048636 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.329672 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.610167 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:27.890445 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.169502 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.448121 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:28.726263 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.004132 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.282018 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.560548 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:29.839452 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.117736 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.396077 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.674833 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:30.953202 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.231966 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.510442 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:31.790021 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.069907 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.349325 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.628908 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:32.908448 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.189169 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.468468 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:33.747443 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.026808 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.306295 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.585696 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:34.864802 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.144647 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.423267 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.702519 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:35.980966 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.260190 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.539527 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:36.818450 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.097413 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.377585 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.657660 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:37.938357 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.216727 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.495106 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:38.773736 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.052786 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.331830 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.610966 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:39.891065 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.169335 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.448099 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:40.727167 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.006150 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.284387 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.562726 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:41.841689 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.119965 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.398803 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.677417 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:42.956563 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.235512 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.514316 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:43.793185 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.071661 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.350077 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.629098 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:44.907361 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.186101 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.465092 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:45.743679 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.022760 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.301594 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.580870 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:46.859512 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.137705 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.416782 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.696235 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:47.974819 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.253364 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.533226 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:48.812058 [2] Warning: no training nodes in this partition! Backward fake loss.
09:53:49.090734 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:13.920283 [2] proc begin: <DistEnv 2/4 nccl>
09:54:18.939906 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:54:18.948214 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:54:22.804247 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:23.886672 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.168206 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.449363 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:24.733261 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.014735 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.295702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.576749 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:25.858354 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.139286 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.420112 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.700424 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:26.981452 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.264296 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.545482 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:27.826580 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.108659 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.390385 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.670758 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:28.952146 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.233175 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.513832 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:29.794866 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.076243 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.356469 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.637857 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:30.919084 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.200548 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.480238 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:31.761026 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.041925 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.322957 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.603608 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:32.883715 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.164726 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.445644 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:33.726032 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.006829 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.286773 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.567634 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:34.848637 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.128903 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.409128 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.689432 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:35.970034 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.250537 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.530235 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:36.810011 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.090525 [2] Warning: no training nodes in this partition! Backward fake loss.
09:54:37.370939 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:31.597611 [2] proc begin: <DistEnv 2/4 nccl>
09:55:44.281928 [2] proc begin: <DistEnv 2/4 nccl>
09:55:48.643647 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:55:48.651294 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:55:53.223781 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.482384 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:54.960567 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.439655 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:55.918340 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.396702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:56.878370 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.358366 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:57.842037 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.321773 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:58.803744 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.284709 [2] Warning: no training nodes in this partition! Backward fake loss.
09:55:59.766966 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.247950 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:00.726596 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.204862 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:01.683751 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.163400 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:02.666776 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.170015 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:03.671708 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.153318 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:04.634170 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.114715 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:05.595287 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.079180 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:06.559359 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.040543 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:07.522685 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.005788 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.488943 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:08.970967 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.452142 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:09.932373 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.412757 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:10.894602 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.377254 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:11.858350 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.340881 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:12.823409 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.304230 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:13.785227 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.265309 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:14.747581 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.228093 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:15.708959 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.190537 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:16.670320 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.150773 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:17.630167 [2] Warning: no training nodes in this partition! Backward fake loss.
09:56:52.868493 [2] proc begin: <DistEnv 2/4 nccl>
09:56:59.438576 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:56:59.460853 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:58:03.783978 [2] proc begin: <DistEnv 2/4 nccl>
09:58:11.125165 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:58:11.141387 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:01.538112 [2] proc begin: <DistEnv 2/4 nccl>
10:00:08.511809 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:00:08.528242 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:00:20.868954 [2] proc begin: <DistEnv 2/4 nccl>
10:00:27.512591 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:00:27.538997 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:02:52.196835 [2] proc begin: <DistEnv 2/4 nccl>
10:02:58.013090 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:02:58.036767 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:04:12.717292 [2] proc begin: <DistEnv 2/4 nccl>
10:04:18.616517 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:04:18.633025 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:34.429524 [2] proc begin: <DistEnv 2/4 nccl>
10:25:39.748550 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:25:39.761549 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:25:44.599036 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.182578 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:46.954313 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:47.726245 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:48.497542 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:49.265833 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.035471 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:50.806286 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:51.577622 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:52.350182 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.121953 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:53.890575 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:54.662665 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:55.433769 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.203814 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:56.974146 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:57.743625 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:58.511419 [2] Warning: no training nodes in this partition! Backward fake loss.
10:25:59.281230 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.050756 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:00.818806 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:01.587972 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:02.390698 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.188462 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:03.956975 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:04.724866 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:05.493285 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:06.262091 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.031270 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:07.801261 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:08.570395 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:09.338091 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.107488 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:10.875761 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:11.645221 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:12.413428 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.182146 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:13.952140 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:14.722059 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:15.493934 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:16.263198 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.033852 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:17.803903 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:18.574143 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:19.343981 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.114026 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:20.883798 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:21.653740 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:22.424115 [2] Warning: no training nodes in this partition! Backward fake loss.
10:26:23.194739 [2] Warning: no training nodes in this partition! Backward fake loss.
10:27:50.523702 [2] proc begin: <DistEnv 2/4 nccl>
10:27:55.711250 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:27:55.718652 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:28:01.542070 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:04.195276 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:06.072009 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:07.949779 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:09.825537 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:11.698100 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:13.566259 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:15.433974 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:17.298948 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:19.164717 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:21.029524 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:22.896117 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:24.760864 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:26.625555 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:28.489606 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:30.353444 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:32.218283 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:34.082236 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:35.946386 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:37.810988 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:39.676474 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:41.538273 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:43.404717 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:45.268388 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:47.133869 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:48.998817 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:50.862349 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:52.728292 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:54.592855 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:56.461714 [2] Warning: no training nodes in this partition! Backward fake loss.
10:28:58.327446 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:00.193904 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:02.089848 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:03.987229 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:05.850827 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:07.719059 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:09.588377 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:11.455056 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:13.322158 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:15.185885 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:17.049097 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:18.912629 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:20.776831 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:22.641573 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:24.506023 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:26.371040 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:28.236080 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:30.099770 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:31.962838 [2] Warning: no training nodes in this partition! Backward fake loss.
10:29:33.828046 [2] Warning: no training nodes in this partition! Backward fake loss.
10:31:28.878719 [2] proc begin: <DistEnv 2/4 nccl>
10:31:33.339042 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:31:33.350460 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:31:40.361509 [2] Warning: no training nodes in this partition! Backward fake loss.
10:31:45.282450 [2] Warning: no training nodes in this partition! Backward fake loss.
10:31:49.354513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:31:53.424730 [2] Warning: no training nodes in this partition! Backward fake loss.
10:31:57.491707 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:01.557066 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:05.691641 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:09.761031 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:13.830578 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:17.901203 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:21.967606 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:26.038812 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:30.103834 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:34.168965 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:38.235452 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:42.302876 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:46.370933 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:50.437025 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:54.503177 [2] Warning: no training nodes in this partition! Backward fake loss.
10:32:58.570822 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:02.694702 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:06.773110 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:10.843427 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:14.913227 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:18.982308 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:23.048562 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:27.116274 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:31.182003 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:35.247260 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:39.313097 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:43.379654 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:47.453022 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:51.520002 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:55.585672 [2] Warning: no training nodes in this partition! Backward fake loss.
10:33:59.654832 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:03.806030 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:07.882388 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:11.954146 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:16.024353 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:20.091750 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:24.160004 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:28.226988 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:32.294845 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:36.360830 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:40.429513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:44.500492 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:48.565926 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:52.632692 [2] Warning: no training nodes in this partition! Backward fake loss.
10:34:56.697902 [2] Warning: no training nodes in this partition! Backward fake loss.
10:35:00.763363 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:42.915781 [2] proc begin: <DistEnv 2/4 nccl>
10:51:47.742261 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:51:47.749942 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:49.318062 [2] proc begin: <DistEnv 2/4 nccl>
10:52:54.251539 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:52:54.260264 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:53:50.140227 [2] proc begin: <DistEnv 2/4 nccl>
10:53:55.578870 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:53:55.590359 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:54:42.837876 [2] proc begin: <DistEnv 2/4 nccl>
10:54:47.505626 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:54:47.517004 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:02.132774 [2] proc begin: <DistEnv 2/4 nccl>
10:56:08.094748 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:56:08.104057 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:13.130869 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:14.578837 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.287918 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:15.993415 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:16.700965 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:17.408227 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.115919 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:18.823635 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:19.530610 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.239272 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:20.945506 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:21.652001 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:22.360368 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.068630 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:23.773732 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:24.479076 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.184550 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:25.889566 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:26.595853 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:27.300516 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.006612 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:28.715185 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:29.423259 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.128342 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:30.834826 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:31.540186 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.248660 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:32.954399 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:33.661031 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:34.367261 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.076182 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:35.782884 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:36.490467 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.197940 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:37.905604 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:38.612888 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:39.320792 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.028691 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:40.734493 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:41.440920 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.147562 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:42.854138 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:43.559429 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.264683 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:44.970257 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:45.676200 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:46.380968 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.089576 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:47.797303 [2] Warning: no training nodes in this partition! Backward fake loss.
10:56:48.502948 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:01.122862 [2] proc begin: <DistEnv 2/4 nccl>
10:58:06.222969 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:58:06.233431 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:01.304783 [2] proc begin: <DistEnv 2/4 nccl>
11:00:05.756795 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:00:05.764599 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:56.919167 [2] proc begin: <DistEnv 2/4 nccl>
11:01:02.672995 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:01:02.680609 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:09.757994 [2] proc begin: <DistEnv 2/4 nccl>
11:02:14.805836 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:02:14.820467 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:08:18.114935 [2] proc begin: <DistEnv 2/4 nccl>
11:08:23.212549 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:08:23.222290 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:09:49.607009 [2] proc begin: <DistEnv 2/4 nccl>
11:10:07.499841 [2] proc begin: <DistEnv 2/4 nccl>
11:10:12.188588 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:10:12.197085 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:20:41.967306 [2] proc begin: <DistEnv 2/4 nccl>
11:20:46.875600 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:20:46.883343 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:21:08.511737 [2] proc begin: <DistEnv 2/4 nccl>
11:21:13.492446 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:21:13.500832 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:22:41.555318 [2] proc begin: <DistEnv 2/4 nccl>
11:22:46.105596 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:22:46.113305 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:05.043651 [2] proc begin: <DistEnv 2/4 nccl>
11:23:10.396194 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:23:10.403741 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:40.401086 [2] proc begin: <DistEnv 2/4 nccl>
11:23:44.469493 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:23:44.477772 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:23:51.482302 [2] Warning: no training nodes in this partition! Backward fake loss.
11:23:54.063134 [2] Warning: no training nodes in this partition! Backward fake loss.
11:23:55.884777 [2] Warning: no training nodes in this partition! Backward fake loss.
11:23:57.706868 [2] Warning: no training nodes in this partition! Backward fake loss.
11:23:59.525463 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:01.342164 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:03.225190 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:05.045003 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:06.864414 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:08.689664 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:10.513744 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:12.336403 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:14.157746 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:15.978493 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:17.797494 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:19.615812 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:21.437258 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:23.256633 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:25.077790 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:26.898794 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:28.717377 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:30.534694 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:32.354307 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:34.174659 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:35.995424 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:37.631608 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:39.267528 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:40.899922 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:42.533515 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:44.169929 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:45.802576 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:47.435662 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:49.069991 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:50.706815 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:52.340446 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:53.975309 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:55.610496 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:57.246092 [2] Warning: no training nodes in this partition! Backward fake loss.
11:24:58.883628 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:00.518108 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:02.156502 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:03.849829 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:05.489239 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:07.130343 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:08.774347 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:10.417253 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:12.060929 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:13.702059 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:15.337953 [2] Warning: no training nodes in this partition! Backward fake loss.
11:25:16.972360 [2] Warning: no training nodes in this partition! Backward fake loss.
14:25:06.553679 [2] proc begin: <DistEnv 2/4 nccl>
14:25:10.966073 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:25:10.976995 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:18.070537 [2] Warning: no training nodes in this partition! Backward fake loss.
14:25:47.579135 [2] proc begin: <DistEnv 2/4 nccl>
14:25:52.464474 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:25:52.473935 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:25:59.493037 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:28.175474 [2] proc begin: <DistEnv 2/4 nccl>
14:26:33.665311 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:26:33.678427 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:38.145829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:39.572288 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.277936 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:40.985574 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:49.988278 [2] proc begin: <DistEnv 2/4 nccl>
14:26:55.234333 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:26:55.242092 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:27:00.738311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:02.286631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.027457 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:03.739461 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:04.449069 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.159493 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:05.868897 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:06.580254 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:07.290888 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.001287 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:08.712614 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:09.423946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:27:17.560503 [2] proc begin: <DistEnv 2/4 nccl>
14:27:24.262310 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:27:24.279824 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:06:35.894900 [2] proc begin: <DistEnv 2/4 nccl>
16:06:41.999241 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:06:42.016042 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:09:53.467828 [2] proc begin: <DistEnv 2/4 nccl>
16:09:58.819974 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:09:58.833880 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:11:41.296224 [2] proc begin: <DistEnv 2/4 nccl>
16:11:46.819821 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:11:46.836641 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:23.036390 [2] proc begin: <DistEnv 2/4 nccl>
16:13:23.100039 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:13:23.112724 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:24.477908 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.238774 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.252690 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.260555 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.269009 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.282173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.291126 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.299420 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.314632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.322494 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.333090 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.343710 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.354802 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.362480 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.378200 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.390334 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.398778 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.406404 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.415318 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.423404 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.431114 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.439027 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.447413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.455558 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.465965 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.474145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.487614 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.497271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.506060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.515086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.524821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.533889 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.542957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.555215 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.564227 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.573536 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.582746 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.594676 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.603260 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.612567 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.623443 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.633050 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.642063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.652794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.666849 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.677877 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.687475 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.695374 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.703390 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.711285 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.719006 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.725306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.733846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.740581 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.749552 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.757813 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.770792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.778058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.787790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.795359 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.805610 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.813223 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.823604 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.831154 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.841318 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.851262 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.862662 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.870249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.880040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.887727 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.896984 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.904564 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.914636 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.922189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.931796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.939276 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.949037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.956581 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.966430 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.975564 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.988117 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:25.998576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.008413 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.016518 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.026442 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.034034 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.051449 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.061721 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.072496 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.080382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.090002 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.097431 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.113852 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.123048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.143148 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.153903 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.166371 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.173624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.183933 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.191415 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.201068 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.208231 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.218271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.225483 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.238261 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.247028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.257761 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.264959 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.275376 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.284674 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.299738 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.308873 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.322277 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.330726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.341320 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.352077 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.363645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.371442 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.381911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.389205 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.399328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.406810 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.416741 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.424597 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.437982 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.447104 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.458497 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.467466 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.481594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.491231 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.503877 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.512586 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.524211 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.533551 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.557921 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.569538 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.583228 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.591210 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.604437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.611951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.624742 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.632079 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.642646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.650215 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.660360 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.667942 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.678522 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.686310 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.697414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.706609 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.719723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.727944 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.740794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.748122 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.758682 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.766337 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.780898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.802582 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.812576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.820373 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.834375 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.843606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.854418 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.861947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.873319 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.881390 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.894730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.903808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.913989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.922952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.932606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.941432 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.951266 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.959508 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.974846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.982444 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:26.994336 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.003051 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.015958 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.024023 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.034556 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.045024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.057280 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.064931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.075701 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.083805 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.095568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.103098 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.118315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.125302 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.139613 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.156382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.167202 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.173928 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.182159 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.188171 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.196429 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.202796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.211570 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:27.218163 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:41.556070 [2] proc begin: <DistEnv 2/4 nccl>
16:13:41.617897 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:13:41.630529 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:13:43.192324 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.975977 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:43.999860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.022152 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.043312 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.067703 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.087279 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.106920 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.124248 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.147002 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.168224 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.190625 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.214564 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.234654 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.258306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.277624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.303039 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.326242 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.347219 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.371539 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.394648 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.417651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.441936 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.466088 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.487016 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.508380 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.531249 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.558370 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.582403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.603923 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.621797 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.646771 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.667526 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.694175 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.721058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.745695 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.770974 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.794439 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.818675 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.839295 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.859043 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.879078 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.899204 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.915784 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.938265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.970507 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:44.995058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.017646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.043358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.067225 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.091214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.117454 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.146439 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.170471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.189971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.211645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.234504 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.255328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.278707 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.295846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.315895 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.334372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.352322 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.375229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.391726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.414713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.435284 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.453925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.477179 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.502611 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.525366 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.546139 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.581560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.597800 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.628762 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.652340 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.672104 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.694567 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.714073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.740942 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.762539 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.788551 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.810846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.830964 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.854211 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.872676 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.896236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.918445 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.947305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.970895 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:45.992434 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.014712 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.042447 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.063105 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.089924 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.110515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.134850 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.159016 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.182315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.205700 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.227270 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.253206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.272890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.295692 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.322063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.341168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.363569 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.385989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.405188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.431112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.454678 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.481885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.511125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.538533 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.564477 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.590755 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.616882 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.644575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.667989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.688505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.712456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.737771 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.762496 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.783729 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.806820 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.827460 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.851002 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.869087 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.892096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.913305 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.934169 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.958363 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:46.981358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.006076 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.025927 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.050059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.075774 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.098232 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.122606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.146163 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.172116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.198022 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.223062 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.247011 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.268526 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.290092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.312726 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.338535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.360772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.378275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.399937 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.422544 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.446593 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.470724 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.495207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.518637 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.540999 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.562654 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.593414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.621591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.649968 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.679461 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.710132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.733930 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.755088 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.777766 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.797828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.817560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.838853 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.862377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.881841 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.905713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.927465 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.949899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.970635 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:47.990644 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.010126 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.032922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.053984 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.075681 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.098924 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.116255 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.138692 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.158309 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.178199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.201722 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.225331 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.238553 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.250869 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.264128 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.276674 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.293901 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.309911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.327207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.350341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.374389 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.398877 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.422642 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.446917 [2] Warning: no training nodes in this partition! Backward fake loss.
16:13:48.471085 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:06.493323 [2] proc begin: <DistEnv 2/4 nccl>
16:14:06.551363 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
16:14:06.563942 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:14:08.016839 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.860420 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.901567 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.933724 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:08.970034 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.006591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.039713 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.072705 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.105066 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.132185 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.161073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.192907 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.227440 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.260532 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.287913 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.319802 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.352593 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.386354 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.421923 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.456352 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.488321 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.523448 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.561457 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.591463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.624272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.659951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.691486 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.724838 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.755096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.787409 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.821005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.854511 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.885254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.920118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.951130 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:09.982341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.016283 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.047979 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.083162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.120139 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.156472 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.193278 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.225109 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.264153 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.300554 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.337265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.372038 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.419257 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.462402 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.507890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.550236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.585858 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.623453 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.656813 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.692784 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.727245 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.754518 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.782074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.810934 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.843347 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.876131 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.910688 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.941776 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.968133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:10.994777 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.024561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.056907 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.092007 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.123697 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.155164 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.192824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.226669 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.259059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.287325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.320822 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.352174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.382205 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.420505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.454824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.488601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.526750 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.570987 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.603934 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.635942 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.668380 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.699913 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.743032 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.774474 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.807846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.838806 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.872125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.898410 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.931535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.958569 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:11.982603 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.015732 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.047395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.079185 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.113406 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.149024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.190467 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.223957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.256086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.287975 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.324189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.356289 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.392213 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.430885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.463281 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.500390 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.543184 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.588307 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.621542 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.655168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.683642 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.716555 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.747846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.783689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.820933 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.856945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.894907 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.925000 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.962530 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:12.995683 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.032952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.070350 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.105184 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.136037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.172716 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.206878 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.237850 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.270794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.304327 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.334883 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.365020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.397324 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.435004 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.463082 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.494885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.537525 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.580136 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.611285 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.645396 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.672179 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.703452 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.735705 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.767721 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.795601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.827391 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.861585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.895505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.927829 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.954115 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:13.986555 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.016186 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.047947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.080026 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.115842 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.153537 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.189902 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.227119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.264504 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.301823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.337455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.375280 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.407772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.446883 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.478536 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.513552 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.551177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.597260 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.628133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.656372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.689459 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.726815 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.762721 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.795847 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.831280 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.863096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.898133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.930835 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.962652 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:14.997317 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.030214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.066445 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.103780 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.139271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.173461 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.209580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.248929 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.286699 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.323105 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.357772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.394229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.430275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.477041 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.512419 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.558221 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.603676 [2] Warning: no training nodes in this partition! Backward fake loss.
16:14:15.635689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:25.146045 [2] proc begin: <DistEnv 2/4 nccl>
16:15:29.794832 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:15:29.802654 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:15:34.554302 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.059281 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:36.768102 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:37.477604 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.185576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:38.893767 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:39.602012 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:40.313507 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.021636 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:41.730257 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:42.438053 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.146161 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:43.854674 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:44.562588 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.271372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:45.980183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:46.689723 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:47.396946 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.105056 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:48.814174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:49.522147 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.231267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:50.939352 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:51.647113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:52.355193 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.063213 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:53.769671 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:54.477175 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.185343 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:55.895848 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:56.603505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:57.311736 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.019558 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:58.729160 [2] Warning: no training nodes in this partition! Backward fake loss.
16:15:59.438166 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.145206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:00.853059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:01.574781 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:02.313821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.037797 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:03.746938 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:04.453778 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.161550 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:05.870689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:06.579214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.287590 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:07.994764 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:08.702313 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:09.411188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.119014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:10.827370 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:11.296174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.054087 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:12.523395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.280936 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:13.748109 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.504643 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:14.972363 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:15.730466 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.197907 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:16.956691 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:17.425579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.183523 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:18.652732 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.410329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:19.878128 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:20.636999 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.104856 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:21.861399 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:22.329622 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.086059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:23.553966 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.310630 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:24.778463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:25.536120 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.003799 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:26.761388 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.228725 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:27.985663 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:28.453715 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.210874 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:29.678688 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.436425 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:30.904072 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:31.662669 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.131092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:32.890491 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:33.359764 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.118383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:34.586700 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.343791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:35.812919 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:36.569391 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.037867 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:37.794739 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:38.262982 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.021761 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:39.491203 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.248749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:40.718744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.476798 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:41.946221 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:42.704883 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.174501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:43.931380 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:44.400915 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.158985 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:45.627164 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.384038 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:46.853008 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:47.610624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.078918 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:48.836455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:49.304210 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.062196 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:50.531218 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.288187 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:51.756729 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.517346 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:52.989338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:53.748650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.217017 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:54.973463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:55.442022 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.198576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:56.666963 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.424997 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:57.892746 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:58.649605 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.118652 [2] Warning: no training nodes in this partition! Backward fake loss.
16:16:59.875175 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:00.344130 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.101347 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:01.569950 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.347993 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:02.835600 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:03.611940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.080910 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:04.838189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:05.307964 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.064145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:06.533591 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.290919 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:07.759498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.516511 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:08.985325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:09.742441 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.210859 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:10.967158 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:11.433989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.191268 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:12.658692 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.413696 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:13.881198 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:14.637535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.105753 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:15.861824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:16.329773 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.085929 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:17.553385 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.309005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:18.776434 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:19.531269 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:19.999875 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:20.755827 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.223000 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:21.979381 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:22.446875 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.203214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:23.671563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.427575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:24.895116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:25.651367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.119081 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:26.874423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:27.342523 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.097122 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:28.564546 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.321096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:29.788068 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:30.544182 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.012035 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:31.768668 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.238050 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:32.993908 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:33.460909 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.217855 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:34.684891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.440815 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:35.909047 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:36.664636 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.133763 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:37.889531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:38.357093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.111920 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:39.579298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.335821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:40.803069 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:41.558297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:17:42.025826 [2] Warning: no training nodes in this partition! Backward fake loss.
09:20:40.244796 [2] proc begin: <DistEnv 2/4 nccl>
09:20:43.823115 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:20:43.875709 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:47.569685 [2] proc begin: <DistEnv 2/4 nccl>
09:22:55.126917 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:22:55.174667 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:19.175357 [2] proc begin: <DistEnv 2/4 nccl>
09:25:26.775071 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:25:26.803956 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:05.169591 [2] proc begin: <DistEnv 2/4 nccl>
09:26:11.828305 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:26:11.863049 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:53.472413 [2] proc begin: <DistEnv 2/4 nccl>
09:26:59.834811 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:26:59.858831 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:27:26.551180 [2] proc begin: <DistEnv 2/4 nccl>
09:27:33.294807 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:27:33.332090 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:20.364664 [2] proc begin: <DistEnv 2/4 nccl>
14:13:38.423254 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:13:38.430537 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:13:49.369197 [2] Warning: no training nodes in this partition! Backward fake loss.
14:14:53.239472 [2] proc begin: <DistEnv 2/4 nccl>
14:15:00.950549 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:15:00.954767 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53351 KB |  217029 KB |  301754 KB |  248402 KB |
|       from large pool |   53351 KB |  217029 KB |  279213 KB |  225862 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:15:08.055396 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:10.029938 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:11.890524 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:13.747232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:15.599314 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:17.457484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:19.305354 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:21.156207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:23.007649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:24.860139 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:26.707709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:28.558473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:30.412469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:32.266763 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:34.126270 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:35.973367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:37.829046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:39.683992 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:41.536223 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:43.389218 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:45.242142 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:47.091305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:48.941171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:50.795929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:52.645442 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:54.496980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:56.351371 [2] Warning: no training nodes in this partition! Backward fake loss.
14:15:58.210196 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:00.068206 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:01.936461 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:03.846628 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:05.699892 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:07.557447 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:09.413744 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:11.271512 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:13.128143 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:14.980202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:16.830701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:18.680307 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:20.535138 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:22.387918 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:24.236634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:26.089779 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:27.944192 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:29.796803 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:31.647257 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:33.501385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:35.355645 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:37.209647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:39.063750 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:40.921688 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:42.593686 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:44.450429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:46.118267 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:47.975076 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:49.644219 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:51.503361 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:53.171852 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:55.022198 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:56.682420 [2] Warning: no training nodes in this partition! Backward fake loss.
14:16:58.531637 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:00.197643 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:02.082942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:03.789342 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:05.647253 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:07.321696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:09.178844 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:10.850286 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:12.712952 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:14.388925 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:16.247491 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:17.919256 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:19.783478 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:21.458876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:23.313044 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:24.980721 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:26.838973 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:28.511776 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:30.369456 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:32.038177 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:33.893675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:35.567062 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:37.430419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:39.101224 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:40.957351 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:42.627377 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:44.484666 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:46.158379 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:48.012717 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:49.680327 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:51.534843 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:53.205681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:55.058326 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:56.731295 [2] Warning: no training nodes in this partition! Backward fake loss.
14:17:58.590595 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:00.262728 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:02.127729 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:03.857390 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:05.721020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:07.392013 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:09.251857 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:10.926368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:12.785570 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:14.453656 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:16.312326 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:17.982475 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:19.838816 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:21.512917 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:23.372037 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:25.038720 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:26.894638 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:28.563870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:30.421222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:32.091919 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:33.945971 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:35.619710 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:37.476647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:39.148811 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:41.007171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:42.676178 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:44.535606 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:46.202939 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:48.064703 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:49.732376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:51.585531 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:53.250582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:55.107730 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:56.777217 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:58.641321 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:00.318847 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:02.202298 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:03.919205 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:05.776970 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:07.454293 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:09.313495 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:10.983446 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:12.838398 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:14.517447 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:16.377821 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:18.052751 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:19.910249 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:21.584910 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:23.443204 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:25.113567 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:26.969789 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:28.641660 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:30.495756 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:32.167817 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:34.024624 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:35.693925 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:37.549572 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:39.216836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:41.073078 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:42.746664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:44.608560 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:46.278144 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:48.142349 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:49.809770 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:51.664397 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:53.331046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:55.184583 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:56.854293 [2] Warning: no training nodes in this partition! Backward fake loss.
14:19:58.709070 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:00.385458 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:02.289924 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:03.981993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:05.842181 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:07.516892 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:09.377193 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:11.050726 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:12.906787 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:14.578089 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:16.436569 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:18.108614 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:19.963227 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:21.628851 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:23.480945 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:25.147121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:27.002073 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:28.670968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:30.525948 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:32.192833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:34.050429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:35.719480 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:37.580875 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:39.250250 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:41.104228 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:42.774213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:44.629327 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:46.299062 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:48.153732 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:49.820769 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:51.673232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:53.350067 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:55.210964 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:56.881366 [2] Warning: no training nodes in this partition! Backward fake loss.
14:20:58.740371 [2] Warning: no training nodes in this partition! Backward fake loss.
14:21:00.419385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:21:02.297664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:21:04.023986 [2] Warning: no training nodes in this partition! Backward fake loss.
14:22:14.915620 [2] proc begin: <DistEnv 2/4 nccl>
14:22:18.850570 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:22:18.858762 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:22:24.463736 [2] Warning: no training nodes in this partition! Backward fake loss.
14:22:27.008664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:22:28.836999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:22:30.671584 [2] Warning: no training nodes in this partition! Backward fake loss.
14:22:32.502441 [2] Warning: no training nodes in this partition! Backward fake loss.
14:25:50.272154 [2] proc begin: <DistEnv 2/4 nccl>
14:25:54.821679 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:25:54.829639 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:26:01.871049 [2] Warning: no training nodes in this partition! Backward fake loss.
14:26:25.649025 [2] proc begin: <DistEnv 2/4 nccl>
14:26:32.364086 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:26:32.367171 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| Active memory         |  466840 KB |  505541 KB |  763880 KB |  297040 KB |
|       from large pool |  466840 KB |  505541 KB |  763857 KB |  297016 KB |
|       from small pool |       0 KB |       2 KB |      23 KB |      23 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  522240 KB |  522240 KB |  522240 KB |       0 B  |
|       from large pool |  520192 KB |  520192 KB |  520192 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53351 KB |  217029 KB |  301754 KB |  248402 KB |
|       from large pool |   53351 KB |  217029 KB |  279213 KB |  225862 KB |
|       from small pool |       0 KB |    2047 KB |   22540 KB |   22540 KB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      54    |      37    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |      33    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |      14    |      11    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |      11    |      11    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:25.429616 [2] proc begin: <DistEnv 2/4 nccl>
15:00:25.638580 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:00:25.647892 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:26.832846 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.561247 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.572621 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.589990 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.600446 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.609162 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.616382 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.624280 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.631722 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.644162 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.651642 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.659300 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.666546 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.673991 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.681450 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.691241 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.699149 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.707286 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.714343 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.721425 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.730472 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.737940 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.748392 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.756204 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.766341 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.775108 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.784054 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.791378 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.798814 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.806360 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.814541 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.822603 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.831322 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.842257 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.850897 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.858679 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.870479 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.878076 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.885604 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.892508 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.900338 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.908250 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.915743 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.922729 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.930300 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.937921 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.945342 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.953976 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.961175 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.968043 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.059287 [2] proc begin: <DistEnv 2/4 nccl>
15:01:56.088222 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:01:56.097848 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:01:58.203511 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.925496 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.936059 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.943584 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.951417 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.958952 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.966188 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.973011 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.980369 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.988866 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.996300 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.003965 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.011646 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.019375 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.029850 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.038798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.049071 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.058521 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.066933 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.079084 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.087029 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.094463 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.102029 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.109460 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.116334 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.125843 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.135623 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.146170 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.155900 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.163427 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.171193 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.178308 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.185330 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.192660 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.200180 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.207118 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.214139 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.221406 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.228545 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.236918 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.244230 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.251758 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.258943 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.266612 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.274277 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.280875 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.288410 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.296129 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.303334 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.310800 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:48.528949 [2] proc begin: <DistEnv 2/4 nccl>
15:02:48.581064 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:02:48.590871 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:02:50.736208 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.470394 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.494234 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.515416 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.537174 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.556802 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.583144 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.607676 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.625555 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.646823 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.665301 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.686634 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.709039 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.728622 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.748870 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.762157 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.782221 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.798027 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.819634 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.840657 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.858941 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.876205 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.898642 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.915359 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.930173 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.940095 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.951236 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.967288 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:51.985340 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.005956 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.024457 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.045878 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.066003 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.085415 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.105515 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.126159 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.147812 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.168388 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.182850 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.193265 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.204857 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.227145 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.243104 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.255439 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.266101 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.279124 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.290514 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.304843 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.316430 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:52.328064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.047256 [2] proc begin: <DistEnv 2/4 nccl>
21:53:27.052078 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:53:27.065857 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:06:22.424109 [2] proc begin: <DistEnv 2/4 nccl>
22:06:45.400840 [2] proc begin: <DistEnv 2/4 nccl>
22:06:51.921500 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:06:51.938008 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:08:02.004871 [2] proc begin: <DistEnv 2/4 nccl>
22:08:08.102984 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:08:08.120956 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:31.291233 [2] proc begin: <DistEnv 2/4 nccl>
22:09:38.000872 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:09:38.020444 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:09:52.820397 [2] proc begin: <DistEnv 2/4 nccl>
22:09:58.216430 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:09:58.234017 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:10:56.433025 [2] proc begin: <DistEnv 2/4 nccl>
22:11:03.945224 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:11:03.961869 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:18.901577 [2] proc begin: <DistEnv 2/4 nccl>
22:12:24.606065 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:12:24.622892 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:12:59.811473 [2] proc begin: <DistEnv 2/4 nccl>
22:13:05.511246 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
22:13:05.527847 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:32.408815 [2] proc begin: <DistEnv 2/4 nccl>
14:51:32.588932 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:51:32.619708 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:06.278195 [2] proc begin: <DistEnv 2/4 nccl>
14:52:06.367634 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:52:06.384279 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:52:48.876303 [2] proc begin: <DistEnv 2/4 nccl>
14:52:48.930585 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:52:48.940286 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:06.894267 [2] proc begin: <DistEnv 2/4 nccl>
14:53:06.945888 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:53:06.955097 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:56.162450 [2] proc begin: <DistEnv 2/4 nccl>
14:53:56.229327 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:53:56.239815 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:22.037715 [2] proc begin: <DistEnv 2/4 nccl>
14:55:22.073605 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:55:22.082672 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:23.406322 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.254378 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.302955 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.367596 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.392289 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.402974 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.414482 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.423781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.430766 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.438595 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.445560 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.451775 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.459700 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.467251 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.473963 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.481163 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.488478 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.495113 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.502707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.511060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.519053 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.540638 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.643745 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.716142 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.825946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.912858 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.946425 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:24.988468 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.031358 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.047839 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.115146 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.157166 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.179632 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.241425 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.286017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.302692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.372959 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.416303 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.433937 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.500418 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.541232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.559230 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.627649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.669060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.687001 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.753298 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.795700 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.816777 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.825117 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.836707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.846276 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.854711 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.862407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.868542 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.876450 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.886321 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.892310 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.899830 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.912278 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.919494 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.927472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.934817 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.940973 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.952737 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.961853 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.969358 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.981545 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.991890 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:25.997981 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.006317 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.013580 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.019708 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.028148 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.035993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.044726 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.052432 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.059682 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.065823 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.073638 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.161765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.237253 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.430552 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.574324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.659072 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:26.893484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.076703 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.225865 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.393009 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.589247 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.718472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:27.927944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.086395 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.187389 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.391368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.536415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.648789 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.802843 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:28.934499 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.006387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.189066 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.237880 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.259456 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.322464 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.382817 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.406015 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.479791 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.528364 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.550776 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.627155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.652360 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.696824 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.771537 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.819629 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.842251 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.914191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.961310 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:29.985209 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.056299 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.104641 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.127788 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.188243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.249443 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.269754 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.341812 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.390342 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.411552 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.485490 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.532810 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.555047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.630028 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.677095 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.697870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.772101 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.818509 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.841656 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.914987 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.959741 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:30.982302 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.054795 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.104785 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.125722 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.197307 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.245942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.267069 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.341395 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.366216 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.376072 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.394567 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.404655 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.410577 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.418506 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.425230 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.431354 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.438962 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.446829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.453486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.461337 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.469165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.476124 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.483365 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.491212 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.502949 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.513027 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.522355 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.528516 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.536347 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.544110 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.551613 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.559484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.567144 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.573555 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.583803 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.593761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.600180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.609415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.617417 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.623919 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.632632 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.640576 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.646842 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.655160 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.662688 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.668960 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.677513 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.685319 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.694358 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.701836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.709497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.715974 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.726707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.734765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.740706 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.750605 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.760731 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.768009 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.841467 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:31.978528 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.050589 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.154396 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:32.198273 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:15.950784 [2] proc begin: <DistEnv 2/4 nccl>
21:50:41.649265 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
21:50:41.656276 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:50:51.658385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.166653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:53.638466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.402897 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.114845 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:55.586455 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.349578 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.060809 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:57.534201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.300801 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.014818 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:59.490250 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.253116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.966085 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:01.442607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.227703 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.970328 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:03.452073 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.218253 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.931084 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:05.406014 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.170300 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.882816 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:07.358099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.122826 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.834669 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:09.307039 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.072341 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.784285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:11.257751 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.024022 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.736834 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.210339 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:13.975028 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.688024 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.161753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:15.926014 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.639053 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.112029 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:17.921572 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.705800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:19.216498 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.040708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.808031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:21.280168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.047757 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.938525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:23.758927 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:25.032562 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.248918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.963565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:27.836799 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.594001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.088639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:29.911214 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.668611 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.167073 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:31.973443 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.737278 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:33.242993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.039146 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.800402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:35.306929 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.098784 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.854199 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:37.363476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.151606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.909093 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:39.417026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.211757 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.965708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:41.477094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.255452 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.003947 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:43.516998 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.291668 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.038248 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:45.550829 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.325210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.074015 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:47.586658 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.361704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.108212 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:49.619539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.394988 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.144066 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:51.653562 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.430903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.180754 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:53.686117 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.459560 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.199704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:55.696663 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.500294 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.232608 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:57.718049 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.528121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.269464 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:59.753996 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.575203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.329474 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:01.842198 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:02.687967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.454474 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:03.967667 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:04.746893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:05.497232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:06.009111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:06.793608 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:07.547515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.056508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.836296 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:09.585933 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.097323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:10.877329 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:11.625677 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.137874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:12.917380 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:13.667357 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.179970 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:14.956236 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:15.703679 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.216169 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:16.992272 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:17.740759 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:18.254356 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.031119 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:19.780025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:20.294259 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.070050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:21.819986 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:22.331077 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.107030 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:23.855696 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:24.368783 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.145928 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:25.894391 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:26.407329 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.181787 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:27.930377 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:28.444279 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.220887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:29.970897 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:30.483089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:31.257649 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.004195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:32.517195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:33.292317 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.041048 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:34.546670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:35.321636 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.071601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:36.584791 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:37.360587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.108043 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:38.621198 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:39.396367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.145581 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:40.656719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:41.432585 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.184499 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:42.695071 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:43.470635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.217702 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:44.728266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:45.502712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.250899 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:46.764341 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:47.537998 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.284972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:48.798291 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:49.574755 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.322781 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:50.835782 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:51.611813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.362657 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:52.878487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:53.652968 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.402105 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:54.914684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:55.694418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.442810 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:56.947670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:57.723284 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.474255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:58.979569 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:59.757808 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:00.507558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.020087 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:01.795712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:02.565937 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.082337 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.898655 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:04.633889 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.130060 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.936463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.672436 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.168683 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.976007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.711896 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.040342 [2] proc begin: <DistEnv 2/4 nccl>
21:55:59.127176 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:55:59.139126 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:56:00.649566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.412930 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.424058 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.435872 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.443802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.450139 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.458689 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.467287 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.475236 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.484008 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.491557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.497841 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.505775 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.514209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.520487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.528750 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.536467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.543162 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.551882 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.559137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.565290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.573168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.580668 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.587352 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.595276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.602921 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.609353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.617894 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.625215 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.631463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.639811 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.647832 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.654171 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.666294 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.676809 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.685635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.694660 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.703052 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.710418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.720469 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.732314 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.739312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.747434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.761444 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.768061 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.779300 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.789971 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.797887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.807418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.815190 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.822641 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.831096 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.839864 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.846526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.856684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.868241 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.876600 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.885252 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.893744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.900703 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.910713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.918894 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.926460 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.937797 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.946416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.954496 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.964943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.975238 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.983858 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.995982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.007428 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.014893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.026858 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.034746 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.044679 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.053362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.062341 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.069839 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.079258 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.091379 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.098330 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.110661 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.122079 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.128943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.139591 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.147675 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.156047 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.166163 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.175525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.182408 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.192674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.202803 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.210145 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.219808 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.227965 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.236061 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.244416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.252665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.260001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.269485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.277410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.285214 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.294199 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.302622 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.310278 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.320105 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.328811 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.337789 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.346229 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.355261 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.363667 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.371673 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.379777 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.386765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.395610 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.404275 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.412031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.422457 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.435584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.444019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.453917 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.462822 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.469269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.479191 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.486534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.493345 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.502870 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.511446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.518990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.528097 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.537557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.545466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.554690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.562794 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.569762 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.579599 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.588099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.596354 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.606774 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.614849 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.623208 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.632992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.641661 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.652860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.661381 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.669871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.677123 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.685686 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.695102 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.702029 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.711406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.719139 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.726951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.735983 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.747231 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.755165 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.764802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.774068 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.781638 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.791083 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.799290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.806470 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.815441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.823584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.830613 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.842166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.854346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.861702 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.870431 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.880135 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.887540 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.896480 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.906664 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.914181 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.924905 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.934255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.941116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.950726 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.959799 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.967186 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.979894 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.988972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:02.997367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.006658 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.014008 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.020985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.030977 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.043528 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.051397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.061374 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.070203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.078266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.087290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.095637 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.104730 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.115401 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.128351 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.135323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.144133 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.151732 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.565979 [2] proc begin: <DistEnv 2/4 nccl>
21:58:32.451185 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:58:32.469916 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:07.081872 [2] proc begin: <DistEnv 2/4 nccl>
22:49:11.968673 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
22:49:11.981190 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:49:54.822101 [2] proc begin: <DistEnv 2/4 nccl>
22:49:56.286058 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
22:49:56.287011 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:50:02.039023 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.376210 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.410628 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.439710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.471605 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.502711 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.534755 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.571035 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.608193 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.638121 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.666947 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.697424 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.732377 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.765279 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.802132 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.837727 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.872138 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.909178 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.943048 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:02.976811 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.011813 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.045650 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.076753 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.112309 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.143509 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.182178 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.213611 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.248130 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.283077 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.310717 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.346294 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.378442 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.414280 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.448107 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.479511 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.514549 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.546655 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.583162 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.621241 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.651248 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.682036 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.716483 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.747731 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.777898 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.816014 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.850541 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.888266 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.923547 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.952908 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:03.985083 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.017681 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.055697 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.085858 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.118990 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.152583 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.181892 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.213035 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.245830 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.278252 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.308242 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.342993 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.372827 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.405426 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.431062 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.466167 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.496730 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.531031 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.563199 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.596029 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.628533 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.661946 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.693376 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.727289 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.758340 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.788009 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.826108 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.856272 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.889769 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.920839 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.954252 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:04.984756 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.020902 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.053899 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.087053 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.119863 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.149753 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.181868 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.214484 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.248977 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.282073 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.315194 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.348366 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.377269 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.409731 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.443025 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.476745 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.508547 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.535938 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.569683 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.604538 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.634778 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.668268 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.698153 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.729883 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.762342 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.792798 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.826719 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.856601 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.893152 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.925170 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.961425 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:05.996297 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.032641 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.063864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.095682 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.129608 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.163215 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.192831 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.230361 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.262198 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.293058 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.325752 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.355609 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.388271 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.417905 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.449107 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.483763 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.515526 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.544488 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.576922 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.609874 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.640598 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.669168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.704885 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.737584 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.768992 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.805809 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.840596 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.872997 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.910380 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.940569 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:06.968928 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.005710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.039163 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.066864 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.099709 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.132296 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.167124 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.200769 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.237010 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.268937 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.300412 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.329851 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.361944 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.390290 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.420549 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.453808 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.484461 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.515755 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.549691 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.582463 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.615658 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.648294 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.681244 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.712542 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.746844 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.776601 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.806575 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.839662 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.874469 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.909117 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.942188 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:07.979444 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.008189 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.043045 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.073009 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.106533 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.139935 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.172407 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.204653 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.235668 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.266551 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.297401 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.333401 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.369011 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.400671 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.432931 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.469566 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.500330 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.534359 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.565721 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.603800 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.637619 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.671525 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.703516 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.736146 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.766349 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.797765 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.831445 [2] Warning: no training nodes in this partition! Backward fake loss.
22:50:08.862519 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:24.921355 [2] proc begin: <DistEnv 2/4 nccl>
10:38:26.603693 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
10:38:26.604927 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| Active memory         |    3864 KB |    3886 KB |    3937 KB |   74752 B  |
|       from large pool |    3790 KB |    3790 KB |    3790 KB |       0 B  |
|       from small pool |      74 KB |      96 KB |     147 KB |   74752 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22528 KB |   22528 KB |   22528 KB |       0 B  |
|       from large pool |   20480 KB |   20480 KB |   20480 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   18663 KB |   18707 KB |   18808 KB |  147968 B  |
|       from large pool |   16690 KB |   16690 KB |   16690 KB |       0 B  |
|       from small pool |    1973 KB |    2045 KB |    2118 KB |  147968 B  |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      32    |      15    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      31    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:28.162624 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.298146 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.331274 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.368259 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.402104 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.433220 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.464975 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.492866 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.523429 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.558550 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.587699 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.623450 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.653717 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.697486 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.731096 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.760602 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.804389 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.837513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.871894 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.912097 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.942072 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:28.977963 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.023084 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.139278 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.175663 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.211646 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.246111 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.283697 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.320692 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.355919 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.391861 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.426376 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.462604 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.500548 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.537806 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.952977 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:29.985571 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.017311 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.054883 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.089325 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.123015 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.164765 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.196814 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.226148 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.258771 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.297299 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.331561 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.362376 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.392687 [2] Warning: no training nodes in this partition! Backward fake loss.
10:38:30.428423 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:23.790595 [2] proc begin: <DistEnv 2/4 nccl>
10:48:23.896772 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
10:48:23.906355 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:48:25.221752 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.887801 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.899355 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.906356 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.913095 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.919448 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.926918 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.936117 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.944746 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.953354 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.961529 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.969861 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.980295 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.988297 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:25.997373 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.005862 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.013084 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.024676 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.037103 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.050455 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.061724 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.068774 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.079974 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.088113 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.098179 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.105726 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.112303 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.119541 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.127006 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.135749 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.144741 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.154134 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.160929 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.168241 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.175469 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.182632 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.190817 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.197759 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.204824 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.211708 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.219813 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.228571 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.235407 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.242832 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.249783 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.256057 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.263978 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.273813 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.282775 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.290991 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.301195 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.310021 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.317281 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.324345 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.332169 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.342157 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.349033 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.358707 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.366047 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.372627 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.379661 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.390022 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.397534 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.405058 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.414066 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.421203 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.428443 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.435753 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.443033 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.453324 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.459975 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.472184 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.485654 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.496183 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.506156 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.518057 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.529102 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.537073 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.545781 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.552784 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.560082 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.567630 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.575442 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.586857 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.595134 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.602822 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.610104 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.617393 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.624663 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.634274 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.641583 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.649090 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.656538 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.663685 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.675105 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.685023 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.692063 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.701602 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.708490 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.715518 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.723218 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.731417 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.741541 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.748877 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.755219 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.762839 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.778719 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.788017 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.804571 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.811866 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.819258 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.828903 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.846810 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.856115 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.866220 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.873558 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.884959 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.895129 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.906616 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.915833 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.925048 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.931574 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.938461 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.946695 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.955448 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.962907 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.969348 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.976085 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.983019 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:26.990303 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.003788 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.013905 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.023280 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.030773 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.037877 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.044311 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.052134 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.061289 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.071508 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.078028 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.087277 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.099230 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.107766 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.116914 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.125583 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.133460 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.141519 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.149071 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.159513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.168246 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.180073 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.192465 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.208625 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.220868 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.228339 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.235410 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.242483 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.252650 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.260370 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.267534 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.275261 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.282709 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.290366 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.297743 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.304956 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.312492 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.320555 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.328527 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.335712 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.347190 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.357328 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.364526 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.375867 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.385084 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.394200 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.401126 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.410784 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.417805 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.424955 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.433109 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.442320 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.450584 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.457462 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.466747 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.473965 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.481123 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.489252 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.496582 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.506235 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.513187 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.520142 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.528017 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.537420 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.544454 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.551904 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.558899 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.572343 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.584381 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.590939 [2] Warning: no training nodes in this partition! Backward fake loss.
10:48:27.598180 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:08.899885 [2] proc begin: <DistEnv 2/4 nccl>
10:51:09.319394 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
10:51:09.332508 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:51:10.687346 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.412598 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.435358 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.455208 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.473099 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.494391 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.512383 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.536276 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.557633 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.579080 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.600322 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.616893 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.631179 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.641832 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.658467 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.672606 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.691014 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.709321 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.718731 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.729666 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.740438 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.753167 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.763341 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.775617 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.786218 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.802149 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.822832 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.839923 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.860743 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.878221 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.897767 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.913547 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.933337 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.951411 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.962531 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.977548 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:11.991365 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.000779 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.015044 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.026074 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.035819 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.046669 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.057231 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.075892 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.089264 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.099906 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.110010 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.124496 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.142200 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.162279 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.180681 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.207508 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.227324 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.242915 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.258844 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.272175 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.290461 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.308511 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.330240 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.346529 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.366567 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.384429 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.403428 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.419549 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.432515 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.446456 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.462207 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.476836 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.497544 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.514509 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.535024 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.552297 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.573466 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.592491 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.611275 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.626757 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.638424 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.649666 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.659689 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.677441 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.698538 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.715866 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.737937 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.754834 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.768271 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.788835 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.809513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.828086 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.847379 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.864606 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.885186 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.914513 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.932461 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.952587 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.972297 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:12.990916 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.007665 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.020692 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.030562 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.042571 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.053042 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.064254 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.095984 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.112242 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.126698 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.136833 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.163115 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.179814 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.204483 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.218952 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.229406 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.241261 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.251606 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.262599 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.272750 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.285548 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.303336 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.322650 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.342408 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.359432 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.381517 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.402051 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.423333 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.438688 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.458388 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.475744 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.494596 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.512701 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.533776 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.553937 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.572085 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.593818 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.611057 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.630638 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.650546 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.670539 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.688376 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.706853 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.725106 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.746027 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.764590 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.783613 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.798741 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.809379 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.820167 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.832604 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.846224 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.854851 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.864787 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.878214 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.891526 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.912449 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.934430 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.954716 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.972577 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:13.992939 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.011626 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.031516 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.048389 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.070323 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.093520 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.108872 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.118491 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.129023 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.141320 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.162214 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.181135 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.202139 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.228569 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.240349 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.250213 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.259801 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.270540 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.279906 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.289477 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.302778 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.316874 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.327947 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.338698 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.347818 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.357125 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.366214 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.386126 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.404909 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.425902 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.445483 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.465694 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.486002 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.504053 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.525933 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.544427 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.565492 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.574503 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.589821 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.604284 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.616413 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.626785 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.637538 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.647918 [2] Warning: no training nodes in this partition! Backward fake loss.
10:51:14.657930 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:32.938754 [2] proc begin: <DistEnv 2/4 nccl>
10:52:32.966714 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
10:52:32.977447 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:52:34.301679 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.165212 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.194875 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.220973 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.258650 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.286707 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.318491 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.335078 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.363302 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.390745 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.417383 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.445084 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.472316 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.493709 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.513312 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.541250 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.560554 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.584189 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.608814 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.638475 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.666560 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.693907 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.724291 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.750515 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.778712 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.805554 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.833895 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.862051 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.889131 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.917234 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.955544 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:35.981430 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.012308 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.039696 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.066898 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.092544 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.117024 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.145291 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.171540 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.197338 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.223379 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.248790 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.272061 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.297635 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.321562 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.349096 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.375422 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.405861 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.432360 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.458194 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.480482 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.506071 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.529628 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.556478 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.583346 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.613190 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.640560 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.666448 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.691846 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.722169 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.750515 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.776586 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.805319 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.832834 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.861257 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.889830 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.922960 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.940140 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.980908 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:36.999526 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.021786 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.048757 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.073092 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.102561 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.128166 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.149616 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.171377 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.196744 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.218408 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.244781 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.271079 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.297877 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.325849 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.353779 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.378704 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.409566 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.433014 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.461932 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.489716 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.514131 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.541353 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.566820 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.593763 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.617807 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.645621 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.672965 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.693643 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.724202 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.750006 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.776065 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.802040 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.830778 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.858306 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.886340 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.912400 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.942780 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:37.987146 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.011371 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.038856 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.068877 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.097834 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.124838 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.154877 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.178110 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.204484 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.226892 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.252715 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.280512 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.306541 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.333003 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.361593 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.389709 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.416179 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.439431 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.466875 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.492829 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.518036 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.544010 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.569170 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.598889 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.626221 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.654018 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.679799 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.706030 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.734402 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.758676 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.786676 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.812390 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.838902 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.854035 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.872582 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.888922 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.910176 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.943121 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:38.983414 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.002441 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.022242 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.042873 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.068657 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.094623 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.120339 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.148679 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.172132 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.196640 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.222283 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.249144 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.273609 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.301020 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.328835 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.356851 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.385114 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.414841 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.439049 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.469445 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.495403 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.521086 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.548085 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.571180 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.597936 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.625495 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.655519 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.683123 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.711346 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.736688 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.762709 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.788984 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.821955 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.842354 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.864218 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.893619 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.923478 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.947795 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:39.990514 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.009281 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.030743 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.055949 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.083327 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.112696 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.141471 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.169452 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.196769 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.224564 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.245904 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.270071 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.302826 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.320202 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.341051 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.368586 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.392118 [2] Warning: no training nodes in this partition! Backward fake loss.
10:52:40.416975 [2] Warning: no training nodes in this partition! Backward fake loss.
10:53:37.270473 [2] proc begin: <DistEnv 2/4 nccl>
10:53:49.768239 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:53:49.784870 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:55:30.930226 [2] proc begin: <DistEnv 2/4 nccl>
10:55:38.243572 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:55:38.257188 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:56:22.471105 [2] proc begin: <DistEnv 2/4 nccl>
10:56:27.156568 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:56:27.172868 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:16.893608 [2] proc begin: <DistEnv 2/4 nccl>
10:58:33.683461 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:58:34.010471 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:58:40.276381 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.212512 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.490714 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:41.767154 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.043998 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.320003 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.595857 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:42.872177 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.149804 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.427183 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.703221 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:43.979366 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.257177 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.533688 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:44.812360 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.088859 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.364664 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.640612 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:45.916295 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.193151 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.469636 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:46.746733 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.023550 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.300916 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.579132 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:47.855700 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.132935 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.410070 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.687660 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:48.965660 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.241852 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.519902 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:49.798285 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.078637 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.357382 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.635178 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:50.912034 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.188547 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.465208 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:51.742234 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.017950 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.294635 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.570508 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:52.846479 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.122547 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.399522 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.675304 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:53.951540 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.228568 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.505451 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:54.782219 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.059130 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.335821 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.612783 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:55.889636 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.166871 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.443976 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.720364 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:56.997847 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.273728 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.550481 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:57.826186 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.103777 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.380953 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.658459 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:58.935274 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.211215 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.488726 [2] Warning: no training nodes in this partition! Backward fake loss.
10:58:59.764956 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.040976 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.317965 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.595811 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:00.872286 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.148892 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.424741 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.700748 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:01.978548 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.266493 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.555561 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:02.843520 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.131611 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.421957 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.704160 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:03.981276 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.257838 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.534542 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:04.811172 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.088363 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.366120 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.643710 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:05.920644 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.197731 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.474649 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:06.751013 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.028904 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.306356 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.582739 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:07.859658 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.136667 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.413466 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.690483 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:08.968223 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.244509 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.521595 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:09.798713 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.076162 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.352855 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.629238 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:10.906701 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.182875 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.459633 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:11.736084 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.012896 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.290187 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.566527 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:12.842825 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.120227 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.397343 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.673676 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:13.950798 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.227099 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.504108 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:14.781923 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.059363 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.335639 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.612366 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:15.889101 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.165461 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.442034 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.718357 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:16.995078 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.271432 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.549047 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:17.825057 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.101295 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.377774 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.653836 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:18.929898 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.206312 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.482916 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:19.759770 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.035670 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.313201 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.589963 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:20.866998 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.142775 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.418265 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.694773 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:21.971196 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.246972 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.524210 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:22.801274 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.076486 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.353385 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.629265 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:23.905330 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.181968 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.458070 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:24.734037 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.010130 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.286437 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.562341 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:25.838735 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.115477 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.392293 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.668114 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:26.944249 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.220549 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.497074 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:27.773694 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.050575 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.328461 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.605968 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:28.882545 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.159156 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.436728 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.713527 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:29.989049 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.265159 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.542333 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:30.818869 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.095794 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.371732 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.649101 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:31.926233 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.202521 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.478367 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:32.755560 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.032243 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.308560 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.585428 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:33.862512 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.138407 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.415120 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.692763 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:34.968714 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.243777 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.519792 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:35.796112 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:36.072774 [2] Warning: no training nodes in this partition! Backward fake loss.
10:59:57.133916 [2] proc begin: <DistEnv 2/4 nccl>
11:00:02.760841 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:00:02.770159 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:00:07.349966 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.501864 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:08.982876 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.461702 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:09.942947 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.423688 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:10.902976 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.382342 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:11.861981 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.340402 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:12.818882 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.298929 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:13.778552 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.257344 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:14.736792 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.217099 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:15.696577 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.176438 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:16.655147 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.134873 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:17.614305 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.093279 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:18.573556 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.053321 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:19.532543 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.011563 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.497624 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:20.977093 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.455656 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:21.934918 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.415164 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:22.893539 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.373504 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:23.853889 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.332269 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:24.811593 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.292303 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:25.771773 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.250472 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:26.729681 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.209266 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:27.687414 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.167294 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:28.648857 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.129165 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:29.608370 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.087231 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:30.567576 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.046896 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:31.525022 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.003567 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.485061 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:32.963149 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.442444 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:33.922394 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.401437 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:34.880881 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.359566 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:35.839561 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.317438 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:36.797103 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.276838 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:37.755838 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.234804 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:38.714045 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.193485 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:39.673365 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.152133 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:40.632137 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.110826 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:41.589077 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.068078 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:42.547001 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.025404 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.503939 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:43.982049 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.460043 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:44.937715 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.415774 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:45.894487 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.373312 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:46.851831 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.329596 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:47.807776 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.285957 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:48.764131 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.242952 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:49.720798 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.198917 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:50.686077 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.166156 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:51.644793 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.123811 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:52.603843 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.083350 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:53.561761 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.041455 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:54.520848 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.000493 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.480961 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:55.963176 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.442396 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:56.922129 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.401377 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:57.880892 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.360368 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:58.841043 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.319855 [2] Warning: no training nodes in this partition! Backward fake loss.
11:00:59.799664 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.278961 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:00.757917 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.237349 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:01.728136 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.229595 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:02.728704 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.219437 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:03.699574 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.181669 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:04.660897 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.141930 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:05.621521 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.101319 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:06.580602 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.059704 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:07.538886 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.018133 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.496893 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:08.975162 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.454521 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:09.934249 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.415005 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:10.895502 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.376312 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:11.856170 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.336869 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:12.817311 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.295921 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:13.775019 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.254365 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:14.733733 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.213257 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:15.692122 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.172365 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:16.652424 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.130897 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:17.609755 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.089631 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:18.568298 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.047711 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:19.528020 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.007145 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.486939 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:20.967009 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.447515 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:21.926811 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.405653 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:22.884899 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.363306 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:23.842326 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.320698 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:24.800138 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.279575 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:25.757977 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.237119 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:26.716632 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.196596 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:27.676317 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.155699 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:28.636294 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.115349 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:29.594241 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.072855 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:30.551701 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.030454 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.509564 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:31.988096 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.466630 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:32.945186 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.424856 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:33.904538 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.383023 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:34.862404 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.340474 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:35.819389 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.298230 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:36.776811 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.255723 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:37.734008 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.213258 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:38.692521 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.171524 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:39.650064 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.130574 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:40.609284 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.087337 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:41.566585 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.045101 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:42.523397 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.002530 [2] Warning: no training nodes in this partition! Backward fake loss.
11:01:43.482137 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:00.460825 [2] proc begin: <DistEnv 2/4 nccl>
11:02:06.067167 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
11:02:06.075130 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:02:11.306118 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:12.988648 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:13.866156 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:14.744584 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:15.625322 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:16.504582 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:17.383157 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:18.262396 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:19.141616 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.019811 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:20.903322 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:21.779552 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:22.658752 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:23.537074 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:24.414319 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:25.291573 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:26.171380 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.048781 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:27.927305 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:28.806100 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:29.683482 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:30.560733 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:31.440759 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:32.318289 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:33.196549 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.075245 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:34.953959 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:35.832962 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:36.711310 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:37.590736 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:38.469429 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:39.348121 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:40.226647 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.107541 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:41.990023 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:42.870776 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:43.750199 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:44.628744 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:45.506433 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:46.385098 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:47.263973 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:48.142697 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.021306 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:49.899435 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:50.780238 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:51.658502 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:52.537736 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:53.415895 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:54.293585 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:55.171699 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.050737 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:56.929604 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:57.807649 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:58.685998 [2] Warning: no training nodes in this partition! Backward fake loss.
11:02:59.566075 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:00.443877 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:01.322415 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:02.206571 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:03.128623 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.029622 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:04.911792 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:05.791950 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:06.672809 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:07.553607 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:08.432727 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:09.312790 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:10.191599 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.071127 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:11.950519 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:12.830265 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:13.709199 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:14.588195 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:15.467845 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:16.347240 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:17.226716 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.105840 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:18.985836 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:19.864686 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:20.744902 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:21.625515 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:22.505895 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:23.384479 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:24.263373 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:25.142831 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.022097 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:26.900857 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:27.779816 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:28.659762 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:29.539102 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:30.417486 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:31.297212 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:32.176872 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.055445 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:33.934339 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:34.813691 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:35.692741 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:36.571173 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:37.451416 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:38.330707 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:39.210240 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.089559 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:40.969832 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:41.849404 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:42.727249 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:43.605743 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:44.483473 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:45.362709 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:46.240987 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.119553 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:47.997185 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:48.876003 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:49.753707 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:50.631944 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:51.510922 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:52.389392 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:53.265796 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:54.142920 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.020862 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:55.899426 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:56.777027 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:57.655467 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:58.532714 [2] Warning: no training nodes in this partition! Backward fake loss.
11:03:59.411486 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:00.290569 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:01.171083 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.069472 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:02.989661 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:03.879079 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:04.759161 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:05.638914 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:06.517797 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:07.398127 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:08.276625 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:09.155401 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.035064 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:10.915584 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:11.795293 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:12.675440 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:13.553870 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:14.433782 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:15.312038 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:16.193608 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.071953 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:17.952078 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:18.831373 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:19.711788 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:20.591447 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:21.472915 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:22.352450 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:23.230366 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.108401 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:24.986454 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:25.866647 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:26.748022 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:27.630528 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:28.515118 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:29.401815 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:30.280811 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:31.162052 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.040422 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:32.919171 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:33.797325 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:34.677485 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:35.556217 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:36.435569 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:37.314418 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:38.192420 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.070058 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:39.954241 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:40.837658 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:41.719745 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:42.600802 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:43.485173 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:44.369632 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:45.254540 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:46.142353 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.036031 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:47.929396 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:48.822746 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:49.712834 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:50.604701 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:51.496247 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:52.387319 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:53.274160 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:54.164507 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.050696 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:55.937353 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:56.823447 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:57.709840 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:58.598396 [2] Warning: no training nodes in this partition! Backward fake loss.
11:04:59.482061 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:00.365771 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:01.245552 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:02.168900 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.080392 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:03.962582 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:04.845200 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:05.728251 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:06.609167 [2] Warning: no training nodes in this partition! Backward fake loss.
11:05:07.492425 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:38.332785 [2] proc begin: <DistEnv 2/4 nccl>
14:44:38.567582 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:44:38.577201 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:44:40.180426 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.357402 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.368872 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.378694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.388198 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.399943 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.411142 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.422172 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.435660 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.447071 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.457876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.468679 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.478755 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.488984 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.498575 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.509685 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.520299 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.529058 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.541236 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.552449 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.562370 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.572275 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.582907 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.590688 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.602943 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.613428 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.622915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.632910 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.643148 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.650888 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.660821 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.671695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.679543 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.689385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.699488 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.707365 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.717772 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.728249 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.736106 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.746927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.755880 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.763961 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.773761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.783030 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.791105 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.801747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.811029 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.819077 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.829081 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.840625 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.850253 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.861156 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.871165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.879848 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.900433 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.914408 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.924578 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.934387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.944804 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.954818 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.964827 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.977859 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.985313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:41.998869 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.008977 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.016539 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.027966 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.037634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.046463 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.056008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.066542 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.075751 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.084238 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.093240 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.102568 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.111978 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.121517 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.129766 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.139438 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.149828 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.158729 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.170303 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.180047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.187694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.197852 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.206456 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.213984 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.239084 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.249782 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.256512 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.266712 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.275635 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.282697 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.294295 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.303577 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.316021 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.328346 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.338071 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.356551 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.365915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.374747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.383189 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.392866 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.409819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.417874 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.428368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.436563 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.444187 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.454151 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.462477 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.470063 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.486875 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.495562 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.506634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.517486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.527654 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.537309 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.549166 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.559779 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.569257 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.581280 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.592349 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.600956 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.611823 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.621986 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.630064 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.640850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.650163 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.658388 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.670540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.680577 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.689942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.703610 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.716287 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.725407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.736735 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.747619 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.757415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.767993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.777619 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.786817 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.797835 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.805310 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.811627 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.820274 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.827517 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.834420 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.847463 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.856004 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.864669 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.878002 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.889296 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.900947 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.917929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.929517 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.938876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.952866 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.964519 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.974135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.996400 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.014588 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.029095 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.039539 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.052013 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.059905 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.073664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.082497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.091179 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.100470 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.110438 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.118025 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.126879 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.135829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.143607 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.152251 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.161415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.169401 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.180017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.188569 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.197065 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.205512 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.214329 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.222502 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.231343 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.242694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.251065 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.260210 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.269376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.276970 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.290359 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.300828 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.317133 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.328833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.339075 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.354837 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.369394 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.380017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.388212 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.399171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.412904 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.746751 [2] proc begin: <DistEnv 2/4 nccl>
14:45:13.812351 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:45:13.823420 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:15.232225 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.828872 [2] proc begin: <DistEnv 2/4 nccl>
14:45:46.910075 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:45:46.922594 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:49.063715 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.881787 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.893258 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.904511 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.914572 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.924306 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.935570 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.945866 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.955582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.965664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.976371 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.985639 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.995413 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.005523 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.014379 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.025091 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.034732 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.043615 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.054526 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.064165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.073999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.084031 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.094029 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.103499 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.113605 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.124379 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.132968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.143487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.154273 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.163141 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.173923 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.183869 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.196519 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.208046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.218531 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.228021 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.237732 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.247517 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.257538 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.267971 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.279048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.288020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.300602 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.310838 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.321219 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.333232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.345698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.355623 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.365646 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.374842 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.382953 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.395997 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.407440 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.417520 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.431293 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.442704 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.452411 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.466282 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.477479 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.489500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.500456 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.512698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.520845 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.541771 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.558707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.569182 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.578937 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.589549 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.598294 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.608207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.618947 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.628005 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.638856 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.652764 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.670063 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.677929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.685324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.692013 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.702676 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.709998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.720026 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.730143 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.745779 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.755759 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.766967 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.780100 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.791023 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.813227 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.823997 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.836501 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.857490 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.869960 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.880007 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.888910 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.898259 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.906977 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.916993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.926173 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.935229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.946975 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.955946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.964293 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.974169 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.986545 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.996941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.006484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.016133 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.024906 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.035491 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.045528 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.053670 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.063830 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.083658 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.093416 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.102846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.112263 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.120999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.131603 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.140791 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.149590 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.160135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.170335 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.178560 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.189555 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.200832 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.210039 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.220651 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.230250 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.238897 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.250294 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.259606 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.267636 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.278204 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.287410 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.295682 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.305393 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.314653 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.323530 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.333071 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.342468 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.351342 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.361014 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.370152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.378587 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.389170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.399692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.407622 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.417630 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.427363 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.435381 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.445191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.456194 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.464421 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.475426 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.486202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.494444 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.506283 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.516276 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.524601 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.535749 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.545580 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.554070 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.566349 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.576373 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.585387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.595761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.605802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.620699 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.637894 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.647863 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.661576 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.673348 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.683135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.690954 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.700473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.710497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.718709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.728047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.738313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.746396 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.756748 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.765849 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.774044 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.783962 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.792992 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.800903 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.816680 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.825957 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.835752 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.845241 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.859691 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.867750 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.877981 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.888251 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.896503 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.906949 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.915802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.923753 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.933665 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.942099 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:40.653887 [2] proc begin: <DistEnv 2/4 nccl>
14:51:40.769466 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:51:40.782507 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:42.260599 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.294671 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.307102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.318917 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.327774 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.336586 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.348311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.358781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.371681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.397668 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.414861 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.423360 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.433871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.444968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.454069 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.465043 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.475615 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.483481 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.494219 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.503644 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.513640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.525001 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.535202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.542712 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.551998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.560793 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.568827 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.581943 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.592920 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.603005 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.614588 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.625135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.637913 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.649904 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.659713 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.668010 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.677265 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.688017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.696810 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.708475 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.721697 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.730316 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.741538 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.751075 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.759317 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.770256 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.779472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.787518 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.798809 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.807685 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.815180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.825379 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.834136 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.841880 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.852763 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.863774 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.870646 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.882121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.891336 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.898945 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.909914 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.918935 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.926537 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.937573 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.946469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.954088 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.971120 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.983387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:43.992881 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.001758 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.010548 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.018890 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.027965 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.038428 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.047078 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.057042 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.066587 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.074498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.087485 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.098865 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.108408 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.118844 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.129189 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.135570 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.145060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.154572 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.161696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.175473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.185284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.192313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.205640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.220721 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.228990 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.240170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.251949 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.261131 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.271941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.282078 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.291158 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.301059 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.310750 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.318997 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.330411 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.340252 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.349565 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.363655 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.374035 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.383113 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.406514 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.418641 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.433081 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.444250 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.457375 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.464901 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.474776 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.484899 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.492587 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.503352 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.514275 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.521994 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.534423 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.543735 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.552684 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.564268 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.574064 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.581781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.592640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.616076 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.624928 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.635850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.647237 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.660863 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.673217 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.683299 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.693826 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.702995 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.713078 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.722046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.732575 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.742576 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.751204 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.763220 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.774313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.782024 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.792503 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.802836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.811617 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.822747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.833769 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.841534 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.855673 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.867032 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.874846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.885694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.894744 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.902870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.913186 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.922114 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.930195 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.941080 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.950226 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.958668 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.969831 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.980497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.989000 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:44.999938 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.007239 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.014088 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.021870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.028753 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.036346 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.051662 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.068480 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.079374 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.090552 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.100638 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.109248 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.122933 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.133450 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.142888 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.154944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.165582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.174587 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.186760 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.198429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.207846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.219917 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.230486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.239664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.250695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.260829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.270747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.282066 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.292130 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.301231 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.311778 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.323641 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.334636 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.346252 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:45.356050 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:00.210981 [2] proc begin: <DistEnv 2/4 nccl>
14:55:00.284134 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:55:00.302120 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:01.906351 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.947102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.959726 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.971186 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.982547 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:02.992822 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.003674 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.021726 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.036291 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.050344 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.063506 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.076448 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.087881 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.099157 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.111885 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.120839 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.131989 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.147719 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.156015 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.164287 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.178701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.188040 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.200467 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.213397 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.222017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.239008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.248229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.259728 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.274835 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.290104 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.300763 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.309427 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.323122 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.332861 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.347859 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.356554 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.374894 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.385087 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.397211 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.409618 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.421384 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.434184 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.442367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.450871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.462336 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.471739 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.480348 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.490023 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.498427 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.507025 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.517675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.525803 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.540105 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.548206 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.556865 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.564616 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.575802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.583779 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.592646 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.602701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.615945 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.625232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.638667 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.647566 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.661341 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.669919 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.682539 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.691814 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.703840 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.713407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.726853 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.735372 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.747714 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.756804 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.770873 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.779387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.793146 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.801813 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.815311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.823302 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.835883 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.845425 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.857693 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.866017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.878778 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.889353 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.907019 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.919882 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.930724 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.940149 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.952714 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.961698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.978856 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:03.987369 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.001046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.008894 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.021536 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.031474 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.043934 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.052463 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.074715 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.089002 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.106401 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.115334 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.128469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.136977 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.149690 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.159357 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.173465 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.181042 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.194283 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.203631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.215805 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.224682 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.243336 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.254910 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.270731 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.280527 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.293612 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.302446 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.314852 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.323492 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.337563 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.347853 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.364892 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.375368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.387815 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.397721 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.409694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.420472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.433368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.442500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.456145 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.464847 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.477225 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.485819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.499343 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.508964 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.522249 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.531944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.543154 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.552193 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.566819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.574494 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.587459 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.597176 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.611682 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.623178 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.650766 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.659303 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.668000 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.676172 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.690236 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.706607 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.719663 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.731021 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.743864 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.756361 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.773729 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.786106 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.803782 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.815804 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.827147 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.835770 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.847095 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.856658 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.865946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.873558 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.886759 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.893922 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.904404 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.912921 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.925934 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.936726 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.948220 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.956207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.968152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.977377 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.987688 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:04.995419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.005696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.013171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.023314 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.030704 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.044284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.054157 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.064029 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.072798 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.083681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.093690 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.105581 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.115916 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.126229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.136236 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.146219 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.155435 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.165929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.175459 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.185931 [2] Warning: no training nodes in this partition! Backward fake loss.
14:55:05.195184 [2] Warning: no training nodes in this partition! Backward fake loss.
14:58:29.565879 [2] proc begin: <DistEnv 2/4 nccl>
14:58:43.243337 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:58:43.259381 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:07.754250 [2] proc begin: <DistEnv 2/4 nccl>
15:00:07.830525 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:00:07.840468 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:00:09.243060 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:09.992476 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.008318 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.020635 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.028453 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.035908 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.043311 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.055148 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.070908 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.079694 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.087767 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.094890 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.103673 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.110667 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.119893 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.127796 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.135295 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.143200 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.151110 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.158581 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.165620 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.172522 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.180308 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.189085 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.197306 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.205266 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.212995 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.220434 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.227887 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.235532 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.243085 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.250894 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.258284 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.265932 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.273711 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.281443 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.289098 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.299798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.308576 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.316613 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.326025 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.335035 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.345975 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.354746 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.362138 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.369982 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.377209 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.384446 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.392399 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.399975 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.407387 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.414292 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.421994 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.427903 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.436530 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.443186 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.451311 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.457736 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.468325 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.474980 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.482963 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.489310 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.498103 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.504514 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.511971 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.522356 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.530890 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.537400 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.545801 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.551588 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.560330 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.567254 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.576443 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.583091 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.593360 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.601462 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.610897 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.621108 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.630887 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.638538 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.645961 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.651838 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.660403 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.668961 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.678058 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.684396 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.693693 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.701807 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.709325 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.715412 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.724183 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.730914 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.739324 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.745924 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.758360 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.767748 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.774860 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.780710 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.788222 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.795025 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.802908 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.809026 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.816631 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.823208 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.830997 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.836845 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.845238 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.851952 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.859902 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.866788 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.874847 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.881405 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.899592 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.906270 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.914997 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.923997 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.937984 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.944450 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.952473 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.959166 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.967212 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.974058 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.982106 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.988065 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.000907 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.011429 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.019673 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.026064 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.034234 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.040069 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.051474 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.059049 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.070823 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.078609 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.087208 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.093367 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.104567 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.114936 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.122798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.132119 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.144810 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.156795 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.167175 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.176237 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.183727 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.190308 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.201365 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.209953 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.219029 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.225905 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.233525 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.239940 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.248103 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.254898 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.269416 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.276339 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.284784 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.290666 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.302649 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.309550 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.318152 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.323938 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.332063 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.338147 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.346145 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.352122 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.360136 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.366493 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.376146 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.382175 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.391767 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.400919 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.410845 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.417214 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.426642 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.432643 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.442213 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.449675 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.458730 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.465761 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.475111 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.480879 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.490383 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.496982 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.505627 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.512156 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.526677 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.537134 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.548955 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.555105 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.564564 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.570762 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.579908 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.586554 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.596075 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.602733 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.612341 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.618742 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.628380 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.634772 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:18.513070 [2] proc begin: <DistEnv 2/4 nccl>
15:18:18.566463 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:18:18.576753 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:18:19.805620 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.463214 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.475002 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.483147 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.491254 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.498789 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.509416 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.519596 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.528131 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.538110 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.545790 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.554617 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.562079 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.570456 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.577569 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.585031 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.592657 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.600464 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.608092 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.615605 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.623094 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.630210 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.638365 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.645918 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.653306 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.660465 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.669059 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.678601 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.686977 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.694968 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.702546 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.713057 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.722929 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.731404 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.743232 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.750826 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.758434 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.767861 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.775716 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.782985 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.790070 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.798411 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.805644 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.813100 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.820935 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.828550 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.835867 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.843114 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.850747 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.858639 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.866571 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.872810 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.880443 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.886907 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.894960 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.901165 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.920777 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.929593 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.938147 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.944957 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.952511 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.958634 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.967573 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.974018 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.981832 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.987487 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:20.995902 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.002161 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.010452 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.016692 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.026600 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.033198 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.041300 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.047844 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.056217 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.063077 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.070839 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.077376 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.085388 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.091903 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.100372 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.106985 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.114740 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.120660 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.128797 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.135715 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.145609 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.153296 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.162872 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.169183 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.179018 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.184917 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.197453 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.203961 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.212968 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.219627 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.230549 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.237655 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.245321 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.251407 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.259448 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.266108 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.279388 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.286269 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.293453 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.299838 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.307783 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.314114 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.322652 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.328557 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.336303 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.342621 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.355072 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.363718 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.376385 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.384415 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.393370 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.399693 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.410729 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.427386 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.442265 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.448398 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.460805 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.471061 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.480171 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.486588 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.494140 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.500821 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.518753 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.528293 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.544665 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.552077 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.561115 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.567144 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.575561 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.582001 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.590656 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.598463 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.606586 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.612812 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.620418 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.627117 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.635931 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.642362 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.650454 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.656631 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.665322 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.671841 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.679710 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.685665 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.693713 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.699917 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.707627 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.714243 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.722171 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.728514 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.742202 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.755868 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.764066 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.770512 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.778500 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.785097 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.792826 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.801985 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.810257 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.816507 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.824270 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.830480 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.842078 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.850798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.858554 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.867513 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.878248 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.885290 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.895295 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.901814 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.909488 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.916027 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.923888 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.930636 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.939125 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.945306 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.965862 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.977484 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.989182 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:21.997829 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.008830 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.017315 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.025638 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.031920 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.039761 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.046566 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.054656 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.060793 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.069581 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.076376 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.084018 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.090623 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.100689 [2] Warning: no training nodes in this partition! Backward fake loss.
15:18:22.107261 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:57.723894 [2] proc begin: <DistEnv 2/4 nccl>
08:58:57.790549 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
08:58:57.798478 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:58:59.117791 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.818272 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.830389 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.837981 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.845856 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.853936 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.860912 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.868915 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.877232 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.885370 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.896878 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.908719 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.917681 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.924790 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.932320 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.939230 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.946899 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.954267 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.962775 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.969684 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.977151 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.984659 [2] Warning: no training nodes in this partition! Backward fake loss.
08:58:59.994320 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.005625 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.014439 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.026749 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.036438 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.043774 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.055321 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.065252 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.075017 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.082121 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.089651 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.096258 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.104395 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.111268 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.118557 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.125887 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.132772 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.140755 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.148358 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.155712 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.164645 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.174622 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.182430 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.192477 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.200856 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.208667 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.216740 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.224812 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.232832 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.241272 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.249395 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.259037 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.267738 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.276617 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.284446 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.292781 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.303734 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.314382 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.324722 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.334651 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.346746 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.356648 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.363771 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.371388 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.378887 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.386255 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.393444 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.401099 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.408148 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.415249 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.433993 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.446672 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.455227 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.462778 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.470537 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.477414 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.484750 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.492406 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.499902 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.507375 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.517270 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.525115 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.533080 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.540832 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.547860 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.555083 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.564200 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.571652 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.579261 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.586579 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.593950 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.611787 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.622764 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.630430 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.637131 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.643801 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.651520 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.658750 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.666728 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.673486 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.680897 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.688532 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.696096 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.702927 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.714575 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.737760 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.749405 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.759444 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.776118 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.787381 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.796289 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.807520 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.819085 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.826056 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.833459 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.840641 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.848163 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.858593 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.872039 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.881307 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.890668 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.898775 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.907165 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.915417 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.923287 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.932070 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.940240 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.948932 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.957286 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.965304 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.973362 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.981637 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.990106 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:00.998475 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.006389 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.014250 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.021837 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.030467 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.038448 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.046793 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.054512 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.062414 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.070326 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.078118 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.086445 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.094646 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.102661 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.110323 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.118051 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.126697 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.134412 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.142629 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.152818 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.164511 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.174900 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.185915 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.193712 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.202176 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.209845 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.218818 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.226528 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.233975 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.241014 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.250631 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.258199 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.265309 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.272233 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.279922 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.287177 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.298081 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.309179 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.316589 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.323002 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.329401 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.338648 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.345747 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.353554 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.360276 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.367938 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.376171 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.383628 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.391050 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.398642 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.406207 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.413090 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.422702 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.432843 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.441880 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.456794 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.467463 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.474697 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.482485 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.490344 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.501272 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.508516 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.515754 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.523046 [2] Warning: no training nodes in this partition! Backward fake loss.
08:59:01.532684 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:05.555654 [2] proc begin: <DistEnv 2/4 nccl>
09:00:05.593684 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
09:00:05.606328 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:00:07.188685 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.893250 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.915355 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.938734 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.962107 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:07.982472 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.002305 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.026423 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.042947 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.058391 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.077912 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.097825 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.123288 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.146596 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.165705 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.187680 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.210067 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.231004 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.254022 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.272586 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.294611 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.312588 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.328348 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.350503 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.369614 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.390684 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.414156 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.434729 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.457810 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.477403 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.503730 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.520434 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.542970 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.571014 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.594339 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.618562 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.643611 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.666605 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.687001 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.710147 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.731692 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.754915 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.774810 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.796847 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.818022 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.838664 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.862462 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.883704 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.904386 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.924094 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.944126 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.958615 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.969926 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:08.982058 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.002378 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.026636 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.045656 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.067095 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.086296 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.106205 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.129067 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.149104 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.170965 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.194324 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.215339 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.234817 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.255179 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.273281 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.288236 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.298673 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.311186 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.324364 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.335830 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.346584 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.357320 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.372620 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.387034 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.398577 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.414390 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.435343 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.458080 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.478999 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.501273 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.521691 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.541264 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.566150 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.585631 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.606436 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.633622 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.653415 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.666626 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.680000 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.691595 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.702942 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.714199 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.725011 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.736318 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.747804 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.759341 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.770111 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.781221 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.794163 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.818242 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.841680 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.860081 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.882751 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.898289 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.913815 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.934810 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.955483 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.972925 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:09.994493 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.014583 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.037122 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.060618 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.081060 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.096692 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.116384 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.133184 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.156080 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.178706 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.198841 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.219121 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.236260 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.258627 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.276953 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.298219 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.322363 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.342997 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.361952 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.379940 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.402135 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.422634 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.446639 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.467822 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.489959 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.509662 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.530680 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.552998 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.586292 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.604956 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.638608 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.655041 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.681166 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.702600 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.726289 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.749726 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.770865 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.794048 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.815190 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.834267 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.850811 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.873702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.892849 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.914793 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.937786 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.959651 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:10.981887 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.001132 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.022510 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.041860 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.062762 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.086083 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.107184 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.129700 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.149775 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.174902 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.198058 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.217786 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.238752 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.258474 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.276381 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.298366 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.316915 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.337012 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.353579 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.368811 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.390931 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.414375 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.435311 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.454195 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.472944 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.494242 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.515329 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.538251 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.559717 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.591232 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.614351 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.645998 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.665822 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.686930 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.706737 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.724716 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.747397 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.770206 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.789212 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.810442 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.834268 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.855006 [2] Warning: no training nodes in this partition! Backward fake loss.
09:00:11.875154 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:09.005341 [2] proc begin: <DistEnv 2/4 nccl>
09:01:09.038637 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
09:01:09.050846 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:01:11.481903 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.218349 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.248580 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.279464 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.309780 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.344498 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.374053 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.405381 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.438279 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.469478 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.504127 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.542775 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.571193 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.599581 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.627986 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.661414 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.692525 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.718946 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.745505 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.777634 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.808714 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.832658 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.862162 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.894438 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.926454 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.958776 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:12.987067 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.015777 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.041319 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.070882 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.096789 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.125666 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.154579 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.181433 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.209322 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.244810 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.271801 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.298999 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.328625 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.354825 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.380807 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.405470 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.434417 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.461966 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.490677 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.517405 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.546587 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.582859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.611307 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.642834 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.674596 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.704726 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.735212 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.763725 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.792926 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.827967 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.856784 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.890080 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.922818 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.955496 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:13.985183 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.025506 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.062221 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.094496 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.124902 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.150296 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.181186 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.211389 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.241970 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.270599 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.298477 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.328742 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.357169 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.388245 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.420869 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.450205 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.477303 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.505317 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.535698 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.565133 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.593114 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.622435 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.651517 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.677922 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.708173 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.733983 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.764260 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.791634 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.820815 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.852225 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.881486 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.915001 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.944871 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:14.979147 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.004975 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.035382 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.069273 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.099470 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.133432 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.161596 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.189856 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.218371 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.248242 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.274423 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.299759 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.329398 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.358222 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.388359 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.417741 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.448845 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.482463 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.512555 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.541343 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.572744 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.601669 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.630465 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.657854 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.687168 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.714390 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.744022 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.773662 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.802064 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.833510 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.869910 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.901426 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.936277 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:15.970921 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.000128 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.027603 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.078394 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.105145 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.133737 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.161654 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.189965 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.218648 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.247278 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.274481 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.299405 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.326014 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.351821 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.381300 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.411540 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.439122 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.468982 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.496943 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.527506 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.557185 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.586220 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.617111 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.647406 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.679594 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.705792 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.733218 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.763241 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.789640 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.817412 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.845905 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.873681 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.903859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.933307 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.956238 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:16.990626 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.021127 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.056489 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.095914 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.125889 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.154578 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.182852 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.203228 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.233224 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.262206 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.292157 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.321077 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.350150 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.378038 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.406997 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.434852 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.463073 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.492499 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.522219 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.554372 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.583804 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.610142 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.639060 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.666288 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.694006 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.724213 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.753889 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.783602 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.813512 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.836276 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.861165 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.886010 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.917222 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.950557 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:17.994291 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.030544 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.067074 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.107138 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:18.135250 [2] Warning: no training nodes in this partition! Backward fake loss.
09:01:42.440768 [2] proc begin: <DistEnv 2/4 nccl>
09:01:47.551077 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:01:47.573176 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:03:00.592937 [2] proc begin: <DistEnv 2/4 nccl>
09:03:06.956582 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:03:06.973698 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:04:05.271210 [2] proc begin: <DistEnv 2/4 nccl>
09:04:11.285835 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:04:11.303058 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:07.882677 [2] proc begin: <DistEnv 2/4 nccl>
09:05:22.184412 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:05:22.192450 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:05:34.318057 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.279039 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.559714 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:35.839931 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.119133 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.399468 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.680324 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:36.960399 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.240830 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.521508 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:37.801697 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.081542 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.363604 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.643456 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:38.923489 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.209170 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.489741 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:39.769674 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.050539 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.331354 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.613597 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:40.896236 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.176834 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.457636 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:41.737957 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.018140 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.298647 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.579167 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:42.859528 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.140605 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.420948 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.703144 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:43.983744 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.265329 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.546498 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:44.828686 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.108171 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.388509 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.668606 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:45.949049 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.229272 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.511199 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:46.792117 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.072348 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.353318 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.633664 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:47.913859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.194193 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.477062 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:48.756914 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.037696 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.320117 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.600079 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:49.880877 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.161280 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.441796 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:50.721984 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.001689 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.281017 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.562438 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:51.842669 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.124200 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.404430 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.685409 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:52.966608 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.246924 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.527699 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:53.807980 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.089178 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.368951 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.649369 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:54.929177 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.209595 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.491713 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:55.773034 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.054442 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.334631 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.614724 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:56.895766 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.175555 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.455542 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:57.735519 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.015777 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.295338 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.575582 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:58.855758 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.136329 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.417292 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.696791 [2] Warning: no training nodes in this partition! Backward fake loss.
09:05:59.977645 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.258932 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.540313 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:00.820410 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.100531 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.382098 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.662027 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:01.945779 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.237096 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.526645 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:02.817793 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.109414 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.399568 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.680389 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:03.961445 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.241998 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.523610 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:04.804221 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.084905 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.365644 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.646987 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:05.927670 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.209695 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.492104 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:06.773750 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.054005 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.335630 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.616209 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:07.897108 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.177532 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.457631 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:08.737935 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.017717 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.298888 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.580247 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:09.861268 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.141776 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.421752 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.701649 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:10.981890 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.263118 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.545122 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:11.825433 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.105573 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.385924 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.666244 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:12.946753 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.227587 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.507746 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:13.787642 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.067420 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.347875 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.628516 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:14.909058 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.189445 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.470226 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:15.750283 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.030663 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.311728 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.591526 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:16.872145 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.153008 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.433329 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.712855 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:17.993601 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.274898 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.555450 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:18.837503 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.118048 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.399104 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.679888 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:19.961224 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.241677 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.522361 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:20.802773 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.084417 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.364948 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.645863 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:21.926121 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.206762 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.487026 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:22.767575 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.048688 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.329082 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.610625 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:23.891776 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.171785 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.452254 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:24.732817 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.014176 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.295042 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.574903 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:25.854712 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.135073 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.415760 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.695780 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:26.975174 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.255548 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.536636 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:27.817735 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.098271 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.377925 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.657705 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:28.937919 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.218165 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.498269 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:29.779089 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.059613 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.340396 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.621692 [2] Warning: no training nodes in this partition! Backward fake loss.
09:06:30.902214 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:37.994170 [2] proc begin: <DistEnv 2/4 nccl>
09:07:42.720424 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:07:42.731074 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:07:47.911879 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.093614 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:49.589973 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.086129 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:50.582440 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.077567 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:51.572977 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.068713 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:52.564131 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.060551 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:53.555635 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.049443 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:54.544789 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.040858 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:55.536760 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.033070 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:56.529415 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.023890 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:57.518458 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.012911 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:58.507956 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.001958 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.496648 [2] Warning: no training nodes in this partition! Backward fake loss.
09:07:59.991924 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.486948 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:00.982702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.479237 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:01.975827 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.486904 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:02.999604 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:03.513015 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.011770 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:04.507490 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.004090 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.499758 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:05.996340 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.490951 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:06.988179 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.484365 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:07.979402 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.475727 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:08.972612 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.469511 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:09.966591 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.463899 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:10.959458 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.455477 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:11.951412 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.448109 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:12.943787 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.441032 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:13.936686 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.432691 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:14.929534 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.426631 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:15.924174 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.420222 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:16.916703 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.412273 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:17.908389 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.404747 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:18.899884 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.395131 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:19.892702 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.388186 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:20.884398 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.380451 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:21.875182 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.370126 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:22.865220 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.360462 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:23.855576 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.350202 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:24.845172 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.341967 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:25.837576 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.332588 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:26.828037 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.324750 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:27.819516 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.314036 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:28.809858 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.306054 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:29.802076 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.298503 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:30.796020 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.292162 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:31.787455 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.283476 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:32.778594 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.275764 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:33.771717 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.266745 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:34.761560 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.258232 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:35.754212 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.249392 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:36.744465 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.241274 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:37.736639 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.231743 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:38.726724 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.222474 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:39.719270 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.214535 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:40.709761 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.205019 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:41.701132 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.197190 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:42.693368 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.188196 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:43.684573 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.180098 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:44.676069 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.172761 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:45.668408 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.163273 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:46.659239 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.155551 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:47.651377 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.146414 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:48.641720 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.136728 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:49.632079 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.127778 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:50.622524 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.117739 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:51.613438 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.108632 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:52.603866 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.099839 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:53.594925 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.090108 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:54.584808 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.079716 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:55.574690 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.070377 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:56.565385 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.060700 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:57.556136 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.050859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:58.545743 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.041414 [2] Warning: no training nodes in this partition! Backward fake loss.
09:08:59.537549 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.033815 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:00.529633 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.025883 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:01.521425 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.026859 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:02.539286 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.054236 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:03.557081 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.053471 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:04.549002 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.044668 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:05.540337 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.036546 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:06.532276 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.030043 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:07.525651 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.021346 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:08.517237 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.012924 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:09.509844 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.006357 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:10.502428 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.000110 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.496941 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:11.992932 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.489065 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:12.985742 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.481690 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:13.976707 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.472192 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:14.967627 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.462883 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:15.958580 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.454183 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:16.949630 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.445610 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:17.941651 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.437066 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:18.932837 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.428003 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:19.926875 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.421946 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:20.918356 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.413708 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:21.908887 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.404736 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:22.900057 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.395145 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:23.890950 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.386476 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:24.882285 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.377258 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:25.874458 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.370409 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:26.865858 [2] Warning: no training nodes in this partition! Backward fake loss.
09:09:27.361760 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:03.547707 [2] proc begin: <DistEnv 2/4 nccl>
09:10:08.019610 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:10:08.027183 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:10:12.674969 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:14.236103 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:15.130751 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.023279 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:16.919899 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:17.811802 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:18.706713 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:19.603506 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:20.498795 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:21.396274 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:22.297014 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:23.194591 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.090836 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:24.988519 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:25.885661 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:26.782438 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:27.675585 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:28.573496 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:29.468709 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:30.365997 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:31.264473 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:32.161590 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.058515 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:33.963044 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:34.862297 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:35.758265 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:36.656262 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:37.552492 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:38.449629 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:39.346134 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:40.245465 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:41.141373 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.035604 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:42.932008 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:43.827481 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:44.723113 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:45.617101 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:46.511805 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:47.403264 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:48.294698 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:49.185725 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.077603 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:50.967662 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:51.857592 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:52.746223 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:53.635462 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:54.526372 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:55.416540 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:56.305783 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:57.197856 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.088076 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:58.978782 [2] Warning: no training nodes in this partition! Backward fake loss.
09:10:59.869296 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:00.760327 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:01.675079 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:02.606693 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:03.505427 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:04.401336 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:05.296309 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:06.192521 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.090103 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:07.984356 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:08.879900 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:09.774893 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:10.672338 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:11.569889 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:12.467497 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:13.363480 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:14.259144 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:15.154701 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.049631 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:16.945919 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:17.835177 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:18.727495 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:19.618191 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:20.509930 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:21.401311 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:22.293035 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:23.183979 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.077245 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:24.968106 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:25.863726 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:26.756766 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:27.650248 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:28.545090 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:29.440473 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:30.335062 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:31.233416 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:32.128243 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.022676 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:33.916153 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:34.807537 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:35.701593 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:36.596161 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:37.494233 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:38.388422 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:39.284503 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:40.181096 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.077449 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:41.971722 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:42.868752 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:43.763504 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:44.659476 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:45.553524 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:46.447751 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:47.343290 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:48.237973 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:49.133622 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.028648 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:50.924536 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:51.819142 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:52.716399 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:53.611105 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:54.506906 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:55.400358 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:56.294424 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:57.189587 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.086792 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:58.983099 [2] Warning: no training nodes in this partition! Backward fake loss.
09:11:59.876808 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:00.772159 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:01.668210 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:02.595946 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:03.517370 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:04.413525 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:05.309055 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:06.205029 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.097861 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:07.996493 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:08.892101 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:09.790737 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:10.685585 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:11.582146 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:12.477488 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:13.371314 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:14.266988 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:15.163243 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.057770 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:16.950251 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:17.841965 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:18.733771 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:19.625989 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:20.516042 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:21.407897 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:22.299493 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:23.191210 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.082537 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:24.973525 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:25.866111 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:26.757407 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:27.648797 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:28.540313 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:29.431890 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:30.322608 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:31.214636 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.105738 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:32.997273 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:33.887961 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:34.778884 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:35.670717 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:36.561830 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:37.454523 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:38.347440 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:39.238561 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:40.130105 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.025584 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:41.924601 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:42.820127 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:43.712194 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:44.603110 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:45.498400 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:46.396180 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:47.292075 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:48.186119 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.081309 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:49.975756 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:50.870591 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:51.766936 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:52.661394 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:53.555919 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:54.450927 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:55.346552 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:56.240489 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:57.134189 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.029740 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:58.925043 [2] Warning: no training nodes in this partition! Backward fake loss.
09:12:59.819318 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:00.714051 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:01.618475 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:02.541912 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:03.460765 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:04.354863 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:05.251054 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:06.148133 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.044010 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:07.940354 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:08.836277 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:09.732661 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:10.628760 [2] Warning: no training nodes in this partition! Backward fake loss.
09:13:11.525334 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:04.861787 [2] proc begin: <DistEnv 2/4 nccl>
14:50:27.758755 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:50:27.765346 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:50:30.856647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:32.295243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.010736 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:33.724529 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:34.436900 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.149202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:35.864416 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:36.578487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:37.294154 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.008136 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:38.723428 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:39.437603 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.148709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:40.861469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:41.574921 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:42.289612 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.002404 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:43.715213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:44.429916 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.143466 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:45.856799 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:46.568855 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.282008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:47.995194 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:48.710569 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:49.425855 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.140549 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:50.854534 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:51.566527 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.281487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:52.994813 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:53.710188 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:54.426158 [2] Warning: no training nodes in this partition! Backward fake loss.
14:50:55.140703 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:10.221072 [2] proc begin: <DistEnv 2/4 nccl>
14:51:10.337833 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
14:51:10.347158 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:51:11.654395 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.357556 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.372477 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.380776 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.392169 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.400271 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.407247 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.415135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.422445 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.432089 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.441001 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.449883 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.456698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.464825 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.472043 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.479767 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.490784 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.498999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.506130 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.513802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.521126 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.528692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.536737 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.544008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.551528 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.559165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.566995 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.573935 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.581313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.589497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.598130 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.606111 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.613394 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.620679 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.628219 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.635503 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.643479 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.651047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.658601 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.666487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.679644 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.688357 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.695959 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.705783 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.714263 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.723075 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.732022 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.742630 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.750424 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.758170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.765606 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.772474 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.779719 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.787643 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.795628 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.802304 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.810829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.816915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.824907 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.830667 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.840093 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.845995 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.853793 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.859757 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.868163 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.875761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.885993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.892979 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.900167 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.906146 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.915490 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.922272 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.930055 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.936116 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.943447 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.949844 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.958606 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.964967 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.972228 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.979361 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:12.993323 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.002429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.010254 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.017024 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.025134 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.031585 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.043393 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.052883 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.064249 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.072165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.084914 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.092972 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.106222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.116162 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.123928 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.130884 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.142294 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.148653 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.156385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.162598 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.171345 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.177492 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.185860 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.192200 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.199698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.209102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.217562 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.228543 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.239288 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.249213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.268886 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.277426 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.286138 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.292404 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.299643 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.305993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.314313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.321046 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.331967 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.341116 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.351156 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.357811 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.364777 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.371275 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.383604 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.389649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.397797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.405799 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.413743 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.422837 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.430156 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.436577 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.444859 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.450431 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.458694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.469367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.480049 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.491865 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.500579 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.512928 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.526226 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.536080 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.547065 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.556656 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.565001 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.570524 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.579167 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.585382 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.593100 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.599147 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.608850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.614980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.623045 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.629441 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.637050 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.646313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.655163 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.661541 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.669359 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.675428 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.685048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.692128 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.699972 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.705818 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.713745 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.720107 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.730666 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.737339 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.744871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.751556 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.764121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.772308 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.781038 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.787272 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.795895 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.802194 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.812387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.822335 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.829772 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.839565 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.847976 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.854575 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.863120 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.868815 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.876329 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.882455 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.890442 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.897155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.907490 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.913906 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.922000 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.927986 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.935453 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.941768 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.949739 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.956267 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.963701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.969825 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.977423 [2] Warning: no training nodes in this partition! Backward fake loss.
14:51:13.983455 [2] Warning: no training nodes in this partition! Backward fake loss.
14:54:38.077431 [2] proc begin: <DistEnv 2/4 nccl>
14:54:54.307499 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:54:54.323834 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:47:44.510230 [2] proc begin: <DistEnv 2/4 nccl>
20:48:16.904274 [2] proc begin: <DistEnv 2/4 nccl>
20:48:24.169438 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:48:24.188508 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:49:01.151822 [2] proc begin: <DistEnv 2/4 nccl>
20:49:06.600485 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:49:06.618045 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:07.196326 [2] proc begin: <DistEnv 2/4 nccl>
20:52:13.912138 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:52:13.928508 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:51.677822 [2] proc begin: <DistEnv 2/4 nccl>
20:52:51.765866 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:52:51.775070 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:52:53.089327 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.814146 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.829084 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.840863 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.850065 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.862145 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.872888 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.886095 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.896156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.905496 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.914663 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.924081 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.935589 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.949760 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.959694 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.968938 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.978583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.987774 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:53.997041 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.006272 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.016144 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.025294 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.034530 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.048081 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.058026 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.068399 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.079991 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.089609 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.098689 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.108966 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.118801 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.127970 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.137097 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.154305 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.166405 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.175768 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.184817 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.194296 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.206566 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.218801 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.229237 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.242128 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.252416 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.265083 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.275286 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.285717 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.296510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.308264 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.320409 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.331636 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.344456 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.354077 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.364763 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.376866 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.386443 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.401022 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.414988 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.426227 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.435349 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.446647 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.456088 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.465737 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.475057 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.484337 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.493531 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.503043 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.514157 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.526869 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.536173 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.548108 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.568062 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.578653 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.587587 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.597065 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.608659 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.620799 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.630876 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.640000 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.652152 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.661386 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.672264 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.692749 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.706943 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.717491 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.727860 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.738160 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.749973 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.761235 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.771161 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.784465 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.796820 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.809530 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.820521 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.836601 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.854485 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.870248 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.880922 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.891556 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.903899 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.915850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.926118 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.937427 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.946624 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.956170 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.965202 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.974365 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.982974 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:54.995679 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.007144 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.017827 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.030366 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.040399 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.053695 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.063776 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.073160 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.083839 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.097870 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.109386 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.118307 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.128647 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.138844 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.150652 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.163541 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.173700 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.182807 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.191389 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.200519 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.209431 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.218587 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.227416 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.236721 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.245913 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.254646 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.264777 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.275475 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.287126 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.297258 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.307369 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.320560 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.329990 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.339472 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.351844 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.360942 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.372665 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.381980 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.393484 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.406260 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.416293 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.428957 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.438279 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.450310 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.463649 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.476894 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.487826 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.497714 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.521621 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.533709 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.543150 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.553392 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.573724 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.586096 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.595472 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.604606 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.614735 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.625208 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.635930 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.647526 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.657810 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.668207 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.678675 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.699701 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.711921 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.722366 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.733826 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.744542 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.754873 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.766510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.777938 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.788834 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.801117 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.812392 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.821990 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.833578 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.843665 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.852926 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.868473 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.878016 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.887069 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.899953 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.908798 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.920443 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.931988 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.946121 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.955197 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.964550 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.973590 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.982581 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:55.991309 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.000281 [2] Warning: no training nodes in this partition! Backward fake loss.
20:52:56.012305 [2] Warning: no training nodes in this partition! Backward fake loss.
20:53:18.822558 [2] proc begin: <DistEnv 2/4 nccl>
20:53:24.252687 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:53:24.281206 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:39.123613 [2] proc begin: <DistEnv 2/4 nccl>
20:54:39.165694 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:54:39.175204 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:40.648456 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.410510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.426234 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.439959 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.454368 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.471558 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.481831 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.495486 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.509296 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.519245 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.530071 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.543254 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.552924 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.562590 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.572247 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.586343 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.595904 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.606063 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.615501 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.625121 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.635255 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.649035 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.661036 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.678344 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.694970 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.706704 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.719967 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.729412 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.740266 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.750161 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.759462 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.772410 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.782387 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.792119 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.804302 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.814531 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.824570 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.840472 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.855172 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.870760 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.881849 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.892451 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.905119 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.915825 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.931642 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.945321 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.957111 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.968713 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.981686 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:41.993692 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.004709 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.017609 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.031776 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.044920 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.055558 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.066503 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.079619 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.090387 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.102924 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.112984 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.127017 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.143527 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.157682 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.169133 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.179715 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.190519 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.201079 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.215276 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.226772 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.239835 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.251319 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.266328 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.285323 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.299621 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.312260 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.325596 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.339337 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.358258 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.374995 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.391317 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.405025 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.416603 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.428972 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.441555 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.457807 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.471385 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.483134 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.492565 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.504653 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.518774 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.532096 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.546962 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.557214 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.567759 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.577542 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.588308 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.605344 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.622775 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.635963 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.647797 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.661772 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.673245 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.692875 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.711053 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.724566 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.737398 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.750612 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.764284 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.775896 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.787681 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.800204 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.811704 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.823612 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.836569 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.848926 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.860588 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.872127 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.886027 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.897432 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.909830 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.926429 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.939728 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.950886 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.963107 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.973583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:42.988634 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.000032 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.010591 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.021080 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.033971 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.047124 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.056560 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.067673 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.078673 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.093637 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.107159 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.118359 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.132172 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.141826 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.154926 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.165574 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.179922 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.189546 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.199079 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.208542 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.218028 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.227544 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.237250 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.246645 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.263341 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.274315 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.290225 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.300450 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.313875 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.323492 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.332887 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.342132 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.351461 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.360471 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.369408 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.378585 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.387570 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.397418 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.414310 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.424233 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.433501 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.443033 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.458220 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.475265 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.490377 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.499824 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.509350 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.520881 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.535403 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.545360 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.555116 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.565615 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.575509 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.585165 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.596699 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.606068 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.619281 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.631493 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.646256 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.656334 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.670033 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.681201 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.692804 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.704800 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.716670 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.729228 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.740401 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.751876 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.762655 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.774678 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.789903 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.801433 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.813910 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.826240 [2] Warning: no training nodes in this partition! Backward fake loss.
20:54:43.842891 [2] Warning: no training nodes in this partition! Backward fake loss.
20:55:42.761360 [2] proc begin: <DistEnv 2/4 nccl>
20:55:48.999839 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:55:49.016037 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:56.292036 [2] proc begin: <DistEnv 2/4 nccl>
20:57:56.389189 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:57:56.401702 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:57:57.704409 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.408504 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.423110 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.435879 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.448273 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.457675 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.468996 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.480936 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.491735 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.501319 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.510476 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.520348 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.529795 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.538859 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.548073 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.557524 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.566733 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.582156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.591942 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.601561 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.610851 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.619939 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.629464 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.638793 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.647816 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.657585 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.670322 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.679859 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.690575 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.700491 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.710013 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.719482 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.728769 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.738172 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.747153 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.757382 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.768521 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.778887 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.788283 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.797952 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.810031 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.824031 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.836012 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.846153 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.855565 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.865269 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.874748 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.884172 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.893381 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.902736 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.914074 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.923275 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.932807 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.943168 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.952859 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.963327 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.972951 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.984519 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:58.994452 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.003661 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.013085 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.022551 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.032212 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.041924 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.051140 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.061119 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.073975 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.086197 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.095499 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.105118 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.114532 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.124850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.135998 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.145715 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.155123 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.168667 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.178407 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.187755 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.197178 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.207103 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.216494 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.226641 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.238933 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.247892 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.259411 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.269368 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.279140 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.292351 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.304499 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.317549 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.326066 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.335847 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.345054 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.354175 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.367190 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.376107 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.385679 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.394753 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.408465 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.419393 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.428886 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.439893 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.451726 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.463610 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.473939 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.501179 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.516017 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.529566 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.539427 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.548974 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.558658 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.568339 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.579458 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.590358 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.602217 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.615221 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.628880 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.640516 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.649871 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.659113 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.670519 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.679483 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.689070 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.700676 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.713675 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.725216 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.734578 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.745921 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.755151 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.764780 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.779087 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.790647 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.801361 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.811475 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.826973 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.837075 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.850992 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.860991 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.870240 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.879283 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.888834 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.901360 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.910701 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.921478 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.933425 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.946499 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.955035 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.968452 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.980898 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.989786 [2] Warning: no training nodes in this partition! Backward fake loss.
20:57:59.998835 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.008164 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.027445 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.041652 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.051480 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.060920 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.070377 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.079566 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.088795 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.100024 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.109417 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.118686 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.127824 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.136867 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.146034 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.162203 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.172129 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.182809 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.194526 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.203274 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.212823 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.224043 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.233831 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.243289 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.252739 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.265749 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.275444 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.284428 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.300110 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.310538 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.322651 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.331915 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.341146 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.353041 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.365210 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.377326 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.386514 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.395911 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.405557 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.415239 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.424850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.437703 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.446985 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.459333 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.470678 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.487808 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.506151 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.519059 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.528771 [2] Warning: no training nodes in this partition! Backward fake loss.
20:58:00.548753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.276564 [2] proc begin: <DistEnv 2/4 nccl>
21:00:48.366381 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:00:48.379091 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:49.823467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.559459 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.573594 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.586625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.596942 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.606497 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.621376 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.632587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.641948 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.651427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.660676 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.673648 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.685761 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.696467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.706184 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.715395 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.725205 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.734752 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.743986 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.753969 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.763185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.772612 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.783821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.793049 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.805307 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.814826 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.824327 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.833411 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.842698 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.851805 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.861387 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.874385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.885209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.897286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.906859 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.916210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.925925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.937817 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.949098 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.958702 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.968159 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.977734 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.987228 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.996538 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.005951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.015179 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.024668 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.034396 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.043532 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.052703 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.064575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.074149 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.083472 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.092909 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.104671 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.113786 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.122824 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.135444 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.153342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.167424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.177592 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.187770 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.197330 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.207071 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.216448 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.226959 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.236360 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.245817 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.260840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.271269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.287547 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.303283 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.314069 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.323382 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.332573 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.341643 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.351028 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.360081 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.369164 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.378383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.387717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.397054 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.406183 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.417386 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.430155 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.441939 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.452985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.462620 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.471957 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.491567 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.504617 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.515898 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.527821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.536972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.546655 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.557219 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.568357 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.577622 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.586856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.596083 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.605699 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.614874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.624034 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.633557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.642771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.651818 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.661196 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.676802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.690647 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.702869 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.718346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.737740 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.754083 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.764640 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.774801 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.783913 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.793128 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.802248 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.813052 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.823239 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.832203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.841907 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.852083 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.861462 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.873611 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.884991 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.893991 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.903316 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.913545 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.925416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.937034 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.948041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.957269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.966767 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.976123 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.985737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.997414 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.006452 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.019195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.030971 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.040194 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.049583 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.058500 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.072507 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.086247 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.095835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.105753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.119538 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.129195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.138488 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.159072 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.172435 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.183012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.192681 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.201604 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.214613 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.225170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.233948 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.243087 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.262037 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.276990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.304530 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.319566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.329478 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.345776 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.355825 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.364860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.375860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.384729 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.394010 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.402984 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.411900 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.421230 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.433577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.442952 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.452155 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.461568 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.473231 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.482470 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.491377 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.502943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.513972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.524200 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.535253 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.544532 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.553598 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.562388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.571502 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.580572 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.589922 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.599201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.608224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.617305 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.626522 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.638338 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.649802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.659238 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.671499 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.682262 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.692583 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:24.368551 [2] proc begin: <DistEnv 2/4 nccl>
21:07:24.397698 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:07:24.407203 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:26.537429 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.238726 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.254064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.266548 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.280345 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.290038 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.299552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.309005 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.321081 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.334115 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.343255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.352660 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.363213 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.373924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.383086 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.395012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.403830 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.413284 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.423092 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.436649 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.446210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.460460 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.476364 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.486545 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.496950 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.508306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.519179 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.528505 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.540275 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.549523 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.559002 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.568368 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.578045 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.587008 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.599832 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.610692 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.619928 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.632600 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.646262 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.655735 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.665712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.675411 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.684813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.694677 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.704263 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.716573 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.729106 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.741368 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.754382 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.767295 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.776612 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.786315 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.795615 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.806976 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.816509 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.825795 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.834930 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.844256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.855342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.866209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.882522 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.896281 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.907073 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.916193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.927087 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.941529 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.951113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.961176 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.972415 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.985346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:27.998919 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.008256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.020853 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.031903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.042089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.053372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.067792 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.078090 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.090854 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.102296 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.115543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.130327 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.143025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.156143 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.165518 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.178155 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.191453 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.204615 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.214452 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.233626 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.245575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.261408 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.284145 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.296089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.308776 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.317499 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.328439 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.340310 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.349121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.372227 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.386355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.395458 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.408489 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.419753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.433607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.443856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.453039 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.462239 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.474363 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.486457 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.496237 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.505120 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.513855 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.522461 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.531185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.539951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.548999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.557815 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.566502 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.575170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.584131 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.592765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.605712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.619597 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.631993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.641891 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.650992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.663349 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.674458 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.690278 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.700958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.715191 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.728542 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.738995 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.748122 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.757021 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.765923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.774692 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.783408 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.792251 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.801080 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.810446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.822018 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.832757 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.842065 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.850912 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.860744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.870209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.878776 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.890769 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.900010 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.910200 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.923701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.935147 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.946380 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.957709 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.969421 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.982347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:28.991999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.000860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.009714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.020476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.030722 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.039355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.048501 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.057178 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.066436 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.075248 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.084021 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.092786 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.104880 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.116097 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.129308 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.141766 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.153972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.163019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.171993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.186991 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.201406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.212499 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.222326 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.231174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.240340 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.251863 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.270428 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.290344 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.306041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.317307 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.326960 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.336178 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.345270 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.365193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.381853 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.394419 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.404256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.413174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.421719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.433118 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.446375 [2] Warning: no training nodes in this partition! Backward fake loss.
21:07:29.456690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:31.456298 [2] proc begin: <DistEnv 2/4 nccl>
21:09:31.569729 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:09:31.583236 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:32.781715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.512112 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.527521 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.538745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.547972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.557569 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.570341 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.582093 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.591736 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.602874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.614143 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.623321 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.636663 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.652167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.662516 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.673610 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.684399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.694416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.704669 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.717230 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.729905 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.740894 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.752920 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.762933 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.774297 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.784890 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.796259 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.806075 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.815248 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.831449 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.844645 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.856438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.869923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.880073 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.889928 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.902111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.912895 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.922250 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.934758 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.946818 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.956205 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.969487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.980741 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:33.991732 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.006166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.018153 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.030243 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.039512 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.048543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.057929 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.067397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.078325 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.089668 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.098763 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.111451 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.121822 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.132779 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.142627 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.154674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.163661 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.172871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.184241 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.193601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.202992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.212569 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.222338 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.231385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.243242 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.254735 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.264665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.275641 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.285476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.295116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.304782 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.322792 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.335059 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.344458 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.356099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.367010 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.377089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.390481 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.401501 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.412642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.424003 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.435397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.444889 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.466396 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.494256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.508501 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.523724 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.535159 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.545515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.558610 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.569562 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.579256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.593680 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.605486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.618065 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.634423 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.644992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.657039 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.671405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.681193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.691025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.701615 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.711226 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.720116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.730244 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.739255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.750289 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.760269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.771439 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.781091 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.791060 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.804544 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.815221 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.825150 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.834611 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.843940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.855230 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.866224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.875304 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.884600 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.895789 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.906313 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.918525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.926948 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.938710 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.947918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.957261 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.966317 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.977825 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.988860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:34.998582 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.007595 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.017318 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.026562 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.038684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.048244 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.057721 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.070996 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.083687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.092763 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.102396 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.111525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.120431 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.130195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.141207 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.153898 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.163584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.181865 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.193848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.206708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.216308 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.227646 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.240524 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.249630 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.262435 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.276859 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.290002 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.301109 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.310259 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.321297 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.330726 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.340712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.349720 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.359364 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.370620 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.382681 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.391824 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.401437 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.410673 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.419747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.429255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.438735 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.451508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.473803 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.497266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.507567 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.517393 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.528544 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.541941 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.550923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.563064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.575771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.585483 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.594515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.607026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.616608 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.627297 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.641107 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.653893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.666406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.675740 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.684907 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.695267 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.704543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.719551 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.730130 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:35.739765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:50.949129 [2] proc begin: <DistEnv 2/4 nccl>
21:09:50.982417 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:09:50.994664 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:52.436342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.178268 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.191818 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.204899 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.217381 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.228493 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.238357 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.247978 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.257418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.269725 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.279040 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.288419 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.297862 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.307160 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.322258 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.335298 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.345833 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.356636 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.369632 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.381863 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.391357 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.401007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.410624 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.420232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.429573 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.439164 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.448505 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.458111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.469882 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.479966 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.489729 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.500771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.513508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.529026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.542430 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.554937 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.563982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.573026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.581840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.592979 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.610558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.624404 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.633000 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.642125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.652736 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.664921 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.673861 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.683703 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.692876 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.702240 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.711243 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.726743 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.738701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.752285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.763852 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.773601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.782613 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.791756 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.800961 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.814373 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.824210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.840747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.852185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.862594 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.874999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.886921 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.895797 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.909038 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.919477 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.929305 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.938982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.960144 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.970808 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.980238 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:53.989307 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.000690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.009876 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.018897 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.027677 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.036687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.046347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.055404 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.064642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.073766 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.086692 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.097010 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.105949 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.115519 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.129332 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.141846 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.151213 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.162690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.173929 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.185653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.195447 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.207575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.218070 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.227306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.236255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.245427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.254161 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.263319 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.272209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.281418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.290447 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.299624 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.308503 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.317486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.326204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.335200 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.344082 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.353816 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.366961 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.378809 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.390854 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.403127 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.412082 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.421201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.430022 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.439109 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.447960 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.458042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.467761 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.479630 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.492204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.504293 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.513041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.522039 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.530729 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.539487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.552230 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.560672 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.574903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.587164 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.595806 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.605487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.614366 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.623303 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.632225 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.641509 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.650401 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.659071 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.668915 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.681981 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.693166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.702234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.714901 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.723975 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.733565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.747287 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.759856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.773871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.783432 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.792421 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.803080 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.814725 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.823539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.834583 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.851073 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.862711 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.871952 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.880854 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.889617 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.898877 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.907596 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.916594 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.925391 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.934370 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.943234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.968057 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.980797 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:54.990847 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.000018 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.009533 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.018594 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.027648 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.036433 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.045531 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.054166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.063110 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.071999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.080956 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.089749 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.098570 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.107536 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.116599 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.127343 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.136904 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.150140 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.159697 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.168575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.177597 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.187086 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.199168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.208099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.216867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.225918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.234681 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.243728 [2] Warning: no training nodes in this partition! Backward fake loss.
21:09:55.255243 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:37.719261 [2] proc begin: <DistEnv 2/4 nccl>
21:12:37.746089 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:12:37.755306 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:39.047774 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.740007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.753919 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.763616 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.773065 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.782486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.792080 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.801835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.812735 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.825806 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.843796 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.857061 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.866470 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.878022 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.887178 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.896607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.907139 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.919099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.928715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.942359 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.951653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.963485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.977369 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.989170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:39.998487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.007987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.018151 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.027804 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.039940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.052667 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.066172 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.075499 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.085701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.095116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.104438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.113815 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.123632 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.134967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.144712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.154024 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.163402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.177959 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.194983 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.205983 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.215630 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.226578 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.239203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.252044 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.261819 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.277822 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.287777 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.297180 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.306225 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.315263 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.324406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.335918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.346681 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.357027 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.367355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.379775 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.392983 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.402114 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.411221 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.420228 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.429521 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.439201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.450630 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.463748 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.474684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.485454 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.494981 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.504769 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.517562 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.534605 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.546915 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.559125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.568161 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.579064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.588047 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.602034 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.611918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.623158 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.632008 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.641164 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.650706 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.663234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.675709 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.685170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.694687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.709974 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.723773 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.755012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.767301 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.776263 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.786128 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.798845 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.807639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.820181 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.829201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.838708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.848019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.859678 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.868920 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.879137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.888249 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.897168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.909327 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.920204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.929187 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.937999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.949212 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.957962 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.966674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.975540 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.987792 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:40.996573 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.006266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.016628 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.026142 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.035128 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.045238 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.054846 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.068409 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.078526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.087321 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.097814 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.106800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.115443 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.125260 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.137200 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.149420 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.160640 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.170748 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.180339 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.189772 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.199171 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.208325 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.217486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.230286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.245819 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.258813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.268282 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.277670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.286771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.295908 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.304899 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.313975 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.327108 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.336705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.349717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.361741 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.370765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.379867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.388842 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.397781 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.406689 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.416985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.429521 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.438534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.447571 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.456451 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.465526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.477971 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.487736 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.498330 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.508486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.517653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.531608 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.540227 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.549319 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.559453 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.568283 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.577644 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.586404 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.595399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.610539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.622715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.631335 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.640475 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.653690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.663383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.672289 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.684241 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.695606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.707322 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.716298 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.727188 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.741813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.764653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.778091 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.787428 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.798331 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.808326 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.821400 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.836715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.848050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.857014 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.869875 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.881515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:12:41.893596 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:12.678041 [2] proc begin: <DistEnv 2/4 nccl>
21:16:12.707527 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:16:12.717199 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:13.985963 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.698855 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.713747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.723121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.732766 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.744821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.756292 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.769637 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.779189 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.787855 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.796836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.807829 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.817190 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.826501 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.835693 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.844498 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.854355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.863365 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.876300 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.885390 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.898296 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.916663 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.928717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.937715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.947049 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.955708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.964895 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.976323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.986013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:14.999746 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.009977 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.022633 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.033387 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.044055 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.055142 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.065621 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.078712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.089342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.099463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.110934 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.120497 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.132498 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.146187 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.161335 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.173490 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.182976 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.196420 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.205857 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.217186 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.227945 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.242574 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.253487 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.262900 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.272704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.281903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.291182 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.300397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.318301 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.333314 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.342939 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.355514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.366325 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.375639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.386043 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.398863 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.410585 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.420163 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.429689 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.438713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.447732 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.457047 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.466604 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.478896 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.492001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.503113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.519882 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.542104 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.552812 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.562299 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.571312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.580223 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.590874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.601992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.612240 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.624664 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.638432 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.651147 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.663309 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.674004 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.699783 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.715320 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.740127 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.753943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.764426 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.776393 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.786045 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.797019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.806593 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.818941 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.828786 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.838367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.847642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.856839 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.869272 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.881121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.891344 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.900922 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.912019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.924832 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.934958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.944165 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.953714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.967748 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.980907 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.990625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:15.999792 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.009453 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.018483 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.029439 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.039383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.048418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.057851 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.070224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.080530 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.089957 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.102225 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.112932 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.122098 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.131433 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.140427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.149780 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.165514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.174947 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.184365 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.194053 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.203101 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.215412 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.224529 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.233719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.242838 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.252159 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.261319 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.271877 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.281042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.293973 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.305087 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.317232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.328509 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.338627 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.347747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.357403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.370498 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.380792 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.390093 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.400201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.412835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.423631 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.432580 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.442399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.451312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.460666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.470832 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.480106 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.489998 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.499384 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.508393 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.517764 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.526751 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.535813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.544979 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.554336 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.566089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.576179 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.585407 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.598018 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.607094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.616494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.627333 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.642711 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.653417 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.663403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.672712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.682281 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.702653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.716933 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.726887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.736227 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.745680 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.755007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.764635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.774203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.783589 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.793395 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.802940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.812898 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.822319 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.831482 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.840828 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.852416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:16:16.860968 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:42.349187 [2] proc begin: <DistEnv 2/4 nccl>
21:20:42.376763 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
21:20:42.386278 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:20:44.442643 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.143842 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.159448 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.169951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.181310 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.192342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.202778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.216015 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.225373 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.238031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.251886 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.263168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.272506 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.284874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.295485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.307920 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.321154 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.335306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.349723 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.359443 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.369270 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.379961 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.389696 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.398717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.408446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.421372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.430531 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.439925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.449127 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.458207 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.469096 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.478810 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.488348 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.501095 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.510684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.520272 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.529771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.538886 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.548354 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.561374 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.578069 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.590700 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.600355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.613243 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.622838 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.632775 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.641993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.651474 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.661054 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.670737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.682187 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.692271 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.702565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.713286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.723984 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.736537 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.746298 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.755450 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.765274 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.778006 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.789392 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.799234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.809450 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.819442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.830276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.840537 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.857099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.871672 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.884312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.895836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.909208 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.920113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.931185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.941732 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.952216 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.965305 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.976125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:45.995639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.009191 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.022557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.033457 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.045068 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.063871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.078715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.090435 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.101682 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.112753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.124390 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.137299 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.155606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.171990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.183953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.194206 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.217999 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.229940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.239868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.248463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.260155 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.272279 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.284353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.303588 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.314020 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.333148 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.345557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.366196 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.378448 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.387713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.396290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.407928 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.417183 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.426556 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.435355 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.445618 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.454576 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.464284 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.473310 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.482345 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.493795 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.507588 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.520508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.530514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.539377 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.548384 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.561016 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.572302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.581554 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.593860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.603878 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.615933 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.625432 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.634268 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.643091 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.651871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.663456 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.673414 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.683143 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.691761 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.701079 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.710296 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.718903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.728052 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.737466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.749680 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.761427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.770558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.779302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.788197 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.797368 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.809486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.818711 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.827569 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.843050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.854935 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.864137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.873116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.881902 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.891395 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.900402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.909508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.921409 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.930693 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.939805 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.948972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.960839 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.969632 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.978618 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.988195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:46.999511 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.010046 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.019929 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.029654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.041861 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.051120 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.064098 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.078126 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.086597 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.095446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.104299 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.113068 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.121835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.130450 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.139224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.148163 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.157249 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.165930 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.174744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.183311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.192406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.201572 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.210590 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.234176 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.243770 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.252987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.262467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.272890 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.281378 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.293848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.306147 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.314672 [2] Warning: no training nodes in this partition! Backward fake loss.
21:20:47.323682 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:32.492069 [2] proc begin: <DistEnv 2/4 nccl>
20:31:44.948224 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:31:44.965946 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:07.185223 [2] proc begin: <DistEnv 2/4 nccl>
20:33:12.621752 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:33:12.649741 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:59:43.756616 [2] proc begin: <DistEnv 2/4 nccl>
20:59:55.468674 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
20:59:55.474395 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:00:09.423190 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:10.974490 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:11.746635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:12.519720 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:13.292433 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.064652 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:14.838946 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:15.610294 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:16.381793 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.153949 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:17.924255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:18.697406 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:19.469876 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:20.239761 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.010469 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:21.783108 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:22.552925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:23.321921 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.091372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:24.859782 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:25.627627 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:26.395623 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.166372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:27.933670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:28.702478 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:29.471462 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:30.240026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.008334 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:31.779063 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:32.586276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:33.359180 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.128893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:34.894986 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:35.663873 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:36.431388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.196457 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:37.961171 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:38.727275 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:39.494255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:40.262784 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.033225 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:41.804705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:42.576776 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:43.347210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.118307 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:44.889461 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:45.660871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:46.430403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.202203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:47.974196 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:48.746417 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:49.516888 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:50.286670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.056533 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:51.826427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:52.596713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:53.366902 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.140438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:54.911759 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:55.684204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:56.454656 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.224111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:57.994885 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:58.766333 [2] Warning: no training nodes in this partition! Backward fake loss.
21:00:59.536745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:00.307869 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.078237 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:01.851331 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:02.656617 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:03.458674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:04.232782 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.002554 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:05.770903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:06.538794 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:07.307304 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.075878 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:08.843508 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:09.716113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:10.488649 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:11.261123 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.027947 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:12.795237 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:13.560900 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:14.327132 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.092543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:15.862036 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:16.628868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:17.401967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.173042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:18.950194 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:19.718690 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:20.491252 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:21.263436 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.035665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:22.808328 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:23.581083 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:24.353058 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.125410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:25.898746 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:26.671471 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:27.443503 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.215512 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:28.988424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:29.759232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:30.530947 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:31.301621 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.072955 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:32.841600 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:33.611358 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:34.382081 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.151917 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:35.921902 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:36.692365 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:37.463699 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:38.233963 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.003778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:39.778728 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:40.547713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:41.316527 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.085172 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:42.855060 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:43.624311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:44.395046 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:45.429041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.200423 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:46.971082 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:47.738694 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:48.506423 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:49.272626 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.037394 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:50.801425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:51.565010 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:52.331584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.096217 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:53.865426 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:54.634778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:55.403468 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.173671 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:56.943553 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:57.714074 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:58.483514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:01:59.253987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.025025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:00.795950 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:01.571652 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:02.378959 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.173087 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:03.944444 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:04.715728 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:05.487977 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:06.258606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.031066 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:07.802324 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:08.573360 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:09.344649 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.113279 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:10.883314 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:11.654166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:12.426023 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.196906 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:13.967666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:14.738653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:15.507247 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:16.274869 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.047342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:17.819359 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:18.587890 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:19.356737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.126036 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:20.896764 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:21.665872 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:22.646068 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:23.798472 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:24.568849 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:25.339201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.105637 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:26.872038 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:27.637282 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:28.402159 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.167934 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:29.932953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:30.699441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:31.465903 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:32.235506 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.006206 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:33.778382 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:34.548880 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:35.319588 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.090359 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:36.861772 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:37.632885 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:38.404441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.176577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:39.948079 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:40.720192 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:41.491799 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:42.264019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.036583 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:43.808674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:02:44.580265 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:05.793542 [2] proc begin: <DistEnv 2/4 nccl>
14:32:27.180913 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:32:27.189255 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:32.107159 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:33.726074 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:34.493467 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:35.261121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.029582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:36.796872 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:37.565276 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:38.332469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.101733 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:39.869103 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:40.636500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:41.405239 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.174284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:42.943430 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:43.712020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:44.481140 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:45.249066 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.017546 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:46.785229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:47.553870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:48.322037 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.090178 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:49.860006 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:50.629998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:51.399154 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.166635 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:52.935052 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:53.703873 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:54.471312 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:55.239131 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.007121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:56.774790 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:57.543061 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:58.311382 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.079835 [2] Warning: no training nodes in this partition! Backward fake loss.
14:32:59.848531 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:00.615907 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:01.383631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.154207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:02.955376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:03.747688 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:04.515031 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:05.282413 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.049427 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:06.816467 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:07.585433 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:08.353110 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.121056 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:09.889963 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:10.658480 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:11.426601 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.195222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:12.964806 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:13.733363 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:14.502830 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:15.273936 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.044193 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:16.813920 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:17.585669 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:18.357782 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.126730 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:19.897781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:20.669107 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:21.438607 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.207306 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:22.976207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:23.745966 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:24.515004 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:25.279855 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.045234 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:26.810338 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:27.575833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:28.341275 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.105889 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:29.870675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:30.636865 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:31.402179 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.167487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:32.933526 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:33.701104 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:34.468037 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:35.235406 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.002067 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:36.768742 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:37.534922 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:38.301516 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.068116 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:39.834341 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:40.607982 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:41.444479 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:42.460413 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:43.709681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:45.002580 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:46.325718 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:47.654026 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:48.982025 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:50.306877 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:51.635912 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:52.956067 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:54.277325 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:55.604463 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:56.935493 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:58.265540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:33:59.600648 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:00.935856 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:01.793401 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:02.594988 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:03.385876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.152981 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:04.920186 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:05.686093 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:06.451941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.218387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:07.984823 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:08.751380 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:09.518436 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:10.285108 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.051330 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:11.817471 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:12.584696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:13.351976 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.118783 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:14.885940 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:15.653915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:16.420911 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.189229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:17.957177 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:18.725325 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:19.493827 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:20.263512 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.030721 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:21.797742 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:22.565794 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:23.333203 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.099750 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:24.866801 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:25.632731 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:26.398956 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.163482 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:27.929403 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:28.695061 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:29.461086 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.227360 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:30.993547 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:31.761168 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:32.527663 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:33.293658 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.060651 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:34.829156 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:35.596940 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:36.364701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.133081 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:37.901291 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:38.670150 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:39.439029 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.207765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:40.975576 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:41.744280 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:42.513191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:43.280781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.049235 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:44.816836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:45.585351 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:46.353334 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.121045 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:47.888371 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:48.656194 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:49.423462 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.192305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:50.962222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:51.731872 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:52.501263 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:53.271032 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.042579 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:54.811728 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:55.578764 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:56.346026 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.113306 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:57.879787 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:58.646388 [2] Warning: no training nodes in this partition! Backward fake loss.
14:34:59.413397 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.179322 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:00.946340 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:01.730838 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:02.533953 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:03.314337 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.082808 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:04.851611 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:05.621594 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:06.390213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.159675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:07.928519 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:08.698413 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:09.466969 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:10.235754 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.004746 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:11.773392 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:12.542786 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:13.309215 [2] Warning: no training nodes in this partition! Backward fake loss.
14:35:14.076203 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:03.951043 [2] proc begin: <DistEnv 2/4 nccl>
14:40:09.053199 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:40:09.062037 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:40:14.482742 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.130395 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.000800 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.868780 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.738540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.609204 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.482223 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.350798 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.222436 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.093898 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.963340 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.832335 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.702624 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.574560 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.445711 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.316477 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.187376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.055834 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.923991 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.793010 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.661594 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.530928 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.399466 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.268285 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.135835 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.002998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.871491 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.739609 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.605669 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.472160 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.339079 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.206144 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.074976 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.945800 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.812415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.679881 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.549609 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.417141 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.286017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.156344 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.025434 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.902108 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.771984 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.640193 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.513704 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.386641 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.259214 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.132414 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.006446 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.876640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.751509 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.626020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.498671 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.372642 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.281739 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.181502 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.052867 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.924676 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.795739 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.666152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.538200 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.410781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.282994 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.155974 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.028115 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.900734 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.772635 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.646846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.517811 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.392048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.264752 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.136498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.009376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.881364 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.753070 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.625165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.495230 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.363774 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.234556 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.103529 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.973909 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.844212 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.712964 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.578939 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.446713 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.315030 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.185725 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.056586 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.928473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.800649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.672500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.543815 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.417161 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.290336 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.163938 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.037433 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.911439 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.784111 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.657368 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.530234 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.400999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.275183 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.148083 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.021797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.895372 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.766653 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.639021 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.512121 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.378941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.245301 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.111674 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.976665 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.842695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.711297 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.580836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.450150 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.321849 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.191540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.059313 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.927829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.795647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.665252 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.533063 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.423799 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.335191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.211124 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.083342 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.956128 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.828289 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.703305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.575382 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.445260 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.313492 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.180836 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.048322 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.913676 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.778254 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.643442 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.508541 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.373818 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.239893 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.105478 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.970550 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.835676 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.701799 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.568893 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.437588 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.305232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.172434 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.039698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.906324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.773235 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.640387 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.507254 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.374484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.241651 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.109176 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.976664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.842634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.710140 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.578026 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.442924 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.308983 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.174529 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.039972 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.904819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.769552 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.635111 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.503645 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.374140 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.242403 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.110915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.980011 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.847864 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.716473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.585069 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.456475 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.330412 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.202738 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.076462 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.949983 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.822193 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.695030 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.569094 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.439373 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.311942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.186695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.060209 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.932946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.808311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.679708 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.553583 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.466171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.365372 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.238505 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.113214 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.986667 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.860301 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.732890 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.604411 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:55.839952 [2] proc begin: <DistEnv 2/4 nccl>
14:45:00.040418 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:45:00.049378 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:45:05.620740 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.207761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.082973 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.957504 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:09.833753 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:10.710052 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.585998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.461667 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.336096 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:14.212497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.085091 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.956108 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.828931 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.698152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:18.568941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.441613 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.314903 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.187675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.057932 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:22.928220 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.801173 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.671598 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.542277 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:26.412147 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.283026 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.155419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.028563 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.900683 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:30.773781 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.648177 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.521672 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.394927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:34.268357 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.141869 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.014508 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.887895 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.762470 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:38.632444 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.504485 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.377944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.249530 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.124798 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.998023 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:43.869269 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.740205 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.614473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.488398 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:47.362473 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.235290 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.108802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.979195 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.853640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:51.727635 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.600212 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.473967 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.345361 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:55.216767 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.089930 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.960453 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.832759 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.704692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:59.578572 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.453243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.328124 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.240979 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:03.135265 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.012704 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.889562 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.764549 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.637100 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:07.512097 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.386070 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.260155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.135347 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.010674 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:11.886584 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.763035 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.636299 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.514148 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.389654 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:16.262246 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.132942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.002498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.874529 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.747583 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:20.622355 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.496437 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.372479 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.249513 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:24.127407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.003077 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.879109 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.750643 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.619616 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.487527 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:29.354564 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.221822 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.090813 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.961582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.831281 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:33.700157 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.568770 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.440058 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.310566 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:37.179679 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.049808 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.918445 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.787076 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.656408 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:41.525346 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.396616 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.269056 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.138824 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.008125 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.879494 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:46.748110 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.617048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.486175 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.356248 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:50.225842 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.095391 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.963795 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.831818 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.700382 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:54.569091 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.437319 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.305533 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.176231 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.046826 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:58.920191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.793015 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.663944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.536259 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:02.434006 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.351577 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.226299 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.098647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.970871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:06.844980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.717238 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.587793 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.458170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:10.327090 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.196127 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.066536 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.935445 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.803995 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:14.672784 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.541927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.411895 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:17.281716 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:18.150582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.018971 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.887346 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.755551 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.626771 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:22.496687 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.364524 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.232171 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.099999 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.969926 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:26.841965 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.712512 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.583141 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.454597 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:30.326133 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.198202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.070940 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.942351 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.814529 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:34.686504 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.557367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.426480 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.302374 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:38.173048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.043133 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.914547 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.785054 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.654631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:42.525230 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.393958 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.263838 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.134280 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.003672 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:46.878246 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.748932 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.618208 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:49.489242 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:50.361284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:51.234020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.106367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:52.978438 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:53.848366 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:54.718534 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:55.590306 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:56.463061 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:57.337655 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:58.211435 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.084770 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:59.955634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:13.730650 [2] proc begin: <DistEnv 2/4 nccl>
14:59:18.000123 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:59:18.008698 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:59:23.847117 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:25.776375 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:27.092951 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:28.412960 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:29.733847 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:31.048678 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:32.356910 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:33.667964 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:34.978902 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:36.293404 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:37.605008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:38.914460 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:40.221213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:41.525980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:42.830881 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:44.134764 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:45.439749 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:46.744491 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:48.047432 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:49.353595 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:50.657993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:51.963327 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:53.270259 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:54.572993 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:55.877852 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:57.180837 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:58.487244 [2] Warning: no training nodes in this partition! Backward fake loss.
14:59:59.793145 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:01.098934 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:02.447080 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:03.764893 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:05.068962 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:06.373393 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:07.679015 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:08.983402 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:10.286748 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:11.597730 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:12.390192 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.152332 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:13.914498 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:14.676068 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:15.436956 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.200100 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:16.961166 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:17.721138 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:18.482218 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:19.243410 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.004310 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:20.764863 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:21.525576 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:22.285937 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.047109 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:23.807118 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:24.567653 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:25.327752 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.087734 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:26.847327 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:27.608516 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:28.368175 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.128361 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:29.887732 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:30.649648 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:31.409149 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.168911 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:32.928163 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:33.687163 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:34.445645 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.204544 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:35.963877 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:36.724472 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:37.483520 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:38.242268 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.001497 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:39.761774 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:40.522383 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:41.282646 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.043591 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:42.803477 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:43.563939 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:44.323818 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.084953 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:45.846167 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:46.606682 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:47.366446 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.126850 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:48.887278 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:49.647150 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:50.407862 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.167909 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:51.927815 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:52.691988 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:53.452345 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.215064 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:54.978588 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:55.740057 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:56.502944 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:57.263180 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.032671 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:58.794544 [2] Warning: no training nodes in this partition! Backward fake loss.
15:00:59.555799 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:00.317702 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.079638 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:01.841780 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:02.633492 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:03.428248 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.189046 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:04.950305 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:05.712292 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:06.484585 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:07.259895 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.036608 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:08.812569 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:09.590471 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:10.366653 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.142709 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:11.920261 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:12.695888 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:13.472841 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:14.248402 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.024915 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:15.800182 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:16.576435 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:17.353633 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.128191 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:18.904301 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:19.679289 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:20.454651 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:21.228568 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.003742 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:22.777621 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:23.553310 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:24.328382 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.103346 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:25.877885 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:26.651985 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:27.426847 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.200705 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:28.974909 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:29.750075 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:30.524897 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:31.299880 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.074071 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:32.849147 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:33.624604 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:34.401081 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.175987 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:35.952267 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:36.727680 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:37.503748 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:38.279236 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.053798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:39.829214 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:40.604682 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:41.380454 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.156168 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:42.931725 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:43.707089 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:44.483703 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:45.259771 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.036013 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:46.812217 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:47.587893 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:48.363253 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.138816 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:49.915573 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:50.690790 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:51.467141 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:52.242921 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.019252 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:53.797020 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:54.573258 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:55.349030 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.124861 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:56.899941 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:57.676144 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:58.451826 [2] Warning: no training nodes in this partition! Backward fake loss.
15:01:59.227336 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.003745 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:00.780430 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:01.556659 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:02.363202 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.166596 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:03.942447 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:04.719101 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:05.495087 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:06.271359 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.047948 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:07.823548 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:08.599310 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:09.376113 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.153433 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:10.929986 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:11.706198 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:12.481508 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:13.257109 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.032872 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:14.809297 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:15.585806 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:16.361659 [2] Warning: no training nodes in this partition! Backward fake loss.
15:02:17.137982 [2] Warning: no training nodes in this partition! Backward fake loss.
14:03:56.426181 [2] proc begin: <DistEnv 2/4 nccl>
14:04:18.859940 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:04:19.081882 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:04:24.052770 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:25.684402 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:26.446345 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.208581 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:27.969888 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:28.732526 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:29.494281 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:30.256260 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.017241 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:31.778927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:32.541109 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:33.301756 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.063970 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:34.825180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:35.588660 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:36.352433 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.116288 [2] Warning: no training nodes in this partition! Backward fake loss.
14:04:37.880107 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.524814 [2] proc begin: <DistEnv 2/4 nccl>
14:09:24.546147 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:09:24.558350 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:09:29.939968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.802556 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.564906 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.327285 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.089840 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.854705 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.617486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.379765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.142521 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.906229 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.669906 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.431516 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.193797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.955725 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.719918 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.483916 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.246455 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.009905 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.773497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.537143 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.299665 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.062462 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.825117 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.586907 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.349403 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.113432 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.791004 [2] proc begin: <DistEnv 2/4 nccl>
14:10:22.157000 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:10:22.212213 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:10:28.615973 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:30.536830 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:31.302168 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.067976 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:32.834670 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:33.602597 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:34.367998 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.133927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:35.900222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:36.663915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:37.428055 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.193324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:38.958846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:39.723681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:40.488922 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:41.253956 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.018341 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:42.783559 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:43.548873 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:44.314500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:45.078996 [2] Warning: no training nodes in this partition! Backward fake loss.
14:18:18.342794 [2] proc begin: <DistEnv 2/4 nccl>
14:18:52.448196 [2] proc begin: <DistEnv 2/4 nccl>
14:18:57.935623 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:18:57.952484 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:34:48.566928 [2] proc begin: <DistEnv 2/4 nccl>
14:34:54.276185 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:34:54.304947 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:13.652693 [2] proc begin: <DistEnv 2/4 nccl>
14:43:18.026708 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:43:18.038593 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:43:23.941327 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:26.623305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:27.953983 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:29.373649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:30.706048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:32.037876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:33.307568 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:34.481396 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:35.871944 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:37.180807 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:38.332276 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:39.695969 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:41.102690 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:42.427217 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:43.695520 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:45.039709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:46.375835 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:47.712817 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:49.051072 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:50.387277 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:51.719476 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:53.054800 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:54.387243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:55.818762 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:57.151862 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:58.486737 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:59.825953 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:01.159523 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:02.509349 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:03.707300 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:05.074615 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:06.423373 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:07.743883 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:08.913450 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:10.206799 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:11.571233 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:12.912500 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:14.254204 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:15.601283 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:16.935805 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:18.268524 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:19.604350 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:20.942375 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:22.274462 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:23.610846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:24.948675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:26.290747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:27.621427 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:28.953180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:30.291331 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:31.622701 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:32.955518 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:34.289445 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:35.629393 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:36.797540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:38.124316 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:39.472223 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:40.814419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:42.018102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:43.401938 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:44.711182 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:46.061200 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:47.401041 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:48.701402 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:49.912048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:51.239838 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:52.583040 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:53.799017 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:55.148130 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:56.499083 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:57.847528 [2] Warning: no training nodes in this partition! Backward fake loss.
14:44:59.198027 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:00.537369 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:01.873540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:03.328372 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:04.661551 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:05.999594 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:07.338538 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:08.674830 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:10.010047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:11.340972 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:12.586021 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:13.813593 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:15.176440 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:16.525786 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:17.732520 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:19.099028 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:20.444605 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:21.660297 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:23.044324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:24.343627 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:25.687715 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:27.033311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:28.376766 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:29.728592 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:31.067939 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:32.408234 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:33.743871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:35.079019 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:36.411847 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:37.642269 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:39.025898 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:40.302994 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:41.528570 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:42.879259 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:44.223006 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:45.569880 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:46.917423 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:48.261259 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:49.602632 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:50.939429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:52.273582 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:53.613971 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:54.954994 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:56.294679 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:57.630385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:45:58.889215 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:00.105351 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:01.487520 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:02.745165 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:04.170173 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:05.427664 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:06.774996 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:08.122064 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:09.300494 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:10.695784 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:12.007240 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:13.187627 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:14.592552 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:15.843181 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:17.193904 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:18.447760 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:19.731287 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:21.119111 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:22.466113 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:23.631149 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:25.025852 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:26.357454 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:27.626681 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:28.971476 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:30.309315 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:31.606927 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:32.815417 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:34.141016 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:35.383784 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:36.725231 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:38.063900 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:39.410470 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:40.753367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:42.095761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:43.267033 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:44.587158 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:45.931213 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:47.174401 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:48.459544 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:49.845491 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:51.186949 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:52.532515 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:53.868777 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:55.204609 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:56.537857 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:57.872333 [2] Warning: no training nodes in this partition! Backward fake loss.
14:46:59.207837 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:00.547270 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:01.997710 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:03.331807 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:04.648644 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:05.983347 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:07.320243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:08.653298 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:09.987367 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:11.322325 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:12.657081 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:13.991700 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:15.323472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:16.661540 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:18.000255 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:19.341119 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:20.643060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:21.959932 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:23.321534 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:24.657215 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:25.915824 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:27.276897 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:28.636930 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:29.972511 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:31.269305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:32.588360 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:33.813925 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:35.214267 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:36.519935 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:37.867913 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:39.214386 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:40.558458 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:41.902045 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:43.245525 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:44.592169 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:45.927911 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:47.137811 [2] Warning: no training nodes in this partition! Backward fake loss.
14:47:48.526639 [2] Warning: no training nodes in this partition! Backward fake loss.
16:31:49.775207 [2] proc begin: <DistEnv 2/4 nccl>
16:31:54.774554 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:31:54.783844 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:31:59.434619 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:01.608432 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:03.051769 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.386219 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:05.735605 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:07.016772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:08.385059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:09.632962 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:11.067333 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:12.410352 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:13.751669 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:15.091613 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.439725 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.781270 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.120631 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.458172 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.878585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:23.204116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:24.530344 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:25.856123 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:27.069497 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:28.429504 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:29.638766 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:31.032597 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:32.337916 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:33.678918 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:35.023104 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:36.362446 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:37.707021 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:38.877229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:40.281471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:41.604263 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:42.824470 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:44.177767 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:45.518940 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:46.861113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:48.209809 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:49.551014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:50.896334 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:52.334906 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:53.660971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:54.942848 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:56.163942 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:57.539828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:58.730559 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:00.155691 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:01.478215 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:02.847142 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:04.178678 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:05.518874 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:06.862226 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:08.199502 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:09.528765 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:10.856500 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:12.192664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:13.525331 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:14.851989 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:16.191133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:17.527071 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:18.853108 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:19.978517 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:21.386294 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:22.750441 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:23.920615 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:25.356428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:26.698030 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:28.039351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:29.198077 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:30.618352 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:31.945351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:33.117437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:34.530301 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:35.873415 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:37.220769 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:38.564091 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:39.903835 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:41.247791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:42.595000 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:43.935738 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:45.272828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:46.700648 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:48.027978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:49.355064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:50.687217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:52.019140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:53.189917 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:54.585740 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:55.921522 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:57.268384 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:58.664562 [2] Warning: no training nodes in this partition! Backward fake loss.
16:33:59.957047 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:01.295561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:02.741095 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:04.101312 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:05.274825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:06.710537 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:08.051611 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:09.346125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:10.687477 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:12.029881 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:13.376356 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:14.721629 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:16.063829 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:17.406210 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:18.638690 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:20.054219 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:21.393573 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:22.609242 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:24.035103 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:25.311196 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:26.660062 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:27.870770 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:29.268415 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:30.585453 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:31.796554 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:33.232279 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:34.569634 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:35.910405 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:37.038096 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:38.465358 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:39.791770 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:41.039272 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:42.426279 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:43.765061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:44.986240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:46.381556 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:47.690800 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:48.945670 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:50.346842 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:51.673627 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:52.926846 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:54.212184 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:55.606341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:56.771387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:58.147313 [2] Warning: no training nodes in this partition! Backward fake loss.
16:34:59.446951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:00.698651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:02.082478 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:03.528296 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:04.745715 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:06.114014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:07.454994 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:08.803536 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:10.043113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:11.457803 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:12.784282 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:14.123083 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:15.461116 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:16.796241 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:18.152990 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:19.492257 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:20.831804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:22.167925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:23.493788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:24.821594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:26.147788 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:27.313088 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:28.682910 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:29.983903 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:31.256338 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:32.667092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:34.005649 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:35.195080 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:36.590965 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:37.758040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:39.167206 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:40.504333 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:41.846377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:43.188119 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:44.528777 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:45.782577 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:47.117128 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:48.532052 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:49.709166 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:51.044559 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:52.456266 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:53.783401 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:55.125636 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:56.429608 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:57.645988 [2] Warning: no training nodes in this partition! Backward fake loss.
16:35:59.070867 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:00.242703 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:01.691601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:03.088441 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:04.428198 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:05.768650 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:07.108812 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:08.451947 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:09.790133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:11.118097 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:12.445459 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:13.543256 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:14.973579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:16.297251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:17.640584 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:18.849745 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:20.308790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:21.651221 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:22.992426 [2] Warning: no training nodes in this partition! Backward fake loss.
16:36:24.333916 [2] Warning: no training nodes in this partition! Backward fake loss.
19:15:53.673406 [2] proc begin: <DistEnv 2/4 nccl>
19:15:58.687526 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:15:58.704278 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:16:55.860363 [2] proc begin: <DistEnv 2/4 nccl>
19:17:01.909896 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:17:01.931024 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:30:46.344896 [2] proc begin: <DistEnv 2/4 nccl>
19:30:52.184997 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:30:52.207067 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:32:54.854353 [2] proc begin: <DistEnv 2/4 nccl>
19:33:00.474140 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:33:00.491164 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:33:50.537442 [2] proc begin: <DistEnv 2/4 nccl>
19:33:56.582106 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:33:56.599444 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:35:02.793870 [2] proc begin: <DistEnv 2/4 nccl>
19:35:08.773922 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:35:08.790941 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:46:46.573342 [2] proc begin: <DistEnv 2/4 nccl>
22:46:50.800533 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:46:50.810067 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:44.486371 [2] proc begin: <DistEnv 2/4 nccl>
08:22:49.433250 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
08:22:49.448807 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:22:55.833969 [2] Warning: no training nodes in this partition! Backward fake loss.
08:22:57.645273 [2] Warning: no training nodes in this partition! Backward fake loss.
08:22:58.353123 [2] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.058793 [2] Warning: no training nodes in this partition! Backward fake loss.
08:22:59.765862 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:00.474259 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.184643 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:01.925000 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:02.662279 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:03.374729 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.085923 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:04.793801 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:05.502559 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.210320 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:06.917444 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:07.625701 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:08.333876 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.040671 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:09.748510 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:10.453224 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.159498 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:11.865543 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:12.571826 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.277923 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:13.983441 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:14.687795 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:15.395043 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.100888 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:16.806339 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:17.511603 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.218434 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:18.924919 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:19.630967 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:20.336494 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.042075 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:21.747217 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:22.453449 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.158828 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:23.863216 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:24.569542 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.276408 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:25.981432 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:26.686877 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:27.391609 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.096454 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:28.801508 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:29.506760 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.212387 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:30.917940 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:31.622400 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.329447 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:32.798483 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:33.554419 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.023693 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:34.781454 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:35.250630 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.005496 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:36.474039 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.230616 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:37.698768 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.454830 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:38.923250 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:39.677320 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.145164 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:40.899702 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:41.368167 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.122598 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:42.590731 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.347071 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:43.814645 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:44.569648 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.036923 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:45.791978 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:46.259545 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.014122 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:47.481874 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.236949 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:48.704826 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.460813 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:49.928209 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:50.683370 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.151297 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:51.906951 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:52.374130 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.128276 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:53.596671 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.351658 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:54.819606 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:55.576507 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.043692 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:56.798566 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:57.266972 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.021580 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:58.488556 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.242360 [2] Warning: no training nodes in this partition! Backward fake loss.
08:23:59.709372 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.463570 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:00.929731 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:01.684938 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.159325 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:02.942984 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:03.426387 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.185883 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:04.655643 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.416424 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:05.886415 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:06.645911 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.115742 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:07.875442 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:08.344870 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.104487 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:09.574441 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.333400 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:10.803839 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:11.564416 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.033897 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:12.795428 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:13.265934 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.024252 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:14.493461 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.248371 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:15.716206 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.473618 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:16.942962 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:17.699851 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.167617 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:18.926311 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:19.393867 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.149465 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:20.616545 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.373517 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:21.841046 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:22.596532 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.063750 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:23.819459 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:24.286050 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.040541 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:25.506436 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.259772 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:26.725946 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.480522 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:27.947580 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:28.701709 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.168463 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:29.922596 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:30.389055 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.143705 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:31.611051 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.363470 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:32.830059 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:33.584684 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.051179 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:34.805511 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:35.271701 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.025928 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:36.493247 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.247717 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:37.714088 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.469345 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:38.936087 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:39.690013 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.156369 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:40.910596 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:41.377014 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.131943 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:42.599068 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.354514 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:43.820852 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:44.575925 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.042225 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:45.796368 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:46.263514 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.017308 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:47.483853 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.238374 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:48.705147 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.460969 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:49.928847 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:50.682100 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.148279 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:51.901984 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:52.369203 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.123391 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:53.590022 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.344869 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:54.811098 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:55.565868 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.032421 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:56.786140 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:57.252710 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.006992 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:58.473558 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.227715 [2] Warning: no training nodes in this partition! Backward fake loss.
08:24:59.694184 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.448723 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:00.915565 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:01.670472 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.155711 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:02.942280 [2] Warning: no training nodes in this partition! Backward fake loss.
08:25:03.416175 [2] Warning: no training nodes in this partition! Backward fake loss.
08:26:31.302078 [2] proc begin: <DistEnv 2/4 nccl>
08:26:37.942583 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
08:26:37.957135 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

08:28:25.738611 [2] proc begin: <DistEnv 2/4 nccl>
08:28:31.913478 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
08:28:31.926909 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:03.469114 [2] proc begin: <DistEnv 2/4 nccl>
10:38:09.794625 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:38:09.814106 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:40:00.801398 [2] proc begin: <DistEnv 2/4 nccl>
10:40:08.087928 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:40:08.111463 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:42:05.220713 [2] proc begin: <DistEnv 2/4 nccl>
10:42:11.326202 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:42:11.344952 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:37.824031 [2] proc begin: <DistEnv 2/4 nccl>
10:43:43.459776 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:43:43.468191 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:47.779410 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.027845 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:49.733197 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:50.438023 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.143018 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:51.848963 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:52.556262 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.260919 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:53.967894 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:54.672902 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:55.377329 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.080863 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:56.786486 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:57.491319 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.196471 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:58.901149 [2] Warning: no training nodes in this partition! Backward fake loss.
10:43:59.606636 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:00.311369 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.015663 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:01.734961 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:02.467229 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.190087 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:03.895670 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:04.603148 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:05.309967 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.017257 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:06.723175 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:07.429456 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.134685 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:08.841024 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:09.549378 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.255245 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:10.961085 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:11.666959 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:12.371375 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.075872 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:13.782576 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:14.488781 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.194907 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:15.900814 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:16.605974 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:17.310631 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.017366 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:18.724356 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:19.430969 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.136629 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:20.840658 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:21.545577 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.250153 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:22.954463 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:23.659292 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.127920 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:24.883608 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:25.351010 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.106649 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:26.575507 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.330748 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:27.799042 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:28.555474 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.023266 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:29.779233 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:30.247820 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.004426 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:31.472556 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.228191 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:32.696186 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.453240 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:33.922056 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:34.679740 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.147503 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:35.904231 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:36.372226 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.129386 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:37.597042 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.353940 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:38.822431 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:39.581672 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.049950 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:40.806766 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:41.274607 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.033163 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:42.500921 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.260645 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:43.728267 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.485522 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:44.952898 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:45.708640 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.177227 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:46.931781 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:47.399108 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.154354 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:48.622068 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.377342 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:49.845655 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:50.600800 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.067947 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:51.823149 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:52.291684 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.046514 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:53.514748 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.270055 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:54.738342 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.493311 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:55.960388 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:56.715505 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.183361 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:57.938002 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:58.404137 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.158754 [2] Warning: no training nodes in this partition! Backward fake loss.
10:44:59.625224 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.379522 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:00.847745 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:01.604026 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.071634 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:02.851918 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:03.336632 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.107504 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:04.576630 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.332790 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:05.801425 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:06.557800 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.027148 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:07.783401 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:08.253063 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.009319 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:09.479764 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.237796 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:10.706122 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.462443 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:11.931628 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:12.690416 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.158951 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:13.918039 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:14.386072 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.143163 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:15.612044 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.369165 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:16.836941 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:17.592525 [2] Warning: no training nodes in this partition! Backward fake loss.
10:45:18.060566 [2] Warning: no training nodes in this partition! Backward fake loss.
10:49:47.357666 [2] proc begin: <DistEnv 2/4 nccl>
10:49:51.478918 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:49:51.487427 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:49:58.515525 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:01.445576 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:03.325095 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:05.151854 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:06.979124 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:08.810103 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:10.639130 [2] Warning: no training nodes in this partition! Backward fake loss.
10:50:12.464971 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:14.551539 [2] proc begin: <DistEnv 2/4 nccl>
14:00:20.338256 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:00:20.348438 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:00:24.909480 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.376905 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:26.807007 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.237482 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:27.667496 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.097431 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.528385 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:28.963248 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.394768 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:29.825156 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.255795 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:30.686862 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.115767 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.546407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:31.977585 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.408104 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:32.838980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.270307 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:33.701142 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.131842 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.561858 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:34.992101 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.422198 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:35.853106 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.283997 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:36.715139 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.145666 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:37.576179 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.007053 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.436841 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:38.867161 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.296291 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:39.726659 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.156951 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:40.587304 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.018439 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.447698 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:41.878138 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.308035 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:42.738027 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.167744 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:43.597484 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.027391 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.456802 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:44.885929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.314955 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:45.744115 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.173873 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:46.603072 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.033336 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.463444 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:47.764076 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.232548 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:48.532319 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.000557 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.300833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:49.768068 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.068224 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.536675 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:50.836251 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.303614 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:51.603548 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.071180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.371339 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:52.838801 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.138983 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.607006 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:53.907019 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.374510 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:54.674903 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.143215 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.443498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:55.911289 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.211470 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.679694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:56.979850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.448634 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:57.748793 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.216929 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.516969 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:58.985647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.286485 [2] Warning: no training nodes in this partition! Backward fake loss.
14:00:59.756112 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.057173 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.525563 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:00.825682 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.294939 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:01.596419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.063604 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.375895 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:02.863487 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.174343 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.655825 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:03.956759 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.427659 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:04.728284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.198815 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.500328 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:05.970283 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.271048 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:06.740561 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.040787 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.512307 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:07.813915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.284521 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:08.586011 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.056657 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.357510 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:09.828407 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.129649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.601706 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:10.903590 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.374102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:11.675879 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.146463 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.446422 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:12.916434 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.217586 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.687801 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:13.988455 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.458380 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:14.759117 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.229827 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:15.530610 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.001629 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.302195 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:16.771914 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.071725 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.539756 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:17.839637 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.309649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:18.609785 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.079511 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.379441 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:19.848380 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.148149 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.617850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:20.918271 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.385949 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:21.684980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.152167 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.451942 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:22.920535 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.219903 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.687604 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:23.987486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.455153 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:24.754953 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.222725 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.522951 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:25.990571 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.289951 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:26.758623 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.058761 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.526222 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:27.825760 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.293819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:28.593797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.062007 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.362796 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:29.830658 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.130968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.599449 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:30.900006 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.367887 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:31.668140 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.136674 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.436620 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:32.905892 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.207820 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.676459 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:33.976300 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.444545 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:34.745329 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.214245 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.514373 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:35.983423 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.284036 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:36.752327 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.052309 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.521448 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:37.824517 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.294170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:38.594279 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.062834 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.363413 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:39.831647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.131957 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.600855 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:40.902849 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.370709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:41.671207 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.139692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.440266 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:42.909127 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.209775 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.678583 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:43.979621 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.449166 [2] Warning: no training nodes in this partition! Backward fake loss.
14:01:44.750447 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:13.724675 [2] proc begin: <DistEnv 2/4 nccl>
16:28:18.606847 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:28:18.749757 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:24.590030 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:26.480204 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.187561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:27.896432 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:28.606749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:29.316370 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.025348 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:30.733772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:31.442906 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.151014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:32.858109 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:33.567291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.275859 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:34.982708 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:35.692372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:36.402965 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.111546 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:37.820373 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:38.529139 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.236784 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:39.945152 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:40.652173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:41.360316 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.067704 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:42.775089 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:43.481953 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.188885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:44.897189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:45.604976 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:46.312094 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.019757 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:47.727170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:48.434782 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.142765 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:49.851234 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:50.558217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.266254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:51.973171 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:52.681074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:53.387929 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.096890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:54.803328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:55.511428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.218278 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:56.926148 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:57.634393 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:58.342009 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.050298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:28:59.759209 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:00.466742 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.174214 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:01.641952 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.431531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:02.916052 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:03.694053 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.161945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:04.927149 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:05.394377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.158876 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:06.626380 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.390649 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:07.857988 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:08.622456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.090334 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:09.853900 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:10.321229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.083684 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:11.550717 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.313517 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:12.781522 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:13.542643 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.009858 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:14.772140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:15.239302 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.002355 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:16.469652 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.232321 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:17.699417 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.462428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:18.929178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:19.691804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.159061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:20.921061 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:21.387960 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.150775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:22.617277 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.379708 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:23.846903 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:24.609635 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.076372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:25.838292 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:26.305544 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.067172 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:27.534541 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.296935 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:28.766437 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.528529 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:29.995750 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:30.758109 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.224722 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:31.987516 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:32.454897 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.219772 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:33.686843 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.449580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:34.916471 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:35.680482 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.146645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:36.910355 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:37.377309 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.139090 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:38.605864 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.368341 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:39.834828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:40.597717 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.064792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:41.829191 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:42.296816 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.062351 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:43.528910 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.294189 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:44.761028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.524958 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:45.993459 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:46.758343 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.226630 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:47.990877 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:48.458307 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.222220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:49.689741 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.453729 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:50.919369 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:51.682653 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.148529 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:52.912914 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:53.379182 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.143410 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:54.609823 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.372796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:55.838548 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:56.599210 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.066020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:57.827671 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:58.294160 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.057552 [2] Warning: no training nodes in this partition! Backward fake loss.
16:29:59.523853 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.285483 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:00.750980 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:01.519922 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.001515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:02.793835 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:03.263335 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.025631 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:04.493462 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.256328 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:05.724153 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.487501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:06.954857 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:07.717240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.184798 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:08.946583 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:09.413646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.177898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:10.645274 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.409443 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:11.875932 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:12.639850 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.106353 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:13.869342 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:14.336764 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.099125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:15.565408 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.328821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:16.794998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:17.557568 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.025423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:18.788484 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:19.255479 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.017414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:20.484523 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.246112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:21.712516 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.473054 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:22.939332 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:23.700176 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.167368 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:24.929118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:25.395018 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.156106 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:26.622854 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.384681 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:27.850253 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:28.612978 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.079725 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:29.841429 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:30.307378 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.068370 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:31.534297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.296152 [2] Warning: no training nodes in this partition! Backward fake loss.
16:30:32.762152 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:04.696478 [2] proc begin: <DistEnv 2/4 nccl>
16:32:09.663353 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:32:09.672152 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:15.077067 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:16.874730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:17.584232 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:18.295118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.005244 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:19.712838 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:20.420911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:21.130303 [2] Warning: no training nodes in this partition! Backward fake loss.
16:32:41.990041 [2] proc begin: <DistEnv 2/4 nccl>
16:32:47.938681 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:32:47.952432 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:40:52.896233 [2] proc begin: <DistEnv 2/4 nccl>
16:40:58.522137 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:40:58.540649 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:03.694062 [2] proc begin: <DistEnv 2/4 nccl>
16:47:08.577468 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:47:08.599633 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:47:13.132207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.984751 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.124725 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.280245 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.435155 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.591632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.747525 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.902364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.057174 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.212353 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.367123 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.522100 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.676730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.831647 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.986605 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.141443 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.296925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.453465 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.608518 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.764275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.919998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.076580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.231297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.386469 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.541302 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.696314 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.851798 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.007826 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.162698 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.318894 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.473645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.629698 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.784364 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.940462 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.095456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.250670 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.406502 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.561129 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.716350 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.870449 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.025299 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.179560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.334457 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.489236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.643889 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.798793 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.954444 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.107957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.262236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.417881 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.571863 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.726689 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.882337 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.038128 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.192267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.347087 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.502106 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.656727 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.811729 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.966089 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.120513 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.276103 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.430372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.585808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.740463 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.895114 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.050400 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.205163 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.360019 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.515857 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.670734 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.827243 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.982703 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.136836 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.291944 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.447060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.603065 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.757163 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.913406 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.067871 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.223388 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.378213 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.532654 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.687668 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.842367 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.997560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.153343 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.307114 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.461941 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.618069 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.772455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.928133 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.082259 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.237606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.393049 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.547420 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.702639 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.857790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.012275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.166993 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.322392 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.478058 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.633412 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.788743 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.942986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.097911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.253371 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.408450 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.562668 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.717876 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.871779 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.028265 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.183491 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.337827 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.491864 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.646199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.802625 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.956872 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.111703 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.266270 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.420974 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.576325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.731804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.885813 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.040329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.194969 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.349646 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.504242 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.659208 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.813962 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.968862 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.123006 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.277787 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.432081 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.587268 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.742100 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.897112 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.051589 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.206513 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.364595 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.517251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.671372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.825805 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.981151 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.135481 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.290384 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.445215 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.599563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.753816 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.907988 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.062389 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.217013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.371308 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.525517 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.679709 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.834259 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.988985 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.142670 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.296781 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.451028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.605026 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.759606 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.914602 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.069093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.223539 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.377935 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.532608 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.686942 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.841875 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.996072 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.150542 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.304936 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.460090 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.614623 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.769173 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.924859 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.078998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.233291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.389677 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.544186 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.699010 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.853680 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.009044 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.163804 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.318397 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.472720 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.627156 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.782791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.937223 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.091928 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.247033 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.401879 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.556596 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.711560 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.866247 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.020767 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.175339 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.330401 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.484609 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.639255 [2] Warning: no training nodes in this partition! Backward fake loss.
15:45:35.448828 [2] proc begin: <DistEnv 2/4 nccl>
15:45:53.442359 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
15:45:53.467644 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:46:08.895210 [2] proc begin: <DistEnv 2/4 nccl>
15:46:09.022567 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:46:09.043675 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:26.703836 [2] proc begin: <DistEnv 2/4 nccl>
15:50:26.902908 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:50:26.918485 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:50:28.021066 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:52:58.419942 [2] proc begin: <DistEnv 2/4 nccl>
15:52:58.511528 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
15:52:58.521799 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:52:59.536018 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:00.314648 [2] L2 tensor(438.5738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:00.356411 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.107285 [2] L1 tensor(91832.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.121428 [2] L2 tensor(442.4138, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.170226 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.200456 [2] L1 tensor(91855.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.247365 [2] L2 tensor(445.8104, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.249803 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.323152 [2] L1 tensor(91963.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.325870 [2] L2 tensor(448.3091, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.352053 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.406353 [2] L1 tensor(92143.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.429174 [2] L2 tensor(450.3099, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.478063 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.508256 [2] L1 tensor(92384.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.555004 [2] L2 tensor(452.1290, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.557412 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.633609 [2] L1 tensor(92675.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.638355 [2] L2 tensor(454.0485, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.661210 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.677700 [2] L1 tensor(93008.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.720661 [2] L2 tensor(456.0366, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.723095 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.797930 [2] L1 tensor(93376.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.800299 [2] L2 tensor(457.9691, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.825603 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.880492 [2] L1 tensor(93775.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.903814 [2] L2 tensor(459.7110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:01.953078 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:01.980764 [2] L1 tensor(94198.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.030764 [2] L2 tensor(461.1994, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.034065 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.106987 [2] L1 tensor(94641.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.112249 [2] L2 tensor(462.4901, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.136225 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.191724 [2] L1 tensor(95100.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.213294 [2] L2 tensor(463.6808, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.262544 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.290266 [2] L1 tensor(95573.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.340128 [2] L2 tensor(464.8087, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.343510 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.416069 [2] L1 tensor(96059.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.419255 [2] L2 tensor(465.8306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.443200 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.499015 [2] L1 tensor(96555.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.521469 [2] L2 tensor(466.6919, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.568240 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.599307 [2] L1 tensor(97059.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.646634 [2] L2 tensor(467.4421, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.649943 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.725379 [2] L1 tensor(97571.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.731864 [2] L2 tensor(468.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.753922 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.805841 [2] L1 tensor(98089.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.814136 [2] L2 tensor(469.1948, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.834895 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.890884 [2] L1 tensor(98612.5547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.912508 [2] L2 tensor(470.4097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:02.962214 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:02.992250 [2] L1 tensor(99140.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.039767 [2] L2 tensor(471.9096, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.043212 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.074813 [2] L1 tensor(99671.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.078540 [2] L2 tensor(473.7235, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.082229 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.137528 [2] L1 tensor(100206.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.158487 [2] L2 tensor(475.8296, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.207712 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.236440 [2] L1 tensor(100741.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.285983 [2] L2 tensor(478.1782, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.289443 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.364267 [2] L1 tensor(101279.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.369279 [2] L2 tensor(480.7126, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.392671 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.446087 [2] L1 tensor(101816.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.468295 [2] L2 tensor(483.3638, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.517413 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.546989 [2] L1 tensor(102353.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.596195 [2] L2 tensor(486.0809, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.599547 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.672701 [2] L1 tensor(102887.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.677349 [2] L2 tensor(488.8194, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.700245 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.756446 [2] L1 tensor(103417.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.776090 [2] L2 tensor(491.5266, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.825063 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.853984 [2] L1 tensor(103939.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.903283 [2] L2 tensor(494.1455, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.905747 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:03.981128 [2] L1 tensor(104451.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:03.985900 [2] L2 tensor(496.6190, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.008784 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.019618 [2] L1 tensor(104950.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.045156 [2] L2 tensor(498.9019, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.069839 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.144199 [2] L1 tensor(105436.0234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.149093 [2] L2 tensor(500.9770, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.172091 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.228110 [2] L1 tensor(105906.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.248069 [2] L2 tensor(502.8625, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.297111 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.326174 [2] L1 tensor(106361.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.375303 [2] L2 tensor(504.6041, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.377891 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.451577 [2] L1 tensor(106804.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.454036 [2] L2 tensor(506.2580, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.479268 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.532466 [2] L1 tensor(107234.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.554628 [2] L2 tensor(507.8826, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.603635 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.632104 [2] L1 tensor(107654.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.681432 [2] L2 tensor(509.5375, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.684317 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.757844 [2] L1 tensor(108064.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.761624 [2] L2 tensor(511.2832, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.784509 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.838678 [2] L1 tensor(108465.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.860907 [2] L2 tensor(513.1110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.909886 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:04.937067 [2] L1 tensor(108857.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.986406 [2] L2 tensor(514.9779, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:04.988865 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.064751 [2] L1 tensor(109240.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.069345 [2] L2 tensor(516.8826, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.092253 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.145571 [2] L1 tensor(109615.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.167811 [2] L2 tensor(518.7771, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.216840 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.243848 [2] L1 tensor(109982.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.293235 [2] L2 tensor(520.5668, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.295719 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.370291 [2] L1 tensor(110338.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.374957 [2] L2 tensor(522.2884, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.398045 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.451371 [2] L1 tensor(110692.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.473410 [2] L2 tensor(524.0200, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.522375 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.549530 [2] L1 tensor(111051.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.598762 [2] L2 tensor(525.9103, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.601201 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.675785 [2] L1 tensor(111417.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.680334 [2] L2 tensor(528.1434, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.703172 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.758329 [2] L1 tensor(111786.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.778485 [2] L2 tensor(530.8064, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.827619 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.854962 [2] L1 tensor(112153.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.904166 [2] L2 tensor(533.7210, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.906786 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:05.980475 [2] L1 tensor(112519.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:05.985120 [2] L2 tensor(536.6836, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.009818 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.063460 [2] L1 tensor(112882.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.085534 [2] L2 tensor(539.6154, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.133483 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.163431 [2] L1 tensor(113245.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.212847 [2] L2 tensor(542.6844, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.215406 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.254144 [2] L1 tensor(113612.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.290519 [2] L2 tensor(546.6610, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.293097 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.369319 [2] L1 tensor(113989.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.371915 [2] L2 tensor(551.9967, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.397253 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.454070 [2] L1 tensor(114380.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.473817 [2] L2 tensor(558.6206, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.522770 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.551753 [2] L1 tensor(114780.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.601096 [2] L2 tensor(565.5466, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.603636 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.678868 [2] L1 tensor(115179.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.683673 [2] L2 tensor(571.8951, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.706736 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.763160 [2] L1 tensor(115564.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.783078 [2] L2 tensor(577.6732, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.832222 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.859263 [2] L1 tensor(115930.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.908422 [2] L2 tensor(582.9789, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.910958 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:06.984478 [2] L1 tensor(116275.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:06.989136 [2] L2 tensor(587.8942, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.011944 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.065141 [2] L1 tensor(116604.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.087265 [2] L2 tensor(592.4735, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.136686 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.165568 [2] L1 tensor(116924.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.215255 [2] L2 tensor(596.7705, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.217863 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.293205 [2] L1 tensor(117243.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.297691 [2] L2 tensor(600.9476, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.320853 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.375321 [2] L1 tensor(117559.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.398129 [2] L2 tensor(605.2831, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.445984 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.475818 [2] L1 tensor(117869.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.525422 [2] L2 tensor(609.3708, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.528297 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.603803 [2] L1 tensor(118170.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.608099 [2] L2 tensor(613.2331, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.632773 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.688780 [2] L1 tensor(118466.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.708492 [2] L2 tensor(616.9452, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.757867 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.786549 [2] L1 tensor(118757.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.835703 [2] L2 tensor(620.6169, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.838196 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.912540 [2] L1 tensor(119055.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.916315 [2] L2 tensor(624.6840, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:07.939272 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:07.995802 [2] L1 tensor(119353.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.016020 [2] L2 tensor(629.0459, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.063952 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.093382 [2] L1 tensor(119651.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.143154 [2] L2 tensor(633.7097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.145702 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.222049 [2] L1 tensor(119951.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.226253 [2] L2 tensor(638.6428, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.249356 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.306260 [2] L1 tensor(120246.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.327430 [2] L2 tensor(643.5590, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.377021 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.406459 [2] L1 tensor(120528.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.455789 [2] L2 tensor(648.2156, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.458418 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.535004 [2] L1 tensor(120794.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.539523 [2] L2 tensor(652.5349, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.563028 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.620920 [2] L1 tensor(121047.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.641087 [2] L2 tensor(656.5295, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.690475 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.719290 [2] L1 tensor(121292.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.725098 [2] L2 tensor(660.2439, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.728306 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.736278 [2] L1 tensor(121528.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.785929 [2] L2 tensor(663.6747, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.788367 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.863580 [2] L1 tensor(121754.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.867752 [2] L2 tensor(666.8158, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.890856 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:08.945540 [2] L1 tensor(121969.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:08.968003 [2] L2 tensor(669.6705, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.017150 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.046275 [2] L1 tensor(122171.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.095571 [2] L2 tensor(672.2507, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.097830 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.172982 [2] L1 tensor(122361.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.178072 [2] L2 tensor(674.5707, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.200505 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.258184 [2] L1 tensor(122540.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.278462 [2] L2 tensor(676.6443, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.327673 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.356825 [2] L1 tensor(122708.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.406269 [2] L2 tensor(678.4858, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.408238 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.484375 [2] L1 tensor(122865.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.486336 [2] L2 tensor(680.1101, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.511766 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.568838 [2] L1 tensor(123012.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.589856 [2] L2 tensor(681.5339, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.638781 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.667620 [2] L1 tensor(123150.4297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.716950 [2] L2 tensor(682.7748, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.719004 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.752713 [2] L1 tensor(123279.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.756099 [2] L2 tensor(683.8511, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.759607 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.769292 [2] L1 tensor(123401.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.818217 [2] L2 tensor(684.7797, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.820239 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.895847 [2] L1 tensor(123515.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.897766 [2] L2 tensor(685.5751, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:09.923444 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:09.980955 [2] L1 tensor(123622.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.000956 [2] L2 tensor(686.2515, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.049790 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.078715 [2] L1 tensor(123723.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.128187 [2] L2 tensor(686.8276, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.130451 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.206536 [2] L1 tensor(123817.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.208390 [2] L2 tensor(687.3495, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.234178 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.291235 [2] L1 tensor(123907.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.311480 [2] L2 tensor(687.9585, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.316764 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.326148 [2] L1 tensor(123994.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.350550 [2] L2 tensor(688.7996, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.375939 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.405869 [2] L1 tensor(124077.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.452771 [2] L2 tensor(689.6537, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.454925 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.531337 [2] L1 tensor(124155.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.533334 [2] L2 tensor(690.4276, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.558890 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.615340 [2] L1 tensor(124230.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.636208 [2] L2 tensor(691.1127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.638353 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.645864 [2] L1 tensor(124302., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.651245 [2] L2 tensor(691.7106, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.656624 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.665137 [2] L1 tensor(124369.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.671864 [2] L2 tensor(692.2195, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.675712 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.686644 [2] L1 tensor(124433.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.690014 [2] L2 tensor(692.6395, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.693190 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.704535 [2] L1 tensor(124492.7578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.710703 [2] L2 tensor(692.9738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.715183 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.722084 [2] L1 tensor(124548.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.725226 [2] L2 tensor(693.2272, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.727755 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.734652 [2] L1 tensor(124601.5703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.737614 [2] L2 tensor(693.4050, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.740748 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.747258 [2] L1 tensor(124651.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.749853 [2] L2 tensor(693.5128, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.752192 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.758509 [2] L1 tensor(124698.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.761486 [2] L2 tensor(693.5555, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.763873 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.770257 [2] L1 tensor(124742.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.773087 [2] L2 tensor(693.5380, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.775615 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.782845 [2] L1 tensor(124784.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.785769 [2] L2 tensor(693.4649, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.788778 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.795901 [2] L1 tensor(124825.0234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.798969 [2] L2 tensor(693.3407, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.801636 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.808494 [2] L1 tensor(124863.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.811536 [2] L2 tensor(693.1694, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.814022 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.821613 [2] L1 tensor(124900.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.824171 [2] L2 tensor(692.9548, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.826544 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.833989 [2] L1 tensor(124935.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.837026 [2] L2 tensor(692.7004, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.840249 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.847727 [2] L1 tensor(124969.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.851134 [2] L2 tensor(692.4095, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.854158 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.860439 [2] L1 tensor(125001.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.863328 [2] L2 tensor(692.0851, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.866348 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.873323 [2] L1 tensor(125033.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.875750 [2] L2 tensor(691.7300, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.878216 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.886694 [2] L1 tensor(125063.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.894085 [2] L2 tensor(691.3472, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.896754 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.905328 [2] L1 tensor(125092.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.908868 [2] L2 tensor(690.9393, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.911940 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.918064 [2] L1 tensor(125120.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.920498 [2] L2 tensor(690.5090, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.922813 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.929455 [2] L1 tensor(125148.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.932255 [2] L2 tensor(690.0588, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.934691 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.942005 [2] L1 tensor(125174.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.944831 [2] L2 tensor(689.5912, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.947961 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.953716 [2] L1 tensor(125199.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.956261 [2] L2 tensor(689.1080, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.958323 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.964166 [2] L1 tensor(125224.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.967925 [2] L2 tensor(688.6113, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.970517 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.976787 [2] L1 tensor(125248.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.979363 [2] L2 tensor(688.1028, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.982195 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:10.988194 [2] L1 tensor(125271.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.990653 [2] L2 tensor(687.5839, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:10.993069 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.000869 [2] L1 tensor(125294.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.003506 [2] L2 tensor(687.0554, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.006183 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.013320 [2] L1 tensor(125316.4453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.016194 [2] L2 tensor(686.5184, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.019072 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.027786 [2] L1 tensor(125337.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.031270 [2] L2 tensor(685.9733, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.034954 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.042392 [2] L1 tensor(125359.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.045195 [2] L2 tensor(685.4206, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.047710 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.055716 [2] L1 tensor(125379.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.058347 [2] L2 tensor(684.8604, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.061644 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.068766 [2] L1 tensor(125399.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.071305 [2] L2 tensor(684.2928, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.074034 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.081034 [2] L1 tensor(125419.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.083498 [2] L2 tensor(683.7180, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.085623 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.094077 [2] L1 tensor(125439.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.097943 [2] L2 tensor(683.1360, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.102136 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.109140 [2] L1 tensor(125459.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.112436 [2] L2 tensor(682.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.116190 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.124851 [2] L1 tensor(125478.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.127554 [2] L2 tensor(681.9510, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.129648 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.136404 [2] L1 tensor(125497.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.139179 [2] L2 tensor(681.3486, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.142323 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.150238 [2] L1 tensor(125517.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.152936 [2] L2 tensor(680.7401, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.155818 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.162970 [2] L1 tensor(125536.6797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.165598 [2] L2 tensor(680.1261, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.168127 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.175681 [2] L1 tensor(125556.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.178579 [2] L2 tensor(679.5076, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.181994 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.191953 [2] L1 tensor(125576.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.198336 [2] L2 tensor(678.8854, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.202918 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.213400 [2] L1 tensor(125596.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.217974 [2] L2 tensor(678.2612, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.222729 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.231911 [2] L1 tensor(125617.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.238351 [2] L2 tensor(677.6365, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.243096 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.253378 [2] L1 tensor(125639.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.257612 [2] L2 tensor(677.0134, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.261019 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.267799 [2] L1 tensor(125663.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.270434 [2] L2 tensor(676.3940, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.273072 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.278951 [2] L1 tensor(125687.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.281896 [2] L2 tensor(675.7798, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.284120 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.292165 [2] L1 tensor(125712.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.295772 [2] L2 tensor(675.1722, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.298969 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.306126 [2] L1 tensor(125739.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.308559 [2] L2 tensor(674.5723, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.310778 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.316670 [2] L1 tensor(125766.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.319317 [2] L2 tensor(673.9815, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.321607 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.327474 [2] L1 tensor(125795.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.331494 [2] L2 tensor(673.4016, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.334038 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.343181 [2] L1 tensor(125824.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.346032 [2] L2 tensor(672.8350, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.348912 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.355085 [2] L1 tensor(125854.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.357608 [2] L2 tensor(672.2845, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.359798 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.366453 [2] L1 tensor(125885.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.369300 [2] L2 tensor(671.7540, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.371808 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.378598 [2] L1 tensor(125916.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.381216 [2] L2 tensor(671.2478, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.383867 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.389694 [2] L1 tensor(125948.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.392327 [2] L2 tensor(670.7714, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.394389 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.400690 [2] L1 tensor(125981.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.403499 [2] L2 tensor(670.3314, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.405834 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.412379 [2] L1 tensor(126013.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.415088 [2] L2 tensor(669.9373, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.417733 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.423635 [2] L1 tensor(126045.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.426085 [2] L2 tensor(669.6034, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.428597 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.436895 [2] L1 tensor(126077., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.439405 [2] L2 tensor(669.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.441906 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.448674 [2] L1 tensor(126108.1641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.451313 [2] L2 tensor(669.2137, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.453790 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.459959 [2] L1 tensor(126138.6797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.462850 [2] L2 tensor(669.2305, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.465515 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.473177 [2] L1 tensor(126168.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.478517 [2] L2 tensor(669.4468, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.482949 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.493531 [2] L1 tensor(126197.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.497633 [2] L2 tensor(669.9043, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.501485 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.512390 [2] L1 tensor(126226.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.517749 [2] L2 tensor(670.6352, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.520277 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.529696 [2] L1 tensor(126254.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.532598 [2] L2 tensor(671.6612, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.535621 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.541608 [2] L1 tensor(126281.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.544326 [2] L2 tensor(672.9954, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.546842 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.553146 [2] L1 tensor(126309.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.555755 [2] L2 tensor(674.6449, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.558524 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.566443 [2] L1 tensor(126336.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.570137 [2] L2 tensor(676.6127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.573788 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.580461 [2] L1 tensor(126364.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.583626 [2] L2 tensor(678.8989, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.586601 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.592448 [2] L1 tensor(126392.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.594976 [2] L2 tensor(681.5011, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.597403 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.603457 [2] L1 tensor(126423.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.605948 [2] L2 tensor(684.4136, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.608370 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.615345 [2] L1 tensor(126455.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.617842 [2] L2 tensor(687.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.620555 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.626888 [2] L1 tensor(126491.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.629606 [2] L2 tensor(691.1115, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.632327 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.638585 [2] L1 tensor(126529.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.641384 [2] L2 tensor(694.8279, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.644212 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.650562 [2] L1 tensor(126570.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.653310 [2] L2 tensor(698.7007, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.656080 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.663688 [2] L1 tensor(126612.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.666446 [2] L2 tensor(702.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.669254 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.676986 [2] L1 tensor(126656.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.679824 [2] L2 tensor(706.5289, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.682368 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.688871 [2] L1 tensor(126701.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.691277 [2] L2 tensor(710.3220, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.693851 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.701096 [2] L1 tensor(126748.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.704337 [2] L2 tensor(713.9838, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.707122 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.715112 [2] L1 tensor(126795.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.717604 [2] L2 tensor(717.5139, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.720552 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.730180 [2] L1 tensor(126842.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.734550 [2] L2 tensor(720.9189, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.738553 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.749037 [2] L1 tensor(126890.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.753071 [2] L2 tensor(724.1981, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.757196 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.765320 [2] L1 tensor(126936.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.768479 [2] L2 tensor(727.3528, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.772068 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.781678 [2] L1 tensor(126980.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.784033 [2] L2 tensor(730.3943, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.786443 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.792288 [2] L1 tensor(127022.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.794740 [2] L2 tensor(733.3419, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.797121 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.804122 [2] L1 tensor(127063.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.809939 [2] L2 tensor(736.2206, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.814746 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.824807 [2] L1 tensor(127101.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.828839 [2] L2 tensor(739.0592, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.832581 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.843990 [2] L1 tensor(127138.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.849880 [2] L2 tensor(741.8875, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.852313 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.862410 [2] L1 tensor(127172.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.865764 [2] L2 tensor(744.7334, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.869186 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.878792 [2] L1 tensor(127205.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.883009 [2] L2 tensor(747.6213, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.886807 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.893882 [2] L1 tensor(127237.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.896395 [2] L2 tensor(750.5713, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.899136 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.905332 [2] L1 tensor(127267.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.908020 [2] L2 tensor(753.6005, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.910697 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.916899 [2] L1 tensor(127297.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.919389 [2] L2 tensor(756.7241, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.921833 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.927753 [2] L1 tensor(127326.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.930388 [2] L2 tensor(759.9565, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.932797 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.938865 [2] L1 tensor(127356.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.941261 [2] L2 tensor(763.3117, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.943841 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.950346 [2] L1 tensor(127388.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.956181 [2] L2 tensor(766.8024, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.959788 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.967942 [2] L1 tensor(127423.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.970460 [2] L2 tensor(770.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.973530 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.979027 [2] L1 tensor(127462.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.981884 [2] L2 tensor(774.2180, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.984306 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:11.992353 [2] L1 tensor(127508.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.995128 [2] L2 tensor(778.1348, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:11.997628 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.005008 [2] L1 tensor(127560.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:12.007907 [2] L2 tensor(782.1676, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:12.011170 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:12.018599 [2] L1 tensor(127616.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
15:53:12.021129 [2] L2 tensor(786.2880, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
15:53:12.023573 [2] Warning: no training nodes in this partition! Backward fake loss.
15:53:58.358934 [2] proc begin: <DistEnv 2/4 nccl>
15:54:15.437967 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
15:54:15.449810 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:54:16.363146 [2] L1 tensor(6418.3506, device='cuda:2', grad_fn=<SumBackward0>) tensor(131.0658, device='cuda:2', grad_fn=<SumBackward0>)
16:28:41.093906 [2] proc begin: <DistEnv 2/4 nccl>
16:28:54.388898 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:28:54.408754 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:55.893663 [2] L1 tensor(38548.0586, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:2', grad_fn=<SumBackward0>)
17:02:50.153979 [2] proc begin: <DistEnv 2/4 nccl>
17:03:07.129646 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:03:07.149112 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:08.185880 [2] L1 tensor(38548.0586, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:2', grad_fn=<SumBackward0>)
17:04:43.452200 [2] proc begin: <DistEnv 2/4 nccl>
17:04:49.690946 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:04:49.711637 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:04:50.725951 [2] L1 tensor(38548.0586, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:2', grad_fn=<SumBackward0>)
17:05:11.577333 [2] proc begin: <DistEnv 2/4 nccl>
17:05:17.873003 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:05:17.892896 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:05:18.844374 [2] L1 tensor(38548.0586, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:2', grad_fn=<SumBackward0>)
17:19:05.463315 [2] proc begin: <DistEnv 2/4 nccl>
17:19:05.766313 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:19:05.779761 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:19:06.865950 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:07.700662 [2] L2 tensor(438.5738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:07.762713 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.643266 [2] L1 tensor(91832.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.646888 [2] L2 tensor(442.4138, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.649907 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.661480 [2] L1 tensor(91855.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.665316 [2] L2 tensor(445.8104, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.669195 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.677655 [2] L1 tensor(91963.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.680490 [2] L2 tensor(448.3092, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.683235 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.694729 [2] L1 tensor(92143.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.701398 [2] L2 tensor(450.3099, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.704180 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.715161 [2] L1 tensor(92384.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.720487 [2] L2 tensor(452.1290, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.725424 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.735288 [2] L1 tensor(92675.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.739450 [2] L2 tensor(454.0485, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.743272 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.752818 [2] L1 tensor(93008.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.755951 [2] L2 tensor(456.0366, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.759082 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.767788 [2] L1 tensor(93376.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.770933 [2] L2 tensor(457.9691, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.773846 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.783597 [2] L1 tensor(93775.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.786144 [2] L2 tensor(459.7110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.788573 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.795108 [2] L1 tensor(94198.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.798018 [2] L2 tensor(461.1994, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.800724 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.807324 [2] L1 tensor(94641.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.810176 [2] L2 tensor(462.4901, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.812795 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.819111 [2] L1 tensor(95100.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.821950 [2] L2 tensor(463.6807, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.824461 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.833895 [2] L1 tensor(95573.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.841028 [2] L2 tensor(464.8087, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.843630 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.851335 [2] L1 tensor(96059.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.854254 [2] L2 tensor(465.8305, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.857190 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.863393 [2] L1 tensor(96555.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.866125 [2] L2 tensor(466.6918, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.868609 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.874873 [2] L1 tensor(97059.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.877721 [2] L2 tensor(467.4421, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.880361 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.887424 [2] L1 tensor(97571.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.890363 [2] L2 tensor(468.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.893051 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.899844 [2] L1 tensor(98089.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.902552 [2] L2 tensor(469.1947, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.905180 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.912049 [2] L1 tensor(98612.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.915464 [2] L2 tensor(470.4096, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.919368 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.926225 [2] L1 tensor(99140.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.932878 [2] L2 tensor(471.9096, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.935783 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.946636 [2] L1 tensor(99671.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.952563 [2] L2 tensor(473.7235, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.955686 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.965920 [2] L1 tensor(100206.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.969106 [2] L2 tensor(475.8296, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.972075 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.980512 [2] L1 tensor(100741.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.983976 [2] L2 tensor(478.1783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.987651 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:08.996015 [2] L1 tensor(101279.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:08.998503 [2] L2 tensor(480.7126, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.001095 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.008800 [2] L1 tensor(101816.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.012393 [2] L2 tensor(483.3637, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.015795 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.025606 [2] L1 tensor(102353.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.031714 [2] L2 tensor(486.0808, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.035398 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.046355 [2] L1 tensor(102887.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.053109 [2] L2 tensor(488.8194, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.056324 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.067091 [2] L1 tensor(103417.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.072974 [2] L2 tensor(491.5267, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.077908 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.086563 [2] L1 tensor(103939.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.089349 [2] L2 tensor(494.1456, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.091783 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.102256 [2] L1 tensor(104451.4453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.105422 [2] L2 tensor(496.6192, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.108441 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.120446 [2] L1 tensor(104950.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.125873 [2] L2 tensor(498.9021, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.128666 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.137982 [2] L1 tensor(105436.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.140857 [2] L2 tensor(500.9772, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.143359 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.153440 [2] L1 tensor(105906.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.156506 [2] L2 tensor(502.8627, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.159604 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.165988 [2] L1 tensor(106361.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.168592 [2] L2 tensor(504.6043, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.171045 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.180010 [2] L1 tensor(106804.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.183006 [2] L2 tensor(506.2584, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.185875 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.192219 [2] L1 tensor(107234.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.195014 [2] L2 tensor(507.8830, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.197512 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.205237 [2] L1 tensor(107654.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.208041 [2] L2 tensor(509.5380, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.210788 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.218292 [2] L1 tensor(108064.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.221472 [2] L2 tensor(511.2837, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.224709 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.234320 [2] L1 tensor(108465.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.237683 [2] L2 tensor(513.1116, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.240843 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.251114 [2] L1 tensor(108857.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.255986 [2] L2 tensor(514.9783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.260424 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.267542 [2] L1 tensor(109240.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.270307 [2] L2 tensor(516.8829, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.273126 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.279539 [2] L1 tensor(109615.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.282124 [2] L2 tensor(518.7773, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.284556 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.290897 [2] L1 tensor(109982.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.293551 [2] L2 tensor(520.5670, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.296009 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.304076 [2] L1 tensor(110338.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.307206 [2] L2 tensor(522.2886, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.310363 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.316780 [2] L1 tensor(110692.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.319331 [2] L2 tensor(524.0200, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.321788 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.329301 [2] L1 tensor(111051.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.333046 [2] L2 tensor(525.9098, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.336230 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.343387 [2] L1 tensor(111416.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.346326 [2] L2 tensor(528.1423, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.349421 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.359595 [2] L1 tensor(111785.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.362663 [2] L2 tensor(530.8048, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.365356 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.376678 [2] L1 tensor(112153.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.381245 [2] L2 tensor(533.7192, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.385574 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.396424 [2] L1 tensor(112519.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.400533 [2] L2 tensor(536.6816, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.404807 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.411016 [2] L1 tensor(112882.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.415365 [2] L2 tensor(539.6133, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.420329 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.430146 [2] L1 tensor(113244.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.434552 [2] L2 tensor(542.6822, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.438141 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.446970 [2] L1 tensor(113612.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.451735 [2] L2 tensor(546.6586, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.455396 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.467332 [2] L1 tensor(113989.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.473532 [2] L2 tensor(551.9941, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.478067 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.488787 [2] L1 tensor(114379.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.492871 [2] L2 tensor(558.6183, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.496921 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.502759 [2] L1 tensor(114780.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.505528 [2] L2 tensor(565.5444, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.508545 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.514834 [2] L1 tensor(115178.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.517705 [2] L2 tensor(571.8928, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.520348 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.527549 [2] L1 tensor(115564.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.530613 [2] L2 tensor(577.6707, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.533047 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.539460 [2] L1 tensor(115930.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.542293 [2] L2 tensor(582.9760, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.544753 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.551299 [2] L1 tensor(116275.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.553894 [2] L2 tensor(587.8910, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.556385 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.565227 [2] L1 tensor(116603.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.567929 [2] L2 tensor(592.4698, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.570479 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.578607 [2] L1 tensor(116924.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.581751 [2] L2 tensor(596.7665, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.584983 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.594253 [2] L1 tensor(117242.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.597095 [2] L2 tensor(600.9441, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.599764 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.606265 [2] L1 tensor(117559.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.609113 [2] L2 tensor(605.2783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.611750 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.620216 [2] L1 tensor(117869.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.623161 [2] L2 tensor(609.3643, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.626023 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.632821 [2] L1 tensor(118170.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.635663 [2] L2 tensor(613.2253, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.640026 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.647043 [2] L1 tensor(118465.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.650471 [2] L2 tensor(616.9367, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.653333 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.663512 [2] L1 tensor(118757.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.666437 [2] L2 tensor(620.6086, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.669388 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.677379 [2] L1 tensor(119055.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.680096 [2] L2 tensor(624.6782, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.682623 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.693431 [2] L1 tensor(119354.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.696794 [2] L2 tensor(629.0445, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.700203 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.709898 [2] L1 tensor(119652.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.714017 [2] L2 tensor(633.7127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.718173 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.728313 [2] L1 tensor(119952.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.733114 [2] L2 tensor(638.6491, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.737020 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.747706 [2] L1 tensor(120247.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.751906 [2] L2 tensor(643.5670, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.755697 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.762961 [2] L1 tensor(120529.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.766070 [2] L2 tensor(648.2248, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.768887 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.775086 [2] L1 tensor(120795.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.777727 [2] L2 tensor(652.5451, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.780251 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.787197 [2] L1 tensor(121048.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.791335 [2] L2 tensor(656.5410, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.793941 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.801059 [2] L1 tensor(121293.3203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.803653 [2] L2 tensor(660.2567, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.806155 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.812086 [2] L1 tensor(121529.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.814851 [2] L2 tensor(663.6885, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.817570 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.824612 [2] L1 tensor(121756.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.827509 [2] L2 tensor(666.8307, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.830485 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.845232 [2] L1 tensor(121970.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.850927 [2] L2 tensor(669.6866, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.855147 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.864385 [2] L1 tensor(122173.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.867538 [2] L2 tensor(672.2680, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.870301 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.878662 [2] L1 tensor(122363.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.881378 [2] L2 tensor(674.5891, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.884148 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.892204 [2] L1 tensor(122542.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.895118 [2] L2 tensor(676.6639, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.898068 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.904099 [2] L1 tensor(122710.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.906785 [2] L2 tensor(678.5065, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.909311 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.915705 [2] L1 tensor(122867.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.921100 [2] L2 tensor(680.1316, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.923958 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.931454 [2] L1 tensor(123014.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.934271 [2] L2 tensor(681.5560, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.936752 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.946679 [2] L1 tensor(123152.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.949928 [2] L2 tensor(682.7974, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.953423 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.960738 [2] L1 tensor(123282.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.963743 [2] L2 tensor(683.8741, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.967170 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.973269 [2] L1 tensor(123403.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.975755 [2] L2 tensor(684.8029, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.978201 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.984747 [2] L1 tensor(123517.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.987447 [2] L2 tensor(685.5984, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:09.989982 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.997762 [2] L1 tensor(123625.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.000717 [2] L2 tensor(686.2748, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.003692 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.011081 [2] L1 tensor(123725.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.013755 [2] L2 tensor(686.8505, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.016808 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.023280 [2] L1 tensor(123819.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.025858 [2] L2 tensor(687.3707, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.028377 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.037347 [2] L1 tensor(123909.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.040092 [2] L2 tensor(687.9740, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.042557 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.050925 [2] L1 tensor(123996.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.054036 [2] L2 tensor(688.8097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.057366 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.063906 [2] L1 tensor(124079.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.066587 [2] L2 tensor(689.6624, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.069044 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.075212 [2] L1 tensor(124158.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.077870 [2] L2 tensor(690.4357, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.080290 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.086888 [2] L1 tensor(124233.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.089870 [2] L2 tensor(691.1203, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.092677 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.100622 [2] L1 tensor(124304.3203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.103220 [2] L2 tensor(691.7180, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.106051 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.113383 [2] L1 tensor(124371.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.116171 [2] L2 tensor(692.2268, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.118743 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.125199 [2] L1 tensor(124435.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.128398 [2] L2 tensor(692.6467, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.131313 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.137763 [2] L1 tensor(124495.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.141350 [2] L2 tensor(692.9810, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.143899 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.150309 [2] L1 tensor(124551.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.153251 [2] L2 tensor(693.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.156053 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.162489 [2] L1 tensor(124603.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.165640 [2] L2 tensor(693.4122, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.168559 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.179210 [2] L1 tensor(124653.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.185853 [2] L2 tensor(693.5199, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.190489 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.198736 [2] L1 tensor(124700.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.201699 [2] L2 tensor(693.5624, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.204203 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.210828 [2] L1 tensor(124744.7734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.213512 [2] L2 tensor(693.5448, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.216295 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.223079 [2] L1 tensor(124786.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.225786 [2] L2 tensor(693.4716, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.228381 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.236291 [2] L1 tensor(124827.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.239558 [2] L2 tensor(693.3471, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.243024 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.250432 [2] L1 tensor(124865.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.253477 [2] L2 tensor(693.1757, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.256670 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.263174 [2] L1 tensor(124902.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.266098 [2] L2 tensor(692.9608, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.268617 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.275015 [2] L1 tensor(124937.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.277889 [2] L2 tensor(692.7062, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.280405 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.289411 [2] L1 tensor(124971.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.294002 [2] L2 tensor(692.4150, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.298535 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.307071 [2] L1 tensor(125003.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.310493 [2] L2 tensor(692.0902, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.313969 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.320517 [2] L1 tensor(125035.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.323013 [2] L2 tensor(691.7349, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.325612 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.331815 [2] L1 tensor(125065.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.334425 [2] L2 tensor(691.3517, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.336929 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.342985 [2] L1 tensor(125094.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.345875 [2] L2 tensor(690.9435, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.348599 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.355285 [2] L1 tensor(125122.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.358223 [2] L2 tensor(690.5129, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.360833 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.367149 [2] L1 tensor(125149.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.369800 [2] L2 tensor(690.0624, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.372229 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.378526 [2] L1 tensor(125176., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.381256 [2] L2 tensor(689.5944, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.384314 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.390215 [2] L1 tensor(125201.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.393276 [2] L2 tensor(689.1108, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.395781 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.402699 [2] L1 tensor(125226.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.405259 [2] L2 tensor(688.6139, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.407685 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.413907 [2] L1 tensor(125249.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.416733 [2] L2 tensor(688.1051, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.419260 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.428139 [2] L1 tensor(125273.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.431887 [2] L2 tensor(687.5858, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.434920 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.442389 [2] L1 tensor(125295.7734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.444914 [2] L2 tensor(687.0573, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.447713 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.453867 [2] L1 tensor(125317.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.456673 [2] L2 tensor(686.5200, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.459327 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.465851 [2] L1 tensor(125339.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.468642 [2] L2 tensor(685.9749, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.471167 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.477866 [2] L1 tensor(125360.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.480766 [2] L2 tensor(685.4220, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.483486 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.490183 [2] L1 tensor(125380.9922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.493087 [2] L2 tensor(684.8618, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.495860 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.502401 [2] L1 tensor(125401.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.506005 [2] L2 tensor(684.2943, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.508601 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.515432 [2] L1 tensor(125421.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.518181 [2] L2 tensor(683.7194, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.521264 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.527391 [2] L1 tensor(125440.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.529916 [2] L2 tensor(683.1373, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.532441 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.538450 [2] L1 tensor(125460.4453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.541228 [2] L2 tensor(682.5483, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.543672 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.549881 [2] L1 tensor(125479.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.552436 [2] L2 tensor(681.9523, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.555083 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.561151 [2] L1 tensor(125499.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.563857 [2] L2 tensor(681.3497, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.566503 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.572205 [2] L1 tensor(125518.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.575045 [2] L2 tensor(680.7411, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.577595 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.585754 [2] L1 tensor(125537.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.588453 [2] L2 tensor(680.1269, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.591393 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.598449 [2] L1 tensor(125557.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.601563 [2] L2 tensor(679.5081, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.604487 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.611532 [2] L1 tensor(125577.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.614539 [2] L2 tensor(678.8856, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.617279 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.628317 [2] L1 tensor(125597.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.633391 [2] L2 tensor(678.2609, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.637600 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.647865 [2] L1 tensor(125618.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.652482 [2] L2 tensor(677.6357, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.655899 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.665483 [2] L1 tensor(125640.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.668827 [2] L2 tensor(677.0120, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.672110 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.678426 [2] L1 tensor(125663.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.681189 [2] L2 tensor(676.3918, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.683602 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.689844 [2] L1 tensor(125688.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.692534 [2] L2 tensor(675.7770, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.695127 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.702073 [2] L1 tensor(125713.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.704906 [2] L2 tensor(675.1686, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.707587 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.715266 [2] L1 tensor(125739.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.717817 [2] L2 tensor(674.5680, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.720284 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.726765 [2] L1 tensor(125767.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.729451 [2] L2 tensor(673.9763, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.731871 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.739642 [2] L1 tensor(125795.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.742461 [2] L2 tensor(673.3954, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.744924 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.755910 [2] L1 tensor(125825.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.759792 [2] L2 tensor(672.8276, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.762739 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.773575 [2] L1 tensor(125855.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.777066 [2] L2 tensor(672.2759, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.780538 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.791512 [2] L1 tensor(125886.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.796349 [2] L2 tensor(671.7439, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.800779 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.807520 [2] L1 tensor(125917.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.810455 [2] L2 tensor(671.2361, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.813404 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.821825 [2] L1 tensor(125949.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.827703 [2] L2 tensor(670.7574, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.831148 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.838150 [2] L1 tensor(125981.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.840607 [2] L2 tensor(670.3148, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.843084 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.852378 [2] L1 tensor(126013.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.855888 [2] L2 tensor(669.9174, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.859093 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.866265 [2] L1 tensor(126045.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.868894 [2] L2 tensor(669.5789, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.871333 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.877926 [2] L1 tensor(126077.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.880968 [2] L2 tensor(669.3206, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.883699 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.890588 [2] L1 tensor(126108.7578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.893426 [2] L2 tensor(669.1735, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.896268 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.902179 [2] L1 tensor(126139.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.904655 [2] L2 tensor(669.1777, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.907132 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.913972 [2] L1 tensor(126169.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.919042 [2] L2 tensor(669.3781, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.922969 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.931719 [2] L1 tensor(126198.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.934370 [2] L2 tensor(669.8165, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.936825 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.947820 [2] L1 tensor(126226.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.954320 [2] L2 tensor(670.5261, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.958678 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.967170 [2] L1 tensor(126254.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.969852 [2] L2 tensor(671.5295, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.972292 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.978573 [2] L1 tensor(126282.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.981241 [2] L2 tensor(672.8403, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.983743 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:10.991989 [2] L1 tensor(126309.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.995054 [2] L2 tensor(674.4662, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:10.998014 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.006931 [2] L1 tensor(126337.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.010894 [2] L2 tensor(676.4105, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.014444 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.023248 [2] L1 tensor(126364.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.027673 [2] L2 tensor(678.6738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.031163 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.040228 [2] L1 tensor(126393.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.043488 [2] L2 tensor(681.2538, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.046840 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.058988 [2] L1 tensor(126423.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.064949 [2] L2 tensor(684.1448, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.069511 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.079471 [2] L1 tensor(126456.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.083851 [2] L2 tensor(687.3356, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.087387 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.097260 [2] L1 tensor(126491.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.100736 [2] L2 tensor(690.8032, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.103826 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.115965 [2] L1 tensor(126529.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.121687 [2] L2 tensor(694.5038, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.126004 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.134805 [2] L1 tensor(126569.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.137588 [2] L2 tensor(698.3648, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.140054 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.146359 [2] L1 tensor(126611.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.149050 [2] L2 tensor(702.2902, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.151820 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.159629 [2] L1 tensor(126655.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.162449 [2] L2 tensor(706.1835, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.165271 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.171938 [2] L1 tensor(126700.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.174583 [2] L2 tensor(709.9762, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.177081 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.184359 [2] L1 tensor(126747.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.187326 [2] L2 tensor(713.6380, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.190126 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.196959 [2] L1 tensor(126794.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.199910 [2] L2 tensor(717.1677, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.202830 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.209608 [2] L1 tensor(126841.7734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.213034 [2] L2 tensor(720.5723, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.215872 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.224394 [2] L1 tensor(126889.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.227830 [2] L2 tensor(723.8508, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.230879 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.240392 [2] L1 tensor(126935.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.244546 [2] L2 tensor(727.0045, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.247772 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.259041 [2] L1 tensor(126979.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.265002 [2] L2 tensor(730.0438, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.269794 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.279475 [2] L1 tensor(127021.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.283907 [2] L2 tensor(732.9879, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.290865 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.299190 [2] L1 tensor(127062.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.303313 [2] L2 tensor(735.8609, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.307043 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.314531 [2] L1 tensor(127100.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.317329 [2] L2 tensor(738.6917, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.320011 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.330684 [2] L1 tensor(127137.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.334254 [2] L2 tensor(741.5101, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.337755 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.346912 [2] L1 tensor(127171.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.351701 [2] L2 tensor(744.3444, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.355487 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.363934 [2] L1 tensor(127204.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.367673 [2] L2 tensor(747.2191, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.370913 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.379795 [2] L1 tensor(127236.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.382520 [2] L2 tensor(750.1547, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.385811 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.392612 [2] L1 tensor(127266.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.395573 [2] L2 tensor(753.1685, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.398663 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.404789 [2] L1 tensor(127296.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.407265 [2] L2 tensor(756.2761, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.409665 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.415718 [2] L1 tensor(127325.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.418419 [2] L2 tensor(759.4919, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.420941 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.428806 [2] L1 tensor(127355.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.431584 [2] L2 tensor(762.8299, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.434066 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.441461 [2] L1 tensor(127386.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.444062 [2] L2 tensor(766.3027, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.446723 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.457386 [2] L1 tensor(127421.0234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.461267 [2] L2 tensor(769.9196, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.464450 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.476137 [2] L1 tensor(127460.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.481572 [2] L2 tensor(773.6823, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.485188 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.494084 [2] L1 tensor(127505.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.500651 [2] L2 tensor(777.5822, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.503640 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.515185 [2] L1 tensor(127555.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.518860 [2] L2 tensor(781.6003, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.521786 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.531286 [2] L1 tensor(127611.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.534514 [2] L2 tensor(785.7085, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:19:11.537763 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:06.037093 [2] proc begin: <DistEnv 2/4 nccl>
17:23:11.614644 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:23:11.636318 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:24:27.816602 [2] proc begin: <DistEnv 2/4 nccl>
17:25:02.838164 [2] proc begin: <DistEnv 2/4 nccl>
17:25:02.907168 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:25:02.942361 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:25:04.239361 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:04.959302 [2] L2 tensor(438.5738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:04.991637 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.796829 [2] L1 tensor(91832.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.801194 [2] L2 tensor(442.4138, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.804164 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.810192 [2] L1 tensor(91855.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.813032 [2] L2 tensor(445.8104, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.815696 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.822324 [2] L1 tensor(91963.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.825133 [2] L2 tensor(448.3091, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.827995 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.834121 [2] L1 tensor(92143.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.836614 [2] L2 tensor(450.3099, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.839096 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.845042 [2] L1 tensor(92384.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.847776 [2] L2 tensor(452.1290, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.850251 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.856733 [2] L1 tensor(92675.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.859784 [2] L2 tensor(454.0485, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.862298 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.869131 [2] L1 tensor(93008.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.871989 [2] L2 tensor(456.0366, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.874914 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.881130 [2] L1 tensor(93376.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.883673 [2] L2 tensor(457.9691, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.886238 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.892595 [2] L1 tensor(93775.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.895744 [2] L2 tensor(459.7110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.898444 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.905592 [2] L1 tensor(94198.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.908705 [2] L2 tensor(461.1994, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.911849 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.917863 [2] L1 tensor(94641.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.920322 [2] L2 tensor(462.4901, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.922798 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.929055 [2] L1 tensor(95100.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.932013 [2] L2 tensor(463.6808, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.934552 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.941321 [2] L1 tensor(95573.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.944036 [2] L2 tensor(464.8087, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.946760 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.953539 [2] L1 tensor(96059.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.956131 [2] L2 tensor(465.8306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.958725 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.965208 [2] L1 tensor(96555.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.968055 [2] L2 tensor(466.6918, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.971155 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:05.981416 [2] L1 tensor(97059.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.989414 [2] L2 tensor(467.4421, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:05.994211 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.006100 [2] L1 tensor(97571.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.010287 [2] L2 tensor(468.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.014447 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.030465 [2] L1 tensor(98089.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.037324 [2] L2 tensor(469.1948, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.039801 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.049653 [2] L1 tensor(98612.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.054275 [2] L2 tensor(470.4097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.058188 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.068630 [2] L1 tensor(99140.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.072948 [2] L2 tensor(471.9097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.077959 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.087647 [2] L1 tensor(99672., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.091174 [2] L2 tensor(473.7236, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.094798 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.102358 [2] L1 tensor(100206.1953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.109451 [2] L2 tensor(475.8297, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.112365 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.122942 [2] L1 tensor(100741.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.126507 [2] L2 tensor(478.1783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.129810 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.139304 [2] L1 tensor(101279.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.144189 [2] L2 tensor(480.7127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.148175 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.157984 [2] L1 tensor(101816.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.162716 [2] L2 tensor(483.3639, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.166841 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.174805 [2] L1 tensor(102353.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.177786 [2] L2 tensor(486.0810, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.180845 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.190088 [2] L1 tensor(102887.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.192841 [2] L2 tensor(488.8195, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.195245 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.202944 [2] L1 tensor(103417.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.205592 [2] L2 tensor(491.5269, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.208238 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.214475 [2] L1 tensor(103939.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.217466 [2] L2 tensor(494.1458, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.220172 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.226473 [2] L1 tensor(104451.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.229106 [2] L2 tensor(496.6194, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.231582 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.237801 [2] L1 tensor(104950.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.240355 [2] L2 tensor(498.9023, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.243010 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.249397 [2] L1 tensor(105436.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.252245 [2] L2 tensor(500.9773, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.255177 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.261577 [2] L1 tensor(105906.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.264251 [2] L2 tensor(502.8629, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.266946 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.273710 [2] L1 tensor(106361.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.276951 [2] L2 tensor(504.6045, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.279735 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.287046 [2] L1 tensor(106804.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.290091 [2] L2 tensor(506.2585, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.293234 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.300044 [2] L1 tensor(107234.6641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.302954 [2] L2 tensor(507.8830, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.305838 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.314985 [2] L1 tensor(107654.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.318598 [2] L2 tensor(509.5378, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.321954 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.329991 [2] L1 tensor(108064.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.333242 [2] L2 tensor(511.2835, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.336254 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.343787 [2] L1 tensor(108465.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.347323 [2] L2 tensor(513.1113, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.350838 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.359019 [2] L1 tensor(108857.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.362329 [2] L2 tensor(514.9783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.365579 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.376699 [2] L1 tensor(109240.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.381648 [2] L2 tensor(516.8831, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.385639 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.396424 [2] L1 tensor(109615.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.400760 [2] L2 tensor(518.7777, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.404156 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.411325 [2] L1 tensor(109982.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.414014 [2] L2 tensor(520.5676, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.416683 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.422967 [2] L1 tensor(110338.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.425601 [2] L2 tensor(522.2892, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.428117 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.436002 [2] L1 tensor(110692.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.438680 [2] L2 tensor(524.0209, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.441166 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.447529 [2] L1 tensor(111052.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.450334 [2] L2 tensor(525.9114, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.452883 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.459199 [2] L1 tensor(111417.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.462029 [2] L2 tensor(528.1450, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.464842 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.471482 [2] L1 tensor(111786.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.474299 [2] L2 tensor(530.8081, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.477084 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.483409 [2] L1 tensor(112154.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.486235 [2] L2 tensor(533.7229, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.488941 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.495530 [2] L1 tensor(112519.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.498871 [2] L2 tensor(536.6855, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.501960 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.511894 [2] L1 tensor(112882.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.515398 [2] L2 tensor(539.6174, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.518884 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.525976 [2] L1 tensor(113245.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.529107 [2] L2 tensor(542.6865, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.531676 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.543508 [2] L1 tensor(113612.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.549943 [2] L2 tensor(546.6628, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.554620 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.564258 [2] L1 tensor(113990., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.568195 [2] L2 tensor(551.9982, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.571431 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.578683 [2] L1 tensor(114380.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.581518 [2] L2 tensor(558.6219, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.584415 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.590379 [2] L1 tensor(114780.8594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.593094 [2] L2 tensor(565.5477, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.595509 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.602062 [2] L1 tensor(115179.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.605112 [2] L2 tensor(571.8963, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.607723 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.615318 [2] L1 tensor(115564.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.621865 [2] L2 tensor(577.6746, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.626582 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.636524 [2] L1 tensor(115930.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.640538 [2] L2 tensor(582.9805, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.643750 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.655532 [2] L1 tensor(116275.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.661762 [2] L2 tensor(587.8960, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.664567 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.672893 [2] L1 tensor(116604.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.675798 [2] L2 tensor(592.4757, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.678554 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.685179 [2] L1 tensor(116924.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.687914 [2] L2 tensor(596.7731, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.690318 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.699201 [2] L1 tensor(117243.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.702684 [2] L2 tensor(600.9498, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.706263 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.715583 [2] L1 tensor(117559.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.718681 [2] L2 tensor(605.2867, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.721710 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.727580 [2] L1 tensor(117869.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.730296 [2] L2 tensor(609.3760, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.732740 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.739085 [2] L1 tensor(118170.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.741716 [2] L2 tensor(613.2397, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.744109 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.751977 [2] L1 tensor(118465.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.754799 [2] L2 tensor(616.9527, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.757209 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.765759 [2] L1 tensor(118757.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.768410 [2] L2 tensor(620.6242, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.770995 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.780831 [2] L1 tensor(119055.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.786837 [2] L2 tensor(624.6888, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.791061 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.797425 [2] L1 tensor(119353.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.800191 [2] L2 tensor(629.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.802654 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.809487 [2] L1 tensor(119651.1641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.812182 [2] L2 tensor(633.7066, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.814651 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.822475 [2] L1 tensor(119951.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.825161 [2] L2 tensor(638.6360, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.827858 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.834564 [2] L1 tensor(120246., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.837427 [2] L2 tensor(643.5498, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.840114 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.849946 [2] L1 tensor(120527.7109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.852529 [2] L2 tensor(648.2047, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.855161 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.861455 [2] L1 tensor(120793.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.864138 [2] L2 tensor(652.5225, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.866641 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.872897 [2] L1 tensor(121046.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.875733 [2] L2 tensor(656.5157, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.878435 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.884807 [2] L1 tensor(121290.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.887463 [2] L2 tensor(660.2286, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.889965 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.896131 [2] L1 tensor(121526.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.898995 [2] L2 tensor(663.6585, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.901550 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.908918 [2] L1 tensor(121753.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.912311 [2] L2 tensor(666.7991, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.915236 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.922016 [2] L1 tensor(121967.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.924747 [2] L2 tensor(669.6534, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.927132 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.933303 [2] L1 tensor(122169.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.936042 [2] L2 tensor(672.2335, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.938493 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.944760 [2] L1 tensor(122360.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.949459 [2] L2 tensor(674.5535, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.954284 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.964207 [2] L1 tensor(122539., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.968628 [2] L2 tensor(676.6274, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.972118 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:06.982689 [2] L1 tensor(122706.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.988164 [2] L2 tensor(678.4692, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:06.993071 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.001368 [2] L1 tensor(122863.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.005211 [2] L2 tensor(680.0939, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.008614 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.018793 [2] L1 tensor(123010.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.021598 [2] L2 tensor(681.5181, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.024372 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.033738 [2] L1 tensor(123148.7109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.038283 [2] L2 tensor(682.7595, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.042697 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.049916 [2] L1 tensor(123278.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.053020 [2] L2 tensor(683.8363, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.056039 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.065546 [2] L1 tensor(123399.5547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.068425 [2] L2 tensor(684.7654, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.071085 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.077308 [2] L1 tensor(123513.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.080091 [2] L2 tensor(685.5615, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.082600 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.089884 [2] L1 tensor(123621.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.093353 [2] L2 tensor(686.2384, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.096399 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.102810 [2] L1 tensor(123721.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.105371 [2] L2 tensor(686.8153, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.107767 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.114998 [2] L1 tensor(123816.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.118206 [2] L2 tensor(687.3386, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.121232 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.130515 [2] L1 tensor(123905.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.133158 [2] L2 tensor(687.9507, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.135613 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.143274 [2] L1 tensor(123993.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.146502 [2] L2 tensor(688.7937, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.149218 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.155355 [2] L1 tensor(124076.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.158059 [2] L2 tensor(689.6476, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.160445 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.166731 [2] L1 tensor(124155.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.170363 [2] L2 tensor(690.4208, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.173641 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.180849 [2] L1 tensor(124230.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.184561 [2] L2 tensor(691.1049, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.187847 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.194981 [2] L1 tensor(124301.6641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.198583 [2] L2 tensor(691.7021, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.201794 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.209086 [2] L1 tensor(124369.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.212635 [2] L2 tensor(692.2105, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.215952 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.225929 [2] L1 tensor(124433.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.232456 [2] L2 tensor(692.6302, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.235540 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.246181 [2] L1 tensor(124492.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.251742 [2] L2 tensor(692.9646, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.255285 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.265058 [2] L1 tensor(124548.9453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.270018 [2] L2 tensor(693.2184, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.274312 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.284483 [2] L1 tensor(124601.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.288417 [2] L2 tensor(693.3970, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.291820 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.297797 [2] L1 tensor(124651.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.300422 [2] L2 tensor(693.5057, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.302996 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.310488 [2] L1 tensor(124698.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.315265 [2] L2 tensor(693.5497, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.319129 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.325684 [2] L1 tensor(124742.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.328157 [2] L2 tensor(693.5336, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.330698 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.337399 [2] L1 tensor(124784.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.340576 [2] L2 tensor(693.4623, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.343312 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.349870 [2] L1 tensor(124824.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.352533 [2] L2 tensor(693.3398, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.355064 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.361468 [2] L1 tensor(124862.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.364090 [2] L2 tensor(693.1705, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.366618 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.373320 [2] L1 tensor(124899.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.376054 [2] L2 tensor(692.9579, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.378566 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.388348 [2] L1 tensor(124934.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.392414 [2] L2 tensor(692.7056, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.396016 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.402455 [2] L1 tensor(124968.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.405484 [2] L2 tensor(692.4167, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.407933 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.418515 [2] L1 tensor(125000.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.423233 [2] L2 tensor(692.0943, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.428037 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.436991 [2] L1 tensor(125032.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.440953 [2] L2 tensor(691.7413, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.444096 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.455497 [2] L1 tensor(125062.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.461459 [2] L2 tensor(691.3604, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.466200 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.476977 [2] L1 tensor(125091.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.480738 [2] L2 tensor(690.9545, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.484329 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.493578 [2] L1 tensor(125119.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.498174 [2] L2 tensor(690.5261, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.502417 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.512453 [2] L1 tensor(125146.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.517142 [2] L2 tensor(690.0777, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.520747 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.532411 [2] L1 tensor(125172.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.537310 [2] L2 tensor(689.6117, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.541355 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.552004 [2] L1 tensor(125198.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.556513 [2] L2 tensor(689.1302, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.560322 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.569909 [2] L1 tensor(125223.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.574794 [2] L2 tensor(688.6351, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.578735 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.587958 [2] L1 tensor(125246.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.591447 [2] L2 tensor(688.1282, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.594543 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.604260 [2] L1 tensor(125270.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.606893 [2] L2 tensor(687.6105, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.609352 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.619854 [2] L1 tensor(125292.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.624224 [2] L2 tensor(687.0834, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.628661 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.634721 [2] L1 tensor(125314.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.637491 [2] L2 tensor(686.5475, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.640172 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.650958 [2] L1 tensor(125336.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.653855 [2] L2 tensor(686.0035, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.656522 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.663040 [2] L1 tensor(125357.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.665717 [2] L2 tensor(685.4519, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.668520 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.674844 [2] L1 tensor(125378.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.677674 [2] L2 tensor(684.8926, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.680285 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.688846 [2] L1 tensor(125398.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.691400 [2] L2 tensor(684.3259, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.693840 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.702608 [2] L1 tensor(125418.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.705638 [2] L2 tensor(683.7520, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.708774 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.718162 [2] L1 tensor(125438.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.720763 [2] L2 tensor(683.1707, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.723370 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.729331 [2] L1 tensor(125457.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.732079 [2] L2 tensor(682.5823, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.734560 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.740644 [2] L1 tensor(125477.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.743688 [2] L2 tensor(681.9871, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.746331 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.756756 [2] L1 tensor(125496.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.760348 [2] L2 tensor(681.3853, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.763851 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.770168 [2] L1 tensor(125516.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.772812 [2] L2 tensor(680.7774, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.775240 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.782564 [2] L1 tensor(125535.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.785327 [2] L2 tensor(680.1642, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.787851 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.794088 [2] L1 tensor(125555.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.796815 [2] L2 tensor(679.5466, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.799514 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.805886 [2] L1 tensor(125575.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.808826 [2] L2 tensor(678.9255, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.811612 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.818798 [2] L1 tensor(125596.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.821527 [2] L2 tensor(678.3028, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.824124 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.830115 [2] L1 tensor(125617.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.832749 [2] L2 tensor(677.6799, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.835370 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.842090 [2] L1 tensor(125640.1641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.844817 [2] L2 tensor(677.0592, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.847517 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.853731 [2] L1 tensor(125663.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.856194 [2] L2 tensor(676.4423, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.858604 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.864626 [2] L1 tensor(125688.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.867351 [2] L2 tensor(675.8309, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.869792 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.877183 [2] L1 tensor(125713.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.880077 [2] L2 tensor(675.2263, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.882934 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.890348 [2] L1 tensor(125740.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.893777 [2] L2 tensor(674.6296, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.896855 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.904003 [2] L1 tensor(125768.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.908151 [2] L2 tensor(674.0422, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.913411 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.921619 [2] L1 tensor(125797.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.925229 [2] L2 tensor(673.4661, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.931161 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.939180 [2] L1 tensor(125826.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.942590 [2] L2 tensor(672.9038, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.948753 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.958088 [2] L1 tensor(125857.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.961182 [2] L2 tensor(672.3586, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.965180 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.974611 [2] L1 tensor(125888.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.978310 [2] L2 tensor(671.8345, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.982025 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.991791 [2] L1 tensor(125919.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.995304 [2] L2 tensor(671.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:07.998906 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.004847 [2] L1 tensor(125951.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.007603 [2] L2 tensor(670.8690, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.010095 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.018331 [2] L1 tensor(125984.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.021467 [2] L2 tensor(670.4412, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.024248 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.030389 [2] L1 tensor(126016.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.033140 [2] L2 tensor(670.0634, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.035750 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.041864 [2] L1 tensor(126048.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.044491 [2] L2 tensor(669.7524, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.050682 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.057791 [2] L1 tensor(126079.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.060803 [2] L2 tensor(669.5336, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.063744 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.072924 [2] L1 tensor(126110.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.076380 [2] L2 tensor(669.4422, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.081304 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.088227 [2] L1 tensor(126141.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.090901 [2] L2 tensor(669.5217, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.093834 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.099723 [2] L1 tensor(126170.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.102194 [2] L2 tensor(669.8168, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.104683 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.115511 [2] L1 tensor(126199.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.119669 [2] L2 tensor(670.3663, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.122894 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.128631 [2] L1 tensor(126228.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.131296 [2] L2 tensor(671.1985, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.133751 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.141712 [2] L1 tensor(126256.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.144869 [2] L2 tensor(672.3314, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.147532 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.154597 [2] L1 tensor(126283.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.157433 [2] L2 tensor(673.7753, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.160299 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.168973 [2] L1 tensor(126310.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.171691 [2] L2 tensor(675.5351, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.174286 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.180938 [2] L1 tensor(126338.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.183715 [2] L2 tensor(677.6124, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.186402 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.193132 [2] L1 tensor(126366.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.196281 [2] L2 tensor(680.0062, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.199083 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.205511 [2] L1 tensor(126395.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.208501 [2] L2 tensor(682.7129, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.210937 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.217164 [2] L1 tensor(126426.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.219685 [2] L2 tensor(685.7258, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.222173 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.228255 [2] L1 tensor(126459.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.231065 [2] L2 tensor(689.0309, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.233785 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.240537 [2] L1 tensor(126495.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.243441 [2] L2 tensor(692.5996, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.246016 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.252292 [2] L1 tensor(126534.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.254868 [2] L2 tensor(696.3790, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.257434 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.264170 [2] L1 tensor(126575.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.267179 [2] L2 tensor(700.2872, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.270133 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.276860 [2] L1 tensor(126618.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.279826 [2] L2 tensor(704.2246, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.282507 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.289480 [2] L1 tensor(126662.8594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.292544 [2] L2 tensor(708.1017, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.295324 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.302197 [2] L1 tensor(126708.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.305649 [2] L2 tensor(711.8633, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.308934 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.315426 [2] L1 tensor(126754.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.320460 [2] L2 tensor(715.4906, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.323832 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.332091 [2] L1 tensor(126802.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.335273 [2] L2 tensor(718.9883, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.338322 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.344295 [2] L1 tensor(126849.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.346996 [2] L2 tensor(722.3608, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.349468 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.356120 [2] L1 tensor(126896.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.358674 [2] L2 tensor(725.6066, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.361177 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.367618 [2] L1 tensor(126942.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.370395 [2] L2 tensor(728.7300, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.372959 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.379583 [2] L1 tensor(126985.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.382570 [2] L2 tensor(731.7451, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.385517 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.391993 [2] L1 tensor(127027.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.394629 [2] L2 tensor(734.6733, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.397251 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.403720 [2] L1 tensor(127067.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.406642 [2] L2 tensor(737.5409, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.409375 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.417949 [2] L1 tensor(127104.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.421269 [2] L2 tensor(740.3770, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.424631 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.431296 [2] L1 tensor(127140.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.434369 [2] L2 tensor(743.2106, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.437580 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.444627 [2] L1 tensor(127174.9922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.447471 [2] L2 tensor(746.0685, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.449901 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.455866 [2] L1 tensor(127207.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.458607 [2] L2 tensor(748.9731, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.461047 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.472393 [2] L1 tensor(127238.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.477170 [2] L2 tensor(751.9435, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.481526 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.492351 [2] L1 tensor(127268.9922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.496840 [2] L2 tensor(754.9952, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.499763 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.506962 [2] L1 tensor(127298.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.510198 [2] L2 tensor(758.1432, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.513133 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.519502 [2] L1 tensor(127327.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.522263 [2] L2 tensor(761.4014, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.524748 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.530751 [2] L1 tensor(127358.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.533582 [2] L2 tensor(764.7833, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.536082 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.543232 [2] L1 tensor(127390.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.550197 [2] L2 tensor(768.3010, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.554503 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.564424 [2] L1 tensor(127426.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.567759 [2] L2 tensor(771.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.571158 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.577483 [2] L1 tensor(127468.4453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.579962 [2] L2 tensor(775.7610, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.582491 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.588750 [2] L1 tensor(127515.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.591504 [2] L2 tensor(779.6886, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.594344 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.600377 [2] L1 tensor(127568.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.603087 [2] L2 tensor(783.7209, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.605608 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:08.612384 [2] L1 tensor(127625.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.615180 [2] L2 tensor(787.8282, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
17:25:08.617604 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:41.069828 [2] proc begin: <DistEnv 2/4 nccl>
17:25:41.166097 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
17:25:41.180443 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:54.072585 [2] proc begin: <DistEnv 2/4 nccl>
20:31:54.356656 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:31:54.376851 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:31:55.579247 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:56.375255 [2] L2 tensor(438.5738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:56.418129 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.343254 [2] L1 tensor(91832.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.347365 [2] L2 tensor(442.4138, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.351032 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.359297 [2] L1 tensor(91855.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.363678 [2] L2 tensor(445.8104, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.367405 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.375010 [2] L1 tensor(91963.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.377882 [2] L2 tensor(448.3091, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.380531 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.387343 [2] L1 tensor(92143.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.390495 [2] L2 tensor(450.3099, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.393465 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.399865 [2] L1 tensor(92384.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.402833 [2] L2 tensor(452.1291, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.405473 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.413898 [2] L1 tensor(92675.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.417795 [2] L2 tensor(454.0486, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.421919 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.432909 [2] L1 tensor(93008.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.437788 [2] L2 tensor(456.0366, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.441474 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.448045 [2] L1 tensor(93376.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.450851 [2] L2 tensor(457.9691, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.454280 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.461842 [2] L1 tensor(93775.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.464785 [2] L2 tensor(459.7110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.467313 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.476539 [2] L1 tensor(94198.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.480225 [2] L2 tensor(461.1994, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.483543 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.490503 [2] L1 tensor(94641.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.493425 [2] L2 tensor(462.4901, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.495966 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.502992 [2] L1 tensor(95100.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.506183 [2] L2 tensor(463.6808, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.508850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.516099 [2] L1 tensor(95573.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.519284 [2] L2 tensor(464.8087, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.522178 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.529248 [2] L1 tensor(96059.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.532126 [2] L2 tensor(465.8305, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.534670 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.541171 [2] L1 tensor(96555.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.544328 [2] L2 tensor(466.6918, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.547161 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.554045 [2] L1 tensor(97059.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.556907 [2] L2 tensor(467.4421, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.559436 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.565877 [2] L1 tensor(97571.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.568779 [2] L2 tensor(468.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.571368 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.578547 [2] L1 tensor(98089.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.581524 [2] L2 tensor(469.1948, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.584455 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.590874 [2] L1 tensor(98612.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.594190 [2] L2 tensor(470.4097, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.599781 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.608882 [2] L1 tensor(99140.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.612188 [2] L2 tensor(471.9096, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.615518 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.622496 [2] L1 tensor(99672., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.625351 [2] L2 tensor(473.7236, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.627935 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.634793 [2] L1 tensor(100206.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.637812 [2] L2 tensor(475.8296, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.640636 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.647468 [2] L1 tensor(100741.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.651066 [2] L2 tensor(478.1783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.654083 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.666721 [2] L1 tensor(101279.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.671193 [2] L2 tensor(480.7127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.674944 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.685068 [2] L1 tensor(101816.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.689177 [2] L2 tensor(483.3638, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.692957 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.703504 [2] L1 tensor(102353.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.708415 [2] L2 tensor(486.0809, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.712606 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.720605 [2] L1 tensor(102887.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.723852 [2] L2 tensor(488.8194, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.727069 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.733685 [2] L1 tensor(103417.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.736545 [2] L2 tensor(491.5267, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.739132 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.746069 [2] L1 tensor(103939.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.749066 [2] L2 tensor(494.1456, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.752142 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.758705 [2] L1 tensor(104451.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.761675 [2] L2 tensor(496.6192, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.764303 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.773895 [2] L1 tensor(104950.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.779953 [2] L2 tensor(498.9020, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.783930 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.794259 [2] L1 tensor(105436.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.797551 [2] L2 tensor(500.9771, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.800903 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.809173 [2] L1 tensor(105906.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.812145 [2] L2 tensor(502.8626, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.815592 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.822092 [2] L1 tensor(106361.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.824857 [2] L2 tensor(504.6041, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.827508 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.836254 [2] L1 tensor(106804.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.839329 [2] L2 tensor(506.2581, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.842295 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.848902 [2] L1 tensor(107234.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.851771 [2] L2 tensor(507.8827, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.854311 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.860424 [2] L1 tensor(107654.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.863303 [2] L2 tensor(509.5375, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.865873 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.877134 [2] L1 tensor(108064.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.882316 [2] L2 tensor(511.2832, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.886608 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.896451 [2] L1 tensor(108465.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.900196 [2] L2 tensor(513.1110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.903373 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.912556 [2] L1 tensor(108857.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.916003 [2] L2 tensor(514.9779, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.919500 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.928340 [2] L1 tensor(109240.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.931245 [2] L2 tensor(516.8826, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.933879 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.943015 [2] L1 tensor(109615.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.946315 [2] L2 tensor(518.7772, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.949325 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.957706 [2] L1 tensor(109982.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.960683 [2] L2 tensor(520.5671, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.963703 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.972644 [2] L1 tensor(110338.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.975854 [2] L2 tensor(522.2888, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.979352 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.990883 [2] L1 tensor(110692.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.994361 [2] L2 tensor(524.0205, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:57.997876 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.007977 [2] L1 tensor(111052., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.012465 [2] L2 tensor(525.9110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.016167 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.026560 [2] L1 tensor(111417.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.030184 [2] L2 tensor(528.1443, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.033361 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.044527 [2] L1 tensor(111786.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.051174 [2] L2 tensor(530.8073, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.055659 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.067106 [2] L1 tensor(112154.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.070861 [2] L2 tensor(533.7219, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.074285 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.089647 [2] L1 tensor(112519.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.095387 [2] L2 tensor(536.6843, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.098205 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.107830 [2] L1 tensor(112882.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.110778 [2] L2 tensor(539.6161, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.113764 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.122926 [2] L1 tensor(113245.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.126738 [2] L2 tensor(542.6851, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.129855 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.139336 [2] L1 tensor(113612.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.142346 [2] L2 tensor(546.6616, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.145027 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.155891 [2] L1 tensor(113989.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.159306 [2] L2 tensor(551.9972, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.162664 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.169960 [2] L1 tensor(114380.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.173681 [2] L2 tensor(558.6211, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.176804 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.184652 [2] L1 tensor(114780.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.187597 [2] L2 tensor(565.5475, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.190134 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.196829 [2] L1 tensor(115179.3203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.199594 [2] L2 tensor(571.8965, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.202098 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.208235 [2] L1 tensor(115564.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.211260 [2] L2 tensor(577.6750, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.214009 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.220993 [2] L1 tensor(115930.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.223884 [2] L2 tensor(582.9812, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.226377 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.233181 [2] L1 tensor(116275.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.236072 [2] L2 tensor(587.8970, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.238569 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.244886 [2] L1 tensor(116604.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.247982 [2] L2 tensor(592.4769, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.250821 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.258093 [2] L1 tensor(116924.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.260993 [2] L2 tensor(596.7745, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.263641 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.273244 [2] L1 tensor(117243.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.276496 [2] L2 tensor(600.9517, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.280080 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.286731 [2] L1 tensor(117560.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.289580 [2] L2 tensor(605.2882, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.292023 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.298485 [2] L1 tensor(117869.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.301361 [2] L2 tensor(609.3769, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.303944 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.310466 [2] L1 tensor(118170.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.313838 [2] L2 tensor(613.2401, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.316687 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.323337 [2] L1 tensor(118466.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.326335 [2] L2 tensor(616.9527, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.329118 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.335777 [2] L1 tensor(118757.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.338803 [2] L2 tensor(620.6245, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.341641 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.348715 [2] L1 tensor(119055.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.351594 [2] L2 tensor(624.6906, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.354051 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.360432 [2] L1 tensor(119353.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.363465 [2] L2 tensor(629.0503, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.366177 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.372941 [2] L1 tensor(119651.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.375838 [2] L2 tensor(633.7117, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.378297 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.385876 [2] L1 tensor(119951.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.389143 [2] L2 tensor(638.6426, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.392373 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.399277 [2] L1 tensor(120246.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.402707 [2] L2 tensor(643.5576, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.405428 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.412682 [2] L1 tensor(120528.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.415515 [2] L2 tensor(648.2136, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.418112 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.424705 [2] L1 tensor(120794.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.427642 [2] L2 tensor(652.5324, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.430216 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.439865 [2] L1 tensor(121047.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.444230 [2] L2 tensor(656.5266, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.448639 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.458580 [2] L1 tensor(121291.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.462968 [2] L2 tensor(660.2404, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.466911 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.474915 [2] L1 tensor(121527.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.478158 [2] L2 tensor(663.6710, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.480873 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.488192 [2] L1 tensor(121754.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.491055 [2] L2 tensor(666.8120, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.494182 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.502627 [2] L1 tensor(121968.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.505487 [2] L2 tensor(669.6666, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.508274 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.514619 [2] L1 tensor(122170.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.517527 [2] L2 tensor(672.2468, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.520165 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.527155 [2] L1 tensor(122361.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.530090 [2] L2 tensor(674.5669, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.532906 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.539942 [2] L1 tensor(122540.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.542916 [2] L2 tensor(676.6407, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.545868 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.552182 [2] L1 tensor(122707.8359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.555268 [2] L2 tensor(678.4823, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.557878 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.564900 [2] L1 tensor(122864.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.567777 [2] L2 tensor(680.1068, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.570268 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.576778 [2] L1 tensor(123011.9922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.579895 [2] L2 tensor(681.5306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.582767 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.589529 [2] L1 tensor(123149.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.592384 [2] L2 tensor(682.7717, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.594873 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.601489 [2] L1 tensor(123279.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.604382 [2] L2 tensor(683.8482, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.606901 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.615841 [2] L1 tensor(123400.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.620273 [2] L2 tensor(684.7769, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.623822 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.633009 [2] L1 tensor(123514.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.636144 [2] L2 tensor(685.5725, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.639279 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.650484 [2] L1 tensor(123622.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.654120 [2] L2 tensor(686.2490, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.657857 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.666641 [2] L1 tensor(123722.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.670445 [2] L2 tensor(686.8252, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.674156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.683629 [2] L1 tensor(123817.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.687787 [2] L2 tensor(687.3473, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.691490 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.698615 [2] L1 tensor(123906.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.701297 [2] L2 tensor(687.9568, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.703878 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.710825 [2] L1 tensor(123993.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.714153 [2] L2 tensor(688.7982, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.716781 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.723840 [2] L1 tensor(124077.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.726676 [2] L2 tensor(689.6525, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.729642 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.736132 [2] L1 tensor(124155.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.738625 [2] L2 tensor(690.4263, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.741087 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.747246 [2] L1 tensor(124230.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.749973 [2] L2 tensor(691.1112, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.752472 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.758564 [2] L1 tensor(124302.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.761333 [2] L2 tensor(691.7091, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.763865 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.769872 [2] L1 tensor(124370.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.772661 [2] L2 tensor(692.2183, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.775235 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.781566 [2] L1 tensor(124433.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.784727 [2] L2 tensor(692.6386, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.787692 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.794064 [2] L1 tensor(124493.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.796938 [2] L2 tensor(692.9735, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.799584 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.806358 [2] L1 tensor(124549.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.809231 [2] L2 tensor(693.2279, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.812024 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.818725 [2] L1 tensor(124602.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.821917 [2] L2 tensor(693.4070, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.824749 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.830964 [2] L1 tensor(124651.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.833717 [2] L2 tensor(693.5162, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.836280 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.842795 [2] L1 tensor(124698.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.845377 [2] L2 tensor(693.5605, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.847994 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.854353 [2] L1 tensor(124743.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.856959 [2] L2 tensor(693.5450, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.859553 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.865952 [2] L1 tensor(124785.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.869171 [2] L2 tensor(693.4740, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.872108 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.878688 [2] L1 tensor(124825.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.881466 [2] L2 tensor(693.3520, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.884205 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.890935 [2] L1 tensor(124863.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.893868 [2] L2 tensor(693.1831, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.896754 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.903322 [2] L1 tensor(124900.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.906786 [2] L2 tensor(692.9709, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.909547 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.916749 [2] L1 tensor(124935.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.919990 [2] L2 tensor(692.7190, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.922903 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.929738 [2] L1 tensor(124969.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.932938 [2] L2 tensor(692.4306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.935934 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.943728 [2] L1 tensor(125001.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.946548 [2] L2 tensor(692.1086, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.949037 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.955274 [2] L1 tensor(125032.9453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.958084 [2] L2 tensor(691.7561, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.960621 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.967282 [2] L1 tensor(125063.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.970352 [2] L2 tensor(691.3757, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.972898 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.979913 [2] L1 tensor(125092.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.985147 [2] L2 tensor(690.9703, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:58.988434 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:58.997521 [2] L1 tensor(125120.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.000696 [2] L2 tensor(690.5424, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.004252 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.011448 [2] L1 tensor(125147.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.014485 [2] L2 tensor(690.0947, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.019339 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.025797 [2] L1 tensor(125173.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.028456 [2] L2 tensor(689.6293, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.030996 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.037489 [2] L1 tensor(125199.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.042413 [2] L2 tensor(689.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.047095 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.053583 [2] L1 tensor(125223.9922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.056138 [2] L2 tensor(688.6539, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.059579 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.066792 [2] L1 tensor(125247.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.069460 [2] L2 tensor(688.1476, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.072814 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.079487 [2] L1 tensor(125271.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.082532 [2] L2 tensor(687.6306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.084962 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.091343 [2] L1 tensor(125293.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.094189 [2] L2 tensor(687.1043, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.096643 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.103281 [2] L1 tensor(125315.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.106002 [2] L2 tensor(686.5692, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.108622 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.118238 [2] L1 tensor(125337.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.121133 [2] L2 tensor(686.0261, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.124154 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.130347 [2] L1 tensor(125358.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.133182 [2] L2 tensor(685.4752, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.135839 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.142650 [2] L1 tensor(125379.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.145467 [2] L2 tensor(684.9169, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.148332 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.154590 [2] L1 tensor(125399.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.157186 [2] L2 tensor(684.3511, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.159663 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.166049 [2] L1 tensor(125419.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.169201 [2] L2 tensor(683.7780, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.171757 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.178601 [2] L1 tensor(125439.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.182195 [2] L2 tensor(683.1976, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.185279 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.192015 [2] L1 tensor(125458.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.194561 [2] L2 tensor(682.6101, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.196969 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.203473 [2] L1 tensor(125478.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.206491 [2] L2 tensor(682.0157, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.208991 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.215208 [2] L1 tensor(125497.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.218041 [2] L2 tensor(681.4148, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.220623 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.226969 [2] L1 tensor(125516.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.229553 [2] L2 tensor(680.8077, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.232157 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.238491 [2] L1 tensor(125536.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.241397 [2] L2 tensor(680.1953, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.244144 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.250429 [2] L1 tensor(125556.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.253350 [2] L2 tensor(679.5782, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.255867 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.262691 [2] L1 tensor(125576.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.265478 [2] L2 tensor(678.9578, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.268085 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.274749 [2] L1 tensor(125596.7734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.277524 [2] L2 tensor(678.3353, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.280662 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.288942 [2] L1 tensor(125618.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.292150 [2] L2 tensor(677.7127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.295144 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.301726 [2] L1 tensor(125640.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.304398 [2] L2 tensor(677.0919, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.306881 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.312950 [2] L1 tensor(125663.6641, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.315954 [2] L2 tensor(676.4749, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.318696 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.324944 [2] L1 tensor(125688.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.327791 [2] L2 tensor(675.8633, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.330242 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.336569 [2] L1 tensor(125713.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.339497 [2] L2 tensor(675.2583, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.342004 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.350351 [2] L1 tensor(125740.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.355493 [2] L2 tensor(674.6611, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.359673 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.366288 [2] L1 tensor(125768.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.369286 [2] L2 tensor(674.0732, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.371947 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.378594 [2] L1 tensor(125796.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.381362 [2] L2 tensor(673.4963, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.384138 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.393794 [2] L1 tensor(125826.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.396685 [2] L2 tensor(672.9331, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.399242 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.405850 [2] L1 tensor(125856.5703, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.408657 [2] L2 tensor(672.3866, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.411170 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.417815 [2] L1 tensor(125887.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.421252 [2] L2 tensor(671.8606, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.424039 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.431254 [2] L1 tensor(125919.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.434153 [2] L2 tensor(671.3599, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.437148 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.443340 [2] L1 tensor(125951.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.445999 [2] L2 tensor(670.8901, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.448761 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.455237 [2] L1 tensor(125983.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.458142 [2] L2 tensor(670.4584, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.460964 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.469998 [2] L1 tensor(126015.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.473950 [2] L2 tensor(670.0754, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.477749 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.488038 [2] L1 tensor(126047.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.491606 [2] L2 tensor(669.7568, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.495435 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.503715 [2] L1 tensor(126079.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.509183 [2] L2 tensor(669.5269, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.513048 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.520618 [2] L1 tensor(126110.1797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.523345 [2] L2 tensor(669.4199, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.525849 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.532346 [2] L1 tensor(126140.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.534972 [2] L2 tensor(669.4784, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.537451 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.543555 [2] L1 tensor(126170.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.546055 [2] L2 tensor(669.7474, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.548562 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.558344 [2] L1 tensor(126199.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.562694 [2] L2 tensor(670.2666, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.566617 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.573087 [2] L1 tensor(126227.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.576028 [2] L2 tensor(671.0658, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.578880 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.588702 [2] L1 tensor(126255.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.591603 [2] L2 tensor(672.1641, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.594459 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.601025 [2] L1 tensor(126283.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.604109 [2] L2 tensor(673.5726, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.606850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.613869 [2] L1 tensor(126310.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.616590 [2] L2 tensor(675.2971, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.619058 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.625333 [2] L1 tensor(126337.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.628069 [2] L2 tensor(677.3395, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.630581 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.636909 [2] L1 tensor(126365.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.639706 [2] L2 tensor(679.6990, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.642597 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.649229 [2] L1 tensor(126394.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.652297 [2] L2 tensor(682.3726, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.655970 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.665271 [2] L1 tensor(126425.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.668437 [2] L2 tensor(685.3538, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.671720 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.678317 [2] L1 tensor(126458.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.681015 [2] L2 tensor(688.6296, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.683493 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.694831 [2] L1 tensor(126494.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.701350 [2] L2 tensor(692.1733, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.704603 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.715501 [2] L1 tensor(126532.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.723670 [2] L2 tensor(695.9346, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.730192 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.740706 [2] L1 tensor(126573.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.745238 [2] L2 tensor(699.8345, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.749384 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.760370 [2] L1 tensor(126616.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.765941 [2] L2 tensor(703.7738, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.770376 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.780397 [2] L1 tensor(126661., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.784546 [2] L2 tensor(707.6608, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.788358 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.795851 [2] L1 tensor(126706.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.799240 [2] L2 tensor(711.4360, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.802268 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.808607 [2] L1 tensor(126752.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.811449 [2] L2 tensor(715.0779, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.813962 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.820440 [2] L1 tensor(126800.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.823380 [2] L2 tensor(718.5892, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.826092 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.834672 [2] L1 tensor(126847.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.837600 [2] L2 tensor(721.9753, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.840574 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.847288 [2] L1 tensor(126894.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.850028 [2] L2 tensor(725.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.852534 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.859582 [2] L1 tensor(126940.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.862757 [2] L2 tensor(728.3699, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.865309 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.872776 [2] L1 tensor(126984.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.876483 [2] L2 tensor(731.3950, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.879644 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.886393 [2] L1 tensor(127026.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.889031 [2] L2 tensor(734.3306, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.891763 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.902410 [2] L1 tensor(127066.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.906686 [2] L2 tensor(737.2029, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.910094 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.923479 [2] L1 tensor(127104.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.928027 [2] L2 tensor(740.0406, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.932046 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.944555 [2] L1 tensor(127140.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.947685 [2] L2 tensor(742.8735, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.951011 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.961384 [2] L1 tensor(127174.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.966557 [2] L2 tensor(745.7286, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.970478 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.980033 [2] L1 tensor(127207.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.984478 [2] L2 tensor(748.6293, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:31:59.988461 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.996387 [2] L1 tensor(127238.6797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.000147 [2] L2 tensor(751.5946, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.003167 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.009928 [2] L1 tensor(127268.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.012699 [2] L2 tensor(754.6411, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.015231 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.022177 [2] L1 tensor(127298.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.027100 [2] L2 tensor(757.7836, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.029807 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.037145 [2] L1 tensor(127327.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.040565 [2] L2 tensor(761.0363, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.043613 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.051647 [2] L1 tensor(127358.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.054883 [2] L2 tensor(764.4130, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.057910 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.064919 [2] L1 tensor(127390.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.067834 [2] L2 tensor(767.9260, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.070583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.080180 [2] L1 tensor(127426.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.083018 [2] L2 tensor(771.5828, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.086412 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.093055 [2] L1 tensor(127466.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.097016 [2] L2 tensor(775.3823, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.099636 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.107169 [2] L1 tensor(127513.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.110111 [2] L2 tensor(779.3130, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.112892 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.119086 [2] L1 tensor(127566.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.121983 [2] L2 tensor(783.3530, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.124794 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:00.131229 [2] L1 tensor(127623.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.134028 [2] L2 tensor(787.4728, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.1793, device='cuda:2', grad_fn=<SumBackward0>)
20:32:00.137156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.314719 [2] proc begin: <DistEnv 2/4 nccl>
20:33:10.376851 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:33:10.410882 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:11.553895 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:12.328687 [2] L2 tensor(8166.0913, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:12.338520 [2] L3 tensor(8193.4414, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:12.342367 [2] L4 tensor(460.0110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:12.370248 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.197549 [2] L1 tensor(92158.0234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.201193 [2] L2 tensor(8003.2510, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.203998 [2] L3 tensor(8178.0820, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.206332 [2] L4 tensor(466.4110, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.209274 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.225053 [2] L1 tensor(92284.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.230744 [2] L2 tensor(7882.0869, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.233824 [2] L3 tensor(8160.8027, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.236609 [2] L4 tensor(471.2665, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.239245 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.248172 [2] L1 tensor(92265.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.250861 [2] L2 tensor(7761.1973, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.254052 [2] L3 tensor(8135.3574, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.257187 [2] L4 tensor(474.9828, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.260090 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.272092 [2] L1 tensor(92279.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.277326 [2] L2 tensor(7658.1450, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.280276 [2] L3 tensor(8111.2231, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.282727 [2] L4 tensor(478.8577, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.285270 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.303092 [2] L1 tensor(92242.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.308799 [2] L2 tensor(7563.7065, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.312343 [2] L3 tensor(8093.5737, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.315270 [2] L4 tensor(481.4165, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.317842 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.334804 [2] L1 tensor(92148.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.339986 [2] L2 tensor(7474.9492, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.343983 [2] L3 tensor(8078.6621, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.346263 [2] L4 tensor(483.2287, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.348717 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.361373 [2] L1 tensor(92004.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.367230 [2] L2 tensor(7388.9067, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.371604 [2] L3 tensor(8064.4102, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.374211 [2] L4 tensor(484.1880, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.376640 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.392303 [2] L1 tensor(91864.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.395879 [2] L2 tensor(7305.5322, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.398187 [2] L3 tensor(8050.8521, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.400963 [2] L4 tensor(485.2452, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.403596 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.418390 [2] L1 tensor(91703.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.422677 [2] L2 tensor(7224.3052, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.426323 [2] L3 tensor(8039.9307, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.430244 [2] L4 tensor(486.2715, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.432674 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.445866 [2] L1 tensor(91546.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.449262 [2] L2 tensor(7142.9131, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.452413 [2] L3 tensor(8030.0137, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.455755 [2] L4 tensor(486.3793, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.458979 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.472603 [2] L1 tensor(91400.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.476184 [2] L2 tensor(7063.3184, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.479212 [2] L3 tensor(8020.9849, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.481690 [2] L4 tensor(486.5335, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.484581 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.498350 [2] L1 tensor(91263.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.504299 [2] L2 tensor(6990.2002, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.508104 [2] L3 tensor(8012.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.510376 [2] L4 tensor(486.5843, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.512826 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.531196 [2] L1 tensor(91150.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.537040 [2] L2 tensor(6922.4463, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.539919 [2] L3 tensor(8003.8706, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.542305 [2] L4 tensor(486.6861, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.544738 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.554988 [2] L1 tensor(91041.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.558187 [2] L2 tensor(6858.5234, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.560661 [2] L3 tensor(7995.8789, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.563109 [2] L4 tensor(486.6684, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.565531 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.578437 [2] L1 tensor(90944.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.581155 [2] L2 tensor(6798.0029, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.583680 [2] L3 tensor(7988.0479, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.586246 [2] L4 tensor(486.6975, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.589024 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.604679 [2] L1 tensor(90839.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.610310 [2] L2 tensor(6739.6226, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.612560 [2] L3 tensor(7982.2271, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.614814 [2] L4 tensor(486.7326, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.617231 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.634082 [2] L1 tensor(90740.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.640618 [2] L2 tensor(6684.4434, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.644054 [2] L3 tensor(7978.1631, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.646349 [2] L4 tensor(486.8357, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.654005 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.667501 [2] L1 tensor(90655.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.673311 [2] L2 tensor(6633.7432, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.676060 [2] L3 tensor(7974.1128, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.678305 [2] L4 tensor(486.9752, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.680715 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.692745 [2] L1 tensor(90576.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.698688 [2] L2 tensor(6587.6748, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.703211 [2] L3 tensor(7970.1611, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.705449 [2] L4 tensor(487.1800, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.707851 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.716694 [2] L1 tensor(90507.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.719910 [2] L2 tensor(6544.5615, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.723065 [2] L3 tensor(7965.9619, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.726242 [2] L4 tensor(487.3255, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.728697 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.739915 [2] L1 tensor(90447.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.743142 [2] L2 tensor(6504.0693, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.745624 [2] L3 tensor(7961.7642, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.748697 [2] L4 tensor(487.4957, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.751634 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.767712 [2] L1 tensor(90383.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.772854 [2] L2 tensor(6466.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.776228 [2] L3 tensor(7957.6348, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.779333 [2] L4 tensor(487.7361, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.781765 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.792664 [2] L1 tensor(90323.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.797900 [2] L2 tensor(6432.1631, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.800278 [2] L3 tensor(7953.8613, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.803395 [2] L4 tensor(488.0311, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.805805 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.822597 [2] L1 tensor(90261.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.827488 [2] L2 tensor(6398.6284, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.831718 [2] L3 tensor(7950.1064, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.834133 [2] L4 tensor(488.2441, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.836609 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.851719 [2] L1 tensor(90206.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.857703 [2] L2 tensor(6365.5928, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.860195 [2] L3 tensor(7946.6143, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.862782 [2] L4 tensor(488.3713, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.865171 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.873951 [2] L1 tensor(90156.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.877753 [2] L2 tensor(6332.5015, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.881408 [2] L3 tensor(7944.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.884313 [2] L4 tensor(488.5137, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.886763 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.904159 [2] L1 tensor(90114.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.910463 [2] L2 tensor(6298.9160, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.912682 [2] L3 tensor(7941.5576, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.914917 [2] L4 tensor(488.6197, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.917306 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.926457 [2] L1 tensor(90077.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.929144 [2] L2 tensor(6265.0801, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.931384 [2] L3 tensor(7938.5840, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.933618 [2] L4 tensor(488.7382, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.936080 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.947217 [2] L1 tensor(90045.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.950433 [2] L2 tensor(6230.8857, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.953298 [2] L3 tensor(7935.4272, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.955849 [2] L4 tensor(488.8261, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.958399 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.975890 [2] L1 tensor(90015.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.981976 [2] L2 tensor(6196.9541, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.984203 [2] L3 tensor(7932.2021, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.986464 [2] L4 tensor(488.9282, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:13.988872 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.999844 [2] L1 tensor(89986.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.002482 [2] L2 tensor(6162.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.005385 [2] L3 tensor(7929.9180, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.008554 [2] L4 tensor(489.0490, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.011491 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.027843 [2] L1 tensor(89958.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.034265 [2] L2 tensor(6129.5806, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.036507 [2] L3 tensor(7927.6050, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.038711 [2] L4 tensor(489.1157, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.041222 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.050479 [2] L1 tensor(89930.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.055110 [2] L2 tensor(6097.7295, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.058894 [2] L3 tensor(7925.4434, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.062767 [2] L4 tensor(489.2115, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.065278 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.082557 [2] L1 tensor(89903.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.088542 [2] L2 tensor(6066.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.091778 [2] L3 tensor(7923.3789, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.094018 [2] L4 tensor(489.3509, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.096452 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.107385 [2] L1 tensor(89876.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.113940 [2] L2 tensor(6037.3926, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.116547 [2] L3 tensor(7921.4487, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.119030 [2] L4 tensor(489.5436, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.121444 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.133737 [2] L1 tensor(89850.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.139413 [2] L2 tensor(6008.5723, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.143721 [2] L3 tensor(7920.0205, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.146858 [2] L4 tensor(489.7487, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.149248 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.162504 [2] L1 tensor(89827.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.165877 [2] L2 tensor(5980.7783, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.168831 [2] L3 tensor(7919.0127, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.171941 [2] L4 tensor(489.9792, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.174934 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.188888 [2] L1 tensor(89805.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.192093 [2] L2 tensor(5954.4502, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.194733 [2] L3 tensor(7918.4092, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.197564 [2] L4 tensor(490.2450, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.200374 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.215835 [2] L1 tensor(89782.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.221510 [2] L2 tensor(5929.5557, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.224219 [2] L3 tensor(7917.1353, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.226495 [2] L4 tensor(490.4838, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.228937 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.237500 [2] L1 tensor(89760.8828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.240539 [2] L2 tensor(5906.1587, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.243744 [2] L3 tensor(7915.2920, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.247066 [2] L4 tensor(490.7103, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.250510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.261740 [2] L1 tensor(89742.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.265848 [2] L2 tensor(5883.9072, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.269206 [2] L3 tensor(7913.0674, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.271966 [2] L4 tensor(490.8849, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.274424 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.291477 [2] L1 tensor(89725.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.297465 [2] L2 tensor(5863.2334, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.300189 [2] L3 tensor(7910.7129, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.302436 [2] L4 tensor(491.0353, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.304918 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.318181 [2] L1 tensor(89711.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.323076 [2] L2 tensor(5844.0439, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.327068 [2] L3 tensor(7908.2539, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.329567 [2] L4 tensor(491.1834, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.332035 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.341344 [2] L1 tensor(89696.6797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.344785 [2] L2 tensor(5826.1553, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.348040 [2] L3 tensor(7906.1543, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.350884 [2] L4 tensor(491.3640, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.353315 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.363698 [2] L1 tensor(89685.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.368533 [2] L2 tensor(5809.2324, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.372959 [2] L3 tensor(7903.4854, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.376107 [2] L4 tensor(491.2783, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.378622 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.390734 [2] L1 tensor(89668.5156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.396455 [2] L2 tensor(5782.9229, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.399760 [2] L3 tensor(7899.7290, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.402163 [2] L4 tensor(491.1868, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.404702 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.415494 [2] L1 tensor(89654.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.422199 [2] L2 tensor(5758.6558, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.424859 [2] L3 tensor(7895.5166, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.427338 [2] L4 tensor(491.1088, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.429997 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.442662 [2] L1 tensor(89639.9453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.447419 [2] L2 tensor(5733.5200, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.451266 [2] L3 tensor(7891.9409, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.454509 [2] L4 tensor(491.0822, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.457064 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.469048 [2] L1 tensor(89627.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.472610 [2] L2 tensor(5711.6113, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.475500 [2] L3 tensor(7888.9756, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.478625 [2] L4 tensor(491.1104, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.481848 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.497192 [2] L1 tensor(89618.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.502365 [2] L2 tensor(5692.4775, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.506125 [2] L3 tensor(7886.5928, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.510736 [2] L4 tensor(491.1937, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.513122 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.528519 [2] L1 tensor(89613.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.531890 [2] L2 tensor(5675.2920, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.535848 [2] L3 tensor(7883.5737, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.539479 [2] L4 tensor(491.2465, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.541893 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.558381 [2] L1 tensor(89608.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.564002 [2] L2 tensor(5659.4434, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.568012 [2] L3 tensor(7879.7822, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.570884 [2] L4 tensor(491.3054, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.573261 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.586420 [2] L1 tensor(89605.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.592571 [2] L2 tensor(5645.0059, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.596067 [2] L3 tensor(7875.9121, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.599077 [2] L4 tensor(491.3718, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.601450 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.614508 [2] L1 tensor(89606.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.620461 [2] L2 tensor(5631.7451, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.623861 [2] L3 tensor(7871.9946, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.626425 [2] L4 tensor(491.4679, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.628928 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.642022 [2] L1 tensor(89606.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.645777 [2] L2 tensor(5619.5215, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.649338 [2] L3 tensor(7867.1426, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.652812 [2] L4 tensor(491.5798, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.655859 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.670321 [2] L1 tensor(89606.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.674502 [2] L2 tensor(5607.9160, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.677999 [2] L3 tensor(7862.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.681497 [2] L4 tensor(491.7184, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.684161 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.698758 [2] L1 tensor(89606.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.704499 [2] L2 tensor(5597.3418, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.708146 [2] L3 tensor(7857.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.710616 [2] L4 tensor(491.8526, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.713248 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.727865 [2] L1 tensor(89612.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.733591 [2] L2 tensor(5587.6641, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.736115 [2] L3 tensor(7851.6646, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.738323 [2] L4 tensor(491.7527, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.740732 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.756224 [2] L1 tensor(89632.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.761362 [2] L2 tensor(5579.0195, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.764044 [2] L3 tensor(7845.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.767540 [2] L4 tensor(491.6373, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.769927 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.784193 [2] L1 tensor(89654.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.790104 [2] L2 tensor(5570.6724, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.792295 [2] L3 tensor(7839.6465, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.795567 [2] L4 tensor(491.5720, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.797964 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.814538 [2] L1 tensor(89675.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.820539 [2] L2 tensor(5563.5771, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.823788 [2] L3 tensor(7834.9033, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.826004 [2] L4 tensor(491.5689, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.828406 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.843952 [2] L1 tensor(89687.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.849786 [2] L2 tensor(5557.3779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.852251 [2] L3 tensor(7831.0566, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.854472 [2] L4 tensor(491.6295, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.856856 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.870504 [2] L1 tensor(89693.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.877038 [2] L2 tensor(5551.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.879818 [2] L3 tensor(7828.0684, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.882054 [2] L4 tensor(491.7559, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.884448 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.898765 [2] L1 tensor(89693., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.904027 [2] L2 tensor(5545.9360, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.907919 [2] L3 tensor(7824.0986, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.911378 [2] L4 tensor(491.8821, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.913802 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.930303 [2] L1 tensor(89679.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.936821 [2] L2 tensor(5538.7637, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.940287 [2] L3 tensor(7819.1602, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.942677 [2] L4 tensor(492.0254, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.945233 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.957372 [2] L1 tensor(89648.5234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.962156 [2] L2 tensor(5529.5820, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.964371 [2] L3 tensor(7814.4331, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.966766 [2] L4 tensor(492.1658, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.969152 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:14.985754 [2] L1 tensor(89593.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.991858 [2] L2 tensor(5518.6353, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.995546 [2] L3 tensor(7808.9355, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:14.997793 [2] L4 tensor(492.2938, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.000194 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.015072 [2] L1 tensor(89530.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.020326 [2] L2 tensor(5505.3262, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.023934 [2] L3 tensor(7803.7461, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.027458 [2] L4 tensor(492.4462, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.029849 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.046843 [2] L1 tensor(89445.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.052811 [2] L2 tensor(5488.7334, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.056009 [2] L3 tensor(7797.5410, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.058515 [2] L4 tensor(492.5581, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.060952 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.078634 [2] L1 tensor(89355.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.084426 [2] L2 tensor(5471.3774, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.087610 [2] L3 tensor(7791.6758, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.090018 [2] L4 tensor(492.6813, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.092436 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.109489 [2] L1 tensor(89248.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.115294 [2] L2 tensor(5452.1006, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.119197 [2] L3 tensor(7784.8213, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.121415 [2] L4 tensor(492.5934, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.123833 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.139559 [2] L1 tensor(89145.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.145391 [2] L2 tensor(5433.4092, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.148133 [2] L3 tensor(7779.3276, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.150398 [2] L4 tensor(492.5703, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.152779 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.166980 [2] L1 tensor(89048.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.172672 [2] L2 tensor(5415.8945, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.176203 [2] L3 tensor(7774.9517, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.179022 [2] L4 tensor(492.6166, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.182104 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.196710 [2] L1 tensor(88959.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.201465 [2] L2 tensor(5399.6489, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.204553 [2] L3 tensor(7770.6099, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.207023 [2] L4 tensor(492.6955, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.209709 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.226987 [2] L1 tensor(88878.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.232041 [2] L2 tensor(5384.5879, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.235899 [2] L3 tensor(7765.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.239002 [2] L4 tensor(492.7455, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.241383 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.255515 [2] L1 tensor(88802.2422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.261428 [2] L2 tensor(5370.6543, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.264438 [2] L3 tensor(7758.4502, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.267106 [2] L4 tensor(492.7861, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.270055 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.288031 [2] L1 tensor(88732.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.294076 [2] L2 tensor(5357.8354, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.298272 [2] L3 tensor(7752.1958, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.300486 [2] L4 tensor(492.8394, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.302880 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.317895 [2] L1 tensor(88671.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.323640 [2] L2 tensor(5346.4995, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.327321 [2] L3 tensor(7746.3706, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.329513 [2] L4 tensor(492.9205, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.331935 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.347243 [2] L1 tensor(88615.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.352969 [2] L2 tensor(5336.1084, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.355965 [2] L3 tensor(7739.6787, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.358183 [2] L4 tensor(492.9848, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.360578 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.375265 [2] L1 tensor(88563.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.380980 [2] L2 tensor(5327.1206, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.384274 [2] L3 tensor(7734.1802, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.387004 [2] L4 tensor(493.0863, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.389382 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.407287 [2] L1 tensor(88516.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.413006 [2] L2 tensor(5318.8306, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.415895 [2] L3 tensor(7729.6104, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.418125 [2] L4 tensor(493.2258, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.420499 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.434041 [2] L1 tensor(88471.7578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.440147 [2] L2 tensor(5310.9170, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.444065 [2] L3 tensor(7724.9180, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.447293 [2] L4 tensor(493.3699, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.449679 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.466657 [2] L1 tensor(88430.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.472811 [2] L2 tensor(5303.4268, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.475872 [2] L3 tensor(7718.8301, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.478161 [2] L4 tensor(493.3211, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.480602 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.494859 [2] L1 tensor(88391.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.500779 [2] L2 tensor(5296.4053, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.504424 [2] L3 tensor(7711.6699, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.507806 [2] L4 tensor(493.2770, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.510555 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.526731 [2] L1 tensor(88354.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.533183 [2] L2 tensor(5289.8555, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.536465 [2] L3 tensor(7704.6060, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.539280 [2] L4 tensor(493.2583, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.541699 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.553304 [2] L1 tensor(88320.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.559881 [2] L2 tensor(5283.6401, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.563884 [2] L3 tensor(7696.5981, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.566121 [2] L4 tensor(493.2222, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.568515 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.584603 [2] L1 tensor(88286.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.587844 [2] L2 tensor(5277.6035, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.590138 [2] L3 tensor(7687.3916, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.592732 [2] L4 tensor(493.1475, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.595338 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.610432 [2] L1 tensor(88254.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.617412 [2] L2 tensor(5272.0479, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.620372 [2] L3 tensor(7679.6270, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.622639 [2] L4 tensor(493.1252, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.625275 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.643279 [2] L1 tensor(88223.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.648843 [2] L2 tensor(5267.0005, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.652401 [2] L3 tensor(7673.1987, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.654995 [2] L4 tensor(493.1563, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.657440 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.670779 [2] L1 tensor(88195.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.673363 [2] L2 tensor(5262.1768, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.675944 [2] L3 tensor(7665.4185, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.678623 [2] L4 tensor(493.1565, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.681513 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.699242 [2] L1 tensor(88169.5234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.705702 [2] L2 tensor(5257.5640, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.708161 [2] L3 tensor(7657.7012, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.710382 [2] L4 tensor(493.1763, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.712772 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.726359 [2] L1 tensor(88146.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.730684 [2] L2 tensor(5253.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.734506 [2] L3 tensor(7649.1050, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.736740 [2] L4 tensor(493.1922, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.739126 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.755729 [2] L1 tensor(88124.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.761460 [2] L2 tensor(5249.3330, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.764033 [2] L3 tensor(7641.1553, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.766672 [2] L4 tensor(493.2054, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.769079 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.778360 [2] L1 tensor(88103.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.784223 [2] L2 tensor(5245.1709, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.787844 [2] L3 tensor(7633.7900, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.790112 [2] L4 tensor(493.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.792543 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.807598 [2] L1 tensor(88083.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.813971 [2] L2 tensor(5241.0825, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.816226 [2] L3 tensor(7625.3706, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.820128 [2] L4 tensor(493.2283, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.823156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.840427 [2] L1 tensor(88063.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.846031 [2] L2 tensor(5237.4404, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.848984 [2] L3 tensor(7618.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.851297 [2] L4 tensor(493.2749, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.854013 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.863450 [2] L1 tensor(88045.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.866768 [2] L2 tensor(5234.2280, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.869895 [2] L3 tensor(7613.3066, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.872106 [2] L4 tensor(493.3685, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.874484 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.886147 [2] L1 tensor(88028.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.892928 [2] L2 tensor(5230.9102, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.896296 [2] L3 tensor(7607.8213, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.899036 [2] L4 tensor(493.4725, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.901430 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.914607 [2] L1 tensor(88012.5234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.917138 [2] L2 tensor(5227.5815, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.919722 [2] L3 tensor(7602.0537, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.922353 [2] L4 tensor(493.5916, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.925241 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.944056 [2] L1 tensor(87998.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.947669 [2] L2 tensor(5224.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.949991 [2] L3 tensor(7595.0410, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.952658 [2] L4 tensor(493.6966, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.955114 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.970727 [2] L1 tensor(87984.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.977397 [2] L2 tensor(5220.7720, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.980887 [2] L3 tensor(7588.8262, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.983621 [2] L4 tensor(493.8004, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.986047 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.994958 [2] L1 tensor(87970.6172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:15.997507 [2] L2 tensor(5216.4814, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.000014 [2] L3 tensor(7580.7461, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.002825 [2] L4 tensor(493.8594, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.005608 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.017647 [2] L1 tensor(87958.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.020432 [2] L2 tensor(5212.1792, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.022749 [2] L3 tensor(7571.4580, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.025054 [2] L4 tensor(493.8966, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.027631 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.044235 [2] L1 tensor(87945.2500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.049614 [2] L2 tensor(5207.4111, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.052854 [2] L3 tensor(7564.4092, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.055853 [2] L4 tensor(493.9751, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.059131 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.070700 [2] L1 tensor(87933.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.077400 [2] L2 tensor(5203.1699, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.080364 [2] L3 tensor(7559.2153, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.082858 [2] L4 tensor(494.0815, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.085354 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.094429 [2] L1 tensor(87923.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.097114 [2] L2 tensor(5198.9482, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.099508 [2] L3 tensor(7551.9668, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.101923 [2] L4 tensor(494.1476, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.104442 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.115290 [2] L1 tensor(87916.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.119651 [2] L2 tensor(5195.6895, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.123903 [2] L3 tensor(7545.8496, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.127593 [2] L4 tensor(494.2112, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.130071 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.140921 [2] L1 tensor(87910.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.145967 [2] L2 tensor(5192.7354, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.150149 [2] L3 tensor(7538.3682, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.153007 [2] L4 tensor(494.2507, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.155653 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.169019 [2] L1 tensor(87903.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.174265 [2] L2 tensor(5189.7148, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.178065 [2] L3 tensor(7530.5679, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.180699 [2] L4 tensor(494.3002, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.183127 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.192050 [2] L1 tensor(87898.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.195921 [2] L2 tensor(5186.5176, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.199013 [2] L3 tensor(7522.4727, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.202272 [2] L4 tensor(494.3649, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.204663 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.216146 [2] L1 tensor(87894.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.219317 [2] L2 tensor(5182.8486, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.221812 [2] L3 tensor(7512.7363, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.224408 [2] L4 tensor(494.2433, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.227217 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.243678 [2] L1 tensor(87889.8359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.249067 [2] L2 tensor(5177.3799, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.252464 [2] L3 tensor(7504.2822, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.255161 [2] L4 tensor(494.1383, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.257667 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.266881 [2] L1 tensor(87877.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.269602 [2] L2 tensor(5158.8506, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.272141 [2] L3 tensor(7497.7524, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.275127 [2] L4 tensor(494.0794, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.278241 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.291689 [2] L1 tensor(87862.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.296740 [2] L2 tensor(5139.9697, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.299897 [2] L3 tensor(7493.0527, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.302627 [2] L4 tensor(494.0676, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.305032 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.318931 [2] L1 tensor(87850.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.325553 [2] L2 tensor(5121.5024, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.328591 [2] L3 tensor(7486.5840, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.331271 [2] L4 tensor(494.0327, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.334146 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.347697 [2] L1 tensor(87847.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.353275 [2] L2 tensor(5107.2832, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.356727 [2] L3 tensor(7478.6582, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.359725 [2] L4 tensor(493.9893, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.362808 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.372580 [2] L1 tensor(87847.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.376479 [2] L2 tensor(5094.4785, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.380752 [2] L3 tensor(7472.2329, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.383909 [2] L4 tensor(493.9891, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.386397 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.398091 [2] L1 tensor(87846.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.404078 [2] L2 tensor(5082.9023, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.407495 [2] L3 tensor(7464.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.410912 [2] L4 tensor(493.9799, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.413398 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.423476 [2] L1 tensor(87845.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.426207 [2] L2 tensor(5071.0684, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.430471 [2] L3 tensor(7454.1279, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.435182 [2] L4 tensor(493.9351, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.437557 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.448382 [2] L1 tensor(87839.1719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.454446 [2] L2 tensor(5058.6934, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.459117 [2] L3 tensor(7445.1729, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.461756 [2] L4 tensor(493.8968, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.464276 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.474819 [2] L1 tensor(87832.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.479576 [2] L2 tensor(5046.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.484143 [2] L3 tensor(7435.6611, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.488425 [2] L4 tensor(493.8749, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.491609 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.508236 [2] L1 tensor(87827.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.514019 [2] L2 tensor(5035.6021, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.516330 [2] L3 tensor(7425.6304, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.519415 [2] L4 tensor(493.8727, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.521811 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.535466 [2] L1 tensor(87824.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.541467 [2] L2 tensor(5026.0786, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.545896 [2] L3 tensor(7415.1670, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.548263 [2] L4 tensor(493.8905, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.550650 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.565231 [2] L1 tensor(87822.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.571931 [2] L2 tensor(5017.4712, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.575769 [2] L3 tensor(7405.8457, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.578530 [2] L4 tensor(493.9127, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.580919 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.590262 [2] L1 tensor(87819.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.596152 [2] L2 tensor(5009.7021, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.599847 [2] L3 tensor(7398.2642, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.602093 [2] L4 tensor(493.9695, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.604489 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.618116 [2] L1 tensor(87820.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.624348 [2] L2 tensor(5002.7212, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.627680 [2] L3 tensor(7392.2432, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.630114 [2] L4 tensor(494.0606, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.634258 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.651673 [2] L1 tensor(87821.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.658276 [2] L2 tensor(4996.1846, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.660478 [2] L3 tensor(7383.5356, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.662699 [2] L4 tensor(494.1093, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.665118 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.675983 [2] L1 tensor(87822.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.678528 [2] L2 tensor(4990.2451, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.680710 [2] L3 tensor(7373.4629, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.683196 [2] L4 tensor(494.1425, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.685739 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.702845 [2] L1 tensor(87823.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.709204 [2] L2 tensor(4984.8652, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.712050 [2] L3 tensor(7365.1074, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.714311 [2] L4 tensor(494.2082, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.716724 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.729434 [2] L1 tensor(87825.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.733022 [2] L2 tensor(4980.0254, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.736278 [2] L3 tensor(7355.3647, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.739546 [2] L4 tensor(494.2617, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.742723 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.757297 [2] L1 tensor(87826.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.762956 [2] L2 tensor(4975.6338, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.765647 [2] L3 tensor(7346.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.768325 [2] L4 tensor(494.3123, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.770984 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.786537 [2] L1 tensor(87828.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.792040 [2] L2 tensor(4971.6523, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.796171 [2] L3 tensor(7339.1299, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.800012 [2] L4 tensor(494.3657, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.804152 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.820236 [2] L1 tensor(87830.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.826041 [2] L2 tensor(4968.1919, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.828291 [2] L3 tensor(7332.8350, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.831456 [2] L4 tensor(494.4484, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.834095 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.843129 [2] L1 tensor(87834.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.845875 [2] L2 tensor(4964.9360, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.848164 [2] L3 tensor(7323.6309, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.850839 [2] L4 tensor(494.4904, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.853591 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.865787 [2] L1 tensor(87839.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.870717 [2] L2 tensor(4962.0059, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.873641 [2] L3 tensor(7316.2178, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.876584 [2] L4 tensor(494.5668, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.879315 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.894725 [2] L1 tensor(87842.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.900601 [2] L2 tensor(4959.1416, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.904255 [2] L3 tensor(7307.6260, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.907108 [2] L4 tensor(494.6472, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.909498 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.921589 [2] L1 tensor(87845.6484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.927990 [2] L2 tensor(4956.2622, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.931230 [2] L3 tensor(7297.9170, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.934339 [2] L4 tensor(494.7349, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.936746 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.945916 [2] L1 tensor(87848.4766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.949594 [2] L2 tensor(4953.4678, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.953168 [2] L3 tensor(7287.2148, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.955933 [2] L4 tensor(494.8544, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.958360 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.968756 [2] L1 tensor(87851.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.972103 [2] L2 tensor(4950.8477, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.974766 [2] L3 tensor(7274.8057, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.978005 [2] L4 tensor(494.9356, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.981079 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:16.994983 [2] L1 tensor(87853.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:16.999147 [2] L2 tensor(4948.4082, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.003168 [2] L3 tensor(7260.9014, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.005673 [2] L4 tensor(494.9958, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.008209 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.023790 [2] L1 tensor(87856.4609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.027972 [2] L2 tensor(4946.2856, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.031734 [2] L3 tensor(7249.3174, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.035384 [2] L4 tensor(495.0984, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.037801 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.054534 [2] L1 tensor(87858.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.060758 [2] L2 tensor(4944.3633, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.064332 [2] L3 tensor(7239.1226, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.067137 [2] L4 tensor(495.1625, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.069522 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.083026 [2] L1 tensor(87860., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.089666 [2] L2 tensor(4942.6050, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.093388 [2] L3 tensor(7230.2114, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.096255 [2] L4 tensor(495.2415, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.098994 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.114552 [2] L1 tensor(87860.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.121002 [2] L2 tensor(4941.0249, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.124265 [2] L3 tensor(7223.1445, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.127157 [2] L4 tensor(495.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.129534 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.146895 [2] L1 tensor(87858., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.152560 [2] L2 tensor(4939.2759, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.155587 [2] L3 tensor(7212.8096, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.158226 [2] L4 tensor(495.4567, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.160721 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.176433 [2] L1 tensor(87852.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.182094 [2] L2 tensor(4937.5049, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.184455 [2] L3 tensor(7200.2646, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.187786 [2] L4 tensor(495.3598, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.190214 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.204764 [2] L1 tensor(87847.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.210319 [2] L2 tensor(4935.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.213150 [2] L3 tensor(7186.3662, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.216222 [2] L4 tensor(495.2668, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.218618 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.235785 [2] L1 tensor(87842.7266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.240515 [2] L2 tensor(4934.3506, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.244065 [2] L3 tensor(7171.2842, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.246306 [2] L4 tensor(495.1876, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.248714 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.264549 [2] L1 tensor(87838.3203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.267992 [2] L2 tensor(4933.0273, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.271976 [2] L3 tensor(7158.6123, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.275413 [2] L4 tensor(495.1689, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.278066 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.290288 [2] L1 tensor(87834.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.295518 [2] L2 tensor(4931.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.299755 [2] L3 tensor(7148.1392, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.302071 [2] L4 tensor(495.2084, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.304539 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.316019 [2] L1 tensor(87830.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.322570 [2] L2 tensor(4930.4761, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.325127 [2] L3 tensor(7136.3931, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.327693 [2] L4 tensor(495.2696, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.330104 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.346900 [2] L1 tensor(87827.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.352584 [2] L2 tensor(4929.1138, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.356193 [2] L3 tensor(7123.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.359115 [2] L4 tensor(495.3551, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.361513 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.373216 [2] L1 tensor(87824.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.376296 [2] L2 tensor(4927.7314, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.379377 [2] L3 tensor(7109.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.382468 [2] L4 tensor(495.4666, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.385841 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.402532 [2] L1 tensor(87821.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.406185 [2] L2 tensor(4926.4697, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.409269 [2] L3 tensor(7097.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.412374 [2] L4 tensor(495.5835, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.415321 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.429422 [2] L1 tensor(87819.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.433163 [2] L2 tensor(4925.3184, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.435921 [2] L3 tensor(7086.5986, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.438876 [2] L4 tensor(495.7116, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.441889 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.458525 [2] L1 tensor(87817.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.462854 [2] L2 tensor(4924.1738, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.467145 [2] L3 tensor(7072.6729, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.470199 [2] L4 tensor(495.8012, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.472591 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.490044 [2] L1 tensor(87816.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.495841 [2] L2 tensor(4923.0361, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.499397 [2] L3 tensor(7055.8457, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.501622 [2] L4 tensor(495.8672, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.504053 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.516036 [2] L1 tensor(87816.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.522051 [2] L2 tensor(4922.0430, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.524801 [2] L3 tensor(7041.5811, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.527401 [2] L4 tensor(495.9786, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.530193 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.546495 [2] L1 tensor(87816.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.552376 [2] L2 tensor(4920.9980, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.556152 [2] L3 tensor(7025.3525, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.558953 [2] L4 tensor(495.9046, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.561426 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.570091 [2] L1 tensor(87816.6328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.572636 [2] L2 tensor(4919.9131, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.575002 [2] L3 tensor(7007.4951, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.577378 [2] L4 tensor(495.8124, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.579851 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.594462 [2] L1 tensor(87816.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.600342 [2] L2 tensor(4918.9038, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.604169 [2] L3 tensor(6989.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.607249 [2] L4 tensor(495.7487, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.609622 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.619364 [2] L1 tensor(87817.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.621914 [2] L2 tensor(4918.0283, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.624174 [2] L3 tensor(6974.7393, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.626463 [2] L4 tensor(495.7438, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.628839 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.642219 [2] L1 tensor(87817.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.646709 [2] L2 tensor(4917.2441, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.650250 [2] L3 tensor(6962.0801, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.653951 [2] L4 tensor(495.7943, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.656560 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.671339 [2] L1 tensor(87817.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.676007 [2] L2 tensor(4916.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.680380 [2] L3 tensor(6951.0078, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.683516 [2] L4 tensor(495.8583, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.685905 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.701300 [2] L1 tensor(87817.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.707190 [2] L2 tensor(4915.6729, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.711506 [2] L3 tensor(6938.9263, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.714186 [2] L4 tensor(495.9427, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.716568 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.731022 [2] L1 tensor(87817.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.736540 [2] L2 tensor(4914.9434, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.740574 [2] L3 tensor(6925.6074, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.743378 [2] L4 tensor(496.0112, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.745765 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.763050 [2] L1 tensor(87817.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.768758 [2] L2 tensor(4914.1479, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.771738 [2] L3 tensor(6911.4238, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.774139 [2] L4 tensor(496.1013, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.776574 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.790769 [2] L1 tensor(87817.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.797429 [2] L2 tensor(4913.3506, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.800565 [2] L3 tensor(6895.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.803595 [2] L4 tensor(496.1635, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.806002 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.816301 [2] L1 tensor(87816.9844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.822286 [2] L2 tensor(4912.5967, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.827114 [2] L3 tensor(6881.6929, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.829816 [2] L4 tensor(496.2386, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.832259 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.847865 [2] L1 tensor(87816.7109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.852555 [2] L2 tensor(4911.8398, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.856935 [2] L3 tensor(6866.4502, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.859805 [2] L4 tensor(496.1516, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.862266 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.878489 [2] L1 tensor(87816.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.885010 [2] L2 tensor(4911.1885, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.887915 [2] L3 tensor(6850.4536, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.890449 [2] L4 tensor(496.0685, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.892839 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.906258 [2] L1 tensor(87816.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.909735 [2] L2 tensor(4910.6191, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.913437 [2] L3 tensor(6837.1035, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.917227 [2] L4 tensor(496.0426, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.920322 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.937595 [2] L1 tensor(87816.3594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.940524 [2] L2 tensor(4909.9902, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.944103 [2] L3 tensor(6822.9180, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.947513 [2] L4 tensor(496.0471, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.950148 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.966365 [2] L1 tensor(87816.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.972228 [2] L2 tensor(4909.4629, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.976035 [2] L3 tensor(6811.2065, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.978975 [2] L4 tensor(496.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.981372 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.990122 [2] L1 tensor(87817.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.992684 [2] L2 tensor(4908.9551, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.996001 [2] L3 tensor(6802.2363, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:17.998640 [2] L4 tensor(496.1695, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.001183 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.014678 [2] L1 tensor(87818.4844, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.020173 [2] L2 tensor(4908.3877, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.023692 [2] L3 tensor(6790.6660, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.026716 [2] L4 tensor(496.2030, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.029526 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.043999 [2] L1 tensor(87819.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.048447 [2] L2 tensor(4907.8096, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.052178 [2] L3 tensor(6776.8120, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.054510 [2] L4 tensor(496.2157, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.056914 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.071799 [2] L1 tensor(87819.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.075663 [2] L2 tensor(4907.1206, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.077974 [2] L3 tensor(6762.2666, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.080539 [2] L4 tensor(496.2554, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.083132 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.099799 [2] L1 tensor(87819.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.106059 [2] L2 tensor(4906.5566, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.108290 [2] L3 tensor(6750.1934, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.111345 [2] L4 tensor(496.3405, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.113760 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.130703 [2] L1 tensor(87820.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.136815 [2] L2 tensor(4905.9561, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.139850 [2] L3 tensor(6737.9741, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.142050 [2] L4 tensor(496.3931, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.144460 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.153372 [2] L1 tensor(87820.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.155885 [2] L2 tensor(4905.3721, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.158744 [2] L3 tensor(6729.5947, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.161815 [2] L4 tensor(496.4587, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.164981 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.175577 [2] L1 tensor(87821.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.180763 [2] L2 tensor(4904.7139, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.185429 [2] L3 tensor(6719.8472, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.188396 [2] L4 tensor(496.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.190987 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.206538 [2] L1 tensor(87822.3906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.211896 [2] L2 tensor(4904.1538, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.215832 [2] L3 tensor(6712.1387, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.218933 [2] L4 tensor(496.6760, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.221336 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.235327 [2] L1 tensor(87823.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.239991 [2] L2 tensor(4903.5703, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.244257 [2] L3 tensor(6702.2544, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.246774 [2] L4 tensor(496.6421, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.249141 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.262459 [2] L1 tensor(87823.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.266074 [2] L2 tensor(4903.0459, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.270148 [2] L3 tensor(6690.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.274120 [2] L4 tensor(496.6011, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.276766 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.285615 [2] L1 tensor(87824.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.288679 [2] L2 tensor(4902.5718, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.291248 [2] L3 tensor(6678.4316, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.293826 [2] L4 tensor(496.5623, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.296304 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.310454 [2] L1 tensor(87824.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.314169 [2] L2 tensor(4902.1846, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.317717 [2] L3 tensor(6668.1768, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.320947 [2] L4 tensor(496.5762, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.323723 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.340250 [2] L1 tensor(87825.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.345190 [2] L2 tensor(4901.8174, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.348379 [2] L3 tensor(6659.1904, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.351465 [2] L4 tensor(496.6059, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.353886 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.370211 [2] L1 tensor(87825.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.376297 [2] L2 tensor(4901.3413, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.380050 [2] L3 tensor(6648.9121, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.382926 [2] L4 tensor(496.6585, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.385307 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.400697 [2] L1 tensor(87825.9297, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.406609 [2] L2 tensor(4900.7842, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.410943 [2] L3 tensor(6636.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.413163 [2] L4 tensor(496.6808, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.415580 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.429952 [2] L1 tensor(87825.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.435449 [2] L2 tensor(4900.2134, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.438868 [2] L3 tensor(6621.6216, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.442609 [2] L4 tensor(496.6862, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.445000 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.457267 [2] L1 tensor(87825.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.462665 [2] L2 tensor(4899.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.467219 [2] L3 tensor(6606.3105, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.469413 [2] L4 tensor(496.7193, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.471811 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.487346 [2] L1 tensor(87824.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.493883 [2] L2 tensor(4899.0005, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.496137 [2] L3 tensor(6593.4424, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.498351 [2] L4 tensor(496.7961, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.500720 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.514558 [2] L1 tensor(87824.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.519279 [2] L2 tensor(4898.4658, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.523378 [2] L3 tensor(6581.9854, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.527189 [2] L4 tensor(496.8827, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.529569 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.544714 [2] L1 tensor(87823.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.550846 [2] L2 tensor(4897.8975, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.555226 [2] L3 tensor(6568.4492, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.557482 [2] L4 tensor(496.9326, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.559928 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.568945 [2] L1 tensor(87823.5547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.572095 [2] L2 tensor(4897.4316, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.574747 [2] L3 tensor(6556.8013, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.577525 [2] L4 tensor(497.0202, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.580113 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.596122 [2] L1 tensor(87823.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.601917 [2] L2 tensor(4896.9404, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.604173 [2] L3 tensor(6543.8242, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.607435 [2] L4 tensor(496.9702, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.609856 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.623306 [2] L1 tensor(87822.9609, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.629029 [2] L2 tensor(4896.4204, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.631705 [2] L3 tensor(6529.0703, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.633917 [2] L4 tensor(496.9082, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.636299 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.652292 [2] L1 tensor(87822.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.658212 [2] L2 tensor(4895.8350, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.660414 [2] L3 tensor(6513.7393, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.662642 [2] L4 tensor(496.8792, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.3378, device='cuda:2', grad_fn=<SumBackward0>)
20:33:18.665375 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:54.545751 [2] proc begin: <DistEnv 2/4 nccl>
20:33:54.633523 [2] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 2, |V|: 677, |E|: 3792>
20:33:54.645873 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |   3864 KiB |   3886 KiB |   3933 KiB |  70656 B   |
|       from large pool |   3790 KiB |   3790 KiB |   3790 KiB |      0 B   |
|       from small pool |     74 KiB |     96 KiB |    143 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   3858 KiB |   3880 KiB |   3925 KiB |  67724 B   |
|       from large pool |   3789 KiB |   3789 KiB |   3789 KiB |      0 B   |
|       from small pool |     69 KiB |     90 KiB |    135 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18663 KiB |  18707 KiB |  18804 KiB | 143872 B   |
|       from large pool |  16690 KiB |  16690 KiB |  16690 KiB |      0 B   |
|       from small pool |   1973 KiB |   2045 KiB |   2114 KiB | 143872 B   |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      19    |      24    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |      16    |      18    |      23    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:33:55.325859 [2] L1 tensor(91906.2109, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.477294 [2] L2 tensor(8166.0913, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.539532 [2] L3 tensor(8193.4414, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.543933 [2] L4 tensor(8182.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.547076 [2] L5 tensor(8131.8223, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.550396 [2] L6 tensor(8150.5381, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.553714 [2] L7 tensor(8157.7505, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.556734 [2] L8 tensor(450.2043, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:56.590427 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.458930 [2] L1 tensor(92030.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.463972 [2] L2 tensor(8282.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.466996 [2] L3 tensor(8357.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.469981 [2] L4 tensor(8181.0479, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.473271 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.476211 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.479452 [2] L7 tensor(7993.9106, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.482937 [2] L8 tensor(459.1643, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.487330 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.510954 [2] L1 tensor(92075.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.515912 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.519872 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.522781 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.525059 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.527371 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.529685 [2] L7 tensor(7922.6777, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.531933 [2] L8 tensor(468.1117, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.534540 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.551088 [2] L1 tensor(92072.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.556318 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.560103 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.563371 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.565726 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.568069 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.570371 [2] L7 tensor(8014.0752, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.572658 [2] L8 tensor(476.9705, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.575292 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.591522 [2] L1 tensor(92088.2344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.597965 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.601280 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.604135 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.606726 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.609048 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.611763 [2] L7 tensor(8104.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.614800 [2] L8 tensor(484.1437, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.618241 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.642244 [2] L1 tensor(92158.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.647215 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.649807 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.652359 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.654850 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.657153 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.659491 [2] L7 tensor(8171.7554, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.661770 [2] L8 tensor(491.6121, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.664565 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.687341 [2] L1 tensor(92215.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.693800 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.696344 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.698705 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.701006 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.703356 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.705709 [2] L7 tensor(8224.4043, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.708096 [2] L8 tensor(497.6973, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.710742 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.727567 [2] L1 tensor(92258.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.733425 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.735976 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.738304 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.740595 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.742906 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.745186 [2] L7 tensor(8195.3975, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.747521 [2] L8 tensor(503.7690, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.750124 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.767510 [2] L1 tensor(92304.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.773517 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.776629 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.779246 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.781539 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.783870 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.786221 [2] L7 tensor(8266.2363, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.788499 [2] L8 tensor(510.6747, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.791050 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.814658 [2] L1 tensor(92351.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.819675 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.822589 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.825484 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.828914 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.831244 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.833492 [2] L7 tensor(8319.0059, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.835727 [2] L8 tensor(517.6276, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.838253 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.854480 [2] L1 tensor(92393.8594, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.860982 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.864347 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.866720 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.869015 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.871324 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.873600 [2] L7 tensor(8378.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.875898 [2] L8 tensor(524.2488, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.878469 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.895757 [2] L1 tensor(92392.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.902174 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.904518 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.906840 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.909154 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.911522 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.913854 [2] L7 tensor(8392.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.916156 [2] L8 tensor(530.5532, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.918733 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.939153 [2] L1 tensor(92393.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.944045 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.947834 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.950849 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.953134 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.955409 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.957692 [2] L7 tensor(8399.4043, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.959933 [2] L8 tensor(537.3357, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.962474 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:57.979381 [2] L1 tensor(92393.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.984776 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.987798 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.990137 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.992417 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.994716 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.996980 [2] L7 tensor(8414.0801, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:57.999248 [2] L8 tensor(544.3667, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.001782 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.025825 [2] L1 tensor(92428.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.032070 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.035698 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.038000 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.040263 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.042528 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.044803 [2] L7 tensor(8413.8184, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.047082 [2] L8 tensor(550.5079, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.049583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.067018 [2] L1 tensor(92428.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.072879 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.075859 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.078287 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.080560 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.082846 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.085164 [2] L7 tensor(8355.0488, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.087504 [2] L8 tensor(555.9419, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.090439 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.108201 [2] L1 tensor(92428.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.114505 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.117065 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.119918 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.122253 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.124538 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.126873 [2] L7 tensor(8290.1934, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.129233 [2] L8 tensor(562.5477, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.131800 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.152122 [2] L1 tensor(92418.9375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.158194 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.160528 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.163446 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.165812 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.168136 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.170464 [2] L7 tensor(8219.0967, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.172758 [2] L8 tensor(569.4016, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.175334 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.198489 [2] L1 tensor(92402.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.205108 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.207976 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.210266 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.212518 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.214761 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.217022 [2] L7 tensor(8148.9014, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.219329 [2] L8 tensor(576.3892, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.221850 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.245184 [2] L1 tensor(92384.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.251493 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.255150 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.257470 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.259799 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.262077 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.264367 [2] L7 tensor(8093.5479, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.266802 [2] L8 tensor(582.6779, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.269354 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.292272 [2] L1 tensor(92361.9453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.298430 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.300709 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.302984 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.305273 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.307594 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.309919 [2] L7 tensor(8046.7803, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.312251 [2] L8 tensor(589.5211, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.314787 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.335888 [2] L1 tensor(92339.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.340679 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.344484 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.347347 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.349629 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.351996 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.354321 [2] L7 tensor(8028.6914, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.356638 [2] L8 tensor(596.7709, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.360725 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.384471 [2] L1 tensor(92315.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.390401 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.392615 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.395007 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.397327 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.399706 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.402050 [2] L7 tensor(7966.2676, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.404336 [2] L8 tensor(604.4969, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.406838 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.425382 [2] L1 tensor(92295.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.430540 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.432760 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.435023 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.437207 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.439511 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.441822 [2] L7 tensor(7895.0420, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.444146 [2] L8 tensor(612.1814, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.446585 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.470190 [2] L1 tensor(92276.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.475796 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.479974 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.482412 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.484607 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.486955 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.489255 [2] L7 tensor(7822.0205, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.491575 [2] L8 tensor(619.7263, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.494039 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.518547 [2] L1 tensor(92255.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.524699 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.527521 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.530083 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.532934 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.535822 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.538652 [2] L7 tensor(7735.8633, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.541486 [2] L8 tensor(627.0621, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.544420 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.567692 [2] L1 tensor(92245.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.571927 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.575784 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.578615 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.581141 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.583983 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.586821 [2] L7 tensor(7640.6553, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.589701 [2] L8 tensor(634.7565, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.592603 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.614794 [2] L1 tensor(92235.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.618483 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.621272 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.624283 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.627055 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.629639 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.634050 [2] L7 tensor(7652.3271, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.637179 [2] L8 tensor(642.3046, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.639994 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.663220 [2] L1 tensor(92220.7734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.669034 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.671943 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.674311 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.677270 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.680204 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.683180 [2] L7 tensor(7614.2900, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.686077 [2] L8 tensor(650.1603, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.689110 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.706655 [2] L1 tensor(92207.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.712837 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.716039 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.718736 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.721395 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.724354 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.727261 [2] L7 tensor(7582.5835, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.730270 [2] L8 tensor(658.2415, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.733309 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.753930 [2] L1 tensor(92190.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.759141 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.761286 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.763467 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.765619 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.767781 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.770096 [2] L7 tensor(7547.9399, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.772252 [2] L8 tensor(665.7829, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.774622 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.796487 [2] L1 tensor(92185.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.801990 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.805350 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.808077 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.810259 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.812445 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.814605 [2] L7 tensor(7515.8604, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.816762 [2] L8 tensor(672.8223, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.819205 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.842060 [2] L1 tensor(92183.1094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.846915 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.849098 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.851991 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.854985 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.860352 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.864904 [2] L7 tensor(7476.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.868051 [2] L8 tensor(680.0869, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.870571 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.893619 [2] L1 tensor(92181.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.899140 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.902839 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.905045 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.907227 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.909414 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.911612 [2] L7 tensor(7443.5044, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.913815 [2] L8 tensor(687.2549, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.916233 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.939155 [2] L1 tensor(92180.3672, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.945677 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.950544 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.954719 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.959522 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.963477 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.965682 [2] L7 tensor(7404.4849, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.967858 [2] L8 tensor(694.3385, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:58.970412 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:58.998610 [2] L1 tensor(92164.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.005167 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.008006 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.010194 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.012481 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.014674 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.016857 [2] L7 tensor(7326.3232, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.019039 [2] L8 tensor(700.2833, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.021400 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.042980 [2] L1 tensor(92148.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.048211 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.051156 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.054240 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.056518 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.058716 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.060908 [2] L7 tensor(7246.6230, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.063126 [2] L8 tensor(706.7782, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.065528 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.087308 [2] L1 tensor(92127.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.093900 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.096216 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.098571 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.100744 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.104608 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.108243 [2] L7 tensor(7189.6465, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.113604 [2] L8 tensor(713.7858, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.116528 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.141328 [2] L1 tensor(92107.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.146920 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.151466 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.153696 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.155881 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.158109 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.160285 [2] L7 tensor(7135.2983, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.162492 [2] L8 tensor(720.7952, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.164871 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.185884 [2] L1 tensor(92092.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.192461 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.195696 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.197944 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.200326 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.202519 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.204728 [2] L7 tensor(7084.1738, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.206955 [2] L8 tensor(728.2336, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.209328 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.231364 [2] L1 tensor(92074.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.237098 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.239832 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.242187 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.244423 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.246620 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.248808 [2] L7 tensor(7037.1904, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.251029 [2] L8 tensor(735.5088, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.253405 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.274801 [2] L1 tensor(92067.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.281328 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.284002 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.286193 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.288354 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.290611 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.292796 [2] L7 tensor(7023.4854, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.294989 [2] L8 tensor(743.4106, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.297406 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.321719 [2] L1 tensor(92060.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.326833 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.331237 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.333447 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.335636 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.337838 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.340029 [2] L7 tensor(6988.1309, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.342200 [2] L8 tensor(752.2716, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.344576 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.362280 [2] L1 tensor(92051.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.367598 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.371787 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.374289 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.376567 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.378736 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.380876 [2] L7 tensor(6953.8452, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.383037 [2] L8 tensor(761.9441, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.385415 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.402439 [2] L1 tensor(92048.3516, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.408328 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.412065 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.414728 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.416865 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.419022 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.421196 [2] L7 tensor(6912.2715, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.423354 [2] L8 tensor(771.5643, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.425731 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.447143 [2] L1 tensor(92044.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.452853 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.455831 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.458019 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.460169 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.462352 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.464497 [2] L7 tensor(6886.2417, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.466684 [2] L8 tensor(781.2713, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.469036 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.486549 [2] L1 tensor(92047.0391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.492788 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.495772 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.498131 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.500389 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.502606 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.504866 [2] L7 tensor(6890.9771, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.507042 [2] L8 tensor(790.4954, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.509426 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.532835 [2] L1 tensor(92048.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.537557 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.541257 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.544167 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.546331 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.548480 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.550669 [2] L7 tensor(6886.4268, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.552809 [2] L8 tensor(799.3633, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.555197 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.577955 [2] L1 tensor(92049.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.584451 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.587709 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.589899 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.592058 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.594263 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.596448 [2] L7 tensor(6894.8955, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.598689 [2] L8 tensor(808.7374, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.601062 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.624400 [2] L1 tensor(92048.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.630991 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.634368 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.636539 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.638687 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.640819 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.642958 [2] L7 tensor(6898.0254, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.645091 [2] L8 tensor(818.4190, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.647511 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.670525 [2] L1 tensor(92054.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.676470 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.679913 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.682418 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.684560 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.686718 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.688860 [2] L7 tensor(6895.3540, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.691008 [2] L8 tensor(826.7911, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.693407 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.716406 [2] L1 tensor(92055.1953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.722368 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.727101 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.732011 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.736798 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.739924 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.742173 [2] L7 tensor(6929.4932, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.744384 [2] L8 tensor(835.1722, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.746793 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.764630 [2] L1 tensor(92060.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.770592 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.772824 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.775024 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.777169 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.779306 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.781455 [2] L7 tensor(6966.1211, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.783668 [2] L8 tensor(843.9580, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.786050 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.804668 [2] L1 tensor(92063.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.810256 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.812840 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.815043 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.817398 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.819671 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.821841 [2] L7 tensor(7004.0137, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.823988 [2] L8 tensor(852.5330, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.826369 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.843477 [2] L1 tensor(92065.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.850060 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.852446 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.854643 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.856802 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.858977 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.861190 [2] L7 tensor(6996.9292, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.863397 [2] L8 tensor(861.7837, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.865780 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.889560 [2] L1 tensor(92066.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.895200 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.897825 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.900614 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.903290 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.905585 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.907810 [2] L7 tensor(6986.4253, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.910054 [2] L8 tensor(871.3632, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.912458 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.935857 [2] L1 tensor(92066.9062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.943359 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.945813 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.948328 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.950828 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.953354 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.955882 [2] L7 tensor(6986.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.958424 [2] L8 tensor(881.7352, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.961048 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:59.983559 [2] L1 tensor(92066.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.989809 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.993258 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.995818 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:33:59.997978 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.000389 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.002973 [2] L7 tensor(6997.6528, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.005539 [2] L8 tensor(893.2026, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.008190 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.030159 [2] L1 tensor(92064.5859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.036121 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.039614 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.041793 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.044380 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.046980 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.049554 [2] L7 tensor(6996.4224, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.052168 [2] L8 tensor(905.3839, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.054928 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.076941 [2] L1 tensor(92061.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.082468 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.085383 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.088466 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.090692 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.092867 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.095135 [2] L7 tensor(7007.0684, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.097340 [2] L8 tensor(917.2354, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.099770 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.122652 [2] L1 tensor(92057.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.128767 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.134198 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.136619 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.139025 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.141602 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.144223 [2] L7 tensor(7016.7778, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.146827 [2] L8 tensor(928.8240, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.149483 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.174277 [2] L1 tensor(92052.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.180024 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.183305 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.185696 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.188176 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.190602 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.193186 [2] L7 tensor(7031.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.195778 [2] L8 tensor(938.5357, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.198489 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.219446 [2] L1 tensor(92047.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.225500 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.228995 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.231499 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.233650 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.236136 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.238689 [2] L7 tensor(7052.1240, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.241287 [2] L8 tensor(948.2116, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.243924 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.266348 [2] L1 tensor(92040.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.272096 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.275596 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.277773 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.280259 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.282823 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.285335 [2] L7 tensor(7068.1011, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.287951 [2] L8 tensor(958.4678, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.290605 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.314300 [2] L1 tensor(92032.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.320028 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.323297 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.325890 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.328690 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.331287 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.333855 [2] L7 tensor(7079.6675, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.336457 [2] L8 tensor(967.2245, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.339169 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.358151 [2] L1 tensor(92024.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.363085 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.367532 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.370130 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.372332 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.375062 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.377483 [2] L7 tensor(7084.8979, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.379708 [2] L8 tensor(976.2678, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.382116 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.405353 [2] L1 tensor(92016.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.410630 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.414184 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.416765 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.419324 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.421934 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.424420 [2] L7 tensor(7097.3198, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.427022 [2] L8 tensor(985.8372, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.429693 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.452703 [2] L1 tensor(92006.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.458351 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.460703 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.463952 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.466259 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.468906 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.471437 [2] L7 tensor(7090.5596, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.474029 [2] L8 tensor(996.5497, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.476745 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.499927 [2] L1 tensor(91997.3281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.505707 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.508001 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.510614 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.512946 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.515528 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.518097 [2] L7 tensor(7083.6445, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.520664 [2] L8 tensor(1006.6285, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.523344 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.548136 [2] L1 tensor(91987.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.551752 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.554436 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.557032 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.559450 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.561643 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.564167 [2] L7 tensor(7090.2759, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.566761 [2] L8 tensor(1017.0908, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.569590 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.593968 [2] L1 tensor(91976.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.598389 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.603214 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.605622 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.608153 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.610776 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.613351 [2] L7 tensor(7101.8350, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.615989 [2] L8 tensor(1028.5674, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.618739 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.641818 [2] L1 tensor(91965.6250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.646113 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.649264 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.652226 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.654791 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.657377 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.659937 [2] L7 tensor(7102.4204, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.662585 [2] L8 tensor(1040.0034, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.665255 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.683122 [2] L1 tensor(91954.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.687461 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.691713 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.694012 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.696616 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.699243 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.701762 [2] L7 tensor(7109.1299, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.704425 [2] L8 tensor(1050.3302, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.707169 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.730401 [2] L1 tensor(91942.6094, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.735076 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.738002 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.740735 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.743281 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.745825 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.748311 [2] L7 tensor(7115.3667, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.750922 [2] L8 tensor(1061.2329, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.753546 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.770923 [2] L1 tensor(91930.6797, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.776692 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.779805 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.781969 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.784478 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.787070 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.789628 [2] L7 tensor(7121.3765, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.792221 [2] L8 tensor(1071.2010, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.794980 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.815182 [2] L1 tensor(91918.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.819363 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.823505 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.826067 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.828527 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.831153 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.833717 [2] L7 tensor(7129.3828, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.836040 [2] L8 tensor(1080.0259, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.838421 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.858266 [2] L1 tensor(91905.5078, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.864535 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.867953 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.870719 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.872846 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.874960 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.877177 [2] L7 tensor(7137.1338, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.879330 [2] L8 tensor(1087.9192, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.881685 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.898879 [2] L1 tensor(91894.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.905433 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.908046 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.910385 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.912630 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.914769 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.916936 [2] L7 tensor(7156.0615, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.919125 [2] L8 tensor(1097.4600, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.921497 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.938747 [2] L1 tensor(91883.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.945363 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.947815 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.950195 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.952473 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.954670 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.956837 [2] L7 tensor(7170.8271, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.959036 [2] L8 tensor(1107.8916, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.961416 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:00.984976 [2] L1 tensor(91872.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.990469 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.993978 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.996500 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:00.998666 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.000850 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.003052 [2] L7 tensor(7185.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.005259 [2] L8 tensor(1118.9893, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.007659 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.023889 [2] L1 tensor(91861., device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.029213 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.032225 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.034968 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.037713 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.039956 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.042318 [2] L7 tensor(7199.2324, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.044938 [2] L8 tensor(1130.3167, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.047630 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.065418 [2] L1 tensor(91849.3047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.071284 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.076664 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.082157 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.084285 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.086449 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.088924 [2] L7 tensor(7210.0137, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.091512 [2] L8 tensor(1142.5631, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.094196 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.117940 [2] L1 tensor(91838.0625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.123153 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.127493 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.129809 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.132261 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.134826 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.137364 [2] L7 tensor(7226.8770, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.139970 [2] L8 tensor(1154.7158, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.142629 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.162022 [2] L1 tensor(91821.5547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.165704 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.168271 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.170749 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.173163 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.175528 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.177706 [2] L7 tensor(7240.4722, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.179971 [2] L8 tensor(1167.4799, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.182722 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.215744 [2] L1 tensor(91805.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.221947 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.227504 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.231424 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.234748 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.238971 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.245933 [2] L7 tensor(7257.1367, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.249151 [2] L8 tensor(1180.9768, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.252053 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.270569 [2] L1 tensor(91789.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.276115 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.281096 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.286034 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.290999 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.295883 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.300931 [2] L7 tensor(7272.3467, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.303953 [2] L8 tensor(1194.1365, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.307049 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.327737 [2] L1 tensor(91775.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.331244 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.333954 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.336856 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.339457 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.341651 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.344070 [2] L7 tensor(7296.3428, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.346658 [2] L8 tensor(1206.1471, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.349331 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.370758 [2] L1 tensor(91761.8359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.375942 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.378657 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.381629 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.384706 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.388444 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.391479 [2] L7 tensor(7308.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.394393 [2] L8 tensor(1218.3230, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.397218 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.424218 [2] L1 tensor(91748.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.429358 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.435414 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.438908 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.442997 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.445420 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.447840 [2] L7 tensor(7320.4033, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.450047 [2] L8 tensor(1230.9602, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.452903 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.479991 [2] L1 tensor(91733.7422, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.484875 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.490218 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.496229 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.501457 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.508239 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.512218 [2] L7 tensor(7331.7754, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.516835 [2] L8 tensor(1243.7366, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.521280 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.543904 [2] L1 tensor(91725.0156, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.547998 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.551428 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.554994 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.560421 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.563196 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.566358 [2] L7 tensor(7327.8857, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.569432 [2] L8 tensor(1255.2939, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.573805 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.596202 [2] L1 tensor(91716.0234, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.599374 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.601765 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.604126 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.606385 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.608805 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.611148 [2] L7 tensor(7322.5342, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.613506 [2] L8 tensor(1266.2397, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.618826 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.645149 [2] L1 tensor(91704.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.649384 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.653221 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.656572 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.660132 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.663597 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.667026 [2] L7 tensor(7321.3926, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.670404 [2] L8 tensor(1276.5854, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.674825 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.695543 [2] L1 tensor(91693.7188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.703640 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.709690 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.715241 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.720903 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.726444 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.732219 [2] L7 tensor(7320.0356, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.737802 [2] L8 tensor(1287.2778, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.745459 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.766526 [2] L1 tensor(91684.1562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.771693 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.776722 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.782721 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.787321 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.792208 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.797114 [2] L7 tensor(7318.8408, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.800750 [2] L8 tensor(1298.6079, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.805187 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.824799 [2] L1 tensor(91675.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.830663 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.834244 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.837370 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.840014 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.843031 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.846942 [2] L7 tensor(7317.9033, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.849255 [2] L8 tensor(1309.6160, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.852203 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.887296 [2] L1 tensor(91665.1484, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.893952 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.899530 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.905473 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.912194 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.916842 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.922604 [2] L7 tensor(7317.0186, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.927590 [2] L8 tensor(1321.7622, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.930202 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:01.952620 [2] L1 tensor(91655.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.961529 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.965711 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.968332 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.971041 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.974247 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.977461 [2] L7 tensor(7316.6641, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.980621 [2] L8 tensor(1335.4668, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:01.984073 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.015736 [2] L1 tensor(91647.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.020678 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.024452 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.028156 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.031783 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.035746 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.039485 [2] L7 tensor(7315.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.044545 [2] L8 tensor(1349.3062, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.048583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.074232 [2] L1 tensor(91639.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.080255 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.085891 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.091592 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.097327 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.102967 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.108629 [2] L7 tensor(7315.3213, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.114372 [2] L8 tensor(1363.8130, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.120905 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.147142 [2] L1 tensor(91632.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.154310 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.159781 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.165476 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.171127 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.176894 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.182482 [2] L7 tensor(7315.0063, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.188227 [2] L8 tensor(1377.6981, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.194124 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.231708 [2] L1 tensor(91626.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.238460 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.243519 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.249183 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.254604 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.260247 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.266940 [2] L7 tensor(7314.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.272432 [2] L8 tensor(1391.4271, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.278470 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.301165 [2] L1 tensor(91620.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.305249 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.308390 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.311469 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.314716 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.317984 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.320505 [2] L7 tensor(7314.2070, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.323013 [2] L8 tensor(1405.6222, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.325762 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.351052 [2] L1 tensor(91615.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.354743 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.359523 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.363911 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.368568 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.371943 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.375326 [2] L7 tensor(7313.7842, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.377698 [2] L8 tensor(1418.0425, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.380424 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.408692 [2] L1 tensor(91611.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.412420 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.417149 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.422765 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.427385 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.431684 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.436037 [2] L7 tensor(7313.3398, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.440300 [2] L8 tensor(1429.7803, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.444665 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.468005 [2] L1 tensor(91607.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.472466 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.475708 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.478373 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.481005 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.483661 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.486218 [2] L7 tensor(7312.8184, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.488795 [2] L8 tensor(1440.7744, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.491911 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.512366 [2] L1 tensor(91604.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.515862 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.519626 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.522409 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.525365 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.528515 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.532875 [2] L7 tensor(7312.3081, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.537130 [2] L8 tensor(1452.0017, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.541678 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.563896 [2] L1 tensor(91601.4688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.567790 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.572861 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.579928 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.584416 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.589014 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.593890 [2] L7 tensor(7311.5332, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.596813 [2] L8 tensor(1463.4434, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.602002 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.628758 [2] L1 tensor(91598.8984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.637167 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.641393 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.646518 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.651264 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.653935 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.657902 [2] L7 tensor(7310.7427, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.660398 [2] L8 tensor(1475.3798, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.664191 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.690246 [2] L1 tensor(91596.6016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.695320 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.699908 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.704514 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.709029 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.713585 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.717771 [2] L7 tensor(7310.3271, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.720585 [2] L8 tensor(1487.6847, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.723613 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.752771 [2] L1 tensor(91594.5391, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.761364 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.763831 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.766047 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.768241 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.770738 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.773128 [2] L7 tensor(7309.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.776850 [2] L8 tensor(1500.0754, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.779744 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.802319 [2] L1 tensor(91592.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.811904 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.817343 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.820094 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.824626 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.826868 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.829061 [2] L7 tensor(7309.5820, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.831314 [2] L8 tensor(1513.7412, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.833834 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.860394 [2] L1 tensor(91591.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.863526 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.867659 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.870022 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.875876 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.879299 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.881526 [2] L7 tensor(7309.2988, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.883753 [2] L8 tensor(1527.6387, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.886245 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.910052 [2] L1 tensor(91589.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.916666 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.919799 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.922205 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.924457 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.926679 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.929003 [2] L7 tensor(7308.9502, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.931348 [2] L8 tensor(1541.2878, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.933813 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.957994 [2] L1 tensor(91586.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.962845 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.965300 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.967771 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.970185 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.972420 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.974736 [2] L7 tensor(7308.2002, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.977027 [2] L8 tensor(1554.8827, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:02.979467 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:02.997612 [2] L1 tensor(91583.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.002036 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.004734 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.007029 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.009333 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.011680 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.013993 [2] L7 tensor(7307.4434, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.016257 [2] L8 tensor(1566.3264, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.018725 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.041936 [2] L1 tensor(91581.1328, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.048494 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.051617 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.053890 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.056177 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.058431 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.060796 [2] L7 tensor(7306.6514, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.063120 [2] L8 tensor(1578.8008, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.065584 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.089117 [2] L1 tensor(91578.8750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.094598 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.098637 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.100950 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.103158 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.105320 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.107653 [2] L7 tensor(7305.5166, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.109940 [2] L8 tensor(1590.5959, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.112384 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.132131 [2] L1 tensor(91576.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.139554 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.144979 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.147948 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.150160 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.152336 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.154567 [2] L7 tensor(7304.7495, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.156775 [2] L8 tensor(1602.1790, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.159240 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.181917 [2] L1 tensor(91575.0938, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.187404 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.191634 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.194138 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.196333 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.198557 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.200754 [2] L7 tensor(7304.0596, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.203009 [2] L8 tensor(1612.7454, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.205466 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.234959 [2] L1 tensor(91573.5469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.241025 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.244186 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.246939 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.249124 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.251360 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.253565 [2] L7 tensor(7303.4873, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.256014 [2] L8 tensor(1625.0591, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.258560 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.279204 [2] L1 tensor(91572.4141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.284485 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.287998 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.291030 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.293219 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.295406 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.297573 [2] L7 tensor(7303.1309, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.299783 [2] L8 tensor(1637.4131, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.302223 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.327043 [2] L1 tensor(91569.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.333579 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.335989 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.338281 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.340545 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.342785 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.345086 [2] L7 tensor(7303.3247, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.347392 [2] L8 tensor(1649.8020, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.349849 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.367838 [2] L1 tensor(91567.2656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.374183 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.376418 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.378757 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.381004 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.383294 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.385577 [2] L7 tensor(7303.9424, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.387870 [2] L8 tensor(1662.8516, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.390395 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.407826 [2] L1 tensor(91565.0859, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.414258 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.416453 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.418769 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.421110 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.423354 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.425781 [2] L7 tensor(7304.7437, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.428002 [2] L8 tensor(1675.3856, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.430448 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.453786 [2] L1 tensor(91563.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.457003 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.459371 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.461813 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.464220 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.466622 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.468921 [2] L7 tensor(7305.5039, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.471180 [2] L8 tensor(1688.0088, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.473651 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.495498 [2] L1 tensor(91561.4062, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.502093 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.505537 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.507873 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.510057 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.512242 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.514451 [2] L7 tensor(7305.4678, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.516656 [2] L8 tensor(1699.0746, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.519122 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.541901 [2] L1 tensor(91559.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.546423 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.551152 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.553602 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.557273 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.562922 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.567320 [2] L7 tensor(7305.4199, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.569556 [2] L8 tensor(1709.9758, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.572006 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.595426 [2] L1 tensor(91558.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.601937 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.604208 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.606935 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.609113 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.613461 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.618394 [2] L7 tensor(7305.3774, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.622912 [2] L8 tensor(1720.4250, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.625533 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.648080 [2] L1 tensor(91557.2969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.653591 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.657330 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.660176 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.662453 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.664757 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.667152 [2] L7 tensor(7305.5938, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.669458 [2] L8 tensor(1731.5405, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.671923 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.695064 [2] L1 tensor(91556.2188, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.701480 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.704058 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.706635 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.708826 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.711049 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.713357 [2] L7 tensor(7305.9639, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.715694 [2] L8 tensor(1741.6255, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.718164 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.741009 [2] L1 tensor(91555.2812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.747773 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.751135 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.753340 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.755735 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.757927 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.760237 [2] L7 tensor(7305.7988, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.762576 [2] L8 tensor(1751.8569, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.765052 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.787023 [2] L1 tensor(91554.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.792173 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.795788 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.798501 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.800838 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.803198 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.805596 [2] L7 tensor(7305.7085, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.807986 [2] L8 tensor(1761.5076, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.810512 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.828153 [2] L1 tensor(91553.7969, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.834025 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.836450 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.838680 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.840894 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.843290 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.845626 [2] L7 tensor(7305.6274, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.848006 [2] L8 tensor(1771.3596, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.850584 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.871929 [2] L1 tensor(91558.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.876064 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.879021 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.882209 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.884471 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.887060 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.889419 [2] L7 tensor(7305.7729, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.891752 [2] L8 tensor(1778.0099, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.894221 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.917800 [2] L1 tensor(91562.8281, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.922812 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.927070 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.929677 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.932167 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.934546 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.936851 [2] L7 tensor(7305.7598, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.939168 [2] L8 tensor(1782.4081, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.941657 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:03.965684 [2] L1 tensor(91572.8438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.971202 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.975422 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.977765 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.981254 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.983545 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.985760 [2] L7 tensor(7305.9077, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.988018 [2] L8 tensor(1790.0247, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:03.990509 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.014265 [2] L1 tensor(91581.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.020382 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.024053 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.026553 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.028722 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.030917 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.033199 [2] L7 tensor(7305.8701, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.035591 [2] L8 tensor(1799.4231, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.038053 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.061593 [2] L1 tensor(91590.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.067234 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.071364 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.073586 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.075821 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.078168 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.080469 [2] L7 tensor(7305.6919, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.082836 [2] L8 tensor(1807.3804, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.085287 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.103434 [2] L1 tensor(91597.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.109921 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.112150 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.114521 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.116798 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.119070 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.121361 [2] L7 tensor(7305.4805, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.123706 [2] L8 tensor(1815.8324, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.126176 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.149428 [2] L1 tensor(91604.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.154521 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.156687 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.158871 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.161204 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.163603 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.165940 [2] L7 tensor(7305.0278, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.168220 [2] L8 tensor(1826.3904, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.170677 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.188642 [2] L1 tensor(91600.9688, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.194544 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.199898 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.204457 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.209770 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.211930 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.214125 [2] L7 tensor(7305.0186, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.216281 [2] L8 tensor(1836.8647, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.218700 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.239730 [2] L1 tensor(91597.6875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.243474 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.246430 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.249801 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.252215 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.254886 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.257156 [2] L7 tensor(7305.2495, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.259352 [2] L8 tensor(1847.7650, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.261821 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.283014 [2] L1 tensor(91594.6719, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.288388 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.291991 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.294878 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.297021 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.299330 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.301666 [2] L7 tensor(7305.3184, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.303981 [2] L8 tensor(1859.8657, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.306455 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.323845 [2] L1 tensor(91600.2266, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.328880 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.332307 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.335174 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.337331 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.339469 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.341609 [2] L7 tensor(7305.7202, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.343958 [2] L8 tensor(1872.3767, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.346375 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.363021 [2] L1 tensor(91605.2578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.367658 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.371141 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.374338 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.376520 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.378729 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.380918 [2] L7 tensor(7305.9404, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.383144 [2] L8 tensor(1884.2997, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.385598 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.402931 [2] L1 tensor(91609.7891, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.408931 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.412336 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.415037 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.417195 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.419341 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.421549 [2] L7 tensor(7306.0884, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.423865 [2] L8 tensor(1895.4619, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.426336 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.443179 [2] L1 tensor(91613.8359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.449785 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.453048 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.455895 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.458100 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.460354 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.462652 [2] L7 tensor(7306.2393, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.465038 [2] L8 tensor(1905.0767, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.467468 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.491160 [2] L1 tensor(91617.4922, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.496247 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.499475 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.502609 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.504782 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.506934 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.509095 [2] L7 tensor(7306.7227, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.511347 [2] L8 tensor(1914.0349, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.513786 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.532392 [2] L1 tensor(91622.1250, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.537586 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.540089 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.543566 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.545735 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.548023 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.550376 [2] L7 tensor(7307.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.552646 [2] L8 tensor(1921.9152, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.555050 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.574345 [2] L1 tensor(91626.3203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.579158 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.583457 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.586123 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.588518 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.590847 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.593205 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.595580 [2] L8 tensor(1929.7485, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.598034 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.621804 [2] L1 tensor(91630.1016, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.627148 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.631325 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.633683 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.635842 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.638014 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.640344 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.642663 [2] L8 tensor(1938.5867, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.645116 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.663238 [2] L1 tensor(91630.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.668988 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.671950 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.674129 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.676267 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.678603 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.680926 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.683244 [2] L8 tensor(1947.7693, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.685677 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.703597 [2] L1 tensor(91630.8203, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.709518 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.712018 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.714355 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.716911 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.719445 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.721752 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.724132 [2] L8 tensor(1954.5996, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.726548 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.744229 [2] L1 tensor(91631.0547, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.750321 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.752557 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.754918 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.757225 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.759582 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.761905 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.764236 [2] L8 tensor(1961.9805, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.766694 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.784883 [2] L1 tensor(91631.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.790398 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.792579 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.794761 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.797044 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.799233 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.801569 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.803880 [2] L8 tensor(1966.5870, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.806308 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.823878 [2] L1 tensor(91631.2031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.830200 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.832452 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.834808 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.837122 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.839456 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.842113 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.847336 [2] L8 tensor(1970.8381, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.849930 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.868755 [2] L1 tensor(91626.4219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.873596 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.876556 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.879085 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.881568 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.883813 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.886009 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.888164 [2] L8 tensor(1978.1454, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.890592 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.915343 [2] L1 tensor(91621.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.921833 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.924144 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.926760 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.928997 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.931327 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.933628 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.935995 [2] L8 tensor(1984.0890, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.938462 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:04.961608 [2] L1 tensor(91617.7578, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.967046 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.969802 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.972562 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.975090 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.977277 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.979476 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.981665 [2] L8 tensor(1990.6523, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:04.984084 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.002082 [2] L1 tensor(91613.7344, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.007147 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.011508 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.014069 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.016317 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.018660 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.021091 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.023615 [2] L8 tensor(1995.3953, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.026267 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.044081 [2] L1 tensor(91609.9141, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.049875 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.054104 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.056411 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.058619 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.060799 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.063113 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.065436 [2] L8 tensor(2000.4619, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.067928 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.086113 [2] L1 tensor(91606.2734, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.091301 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.095658 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.098154 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.100553 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.102880 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.105238 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.107570 [2] L8 tensor(2007.9070, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.110057 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.129494 [2] L1 tensor(91602.7812, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.134699 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.139139 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.141422 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.143897 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.146210 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.148526 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.150880 [2] L8 tensor(2017.1493, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.153324 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.177198 [2] L1 tensor(91591.8125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.182610 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.186941 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.189346 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.191656 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.194002 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.196318 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.198680 [2] L8 tensor(2026.3612, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.201138 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.222840 [2] L1 tensor(91581.6953, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.229429 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.232208 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.234558 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.236857 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.239198 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.241535 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.243896 [2] L8 tensor(2036.9098, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.246325 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.264093 [2] L1 tensor(91572.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.270247 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.272517 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.274840 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.277128 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.279489 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.281832 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.284113 [2] L8 tensor(2045.9766, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.286552 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.303936 [2] L1 tensor(91563.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.310351 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.312698 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.315158 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.317573 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.320001 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.322480 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.324901 [2] L8 tensor(2055.1167, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.327532 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.347527 [2] L1 tensor(91555.8047, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.353798 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.359074 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.361288 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.363555 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.366698 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.369986 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.372735 [2] L8 tensor(2064.7595, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.375439 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.398341 [2] L1 tensor(91548.4375, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.404063 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.408004 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.410454 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.412633 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.414966 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.417300 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.419615 [2] L8 tensor(2074.9424, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.422077 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.445112 [2] L1 tensor(91536.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.450800 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.454788 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.456975 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.459241 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.461533 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.463913 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.466234 [2] L8 tensor(2086.4458, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.468654 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.491146 [2] L1 tensor(91525.6406, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.496882 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.499929 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.502124 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.504363 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.506533 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.508855 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.511187 [2] L8 tensor(2098.4275, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.513593 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.531917 [2] L1 tensor(91515.6562, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.537786 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.540041 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.542300 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.544620 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.546927 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.549274 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.551614 [2] L8 tensor(2107.1714, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.554109 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.572548 [2] L1 tensor(91506.5000, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.578311 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.580727 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.583840 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.586096 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.588543 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.591053 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.593515 [2] L8 tensor(2116.1304, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.596145 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.614225 [2] L1 tensor(91498.1172, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.617088 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.619845 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.622797 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.625277 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.627620 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.629915 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.632072 [2] L8 tensor(2126.3237, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.634510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.652148 [2] L1 tensor(91488.9219, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.657812 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.660739 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.663250 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.665716 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.668194 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.670703 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.673151 [2] L8 tensor(2136.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.675754 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.694364 [2] L1 tensor(91479.0469, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.699258 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.703622 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.706261 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.708561 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.710840 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.713169 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.715479 [2] L8 tensor(2145.9819, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.717919 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.741860 [2] L1 tensor(91470.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.747434 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.751582 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.753902 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.756200 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.758556 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.760878 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.764295 [2] L8 tensor(2156.6565, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.766872 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.784894 [2] L1 tensor(91460.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.790419 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.792650 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.795006 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.797312 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.799628 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.801926 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.804219 [2] L8 tensor(2167.7449, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.806644 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.823130 [2] L1 tensor(91451.7031, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.829867 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.832861 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.835404 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.837823 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.840240 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.842706 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.845149 [2] L8 tensor(2176.8025, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.847755 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.870411 [2] L1 tensor(91440.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.876992 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.879950 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.882255 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.884562 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.886881 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.889230 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.891583 [2] L8 tensor(2188.1221, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.894057 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.917512 [2] L1 tensor(91429.5625, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.922558 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.927042 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.929407 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.931740 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.934130 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.936620 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.938944 [2] L8 tensor(2197.4336, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.941421 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:05.964148 [2] L1 tensor(91419.4531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.970617 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.974445 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.976646 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.978836 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.981007 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.983214 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.985490 [2] L8 tensor(2208.6543, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:05.987955 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.010397 [2] L1 tensor(91414.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.015636 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.018184 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.020737 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.023131 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.025320 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.027618 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.029950 [2] L8 tensor(2220.6272, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.032363 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.050497 [2] L1 tensor(91411.1875, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.055456 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.059465 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.062161 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.064459 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.066774 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.069048 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.071401 [2] L8 tensor(2233.1575, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.073882 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.097328 [2] L1 tensor(91408.3125, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.102674 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.106999 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.109315 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.111608 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.113941 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.116277 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.118623 [2] L8 tensor(2246.8120, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.121045 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.139418 [2] L1 tensor(91405.7500, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.143816 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.146650 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.149885 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.152358 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.154585 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.156888 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.159243 [2] L8 tensor(2259.6453, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.161752 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.185259 [2] L1 tensor(91403.9531, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.190532 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.194813 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.197154 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.199470 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.201867 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.204143 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.206451 [2] L8 tensor(2271.0356, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.208894 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.230598 [2] L1 tensor(91402.3438, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.236673 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.239793 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.242007 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.244205 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.246419 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.248759 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.251073 [2] L8 tensor(2285.2490, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.253520 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.272429 [2] L1 tensor(91400.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.278026 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.280222 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.282434 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.284611 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.286817 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.289117 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.291435 [2] L8 tensor(2299.2354, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.293915 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.311665 [2] L1 tensor(91399.5781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.317520 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.320237 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.322558 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.324852 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.327211 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.329704 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.332159 [2] L8 tensor(2312.9934, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.334736 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.358307 [2] L1 tensor(91398.3984, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.362840 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.367380 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.369854 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.372131 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.374460 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.376788 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.379165 [2] L8 tensor(2323.4746, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.381586 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.399051 [2] L1 tensor(91397.3359, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.404479 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.408019 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.410888 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.413093 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.415304 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.417518 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.419721 [2] L8 tensor(2333.1545, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.422290 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.439366 [2] L1 tensor(91396.3750, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.445128 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.448640 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.451730 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.454236 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.456607 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.458884 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.461219 [2] L8 tensor(2343.0676, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.463755 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.486136 [2] L1 tensor(91395.5312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.491307 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.493763 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.496217 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.498631 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.500842 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.503496 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.505813 [2] L8 tensor(2350.1367, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.508256 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.526318 [2] L1 tensor(91394.7656, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.532126 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.535588 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.538025 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.540582 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.543033 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.545506 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.548017 [2] L8 tensor(2357.3701, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.550609 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.569712 [2] L1 tensor(91394.0781, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.574635 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.578705 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.580963 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.583448 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.585792 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.588132 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.590446 [2] L8 tensor(2365.2251, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.592888 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.610901 [2] L1 tensor(91393.4453, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.616387 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.620057 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.622928 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.625131 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.627363 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.629698 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.631994 [2] L8 tensor(2376.1802, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.634509 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.658193 [2] L1 tensor(91392.8906, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.665017 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.667822 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.670227 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.672550 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.674901 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.677723 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.680753 [2] L8 tensor(2386.9541, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.683996 [2] Warning: no training nodes in this partition! Backward fake loss.
20:34:06.702537 [2] L1 tensor(91392.0312, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.4266, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.708370 [2] L2 tensor(8343.7402, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.3534, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.711671 [2] L3 tensor(8466.9336, device='cuda:2', grad_fn=<SumBackward0>) tensor(134.0668, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.713898 [2] L4 tensor(8180.1406, device='cuda:2', grad_fn=<SumBackward0>) tensor(123.5495, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.716288 [2] L5 tensor(8129.6323, device='cuda:2', grad_fn=<SumBackward0>) tensor(122.0187, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.718571 [2] L6 tensor(8153.8779, device='cuda:2', grad_fn=<SumBackward0>) tensor(126.8097, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.720901 [2] L7 tensor(7307.2773, device='cuda:2', grad_fn=<SumBackward0>) tensor(129.2913, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.723219 [2] L8 tensor(2398.0083, device='cuda:2', grad_fn=<SumBackward0>) tensor(7.8731, device='cuda:2', grad_fn=<SumBackward0>)
20:34:06.725674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:06:44.117760 [2] proc begin: <DistEnv 2/4 nccl>
21:06:56.839933 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:06:56.866278 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:07:47.854676 [2] proc begin: <DistEnv 2/4 nccl>
21:07:54.137604 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:07:54.158949 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:09:07.975119 [2] proc begin: <DistEnv 2/4 nccl>
21:09:12.495765 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:09:12.525210 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:12:17.082887 [2] proc begin: <DistEnv 2/4 nccl>
21:12:22.160533 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:12:22.178509 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:13:53.428907 [2] proc begin: <DistEnv 2/4 nccl>
21:13:58.503447 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:13:58.542326 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:14:51.555358 [2] proc begin: <DistEnv 2/4 nccl>
21:14:57.057880 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:14:57.081693 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:19:51.099533 [2] proc begin: <DistEnv 2/4 nccl>
21:19:55.665470 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:19:55.692807 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:20:43.790513 [2] proc begin: <DistEnv 2/4 nccl>
09:20:49.520849 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:20:49.537763 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:20.466322 [2] proc begin: <DistEnv 2/4 nccl>
09:22:43.874993 [2] proc begin: <DistEnv 2/4 nccl>
09:22:48.296420 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:22:48.315680 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:10.662487 [2] proc begin: <DistEnv 2/4 nccl>
09:25:17.060595 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:25:17.081313 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:29:00.134952 [2] proc begin: <DistEnv 2/4 nccl>
09:29:06.681958 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
09:29:06.702605 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:30:21.160855 [2] proc begin: <DistEnv 2/4 nccl>
09:30:35.353863 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:30:35.361156 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:08.917298 [2] proc begin: <DistEnv 2/4 nccl>
09:32:13.833359 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:32:13.841924 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:32:20.443786 [2] Warning: no training nodes in this partition! Backward fake loss.
09:33:37.513061 [2] proc begin: <DistEnv 2/4 nccl>
09:33:41.647014 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:33:41.655546 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:33:46.629988 [2] Warning: no training nodes in this partition! Backward fake loss.
09:35:51.056507 [2] proc begin: <DistEnv 2/4 nccl>
09:35:55.593223 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
09:35:55.606921 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:36:00.372423 [2] Warning: no training nodes in this partition! Backward fake loss.
10:10:17.989725 [2] proc begin: <DistEnv 2/4 nccl>
10:10:22.120393 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:10:22.134644 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:10:27.771564 [2] Warning: no training nodes in this partition! Backward fake loss.
10:12:45.393470 [2] proc begin: <DistEnv 2/4 nccl>
10:12:51.259444 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:12:51.290714 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:15:16.521794 [2] proc begin: <DistEnv 2/4 nccl>
10:15:21.065401 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:15:21.085945 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:16:48.994309 [2] proc begin: <DistEnv 2/4 nccl>
10:16:53.280173 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:16:53.299879 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:17:40.777089 [2] proc begin: <DistEnv 2/4 nccl>
10:17:46.011792 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:17:46.031835 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:19:07.175143 [2] proc begin: <DistEnv 2/4 nccl>
10:19:13.203245 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
10:19:13.226741 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:05.872156 [2] proc begin: <DistEnv 2/4 nccl>
10:20:10.073230 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:20:10.082132 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:34.653100 [2] proc begin: <DistEnv 2/4 nccl>
10:20:39.419145 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
10:20:39.428556 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:20:44.578740 [2] Warning: no training nodes in this partition! Backward fake loss.
16:21:09.486826 [2] proc begin: <DistEnv 2/4 nccl>
16:21:26.430907 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:21:26.451536 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:44.906495 [2] proc begin: <DistEnv 2/4 nccl>
16:28:50.943550 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:28:50.965974 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:18.862694 [2] proc begin: <DistEnv 2/4 nccl>
16:29:25.300844 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:29:25.321375 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:30:57.343251 [2] proc begin: <DistEnv 2/4 nccl>
16:31:03.666053 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:31:03.686738 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:02.887194 [2] proc begin: <DistEnv 2/4 nccl>
16:32:09.699540 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:32:09.720751 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:48.065350 [2] proc begin: <DistEnv 2/4 nccl>
16:32:52.592246 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:32:52.614006 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:35:16.341573 [2] proc begin: <DistEnv 2/4 nccl>
16:35:22.744324 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:35:22.764877 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:38:07.703008 [2] proc begin: <DistEnv 2/4 nccl>
16:38:12.675809 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:38:12.705190 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:56:16.394808 [2] proc begin: <DistEnv 2/4 nccl>
16:56:22.195720 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:56:22.218153 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:05.436960 [2] proc begin: <DistEnv 2/4 nccl>
17:03:11.946333 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:03:11.970340 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:03:31.570659 [2] proc begin: <DistEnv 2/4 nccl>
17:03:38.121279 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:03:38.141570 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:06:09.494313 [2] proc begin: <DistEnv 2/4 nccl>
17:06:15.177808 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
17:06:15.198300 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:34:53.617574 [2] proc begin: <DistEnv 2/4 nccl>
18:41:35.461443 [2] proc begin: <DistEnv 2/4 nccl>
18:44:17.516487 [2] proc begin: <DistEnv 2/4 nccl>
18:46:19.154707 [2] proc begin: <DistEnv 2/4 nccl>
18:46:30.723729 [2] proc begin: <DistEnv 2/4 nccl>
18:49:17.992088 [2] proc begin: <DistEnv 2/4 nccl>
18:55:48.394758 [2] proc begin: <DistEnv 2/4 nccl>
19:38:28.148180 [2] proc begin: <DistEnv 2/4 nccl>
19:38:44.940022 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
19:38:44.974856 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:16:19.269413 [2] proc begin: <DistEnv 2/4 nccl>
20:16:24.601932 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:16:24.622050 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:35.370227 [2] proc begin: <DistEnv 2/4 nccl>
20:27:54.213611 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
20:27:54.239820 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:27:59.764254 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:02.629396 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:04.474135 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:06.304757 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:08.133330 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:09.962203 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:11.792663 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:13.616517 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:15.438912 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:17.261779 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:19.085524 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:20.907678 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:22.732744 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:24.553076 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:26.376149 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:28.200533 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:30.026248 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:31.850111 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:33.674734 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:35.498745 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:37.328439 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:39.157295 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:40.979404 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:42.802113 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:44.626190 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:46.450404 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:48.272875 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:50.094997 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:51.916397 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:53.740238 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:55.563132 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:57.385966 [2] Warning: no training nodes in this partition! Backward fake loss.
20:28:59.210589 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:01.033072 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:02.918702 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:04.742185 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:06.563837 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:08.385016 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:10.207323 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:12.029283 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:13.851969 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:15.673103 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:17.495510 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:19.318728 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:21.139464 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:22.961660 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:24.782864 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:26.604997 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:28.427257 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:30.250629 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:32.073872 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:33.712042 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:35.535943 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:37.174129 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:38.996516 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:40.635087 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:42.457879 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:44.095740 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:45.919200 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:47.557495 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:49.380245 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:51.018532 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:52.841794 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:54.479953 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:56.302081 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:57.940838 [2] Warning: no training nodes in this partition! Backward fake loss.
20:29:59.764378 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:01.400155 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:03.285147 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:04.925781 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:06.752247 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:08.394721 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:10.219548 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:11.858781 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:13.681540 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:15.320163 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:17.143480 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:18.782300 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:20.604962 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:22.243350 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:24.067625 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:25.705329 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:27.529367 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:29.168894 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:30.991703 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:32.630341 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:34.454329 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:36.093559 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:37.917533 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:39.556143 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:41.379583 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:43.018204 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:44.840619 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:46.478390 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:48.304431 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:49.942281 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:51.764748 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:53.402913 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:55.226637 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:56.866933 [2] Warning: no training nodes in this partition! Backward fake loss.
20:30:58.690991 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:00.330057 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:02.187839 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:03.854808 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:05.677640 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:07.317895 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:09.141824 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:10.781340 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:12.605285 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:14.244503 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:16.069758 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:17.709868 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:19.532995 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:21.170952 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:22.993763 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:24.631455 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:26.454652 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:28.092982 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:29.916789 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:31.554475 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:33.377579 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:35.014638 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:36.836925 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:38.474800 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:40.297172 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:41.935089 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:43.757347 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:45.400712 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:47.227406 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:48.865228 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:50.689858 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:52.328617 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:54.151922 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:55.789628 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:57.612853 [2] Warning: no training nodes in this partition! Backward fake loss.
20:31:59.253003 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:01.075974 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:02.743404 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:04.591689 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:06.231115 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:08.056503 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:09.696222 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:11.520288 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:13.158896 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:14.982625 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:16.620196 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:18.442019 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:20.079205 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:21.903156 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:23.540595 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:25.362972 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:26.999785 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:28.822499 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:30.461447 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:32.284740 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:33.922693 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:35.745362 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:37.383512 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:39.206456 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:40.843685 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:42.666620 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:44.303641 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:46.126796 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:47.763716 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:49.586735 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:51.224444 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:53.048444 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:54.685762 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:56.507260 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:58.145578 [2] Warning: no training nodes in this partition! Backward fake loss.
20:32:59.969043 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:01.613204 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:03.490723 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:05.130170 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:06.954624 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:08.594629 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:10.419620 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:12.057582 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:13.880322 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:15.517905 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:17.340959 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:18.979384 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:20.802682 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:22.440485 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:24.263975 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:25.904421 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:27.728379 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:29.367692 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:31.190689 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:32.830411 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:34.653493 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:36.292581 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:38.117667 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:39.757388 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:41.581055 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:43.219868 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:45.043893 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:46.681445 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:48.505494 [2] Warning: no training nodes in this partition! Backward fake loss.
20:33:50.143519 [2] Warning: no training nodes in this partition! Backward fake loss.
20:39:45.221494 [2] proc begin: <DistEnv 2/4 nccl>
20:39:50.334869 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:39:50.355638 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:46:10.289566 [2] proc begin: <DistEnv 2/4 nccl>
20:47:48.474287 [2] proc begin: <DistEnv 2/4 nccl>
20:47:54.155271 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:47:54.171170 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:27:35.390178 [2] proc begin: <DistEnv 2/4 nccl>
21:27:40.428026 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:27:40.449565 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:28:45.752227 [2] proc begin: <DistEnv 2/4 nccl>
21:28:51.043934 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:28:51.081052 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:30:37.663171 [2] proc begin: <DistEnv 2/4 nccl>
21:30:42.918762 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:30:42.935893 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:32:23.896188 [2] proc begin: <DistEnv 2/4 nccl>
21:32:43.083636 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:32:43.104739 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:49:23.092944 [2] proc begin: <DistEnv 2/4 nccl>
21:49:28.625517 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
21:49:28.645980 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:00:42.188530 [2] proc begin: <DistEnv 2/4 nccl>
22:00:56.437867 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:00:56.444951 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

22:01:08.104940 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:10.961828 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:12.802165 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:14.634240 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:16.466013 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:18.299581 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:20.134915 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:21.967130 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:23.800265 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:25.631171 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:27.459051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:29.284995 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:31.107446 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:32.931387 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:34.756660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:36.581471 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:38.410032 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:40.235416 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:42.062468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:43.891579 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:45.717614 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:47.544520 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:49.368698 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:51.195311 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:53.024962 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:54.852182 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:56.679325 [2] Warning: no training nodes in this partition! Backward fake loss.
22:01:58.508951 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:00.337424 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:02.176929 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:04.058142 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:05.891453 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:07.726390 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:09.560242 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:11.395503 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:13.228104 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:15.056907 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:16.883870 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:18.712582 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:20.548072 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:22.378565 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:24.203994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:26.029585 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:27.855229 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:29.682135 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:31.508045 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:33.330779 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:35.158784 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:36.985748 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:38.815212 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:40.644101 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:42.287398 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:44.113335 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:45.755123 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:47.582433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:49.222885 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:51.050052 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:52.688377 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:54.511376 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:56.149500 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:57.972712 [2] Warning: no training nodes in this partition! Backward fake loss.
22:02:59.613893 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:01.439234 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:03.139519 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:04.967244 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:06.610556 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:08.441075 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:10.085272 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:11.913380 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:13.557131 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:15.384530 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:17.026825 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:18.854098 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:20.495358 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:22.322426 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:23.962994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:25.789433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:27.427660 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:29.251107 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:30.892974 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:32.719554 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:34.360700 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:36.185750 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:37.827275 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:39.649515 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:41.288032 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:43.110698 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:44.748557 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:46.572201 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:48.209985 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:50.033579 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:51.672329 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:53.495710 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:55.134507 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:56.961320 [2] Warning: no training nodes in this partition! Backward fake loss.
22:03:58.602829 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:00.428657 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:02.103234 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:03.951569 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:05.594548 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:07.424888 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:09.068420 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:10.900068 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:12.540691 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:14.366553 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:16.006407 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:17.835196 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:19.475921 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:21.303468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:22.945087 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:24.773547 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:26.417634 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:28.246200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:29.888327 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:31.718886 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:33.363404 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:35.192244 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:36.831922 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:38.656804 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:40.293839 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:42.118366 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:43.762137 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:45.588155 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:47.228353 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:49.057051 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:50.696377 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:52.521896 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:54.161679 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:55.987235 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:57.630947 [2] Warning: no training nodes in this partition! Backward fake loss.
22:04:59.459227 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:01.102413 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:02.979744 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:04.631172 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:06.457704 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:08.100433 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:09.925634 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:11.566045 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:13.392994 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:15.033543 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:16.859843 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:18.498632 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:20.323262 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:21.963957 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:23.788587 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:25.427313 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:27.251181 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:28.889169 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:30.712406 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:32.354669 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:34.181148 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:35.825166 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:37.651682 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:39.293662 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:41.120272 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:42.762224 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:44.588817 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:46.229208 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:48.056345 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:49.699425 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:51.525196 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:53.167299 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:54.993892 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:56.635758 [2] Warning: no training nodes in this partition! Backward fake loss.
22:05:58.465991 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:00.109511 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:01.967239 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:03.641534 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:05.471187 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:07.118150 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:08.947880 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:10.593158 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:12.423923 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:14.068133 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:15.892248 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:17.532657 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:19.358884 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:20.997332 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:22.822200 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:24.464649 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:26.291837 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:27.931293 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:29.755845 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:31.394939 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:33.220415 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:34.861303 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:36.687721 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:38.329481 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:40.156174 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:41.797274 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:43.625342 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:45.267094 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:47.094909 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:48.736275 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:50.563128 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:52.204342 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:54.031468 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:55.671416 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:57.498431 [2] Warning: no training nodes in this partition! Backward fake loss.
22:06:59.139203 [2] Warning: no training nodes in this partition! Backward fake loss.
22:28:58.234955 [2] proc begin: <DistEnv 2/4 nccl>
22:29:04.047981 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
22:29:04.062176 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:23.091539 [2] proc begin: <DistEnv 2/4 nccl>
14:08:27.703858 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:08:27.722391 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:08:32.470757 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.075083 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:34.599339 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.121989 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:35.646123 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.168611 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:36.690429 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.212774 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:37.736777 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.261419 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:38.783752 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.307223 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:39.830975 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.354936 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:40.877330 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.399765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:41.921618 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.444615 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:42.969245 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:43.492499 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.014014 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:44.536062 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.058818 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:45.580065 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.101760 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:46.624008 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.146061 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:47.667637 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.190641 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:48.713361 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.238446 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:49.760746 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.285465 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:50.812197 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.337846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:51.864260 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.390058 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:52.915190 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.440732 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:53.967252 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:54.492556 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.018702 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:55.544089 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.068957 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:56.594449 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.121104 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:57.646665 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.173034 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:58.699054 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.223674 [2] Warning: no training nodes in this partition! Backward fake loss.
14:08:59.748559 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.276155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:00.801891 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.331442 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:01.876574 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.421968 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:02.955205 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:03.480244 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.005965 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:04.531901 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.058915 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:05.584170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.109488 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:06.633819 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.158774 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:07.683602 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.207708 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:08.734464 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.262390 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:09.788815 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.316195 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:10.843707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.370800 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:11.896722 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.423466 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:12.948202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.472645 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:13.997038 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:14.522909 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.047853 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:15.574129 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.099074 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:16.623415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.147372 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:17.670551 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.194864 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:18.718734 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.243559 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:19.767057 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.290809 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:20.812785 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.340078 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:21.867657 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.393498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:22.920936 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.447713 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:23.972462 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:24.498911 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.025619 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:25.550685 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.076255 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:26.600677 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.126573 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:27.650649 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.175286 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:28.699452 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.226143 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:29.750995 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.277202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:30.803705 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.330410 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:31.857152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.381296 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:32.906640 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.432849 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:33.956695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:34.482076 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.007030 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:35.534277 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.061945 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:36.588049 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.114833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:37.641160 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.166751 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:38.693534 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.219886 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:39.745797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.271858 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:40.798162 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.324147 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:41.850105 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.375777 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:42.903415 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.430930 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:43.956585 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:44.483154 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.009732 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:45.537128 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.064155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:46.593246 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.120829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:47.647206 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.171469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:48.695875 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.219692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:49.743333 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.267694 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:50.792921 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.317167 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:51.842980 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.369300 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:52.895840 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.421135 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:53.948408 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:54.476696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.003469 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:55.530154 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.055638 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:56.580705 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.107045 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:57.633152 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.159295 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:58.684932 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.211240 [2] Warning: no training nodes in this partition! Backward fake loss.
14:09:59.735846 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.261796 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:00.788180 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.314375 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:01.842305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.389191 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:02.935399 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.471800 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:03.997159 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:04.523232 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.047941 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:05.572870 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.098841 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:06.625263 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.151544 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:07.676562 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.202162 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:08.728073 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.252970 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:09.777150 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.301893 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:10.827736 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.353136 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:11.876258 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.402434 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:12.928752 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.453347 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:13.977302 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:14.503059 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.028697 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:15.555273 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.082598 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:16.610391 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.137946 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:17.665765 [2] Warning: no training nodes in this partition! Backward fake loss.
14:10:18.192306 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:40.558450 [2] proc begin: <DistEnv 2/4 nccl>
14:39:45.082605 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
14:39:45.092793 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:39:51.885504 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:53.928614 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:54.936164 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:55.946696 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:56.955364 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:57.959871 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:58.970721 [2] Warning: no training nodes in this partition! Backward fake loss.
14:39:59.975409 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:00.979594 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:02.008671 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:03.040947 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:04.052150 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:05.058707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:06.066090 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:07.072321 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:08.077438 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:09.083611 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:10.090559 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:11.096116 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:12.103992 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:13.110123 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:14.114472 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:15.116685 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:16.121290 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:17.128004 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:18.130550 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:19.133892 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:20.134216 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:21.137117 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:22.139366 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:23.140057 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:24.140625 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:25.141112 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:26.143001 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:27.145003 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:28.147709 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:29.152497 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:30.156508 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:31.158269 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:32.161210 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:33.163645 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:34.166882 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:35.169324 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:36.173412 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:37.180580 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:38.185490 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:39.188011 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:40.191710 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:41.192724 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:42.194596 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:43.194708 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:44.196938 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:45.198667 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:46.200294 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:47.202020 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:48.201967 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:49.202662 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:50.204684 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:51.206612 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:52.206305 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:53.206029 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:54.203793 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:55.202882 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:56.201713 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:57.204068 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:58.201964 [2] Warning: no training nodes in this partition! Backward fake loss.
14:40:59.201895 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:00.199325 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:01.197373 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:02.208260 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:03.248392 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:04.260170 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:05.263654 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:06.264878 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:07.264833 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:08.263536 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:09.264083 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:10.264446 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:11.266486 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:12.269506 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:13.267997 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:14.266848 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:15.265105 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:16.265060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:17.265522 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:18.266330 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:19.268451 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:20.266829 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:21.267060 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:22.267108 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:23.266733 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:24.265856 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:25.266073 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:26.266155 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:27.272284 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:28.281202 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:29.288495 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:30.290564 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:31.289876 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:32.289354 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:33.289221 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:34.289160 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:35.289872 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:36.288081 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:37.288741 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:38.289236 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:39.296278 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:40.298809 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:41.302479 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:42.306300 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:43.309861 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:44.313411 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:45.315838 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:46.317779 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:47.321125 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:48.324689 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:49.326585 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:50.327804 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:51.329320 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:52.331290 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:53.336057 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:54.339032 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:55.341000 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:56.344631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:57.347283 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:58.346376 [2] Warning: no training nodes in this partition! Backward fake loss.
14:41:59.347104 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:00.347335 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:01.347363 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:02.382137 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:03.407713 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:04.417730 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:05.421167 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:06.424288 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:07.427850 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:08.434909 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:09.439692 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:10.445736 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:11.453054 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:12.465456 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:13.473747 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:14.477432 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:15.480297 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:16.480224 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:17.478035 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:18.476975 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:19.476073 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:20.476658 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:21.473806 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:22.473065 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:23.475428 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:24.477243 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:25.478285 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:26.480040 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:27.478987 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:28.477718 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:29.479647 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:30.476877 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:31.474742 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:32.474381 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:33.476025 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:34.477695 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:35.476979 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:36.477168 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:37.476785 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:38.484654 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:39.488461 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:40.490756 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:41.493252 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:42.494338 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:43.494614 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:44.498311 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:45.500797 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:46.504242 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:47.506002 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:48.506707 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:49.508840 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:50.509332 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:51.513986 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:52.516788 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:53.518888 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:54.519631 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:55.520161 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:56.518116 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:57.516950 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:58.515922 [2] Warning: no training nodes in this partition! Backward fake loss.
14:42:59.517114 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:00.515785 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:01.530265 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:02.570988 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:03.582047 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:04.587000 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:05.588246 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:06.590723 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:07.592157 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:08.593102 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:09.595498 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:10.598670 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:11.602201 [2] Warning: no training nodes in this partition! Backward fake loss.
14:43:12.602032 [2] Warning: no training nodes in this partition! Backward fake loss.
15:29:58.798165 [2] proc begin: <DistEnv 2/4 nccl>
15:30:19.663722 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:30:19.684358 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:30:56.270878 [2] proc begin: <DistEnv 2/4 nccl>
15:31:01.201707 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:31:01.233616 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:32:42.901811 [2] proc begin: <DistEnv 2/4 nccl>
15:32:48.427197 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:32:48.458301 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:25:25.062568 [2] proc begin: <DistEnv 2/4 nccl>
18:25:31.412859 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
18:25:31.434504 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:41.090369 [2] proc begin: <DistEnv 2/4 nccl>
18:30:46.584004 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
18:30:46.616821 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:40.716952 [2] proc begin: <DistEnv 2/4 nccl>
20:00:46.789939 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:00:46.808472 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:03:41.495123 [2] proc begin: <DistEnv 2/4 nccl>
20:03:46.527243 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:03:46.558327 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:05:09.904704 [2] proc begin: <DistEnv 2/4 nccl>
20:05:15.531723 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:05:15.558961 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:06:19.659269 [2] proc begin: <DistEnv 2/4 nccl>
20:06:24.445697 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
20:06:24.480305 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:01.563793 [2] proc begin: <DistEnv 2/4 nccl>
11:34:14.412766 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
11:34:14.434230 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:34:15.219623 [2] L1 tensor(77137.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:2', grad_fn=<SumBackward0>)
15:51:57.349330 [2] proc begin: <DistEnv 2/4 nccl>
15:52:23.545488 [2] proc begin: <DistEnv 2/4 nccl>
15:52:28.923644 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:52:28.943868 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:02:34.742859 [2] proc begin: <DistEnv 2/4 nccl>
16:02:40.037839 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:02:40.058447 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:21:50.774702 [2] proc begin: <DistEnv 2/4 nccl>
16:21:56.363769 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:21:56.387928 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:30:03.540388 [2] proc begin: <DistEnv 2/4 nccl>
18:30:08.987350 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
18:30:09.020661 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:51:37.833177 [2] proc begin: <DistEnv 2/4 nccl>
18:51:43.125959 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
18:51:43.154439 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:22.381537 [2] proc begin: <DistEnv 2/4 nccl>
14:31:26.390811 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:31:26.411173 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:31:27.565086 [2] L1 tensor(77137.2891, device='cuda:2', grad_fn=<SumBackward0>) tensor(260.2319, device='cuda:2', grad_fn=<SumBackward0>)
14:31:59.736214 [2] proc begin: <DistEnv 2/4 nccl>
14:32:05.564455 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
14:32:05.584502 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:32:06.681405 [2] L1 tensor(38548.0586, device='cuda:2', grad_fn=<SumBackward0>) tensor(120.5969, device='cuda:2', grad_fn=<SumBackward0>)
16:15:52.516242 [2] proc begin: <DistEnv 2/4 nccl>
16:15:58.235548 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:15:58.256684 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:44:48.282491 [2] proc begin: <DistEnv 2/4 nccl>
16:44:54.193737 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:44:54.214198 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:16:27.755414 [2] proc begin: <DistEnv 2/4 nccl>
21:16:33.918251 [2] proc begin: <DistEnv 2/4 nccl>
21:16:39.750554 [2] proc begin: <DistEnv 2/4 nccl>
21:16:45.436988 [2] proc begin: <DistEnv 2/4 nccl>
21:16:51.173246 [2] proc begin: <DistEnv 2/4 nccl>
21:16:55.904826 [2] proc begin: <DistEnv 2/4 nccl>
21:17:00.636242 [2] proc begin: <DistEnv 2/4 nccl>
21:17:06.544725 [2] proc begin: <DistEnv 2/4 nccl>
21:17:11.646450 [2] proc begin: <DistEnv 2/4 nccl>
21:20:15.207516 [2] proc begin: <DistEnv 2/4 nccl>
21:21:38.722338 [2] proc begin: <DistEnv 2/4 nccl>
21:23:12.560643 [2] proc begin: <DistEnv 2/4 nccl>
21:23:40.694664 [2] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 8413331>
21:23:40.702380 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1072 MiB |   1088 MiB |   1121 MiB |  51113 KiB |
|       from large pool |   1072 MiB |   1088 MiB |   1121 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1072 MiB |   1088 MiB |   1121 MiB |  51113 KiB |
|       from large pool |   1072 MiB |   1088 MiB |   1121 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1069 MiB |   1084 MiB |   1117 MiB |  48832 KiB |
|       from large pool |   1069 MiB |   1084 MiB |   1117 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1104 MiB |   1104 MiB |   1104 MiB |      0 B   |
|       from large pool |   1102 MiB |   1102 MiB |   1102 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14272 KiB |  19998 KiB |  48715 KiB |  34442 KiB |
|       from large pool |  14272 KiB |  19998 KiB |  42566 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:23:46.474436 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:48.236891 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:48.963933 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:49.684689 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:50.404410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:51.124755 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:51.844178 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:52.564284 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:53.284987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:54.004114 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:54.726025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:55.444336 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:56.164425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:56.882669 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:57.602042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:58.322885 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:59.041306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:23:59.761015 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:00.481381 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:01.201224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:01.939264 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:02.678606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:03.400777 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:04.123092 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:04.844059 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:05.565285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:24:06.286985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:15.455143 [2] proc begin: <DistEnv 2/4 nccl>
21:25:17.740280 [2] graph loaded <COO Graph: e80M_f512_l32_t0.5, |V|: 2000000, |E|: 80000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 8413331>
21:25:17.748959 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1072 MiB |   1088 MiB |   1121 MiB |  51113 KiB |
|       from large pool |   1072 MiB |   1088 MiB |   1121 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1072 MiB |   1088 MiB |   1121 MiB |  51113 KiB |
|       from large pool |   1072 MiB |   1088 MiB |   1121 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1069 MiB |   1084 MiB |   1117 MiB |  48832 KiB |
|       from large pool |   1069 MiB |   1084 MiB |   1117 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1104 MiB |   1104 MiB |   1104 MiB |      0 B   |
|       from large pool |   1102 MiB |   1102 MiB |   1102 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14272 KiB |  19998 KiB |  48715 KiB |  34442 KiB |
|       from large pool |  14272 KiB |  19998 KiB |  42566 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:25:28.068082 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:29.864704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:30.589750 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:31.316685 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:32.045258 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:32.771771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:33.499040 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:34.226024 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:34.949734 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:35.673846 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:36.397088 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:37.117821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:37.839986 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:38.563775 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:39.286419 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:40.009765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:40.731514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:41.452605 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:42.173317 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:42.896279 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:43.620157 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:44.341574 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:45.063319 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:45.785055 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:46.507056 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:47.228170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:47.950535 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:48.672247 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:49.393509 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:50.115747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:50.837750 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:51.559982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:52.282817 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:53.006724 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:53.730781 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:54.454710 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:55.178943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:55.904626 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:56.628687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:57.350211 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:58.072798 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:58.795006 [2] Warning: no training nodes in this partition! Backward fake loss.
21:25:59.516756 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:00.237575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:00.959106 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:01.687586 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:02.430392 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:03.166427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:03.892062 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:04.618067 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:05.342746 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:05.723474 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:06.642942 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:07.024323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:07.945202 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:08.326565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:09.248547 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:09.628680 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:10.549539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:10.930107 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:11.851410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:12.229882 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:13.148194 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:13.527084 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:14.445737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:14.824922 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:15.742219 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:16.120312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:17.037472 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:17.416126 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:18.331167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:18.710707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:19.629013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:20.007341 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:20.928013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:21.304626 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:22.222224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:22.600534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:23.519081 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:23.896967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:24.815636 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:25.193670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:26.110822 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:26.489834 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:27.407955 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:27.786686 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:28.705046 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:29.084364 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:30.002425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:30.381134 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:31.298229 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:31.677408 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:32.594192 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:32.974494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:33.890714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:34.270914 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:35.187666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:35.566722 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:36.484149 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:36.862396 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:37.779584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:38.159256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:39.075874 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:39.454973 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:40.371855 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:40.751552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:41.668408 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:42.046869 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:42.963841 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:43.342706 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:44.259588 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:44.638863 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:45.555137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:45.934347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:46.850462 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:47.229684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:48.145576 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:48.524732 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:49.441267 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:49.819740 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:50.737707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:51.116188 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:52.032852 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:52.412965 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:53.328884 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:53.707558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:54.623137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:55.000031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:55.915959 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:56.293586 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:57.207483 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:57.585385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:58.501577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:58.879502 [2] Warning: no training nodes in this partition! Backward fake loss.
21:26:59.797565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:00.175623 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:01.093436 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:01.471443 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:02.398552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:02.791861 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:03.720821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:04.101370 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:05.019246 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:05.399333 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:06.318434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:06.697059 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:07.616752 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:07.995647 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:08.914601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:09.293964 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:10.214335 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:10.593457 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:11.513862 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:11.893351 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:12.812607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:13.192168 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:14.110901 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:14.488893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:15.407051 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:15.784849 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:16.702186 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:17.080454 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:17.998532 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:18.376455 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:19.294803 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:19.673394 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:20.587195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:20.964859 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:21.879899 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:22.257748 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:23.173189 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:23.551658 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:24.466064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:24.844084 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:25.758497 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:26.136216 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:27.050867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:27.428115 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:28.344725 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:28.722369 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:29.637050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:30.014907 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:30.930403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:31.307821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:32.222985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:32.601438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:33.519505 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:33.897456 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:34.817053 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:35.195427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:36.112909 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:36.491381 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:37.409424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:37.787107 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:38.704772 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:39.082917 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:39.999966 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:40.378050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:41.297079 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:41.674587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:27:49.499781 [2] proc begin: <DistEnv 2/4 nccl>
21:28:36.303535 [2] graph loaded <COO Graph: e320M_f512_l32_t0.5, |V|: 2000000, |E|: 320000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 31921155>
21:28:36.317826 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1250 MiB |   1266 MiB |   1300 MiB |  51113 KiB |
|       from large pool |   1250 MiB |   1266 MiB |   1300 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1250 MiB |   1266 MiB |   1300 MiB |  51113 KiB |
|       from large pool |   1250 MiB |   1266 MiB |   1300 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1248 MiB |   1263 MiB |   1296 MiB |  48832 KiB |
|       from large pool |   1248 MiB |   1263 MiB |   1296 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1284 MiB |   1284 MiB |   1284 MiB |      0 B   |
|       from large pool |   1282 MiB |   1282 MiB |   1282 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15783 KiB |  19998 KiB |  50226 KiB |  34442 KiB |
|       from large pool |  15783 KiB |  19998 KiB |  44077 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:29:02.209420 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:04.996413 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:06.611494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:08.220072 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:09.826734 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:11.433310 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:13.038377 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:14.639747 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:16.240442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:17.840980 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:19.441003 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:21.039617 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:22.639263 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:24.238461 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:25.838839 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:27.437174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:29.035859 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:30.635256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:32.237647 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:33.839167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:35.439993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:37.041430 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:38.643133 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:40.242622 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:41.842320 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:43.442679 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:45.041642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:46.641534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:48.240354 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:49.839461 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:51.438919 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:53.037762 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:54.637370 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:56.237535 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:57.836843 [2] Warning: no training nodes in this partition! Backward fake loss.
21:29:59.435213 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:01.032905 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:02.645532 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:04.253892 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:05.856979 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:07.460202 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:09.062386 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:10.665066 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:12.268447 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:13.872966 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:15.476333 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:17.081219 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:18.685220 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:20.287712 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:21.890160 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:23.494139 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:24.220727 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:26.549546 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:27.273526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:29.605054 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:30.329577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:32.656231 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:33.380924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:35.708726 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:36.433707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:38.761539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:39.486013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:41.814604 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:42.538919 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:44.866453 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:45.592324 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:47.919197 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:48.644311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:50.973303 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:51.698480 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:54.026993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:54.753048 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:57.081021 [2] Warning: no training nodes in this partition! Backward fake loss.
21:30:57.804821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:00.132745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:00.857224 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:03.200982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:03.929478 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:06.259910 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:06.985405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:09.319303 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:10.044443 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:12.376480 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:13.102552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:15.430470 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:16.156287 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:18.484275 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:19.209958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:21.541278 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:22.267153 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:24.594467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:25.318306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:27.645968 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:28.370377 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:30.699133 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:31.424109 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:33.751091 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:34.475310 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:36.802347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:37.526829 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:39.855938 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:40.580606 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:42.907672 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:43.631497 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:45.959880 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:46.683989 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:49.012671 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:49.737141 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:52.064007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:52.788131 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:55.112758 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:55.836955 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:58.164835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:31:58.890506 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:01.216016 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:01.950488 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:04.281787 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:05.006837 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:07.336353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:08.062186 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:10.389556 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:11.114639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:13.441677 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:14.165925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:16.494618 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:17.220032 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:19.547698 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:20.273234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:22.599543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:23.325085 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:25.652721 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:26.377020 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:28.703914 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:29.429437 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:31.756156 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:32.480243 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:34.808013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:35.532670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:37.859001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:38.583348 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:40.909177 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:41.633743 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:43.961774 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:44.686288 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:47.011893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:47.735513 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:50.062050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:50.786120 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:53.117549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:53.841575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:56.167783 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:56.891285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:59.218547 [2] Warning: no training nodes in this partition! Backward fake loss.
21:32:59.942757 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:02.278875 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:03.015723 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:05.343450 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:06.067543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:08.395742 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:09.119983 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:11.448709 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:12.175430 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:14.502395 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:15.229015 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:17.556455 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:18.282601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:20.610845 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:21.336332 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:23.664110 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:24.389560 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:26.717515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:27.443127 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:29.771980 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:30.498269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:32.825502 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:33.550840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:35.877687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:36.602106 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:38.929861 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:39.654984 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:41.982268 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:42.706218 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:45.031672 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:45.756317 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:48.084880 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:48.809428 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:51.134778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:51.859235 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:54.186416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:54.910750 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:57.238132 [2] Warning: no training nodes in this partition! Backward fake loss.
21:33:57.962990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:00.290916 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:01.015362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:03.359399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:04.083614 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:06.416688 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:07.142429 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:09.473815 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:10.197609 [2] Warning: no training nodes in this partition! Backward fake loss.
21:34:18.009100 [2] proc begin: <DistEnv 2/4 nccl>
21:34:44.560117 [2] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:34:44.567313 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 652419 KiB | 668806 KiB | 703532 KiB |  51113 KiB |
|       from large pool | 652419 KiB | 668803 KiB | 703525 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 652419 KiB | 668806 KiB | 703532 KiB |  51113 KiB |
|       from large pool | 652419 KiB | 668803 KiB | 703525 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 651468 KiB | 667094 KiB | 700300 KiB |  48832 KiB |
|       from large pool | 651468 KiB | 667093 KiB | 700296 KiB |  48828 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 688128 KiB | 688128 KiB | 688128 KiB |      0 B   |
|       from large pool | 686080 KiB | 686080 KiB | 686080 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  17276 KiB |  20286 KiB |  51719 KiB |  34442 KiB |
|       from large pool |  17276 KiB |  20286 KiB |  45570 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:35:05.940965 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:07.855048 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:08.719057 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:09.583392 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:10.444315 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:11.309350 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:12.171514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.031072 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.891181 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:14.750675 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:15.611085 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:16.471840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:17.334004 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:18.193558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.050496 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.909091 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:20.768427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:21.626564 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:22.485455 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:23.343976 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:24.201679 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.058470 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.916376 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:26.775165 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:27.634156 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:28.491135 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:29.350299 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:30.207419 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.064769 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.923868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:32.780870 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:33.639335 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:34.495953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:35.353804 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:36.211285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.069788 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.928705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:38.787037 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:39.645392 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:40.503773 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:41.361905 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:42.219544 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.076948 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.936570 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:44.796700 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:45.657160 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:46.518121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:47.377798 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:48.237425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.096901 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.956288 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:50.417549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.527953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.989146 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.099630 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.560819 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:54.667539 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:55.126024 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.232848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.691018 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:57.797657 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:58.256374 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.363049 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.821103 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:00.928173 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:01.386213 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.506290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.976799 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.091907 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.552894 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:05.665685 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:06.126769 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.237985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.698871 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:08.811256 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:09.272347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.383407 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.843860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:11.953092 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:12.412879 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.522795 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.982591 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.092872 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.553238 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:16.661568 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:17.121251 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.228917 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.687242 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:19.795476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:20.254715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.360317 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.818199 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:22.924785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:23.383938 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.491620 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.949210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.055546 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.513860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:27.619938 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:28.077878 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.184057 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.644353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:30.750858 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:31.209560 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.316386 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.774731 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:33.880496 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:34.338659 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.444884 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.902665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.009701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.467267 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:38.575281 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:39.033537 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.140174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.599442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:41.706513 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:42.165941 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.273074 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.732741 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:44.839117 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:45.298868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.409119 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.867923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:47.974982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:48.434483 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:49.542086 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:50.000771 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.106716 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.564878 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:52.672469 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:53.130185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.237549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.696789 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:55.802835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:56.261561 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.367745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.826432 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:58.933002 [2] Warning: no training nodes in this partition! Backward fake loss.
21:36:59.392206 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.500982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.960701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.080686 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.550136 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:03.667462 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:04.129229 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.241286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.703155 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:06.821190 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:07.285828 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.402379 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.864471 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:09.975867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:10.434692 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:11.547240 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:12.005330 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.115949 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.574372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:14.685100 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:15.144013 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.254054 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.713582 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:17.825709 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:18.284667 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.396445 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.855026 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:20.962417 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:21.422515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.530367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.989059 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.095651 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.552981 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:25.660133 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:26.118398 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.224546 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.681552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:28.788571 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:29.245694 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.352848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.810675 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:31.917824 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:32.375700 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.482165 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.939291 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.046104 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.504362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:36.611619 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:37.069586 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.175475 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.633322 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:39.739533 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:40.196944 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.303502 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.761405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:42.867843 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:43.325726 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.431740 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.889716 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:45.995589 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:46.454251 [2] Warning: no training nodes in this partition! Backward fake loss.
21:37:53.689877 [2] proc begin: <DistEnv 2/4 nccl>
21:38:51.899525 [2] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:38:51.905683 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2102 MiB |   2118 MiB |   2152 MiB |  51113 KiB |
|       from large pool |   2102 MiB |   2118 MiB |   2152 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2102 MiB |   2118 MiB |   2152 MiB |  51113 KiB |
|       from large pool |   2102 MiB |   2118 MiB |   2152 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2101 MiB |   2116 MiB |   2148 MiB |  48832 KiB |
|       from large pool |   2101 MiB |   2116 MiB |   2148 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2136 MiB |   2136 MiB |   2136 MiB |      0 B   |
|       from large pool |   2134 MiB |   2134 MiB |   2134 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15516 KiB |  18526 KiB |  49959 KiB |  34442 KiB |
|       from large pool |  15516 KiB |  18526 KiB |  43810 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       6    |      10    |       5    |
|       from large pool |       5    |       5    |       7    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:39:10.994809 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:13.383402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:14.741089 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:16.100821 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:17.459379 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:18.814020 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:20.169479 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:21.525241 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:22.879687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:24.235789 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:25.590965 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:26.945633 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:28.299926 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:29.655074 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:31.010077 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:32.365125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:33.719596 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:35.073066 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:36.425203 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:37.779766 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:39.133512 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:40.487682 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:41.841445 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:43.195356 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:44.549482 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:45.903029 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:47.255918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:48.611603 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:49.965574 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:51.319116 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:52.673169 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:54.028785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:55.382137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:56.735563 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:58.089438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:59.442351 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:00.796337 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:02.153702 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:03.526991 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:04.883404 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:06.240069 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:07.595368 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:08.953103 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:10.310416 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:11.668210 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:13.025775 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:14.383268 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:15.740887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:17.099801 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:18.456278 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:19.812523 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:20.382600 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.378801 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.947321 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:24.941848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:25.511177 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:27.504892 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:28.073785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.068050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.636266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:32.628092 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:33.195422 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.189559 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.758829 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:37.751286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:38.318305 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.311072 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.879438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:42.871387 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:43.440252 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.432374 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.999587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:47.993207 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:48.561383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:50.553867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:51.121800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.114419 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.682516 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:55.674935 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:56.243132 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.236925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.805360 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:00.798528 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:01.367827 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.373383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.941986 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:05.936549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:06.504476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:08.500145 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:09.069684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.061724 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.630916 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:13.623733 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:14.191666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.183941 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.752422 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:18.744193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:19.312082 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.305001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.874397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:23.866463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:24.436642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.430337 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.997445 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:28.990780 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:29.558330 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:31.553644 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:32.121205 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.115705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.683807 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:36.677816 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:37.246691 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.238669 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.806206 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:41.799217 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:42.366629 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.359605 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.927695 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:46.922239 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:47.489968 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:49.485428 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:50.053208 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.047373 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.615666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:54.610865 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:55.179886 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.173765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.743684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:41:59.736218 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:00.305466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.307422 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.888589 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:04.882219 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:05.451670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:07.447441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:08.017682 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.013188 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.583476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:12.577848 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:13.145544 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.138095 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.706094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:17.699911 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:18.267654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.260237 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.827291 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:22.819478 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:23.387352 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.380900 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.948911 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:27.941715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:28.509897 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:30.503713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:31.071529 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.064784 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.633137 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:35.625130 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:36.192485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.184559 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.751961 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:40.745097 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:41.311880 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.303494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.870446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:45.861885 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:46.429390 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.421385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.988631 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:50.980985 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:51.548748 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:53.542095 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:54.109211 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.101566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.668526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:58.661041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:42:59.228255 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.221449 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.793723 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:03.799153 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:04.368218 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.362366 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.931645 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:08.925684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:09.493895 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:11.486736 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:12.055093 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.049953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.618463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:16.611851 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:17.179991 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.173357 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.740804 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:21.733442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:22.300494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.294992 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.862676 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:26.857146 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:27.425800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.420281 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.988109 [2] Warning: no training nodes in this partition! Backward fake loss.
21:43:37.463584 [2] proc begin: <DistEnv 2/4 nccl>
21:44:04.223931 [2] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:44:04.231886 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:44:30.371019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:32.334585 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:33.342558 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:34.349840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:35.352676 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:36.355593 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:37.360394 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:38.363806 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:39.368039 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:40.369941 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:41.371190 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:42.372812 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:43.373847 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:44.375705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:45.375924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:46.377133 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:47.378918 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:48.382632 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:49.386342 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:50.390553 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:51.392912 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:52.396605 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:53.400636 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:54.405030 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:55.408025 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:56.411297 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:57.415744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:58.419027 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:59.423062 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:00.428601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:01.441525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:02.462098 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:03.470234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:04.476218 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:05.483201 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:06.490369 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:07.496603 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:08.504028 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:09.510034 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:10.513031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:11.516547 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:12.520446 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:13.524071 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:14.527824 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:15.532397 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:16.536322 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:17.540204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:18.542737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:19.546603 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:20.550367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:21.552997 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:22.028251 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.416012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.888856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.271276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.745420 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.128344 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.601687 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:28.986143 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:29.459877 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:30.842367 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:31.315308 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:32.695147 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:33.168584 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:34.548045 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:35.020610 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.399650 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.872851 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.254535 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.726365 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.107220 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.579647 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:41.959394 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:42.431887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:43.812427 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:44.285595 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:45.665392 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:46.137195 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.517160 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.990904 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.370931 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.843923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.225943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.700000 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.078607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.550958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:54.931265 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:55.404105 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:56.785831 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:57.260117 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:58.641033 [2] Warning: no training nodes in this partition! Backward fake loss.
21:45:59.114608 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.496585 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.971128 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.363121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.848719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.231207 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.704707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.086164 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.560982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:07.942391 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:08.416865 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:09.797670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:10.271103 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:11.651655 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:12.125290 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.505590 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.979189 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.359458 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.831793 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.210804 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.683505 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.064287 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.537323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:20.918654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:21.391917 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:22.773134 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:23.246826 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:24.630297 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:25.103389 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.487346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.959463 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.342833 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.816093 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.198112 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.671851 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.053285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.527125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:33.906939 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:34.379400 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:35.759864 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:36.233072 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:37.617332 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:38.090607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.470405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.944346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.328467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.801113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.185249 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.658643 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.042839 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.515623 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:46.900993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:47.373586 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:48.755982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:49.229113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:50.609790 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:51.083481 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.465575 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.937671 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.319923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.792995 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.175467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.648417 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.029009 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.502110 [2] Warning: no training nodes in this partition! Backward fake loss.
21:46:59.884434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:00.358962 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:01.750189 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:02.236370 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:03.622014 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:04.095619 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.476982 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.950820 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.332789 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.806466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.187844 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.660099 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.041899 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.515034 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:12.896369 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:13.369024 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:14.750371 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:15.223115 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:16.606308 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:17.079735 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.463781 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.936339 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.320798 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.795356 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.178114 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.652006 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.034552 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.510018 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:25.893417 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:26.367522 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:27.747172 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:28.221433 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:29.601439 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:30.075196 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.456817 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.930762 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.311540 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.783795 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.165783 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.639033 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.021697 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.494893 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:38.876096 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:39.349065 [2] Warning: no training nodes in this partition! Backward fake loss.
21:47:47.060748 [2] proc begin: <DistEnv 2/4 nccl>
21:48:20.435901 [2] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:48:20.442884 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:48:36.923800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:38.960044 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:40.041835 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:41.121524 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:42.199944 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:43.280715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:44.361269 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:45.442714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:46.523742 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:47.604046 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:48.683197 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:49.764197 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:50.845251 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:51.924439 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:53.004996 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:54.085134 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:55.165544 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:56.246114 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:57.325909 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:58.403526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:59.480676 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:00.557104 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:01.643967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:02.737567 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:03.817079 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:04.893887 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:05.973132 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:07.051296 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:08.128370 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:09.206112 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:10.282753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:11.360204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:12.437635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:13.515274 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:14.592223 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:15.668036 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:16.745639 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:17.822128 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:18.899857 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:19.977655 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:21.055590 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:22.133734 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:23.211041 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:24.287837 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:25.364803 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:26.441851 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:27.520498 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:28.597444 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:29.674995 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:30.752279 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:31.830287 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:32.378472 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:33.838048 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:34.386577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:35.844765 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:36.393230 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:37.851519 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:38.400327 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:39.858677 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:40.406383 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:41.864282 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:42.412704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:43.872405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:44.420410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:45.876744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:46.424114 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:47.880540 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:48.428090 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:49.884825 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:50.432423 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:51.889482 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:52.437745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:53.895935 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:54.444418 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:55.902197 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:56.449549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:57.905820 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:58.454042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:49:59.914311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:00.462047 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:01.931425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:02.494167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:03.961065 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:04.509684 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:05.970318 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:06.518557 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:07.979653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:08.528686 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:09.989763 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:10.538295 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:11.998852 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:12.546560 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:14.004914 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:14.553425 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.010486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.558320 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.015157 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.563644 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.020311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.568548 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.025449 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.573336 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.031815 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.580138 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.037431 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.585188 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.042679 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.590466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.047066 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.594543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.052505 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.600170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.058494 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.606634 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.062796 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.610548 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.066785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.614368 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.074813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.622717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.079867 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.627794 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.086182 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.634379 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.092050 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.639844 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.096550 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.643943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.105514 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.653736 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.112031 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.661086 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.118551 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.666476 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.123571 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.671343 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.127346 [2] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.675276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.132947 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.681111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.162179 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.721216 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.180540 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.729194 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.186793 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.735706 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.194442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.743963 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.201656 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.749426 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.206442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.755171 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.212642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.759948 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.216543 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.764363 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.221424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.769490 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.227504 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.774234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.230616 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.779125 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.237347 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.786607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.244481 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.792802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.249657 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.798410 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.256332 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.805214 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.263615 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.812216 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.270805 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.818264 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.275725 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.823879 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.280796 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.827956 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.285020 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.831688 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.289325 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.837676 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.295388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.843037 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.300414 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.847753 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.304708 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.852471 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.310216 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.857111 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.314217 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.861550 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.319106 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.867520 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.324714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.872876 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.329847 [2] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.879268 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.335845 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.882910 [2] Warning: no training nodes in this partition! Backward fake loss.
21:52:08.164980 [2] proc begin: <DistEnv 2/4 nccl>
21:52:54.946496 [2] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 2, |V|: 500000, |E|: 15637930>
21:52:54.954525 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:53:03.445534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.438619 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.454462 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.470783 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.490694 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.507972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.526603 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:11.546372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:12.565640 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:13.586602 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:14.607883 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:15.626524 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:16.641442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:17.656719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:18.669923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:19.684966 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:20.700840 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:21.714812 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:22.729232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:23.745193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:24.759868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:25.773860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:26.787071 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:27.798838 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:28.812302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:29.826542 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:30.840283 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:31.852961 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:32.866163 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:33.878924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:34.891528 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:35.906353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:36.919434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:37.931399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:38.944673 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:39.957388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:40.971613 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:41.984005 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:42.995972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:44.009173 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:45.020908 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:46.032565 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:47.044924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:48.057709 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:49.071311 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:50.083444 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:51.095725 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:52.107760 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:53.122990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:54.135515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.147549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.632652 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.028448 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.512629 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:58.906322 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:59.390193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:00.785326 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:01.269012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:02.686174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:03.182169 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:04.579461 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:05.063642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.461442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.945950 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.342579 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.827302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.224569 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.710061 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.107628 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.592500 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:13.988447 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:14.473233 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:15.868349 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:16.351802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:17.745571 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:18.229094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:19.624086 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:20.108363 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.503620 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.986441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.380353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.864993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.260080 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.744931 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.139692 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.623698 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.019778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.504525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:30.897953 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:31.380936 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:32.775121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:33.258549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:34.651426 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:35.133526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.527048 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:37.009836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.403306 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.886102 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.280412 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.763566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.155467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.638424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.031402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.513403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:45.906641 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:46.389577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:47.783515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:48.266960 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:49.660670 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:50.143105 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.535997 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:52.020503 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.415189 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.899951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.294289 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.778891 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.174191 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.657987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.052481 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.537001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:00.931241 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:01.417587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:02.829806 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:03.315967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:04.715166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:05.199940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:06.598358 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:07.083032 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.482107 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.969998 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.369550 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.853934 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.250864 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.734884 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.129601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.615545 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.010009 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.494743 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:17.890940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:18.373401 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:19.771658 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:20.255653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:21.652607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:22.135879 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.533019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:24.015931 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.415567 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.900167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.299613 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.784625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.188714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.675786 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.075654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.560303 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:32.960302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:33.445329 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:34.841247 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:35.325707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:36.721958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:37.206788 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:38.605513 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:39.090214 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.484955 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.968103 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.362869 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.847170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.241813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.726184 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.119915 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.604434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:47.999509 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:48.483364 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:49.879056 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:50.363266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:51.756405 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:52.240312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:53.633160 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:54.116208 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.509362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.993019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.387607 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.872161 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.266625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.751665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.146723 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.630587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.045512 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.534997 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:04.932856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:05.417701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:06.818012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:07.302733 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:08.702373 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:09.187717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:10.586481 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:11.071124 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.466283 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.951485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.347403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.832763 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:22.143705 [2] proc begin: <DistEnv 2/4 nccl>
21:56:54.243465 [2] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:56:54.253511 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:57:25.736372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:27.795534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:28.822402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:29.847925 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:30.874007 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:31.900308 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:32.927813 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:33.955198 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:34.981078 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:36.007088 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:37.033826 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:38.060723 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:39.090972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:40.118154 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:41.142094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:42.166178 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:43.189619 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:44.214038 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:45.235515 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:46.258609 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:47.282927 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:48.304503 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:49.327309 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:50.347682 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:51.369744 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:52.393912 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:53.419204 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:54.444854 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:55.466158 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:56.490424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:57.515291 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:58.540844 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:59.565856 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:00.590473 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:01.623546 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:02.664897 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:03.689766 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:04.713671 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:05.737312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:06.760029 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:07.783154 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:08.805875 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:09.828920 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:10.853033 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:11.876595 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:12.900247 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:13.923574 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:14.946323 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:15.973373 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:16.997815 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.023912 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.521704 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.926785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:20.421745 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:21.826737 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:22.321954 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:23.725885 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:24.220064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:25.623313 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:26.117769 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:27.519466 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:28.014678 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.418154 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.913420 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.315705 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.810078 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.212120 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.706857 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.107126 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.602212 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:37.006170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:37.501884 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:38.906354 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:39.400844 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:40.803811 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:41.299624 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:42.704206 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:43.200654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:44.605385 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:45.101402 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:46.505587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:47.000702 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.407028 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.904292 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.309285 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.805326 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.208275 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.704014 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.108260 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.604244 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.009042 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.504853 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:57.909298 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:58.404912 [2] Warning: no training nodes in this partition! Backward fake loss.
21:58:59.809578 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:00.306566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:01.709113 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:02.216665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:03.634749 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:04.130875 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:05.539496 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:06.036202 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.443388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.939529 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.348525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.844390 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.249674 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.746451 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.152043 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.646399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.050129 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.545838 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:16.950650 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:17.446064 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:18.849943 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:19.344785 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:20.746635 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:21.240719 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:22.644506 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:23.139507 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:24.542080 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:25.036304 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.438438 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.933265 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.336362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.831836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.235836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.731576 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.135507 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.631185 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.035486 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.530798 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:35.939097 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:36.434623 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:37.843448 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:38.338316 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:39.743715 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:40.239454 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:41.644023 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:42.139209 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:43.544276 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:44.039551 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.446127 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.941666 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.346286 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.841109 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.245349 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.741234 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.143469 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.637713 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.041161 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.534876 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:54.937706 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:55.431362 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:56.835176 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:57.329501 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:58.733553 [2] Warning: no training nodes in this partition! Backward fake loss.
21:59:59.227610 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:00.630633 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:01.125047 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:02.552152 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:03.051803 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.454010 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.947934 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.348447 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.843041 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.246295 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.743027 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.144804 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.638437 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.039614 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.533104 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:13.937011 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:14.432719 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:15.834180 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:16.327544 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:17.729114 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:18.223494 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:19.625414 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:20.119602 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:21.521219 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:22.014560 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.415740 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.910265 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.314362 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.809449 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.213063 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.709053 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.112772 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.606160 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.009479 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.503537 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:32.906849 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:33.402203 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:34.805041 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:35.298166 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:36.701168 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:37.195688 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:38.598734 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:39.093326 [2] Warning: no training nodes in this partition! Backward fake loss.
22:00:47.271908 [2] proc begin: <DistEnv 2/4 nccl>
22:00:51.692107 [2] graph loaded <COO Graph: e160M_f512_l32_t0.8, |V|: 2000000, |E|: 160000000, masks: 1600000,200000,200000><Local: 2, |V|: 500000, |E|: 15637930>
22:00:51.700221 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:57:15.237590 [2] proc begin: <DistEnv 2/4 nccl>
15:57:24.819905 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:57:24.864031 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:59:24.517620 [2] proc begin: <DistEnv 2/4 nccl>
15:59:30.711844 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
15:59:30.728275 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:13.928910 [2] proc begin: <DistEnv 2/4 nccl>
16:00:20.211724 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:00:20.232987 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:03:35.347564 [2] proc begin: <DistEnv 2/4 nccl>
16:03:40.895268 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:03:40.917419 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:12:43.437426 [2] proc begin: <DistEnv 2/4 nccl>
16:12:48.514892 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:12:48.548540 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:20:23.368974 [2] proc begin: <DistEnv 2/4 nccl>
16:20:28.783441 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:20:28.820555 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:23:55.781183 [2] proc begin: <DistEnv 2/4 nccl>
16:24:00.521174 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:24:00.555751 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:28:41.837637 [2] proc begin: <DistEnv 2/4 nccl>
16:28:47.854260 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:28:47.873921 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:29:40.484888 [2] proc begin: <DistEnv 2/4 nccl>
16:29:45.822292 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:29:45.839790 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:33:59.115900 [2] proc begin: <DistEnv 2/4 nccl>
16:34:06.176631 [2] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 2, |V|: 58242, |E|: 28825013>
16:34:06.198185 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 367117 KiB | 369001 KiB | 372769 KiB |   5652 KiB |
|       from large pool | 365521 KiB | 367404 KiB | 371169 KiB |   5647 KiB |
|       from small pool |   1596 KiB |   1597 KiB |   1600 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 365567 KiB | 367388 KiB | 371028 KiB |   5460 KiB |
|       from large pool | 363975 KiB | 365795 KiB | 369435 KiB |   5460 KiB |
|       from small pool |   1592 KiB |   1592 KiB |   1593 KiB |      0 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 393216 KiB | 393216 KiB | 393216 KiB |      0 B   |
|       from large pool | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  26098 KiB |  26098 KiB |  33118 KiB |   7020 KiB |
|       from large pool |  25646 KiB |  25646 KiB |  31294 KiB |   5647 KiB |
|       from small pool |    452 KiB |   1820 KiB |   1824 KiB |   1372 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      29    |      12    |
|       from large pool |      10    |      11    |      13    |       3    |
|       from small pool |       7    |      10    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |       9    |       3    |
|       from large pool |       5    |       5    |       8    |       3    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:36:41.245001 [2] proc begin: <DistEnv 2/4 nccl>
16:36:58.610824 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:36:58.617744 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:37:04.394377 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:05.882485 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:06.644906 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:07.405572 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.165655 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:08.925009 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:09.684150 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:10.443829 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.203271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:11.963020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:12.723923 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:13.483687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:14.243620 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.001207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:15.759847 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:16.517498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:17.274885 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.033153 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:18.792808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:19.549815 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:20.308869 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.066749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:21.823717 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:22.582902 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:23.340886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.100620 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:24.859345 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:25.617861 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:26.376046 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.133505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:27.891414 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:28.650060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:29.408702 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.168258 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:30.925710 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:31.684594 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:32.442159 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.200438 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:33.959824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:34.719014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:35.477825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.236512 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:36.995038 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:37.755013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:38.514260 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:39.272019 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.030464 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:40.789852 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:41.546579 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:42.304223 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.061840 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:43.820175 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:44.578204 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:45.335387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.093680 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:46.852722 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:47.609480 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:48.368575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.125113 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:49.882883 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:50.640251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:51.398640 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.156412 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:52.913849 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:53.672291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:54.431275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.188865 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:55.946754 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:56.704651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:57.462247 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.221612 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:58.981649 [2] Warning: no training nodes in this partition! Backward fake loss.
16:37:59.739093 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:00.497794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:01.255898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.035208 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:02.825901 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:03.590645 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:04.351018 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.111893 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:05.872217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:06.631688 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:07.394090 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.154298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:08.913842 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:09.673782 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:10.435102 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.196025 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:11.957182 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:12.716499 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:13.476588 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.236284 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:14.994842 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:15.754993 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:16.514973 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:17.274794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.035762 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:18.796505 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:19.556299 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:20.316791 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.075250 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:21.834193 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:22.593028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:23.352598 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.112387 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:24.872171 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:25.631519 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:26.391284 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.151372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:27.910912 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:28.670030 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:29.428083 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.187542 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:30.945630 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:31.704132 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:32.462866 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.220286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:33.978919 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:34.736056 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:35.493761 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:36.251850 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.009585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:37.768790 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:38.527040 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:39.285631 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.044768 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:40.801653 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:41.560022 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:42.318382 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.077211 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:43.835766 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:44.593642 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:45.352121 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.110032 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:46.868267 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:47.626729 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:48.384640 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.144736 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:49.904147 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:50.662780 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:51.420727 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.178365 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:52.936612 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:53.694873 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:54.453248 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.211831 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:55.968939 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:56.726931 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:57.484698 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:58.242853 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.001794 [2] Warning: no training nodes in this partition! Backward fake loss.
16:38:59.759558 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:00.517315 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:01.275162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.034128 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:02.822695 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:03.605901 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:04.364708 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.122762 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:05.881105 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:06.639566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:07.397957 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.157398 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:08.915443 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:09.672889 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:10.430667 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.189275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:11.947576 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:12.705866 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:13.463976 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.222625 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:14.980756 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:15.739048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:16.498609 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:17.257738 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.017029 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:18.775667 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:19.534457 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:20.293647 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.053056 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:21.812074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:22.569912 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:23.327773 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.086937 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:24.844785 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:25.602968 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:26.361545 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.119369 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:27.879169 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:28.638749 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:29.398431 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.156383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:30.915563 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:31.675188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:32.435168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.194858 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:33.954388 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:34.713239 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:35.473006 [2] Warning: no training nodes in this partition! Backward fake loss.
16:39:36.231896 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:24.968377 [2] proc begin: <DistEnv 2/4 nccl>
16:45:29.130281 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:45:29.139219 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:45:34.703225 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:36.319558 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.073944 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:37.829273 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:38.587811 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:39.344908 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.100676 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:40.858178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:41.615457 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:42.373194 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.130346 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:43.888782 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:44.645312 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:45.402157 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.160014 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:46.917417 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:47.673254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:48.429499 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.186675 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:49.942915 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:50.698675 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:51.455240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.212667 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:52.967894 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:53.725273 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:54.481429 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.236687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:55.992357 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:56.750217 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:57.506618 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:58.261965 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.019430 [2] Warning: no training nodes in this partition! Backward fake loss.
16:45:59.774687 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:00.530715 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:01.292298 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.081941 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:02.857899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:03.615306 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:04.372142 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.128798 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:05.887458 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:06.644419 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:07.401929 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.160943 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:08.920106 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:09.681021 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:10.443123 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.203821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:11.965044 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:12.725750 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:13.486925 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:14.246425 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.006004 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:15.765227 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:16.525775 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:17.284748 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.044023 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:18.804874 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:19.564744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:20.322779 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.081205 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:21.840168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:22.599076 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:23.357222 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.116909 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:24.876048 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:25.635054 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:26.394056 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.152770 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:27.911421 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:28.669835 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:29.429086 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.187344 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:30.945616 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:31.704462 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:32.464419 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.224227 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:33.982301 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:34.741065 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:35.499890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:36.259024 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.018203 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:37.777063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:38.536582 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:39.295207 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.054347 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:40.812964 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:41.571270 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:42.329164 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.087845 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:43.846873 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:44.605748 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:45.367780 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.128754 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:46.890168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:47.650632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:48.412117 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.173639 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:49.932410 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:50.691561 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:51.450596 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.208074 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:52.964492 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:53.722478 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:54.480186 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.236829 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:55.992945 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:56.749162 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:57.504724 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:58.261004 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.018423 [2] Warning: no training nodes in this partition! Backward fake loss.
16:46:59.777013 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:00.533498 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:01.290564 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.064886 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:02.853246 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:03.621506 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:04.383271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.144183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:05.905459 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:06.666183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:07.427322 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.186807 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:08.947763 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:09.709718 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:10.469795 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.229092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:11.986389 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:12.743664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:13.500621 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:14.258028 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.015653 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:15.772521 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:16.529898 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:17.287720 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.044251 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:18.800827 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:19.558881 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:20.315792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.072617 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:21.829547 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:22.586448 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:23.343997 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.102092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:24.859769 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:25.615651 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:26.371515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.127303 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:27.883949 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:28.638938 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:29.396107 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.152248 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:30.908199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:31.663890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:32.419891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.175860 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:33.932065 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:34.687535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:35.443495 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.200199 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:36.956041 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:37.712083 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:38.468998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.226784 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:39.982092 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:40.738524 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:41.495644 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:42.252064 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.007383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:43.764005 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:44.519566 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:45.275730 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.031178 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:46.787167 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:47.543055 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:48.298589 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.054581 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:49.811118 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:50.566998 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:51.322501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.078624 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:52.835475 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:53.592388 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:54.348372 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.103986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:55.859748 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:56.615635 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:57.371793 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.127986 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:58.884681 [2] Warning: no training nodes in this partition! Backward fake loss.
16:47:59.643825 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:00.402429 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.160240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:01.948632 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:02.732555 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:03.490291 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:04.248020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.005240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:05.764454 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:06.522276 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:13.576800 [2] proc begin: <DistEnv 2/4 nccl>
16:48:17.960489 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
16:48:17.969383 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:48:24.415575 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:27.222901 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:29.066984 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:30.908796 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:32.749286 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:34.589620 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:36.433033 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:38.274619 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:40.116899 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:41.957649 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:43.804665 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:45.645282 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:47.487240 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:49.328895 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:51.170271 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:53.013125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:54.853744 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:56.696532 [2] Warning: no training nodes in this partition! Backward fake loss.
16:48:58.538572 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:00.378515 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:02.241297 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:04.119664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:05.961958 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:07.803911 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:09.645604 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:11.487928 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:13.329244 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:15.169503 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:17.010326 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:18.847737 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:20.685554 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:22.521845 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:24.359531 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:26.192841 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:28.027465 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:29.864017 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:31.696600 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:33.530702 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:35.364573 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:37.200403 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:39.036022 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:40.869951 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:42.703060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:44.538027 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:46.371963 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:48.206395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:50.039875 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:51.872840 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:53.705778 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:55.538808 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:57.373147 [2] Warning: no training nodes in this partition! Backward fake loss.
16:49:59.207755 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:01.041663 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:02.934041 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:04.774236 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:06.612936 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:08.453363 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:10.293877 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:12.134602 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:13.975540 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:15.814125 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:17.654065 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:19.490501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:21.328504 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:23.164314 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:25.000880 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:26.837359 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:28.674018 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:30.510155 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:32.348220 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:34.187243 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:36.024137 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:37.861820 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:39.701442 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:41.539580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:43.377254 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:45.214955 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:47.052491 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:48.890177 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:50.729059 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:52.564968 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:54.397666 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:56.236682 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:58.073478 [2] Warning: no training nodes in this partition! Backward fake loss.
16:50:59.912202 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:01.748725 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:03.640550 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:05.478826 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:07.318474 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:09.159445 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:10.997527 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:12.836325 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:14.676501 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:16.514891 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:18.354383 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:20.194604 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:22.033724 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:23.874170 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:25.715416 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:27.555890 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:29.395145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:31.234455 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:33.072354 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:34.910146 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:36.748428 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:38.585188 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:40.421432 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:42.258275 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:44.094009 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:45.931882 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:47.769510 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:49.606037 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:51.442996 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:53.279145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:55.114145 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:56.951016 [2] Warning: no training nodes in this partition! Backward fake loss.
16:51:58.786562 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:00.623165 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:02.505197 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:04.350395 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:06.186994 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:08.024390 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:09.861939 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:11.696060 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:13.532352 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:15.369821 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:17.206780 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:19.042580 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:20.882032 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:22.721345 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:24.557183 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:26.389824 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:28.225897 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:30.062507 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:31.899140 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:33.735629 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:35.570802 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:37.407299 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:39.245100 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:41.086224 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:42.923862 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:44.759844 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:46.596237 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:48.433484 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:50.267639 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:52.101731 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:53.936345 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:55.769585 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:57.603852 [2] Warning: no training nodes in this partition! Backward fake loss.
16:52:59.437858 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:01.272904 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:03.159326 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:05.001279 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:06.842664 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:08.685309 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:10.523181 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:12.361342 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:14.195370 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:16.028523 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:17.864329 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:19.698887 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:21.531874 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:23.364601 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:25.198300 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:27.033415 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:28.867285 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:30.702971 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:32.539792 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:34.376857 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:36.213418 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:38.050456 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:39.885757 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:41.720912 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:43.555063 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:45.388699 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:47.220928 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:49.055020 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:50.889168 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:52.723789 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:54.557373 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:56.393459 [2] Warning: no training nodes in this partition! Backward fake loss.
16:53:58.226894 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:00.062062 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:01.916488 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:03.790587 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:05.628828 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:07.465686 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:09.302184 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:11.141229 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:12.978073 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:14.813512 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:16.647185 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:18.481535 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:20.315697 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:22.148716 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:23.982164 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:25.816955 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:27.650408 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:29.485314 [2] Warning: no training nodes in this partition! Backward fake loss.
16:54:31.320105 [2] Warning: no training nodes in this partition! Backward fake loss.
17:15:58.794656 [2] proc begin: <DistEnv 2/4 nccl>
17:16:03.262271 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
17:16:03.272332 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:16:10.683068 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:13.599756 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:15.448223 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:17.289322 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:19.123094 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:20.958094 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:22.790411 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:24.625494 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:26.458195 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:28.288691 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:30.121906 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:31.953716 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:33.787291 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:35.618091 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:37.451373 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:39.284380 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:41.115486 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:42.949955 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:44.781426 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:46.612651 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:48.445755 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:50.277329 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:52.109215 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:53.940892 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:55.772427 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:57.604105 [2] Warning: no training nodes in this partition! Backward fake loss.
17:16:59.436233 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:01.268551 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:03.147727 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:04.990322 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:06.822814 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:08.660345 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:10.494815 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:12.327072 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:14.160191 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:15.991514 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:17.823603 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:19.657434 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:21.488871 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:23.322082 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:25.156142 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:26.988353 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:28.820616 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:30.653655 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:32.487284 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:34.320138 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:36.151674 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:37.984467 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:39.816900 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:41.650488 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:43.486111 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:45.319991 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:47.152155 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:48.984350 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:50.818263 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:52.651253 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:54.483470 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:56.315883 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:58.147944 [2] Warning: no training nodes in this partition! Backward fake loss.
17:17:59.980416 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:01.825504 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:03.703623 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:05.534753 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:07.371996 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:09.208161 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:11.044026 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:12.876979 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:14.711243 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:16.545434 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:18.379313 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:20.210785 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:22.043673 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:23.874776 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:25.707064 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:27.539637 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:29.371761 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:31.205905 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:33.041147 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:34.872430 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:36.704232 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:38.535455 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:40.367911 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:42.199745 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:44.034028 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:45.866664 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:47.699603 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:49.532519 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:51.363784 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:53.195693 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:55.027915 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:56.861346 [2] Warning: no training nodes in this partition! Backward fake loss.
17:18:58.693805 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:00.525481 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:02.370367 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:04.251446 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:06.088627 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:07.926136 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:09.764047 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:11.598827 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:13.434487 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:15.265843 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:17.096336 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:18.927779 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:20.759892 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:22.591336 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:24.423839 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:26.256360 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:28.086763 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:29.918994 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:31.749916 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:33.582003 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:35.412655 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:37.244649 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:39.079343 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:40.912556 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:42.744536 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:44.582130 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:46.412160 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:48.245459 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:50.078390 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:51.909294 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:53.739879 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:55.573346 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:57.406419 [2] Warning: no training nodes in this partition! Backward fake loss.
17:19:59.237387 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:01.068731 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:02.960329 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:04.798983 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:06.635587 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:08.472876 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:10.309660 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:12.142264 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:13.974635 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:15.807194 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:17.639466 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:19.471015 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:21.303338 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:23.136888 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:24.968872 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:26.802018 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:28.633354 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:30.467073 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:32.298885 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:34.132153 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:35.963999 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:37.795946 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:39.627747 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:41.461216 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:43.294890 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:45.128593 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:46.961832 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:48.794561 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:50.627521 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:52.460519 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:54.292599 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:56.125086 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:57.957475 [2] Warning: no training nodes in this partition! Backward fake loss.
17:20:59.789096 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:01.641040 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:03.514809 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:05.348759 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:07.180480 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:09.012307 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:10.844250 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:12.675945 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:14.508972 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:16.342259 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:18.173660 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:20.006330 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:21.840649 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:23.673372 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:25.505546 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:27.337973 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:29.171529 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:31.003161 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:32.835429 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:34.669232 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:36.503032 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:38.335804 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:40.168535 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:42.000571 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:43.833279 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:45.664983 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:47.497520 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:49.330028 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:51.161806 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:52.995728 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:54.828596 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:56.660998 [2] Warning: no training nodes in this partition! Backward fake loss.
17:21:58.492295 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:00.325929 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:02.179980 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:04.048202 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:05.882863 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:07.717873 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:09.553028 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:11.389172 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:13.221785 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:15.056396 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:16.889213 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:23.889493 [2] proc begin: <DistEnv 2/4 nccl>
17:22:27.873183 [2] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 2, |V|: 612258, |E|: 24492100>
17:22:27.882273 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 466840 KiB | 505541 KiB | 763864 KiB | 297024 KiB |
|       from large pool | 466840 KiB | 505541 KiB | 763857 KiB | 297016 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 466382 KiB | 504634 KiB | 762949 KiB | 296567 KiB |
|       from large pool | 466382 KiB | 504634 KiB | 762944 KiB | 296562 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 522240 KiB | 522240 KiB | 522240 KiB |      0 B   |
|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  53351 KiB | 217029 KiB | 285362 KiB | 232010 KiB |
|       from large pool |  53351 KiB | 217029 KiB | 279213 KiB | 225862 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       6    |       3    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:22:34.957597 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:39.689633 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:43.690015 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:47.689454 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:51.685572 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:55.679515 [2] Warning: no training nodes in this partition! Backward fake loss.
17:22:59.673194 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:03.726228 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:07.721580 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:11.718797 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:15.715427 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:19.709603 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:23.702748 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:27.695903 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:31.688256 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:35.682885 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:39.679575 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:43.674085 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:47.666932 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:51.660160 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:55.655600 [2] Warning: no training nodes in this partition! Backward fake loss.
17:23:59.649162 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:03.699752 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:07.695999 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:11.688803 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:15.684372 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:19.677146 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:23.673000 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:27.667486 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:31.659988 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:35.653596 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:39.647329 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:43.640023 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:47.634270 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:51.629013 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:55.621235 [2] Warning: no training nodes in this partition! Backward fake loss.
17:24:59.616240 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:03.666683 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:07.658048 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:11.650925 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:15.645368 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:19.637243 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:23.628034 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:27.620980 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:31.613426 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:35.606671 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:39.598899 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:43.594123 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:47.589832 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:51.582654 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:55.575986 [2] Warning: no training nodes in this partition! Backward fake loss.
17:25:59.567725 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:03.620603 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:07.618072 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:11.613436 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:15.607681 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:19.602205 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:23.595073 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:27.590586 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:31.586040 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:35.587325 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:39.584377 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:43.579732 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:47.577579 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:51.576332 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:55.574787 [2] Warning: no training nodes in this partition! Backward fake loss.
17:26:59.571022 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:03.626235 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:07.622756 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:11.620045 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:15.616809 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:19.715332 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:23.745574 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:27.816783 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:31.849601 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:35.846734 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:39.844851 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:43.845530 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:47.850411 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:51.844064 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:55.836931 [2] Warning: no training nodes in this partition! Backward fake loss.
17:27:59.830428 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:03.884222 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:07.884017 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:11.879674 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:15.878928 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:19.883587 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:23.877769 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:27.871290 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:31.865929 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:35.858887 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:39.854168 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:43.845945 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:47.842755 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:51.836693 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:55.830907 [2] Warning: no training nodes in this partition! Backward fake loss.
17:28:59.827353 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:03.878557 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:07.873870 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:11.872446 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:15.869108 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:19.864734 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:23.862122 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:27.858682 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:31.853125 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:35.847794 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:39.842226 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:43.836673 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:47.833796 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:51.829000 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:55.825127 [2] Warning: no training nodes in this partition! Backward fake loss.
17:29:59.822117 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:03.877458 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:07.869140 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:11.868796 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:15.867905 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:19.861510 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:23.854375 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:27.846468 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:31.839055 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:35.832070 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:39.825392 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:43.818577 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:47.814397 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:51.808880 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:55.801397 [2] Warning: no training nodes in this partition! Backward fake loss.
17:30:59.795753 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:03.847765 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:07.840382 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:11.833512 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:15.825380 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:19.816551 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:23.809713 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:27.801180 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:31.794814 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:35.786761 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:39.778584 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:43.772063 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:47.766012 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:51.759803 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:55.753178 [2] Warning: no training nodes in this partition! Backward fake loss.
17:31:59.744822 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:03.799171 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:07.794978 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:11.786634 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:15.779964 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:19.772443 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:23.763772 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:27.754505 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:31.748157 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:35.741917 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:39.734405 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:43.725293 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:47.717956 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:51.708887 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:55.701473 [2] Warning: no training nodes in this partition! Backward fake loss.
17:32:59.694075 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:03.742199 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:07.743206 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:11.737631 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:15.732808 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:19.729163 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:23.724717 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:27.718822 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:31.711324 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:35.707455 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:39.700560 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:43.695966 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:47.690425 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:51.684486 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:55.680208 [2] Warning: no training nodes in this partition! Backward fake loss.
17:33:59.672458 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:03.723655 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:07.719301 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:11.713673 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:15.708253 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:19.702393 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:23.696802 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:27.692245 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:31.687954 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:35.681674 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:39.676425 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:43.673357 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:47.668803 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:51.662452 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:55.656979 [2] Warning: no training nodes in this partition! Backward fake loss.
17:34:59.651965 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:03.704303 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:07.700111 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:11.694246 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:15.689456 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:19.683502 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:23.677747 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:27.672068 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:31.667568 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:35.663510 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:39.659433 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:43.652731 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:47.646566 [2] Warning: no training nodes in this partition! Backward fake loss.
17:35:51.641875 [2] Warning: no training nodes in this partition! Backward fake loss.
