no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:47:46.887376 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:47:47.051963 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:47:47.060748 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:47:47.069895 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:47:53.813215 [1] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:47:53.818747 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:48:09.271827 [3] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:48:09.281263 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:48:20.435901 [2] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:48:20.442884 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:48:34.795600 [0] graph loaded <COO Graph: e160M_f512_l64_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 0, |V|: 500000, |E|: 113001879>
21:48:34.804040 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1867 MiB |   1882 MiB |   1914 MiB |  48832 KiB |
|       from large pool |   1867 MiB |   1882 MiB |   1914 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1900 MiB |   1900 MiB |   1900 MiB |      0 B   |
|       from large pool |   1898 MiB |   1898 MiB |   1898 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12184 KiB |  19998 KiB |  46627 KiB |  34442 KiB |
|       from large pool |  12184 KiB |  19998 KiB |  40478 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:48:36.923800 [2] Warning: no training nodes in this partition! Backward fake loss.
21:48:36.962713 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:38.256219 [0] Epoch 00000 | Loss 4.1604
21:48:38.340737 [0] Epoch: 000, Train: 0.0157, Val: 0.0157, Test: 0.0157
21:48:38.953315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:38.960044 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015656
21:48:39.383949 [0] Epoch 00001 | Loss 19302.4062
21:48:39.422402 [0] Epoch: 001, Train: 0.0156, Val: 0.0156, Test: 0.0154
21:48:40.034650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:40.041835 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015429
21:48:40.464397 [0] Epoch 00002 | Loss 4938.5776
21:48:40.502722 [0] Epoch: 002, Train: 0.0157, Val: 0.0156, Test: 0.0156
21:48:41.114434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:41.121524 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015564
21:48:41.542888 [0] Epoch 00003 | Loss 1738.5515
21:48:41.580980 [0] Epoch: 003, Train: 0.0158, Val: 0.0158, Test: 0.0156
21:48:42.193221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:42.199944 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015633
21:48:42.622328 [0] Epoch 00004 | Loss 1437.2269
21:48:42.660575 [0] Epoch: 004, Train: 0.0157, Val: 0.0155, Test: 0.0157
21:48:43.272599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:43.280715 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015699
21:48:43.702075 [0] Epoch 00005 | Loss 934.2902
21:48:43.741528 [0] Epoch: 005, Train: 0.0155, Val: 0.0157, Test: 0.0158
21:48:44.354489 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:44.361269 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015811
21:48:44.784000 [0] Epoch 00006 | Loss 475.8976
21:48:44.823335 [0] Epoch: 006, Train: 0.0156, Val: 0.0151, Test: 0.0155
21:48:45.436089 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:45.442714 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015451
21:48:45.865192 [0] Epoch 00007 | Loss 95.8508
21:48:45.903488 [0] Epoch: 007, Train: 0.0156, Val: 0.0153, Test: 0.0157
21:48:46.516430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:46.523742 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015655
21:48:46.944956 [0] Epoch 00008 | Loss 4.1594
21:48:46.983361 [0] Epoch: 008, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:47.596855 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:47.604046 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:48.025248 [0] Epoch 00009 | Loss 4.1594
21:48:48.063705 [0] Epoch: 009, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:48.677026 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:48.683197 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:49.106120 [0] Epoch 00010 | Loss 4.1594
21:48:49.144315 [0] Epoch: 010, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:49.757566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:49.764197 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:50.186086 [0] Epoch 00011 | Loss 4.1594
21:48:50.224931 [0] Epoch: 011, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:50.836911 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:50.845251 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:51.266881 [0] Epoch 00012 | Loss 4.1594
21:48:51.305052 [0] Epoch: 012, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:51.917719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:51.924439 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:52.344991 [0] Epoch 00013 | Loss 4.1594
21:48:52.383897 [0] Epoch: 013, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:52.997806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:53.004996 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:53.427193 [0] Epoch 00014 | Loss 4.1594
21:48:53.465401 [0] Epoch: 014, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:54.077676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:54.085134 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:54.507667 [0] Epoch 00015 | Loss 4.1594
21:48:54.545776 [0] Epoch: 015, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:55.158405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:55.165544 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:55.587944 [0] Epoch 00016 | Loss 4.1594
21:48:55.626095 [0] Epoch: 016, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:56.238930 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:56.246114 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:56.668313 [0] Epoch 00017 | Loss 4.1594
21:48:56.706834 [0] Epoch: 017, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:57.318654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:57.325909 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:57.745397 [0] Epoch 00018 | Loss 4.1594
21:48:57.783709 [0] Epoch: 018, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:58.396519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:58.403526 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:58.823391 [0] Epoch 00019 | Loss 4.1594
21:48:58.861398 [0] Epoch: 019, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:48:59.473458 [3] Warning: no training nodes in this partition! Backward fake loss.
21:48:59.480676 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:48:59.899505 [0] Epoch 00020 | Loss 4.1594
21:48:59.937672 [0] Epoch: 020, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:00.549842 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:00.557104 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:00.977570 [0] Epoch 00021 | Loss 4.1594
21:49:01.015692 [0] Epoch: 021, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:01.637201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:01.643967 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:02.074657 [0] Epoch 00022 | Loss 4.1594
21:49:02.114216 [0] Epoch: 022, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:02.730104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:02.737567 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:03.158458 [0] Epoch 00023 | Loss 4.1594
21:49:03.197028 [0] Epoch: 023, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:03.810139 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:03.817079 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:04.236301 [0] Epoch 00024 | Loss 4.1594
21:49:04.274533 [0] Epoch: 024, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:04.887120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:04.893887 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:05.314320 [0] Epoch 00025 | Loss 4.1594
21:49:05.352421 [0] Epoch: 025, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:05.965945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:05.973132 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:06.393023 [0] Epoch 00026 | Loss 4.1594
21:49:06.431381 [0] Epoch: 026, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:07.043848 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:07.051296 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:07.470868 [0] Epoch 00027 | Loss 4.1594
21:49:07.508884 [0] Epoch: 027, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:08.121383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:08.128370 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:08.548018 [0] Epoch 00028 | Loss 4.1594
21:49:08.586326 [0] Epoch: 028, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:09.198643 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:09.206112 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:09.624921 [0] Epoch 00029 | Loss 4.1594
21:49:09.663057 [0] Epoch: 029, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:10.275557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:10.282753 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:10.701825 [0] Epoch 00030 | Loss 4.1594
21:49:10.740197 [0] Epoch: 030, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:11.353840 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:11.360204 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:11.780378 [0] Epoch 00031 | Loss 4.1594
21:49:11.818543 [0] Epoch: 031, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:12.429947 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:12.437635 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:12.857646 [0] Epoch 00032 | Loss 4.1594
21:49:12.896100 [0] Epoch: 032, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:13.508318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:13.515274 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:13.934787 [0] Epoch 00033 | Loss 4.1594
21:49:13.972716 [0] Epoch: 033, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:14.585017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:14.592223 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:15.011502 [0] Epoch 00034 | Loss 4.1594
21:49:15.049329 [0] Epoch: 034, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:15.660966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:15.668036 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:16.087898 [0] Epoch 00035 | Loss 4.1594
21:49:16.125803 [0] Epoch: 035, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:16.738562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:16.745639 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:17.165024 [0] Epoch 00036 | Loss 4.1594
21:49:17.203116 [0] Epoch: 036, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:17.815200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:17.822128 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:18.242228 [0] Epoch 00037 | Loss 4.1594
21:49:18.280204 [0] Epoch: 037, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:18.892112 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:18.899857 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:19.320467 [0] Epoch 00038 | Loss 4.1594
21:49:19.358409 [0] Epoch: 038, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:19.970699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:19.977655 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:20.398481 [0] Epoch 00039 | Loss 4.1594
21:49:20.436402 [0] Epoch: 039, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:21.048622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:21.055590 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:21.475727 [0] Epoch 00040 | Loss 4.1594
21:49:21.514083 [0] Epoch: 040, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:22.126598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:22.133734 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:22.554022 [0] Epoch 00041 | Loss 4.1594
21:49:22.591936 [0] Epoch: 041, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:23.203923 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:23.211041 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:23.630278 [0] Epoch 00042 | Loss 4.1594
21:49:23.668545 [0] Epoch: 042, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:24.280858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:24.287837 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:24.707576 [0] Epoch 00043 | Loss 4.1594
21:49:24.745551 [0] Epoch: 043, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:25.357563 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:25.364803 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:25.784817 [0] Epoch 00044 | Loss 4.1594
21:49:25.822789 [0] Epoch: 044, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:26.434716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:26.441851 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:26.862310 [0] Epoch 00045 | Loss 4.1594
21:49:26.900529 [0] Epoch: 045, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:27.512905 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:27.520498 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:27.939838 [0] Epoch 00046 | Loss 4.1594
21:49:27.977731 [0] Epoch: 046, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:28.590152 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:28.597444 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:29.017746 [0] Epoch 00047 | Loss 4.1594
21:49:29.055982 [0] Epoch: 047, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:29.667905 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:29.674995 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:30.095303 [0] Epoch 00048 | Loss 4.1594
21:49:30.133034 [0] Epoch: 048, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:30.745200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:30.752279 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:31.173361 [0] Epoch 00049 | Loss 4.1594
21:49:31.211376 [0] Epoch: 049, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:31.822921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:31.830287 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:32.249396 [0] Epoch 00050 | Loss 4.1594
21:49:32.287814 [0] Epoch: 050, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:32.321851 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:32.378472 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:33.180624 [0] Epoch 00051 | Loss 4.1594
21:49:33.218619 [0] Epoch: 051, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:33.831007 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:33.838048 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:34.258153 [0] Epoch 00052 | Loss 4.1594
21:49:34.295887 [0] Epoch: 052, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:34.329999 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:34.386577 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:35.187530 [0] Epoch 00053 | Loss 4.1594
21:49:35.225708 [0] Epoch: 053, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:35.837830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:35.844765 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:36.264290 [0] Epoch 00054 | Loss 4.1594
21:49:36.302549 [0] Epoch: 054, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:36.336569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:36.393230 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:37.193558 [0] Epoch 00055 | Loss 4.1594
21:49:37.231925 [0] Epoch: 055, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:37.844356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:37.851519 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:38.271331 [0] Epoch 00056 | Loss 4.1594
21:49:38.309641 [0] Epoch: 056, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:38.343699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:38.400327 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:39.201503 [0] Epoch 00057 | Loss 4.1594
21:49:39.239386 [0] Epoch: 057, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:39.851477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:39.858677 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:40.277585 [0] Epoch 00058 | Loss 4.1594
21:49:40.315802 [0] Epoch: 058, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:40.350154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:40.406383 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:41.207544 [0] Epoch 00059 | Loss 4.1594
21:49:41.245353 [0] Epoch: 059, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:41.857328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:41.864282 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:42.283833 [0] Epoch 00060 | Loss 4.1594
21:49:42.322114 [0] Epoch: 060, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:42.356068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:42.412704 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:43.214213 [0] Epoch 00061 | Loss 4.1594
21:49:43.252438 [0] Epoch: 061, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:43.864885 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:43.872405 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:44.290679 [0] Epoch 00062 | Loss 4.1594
21:49:44.329019 [0] Epoch: 062, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:44.363137 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:44.420410 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:45.219566 [0] Epoch 00063 | Loss 4.1594
21:49:45.257306 [0] Epoch: 063, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:45.869622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:45.876744 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:46.295568 [0] Epoch 00064 | Loss 4.1594
21:49:46.333207 [0] Epoch: 064, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:46.367349 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:46.424114 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:47.223075 [0] Epoch 00065 | Loss 4.1594
21:49:47.261372 [0] Epoch: 065, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:47.873386 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:47.880540 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:48.298872 [0] Epoch 00066 | Loss 4.1594
21:49:48.337164 [0] Epoch: 066, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:48.371658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:48.428090 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:49.227878 [0] Epoch 00067 | Loss 4.1594
21:49:49.265681 [0] Epoch: 067, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:49.877736 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:49.884825 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:50.303761 [0] Epoch 00068 | Loss 4.1594
21:49:50.341428 [0] Epoch: 068, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:50.375436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:50.432423 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:51.232603 [0] Epoch 00069 | Loss 4.1594
21:49:51.270388 [0] Epoch: 069, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:51.882320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:51.889482 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:52.308875 [0] Epoch 00070 | Loss 4.1594
21:49:52.346848 [0] Epoch: 070, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:52.380841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:52.437745 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:53.237882 [0] Epoch 00071 | Loss 4.1594
21:49:53.276108 [0] Epoch: 071, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:53.888749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:53.895935 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:54.315573 [0] Epoch 00072 | Loss 4.1594
21:49:54.353487 [0] Epoch: 072, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:54.387562 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:54.444418 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:55.244976 [0] Epoch 00073 | Loss 4.1594
21:49:55.282834 [0] Epoch: 073, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:55.895471 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:55.902197 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:56.320246 [0] Epoch 00074 | Loss 4.1594
21:49:56.358650 [0] Epoch: 074, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:56.392737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:56.449549 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:57.248533 [0] Epoch 00075 | Loss 4.1594
21:49:57.286260 [0] Epoch: 075, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:57.898648 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:57.905820 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:58.325150 [0] Epoch 00076 | Loss 4.1594
21:49:58.362986 [0] Epoch: 076, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:58.398214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:58.454042 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:49:59.254130 [0] Epoch 00077 | Loss 4.1594
21:49:59.292004 [0] Epoch: 077, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:49:59.907157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:49:59.914311 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:00.333231 [0] Epoch 00078 | Loss 4.1594
21:50:00.371092 [0] Epoch: 078, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:00.405150 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:00.462047 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:01.264013 [0] Epoch 00079 | Loss 4.1594
21:50:01.302202 [0] Epoch: 079, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:01.923880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:01.931425 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:02.362918 [0] Epoch 00080 | Loss 4.1594
21:50:02.402369 [0] Epoch: 080, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:02.437259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:02.494167 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:03.300564 [0] Epoch 00081 | Loss 4.1594
21:50:03.338546 [0] Epoch: 081, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:03.953902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:03.961065 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:04.380719 [0] Epoch 00082 | Loss 4.1594
21:50:04.418692 [0] Epoch: 082, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:04.452809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:04.509684 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:05.309791 [0] Epoch 00083 | Loss 4.1594
21:50:05.347949 [0] Epoch: 083, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:05.962791 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:05.970318 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:06.389316 [0] Epoch 00084 | Loss 4.1594
21:50:06.427311 [0] Epoch: 084, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:06.461547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:06.518557 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:07.318766 [0] Epoch 00085 | Loss 4.1594
21:50:07.356694 [0] Epoch: 085, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:07.972359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:07.979653 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:08.399789 [0] Epoch 00086 | Loss 4.1594
21:50:08.437697 [0] Epoch: 086, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:08.471699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:08.528686 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:09.329255 [0] Epoch 00087 | Loss 4.1594
21:50:09.367290 [0] Epoch: 087, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:09.982823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:09.989763 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:10.409470 [0] Epoch 00088 | Loss 4.1594
21:50:10.447453 [0] Epoch: 088, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:10.481627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:10.538295 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:11.338658 [0] Epoch 00089 | Loss 4.1594
21:50:11.376550 [0] Epoch: 089, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:11.991649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:11.998852 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:12.417861 [0] Epoch 00090 | Loss 4.1594
21:50:12.455544 [0] Epoch: 090, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:12.489574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:12.546560 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:13.346765 [0] Epoch 00091 | Loss 4.1594
21:50:13.384894 [0] Epoch: 091, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:13.997586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:14.004914 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:14.424582 [0] Epoch 00092 | Loss 4.1594
21:50:14.462378 [0] Epoch: 092, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:14.496517 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:14.553425 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:15.352899 [0] Epoch 00093 | Loss 4.1594
21:50:15.390627 [0] Epoch: 093, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:16.003116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.010486 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:16.429571 [0] Epoch 00094 | Loss 4.1594
21:50:16.467450 [0] Epoch: 094, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:16.501864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:16.558320 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:17.357748 [0] Epoch 00095 | Loss 4.1594
21:50:17.395702 [0] Epoch: 095, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:18.008192 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.015157 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:18.434897 [0] Epoch 00096 | Loss 4.1594
21:50:18.472797 [0] Epoch: 096, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:18.506745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:18.563644 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:19.362934 [0] Epoch 00097 | Loss 4.1594
21:50:19.401104 [0] Epoch: 097, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:20.013096 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.020311 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:20.439918 [0] Epoch 00098 | Loss 4.1594
21:50:20.477568 [0] Epoch: 098, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:20.511673 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:20.568548 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:21.368631 [0] Epoch 00099 | Loss 4.1594
21:50:21.406566 [0] Epoch: 099, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:22.018325 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.025449 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:22.444647 [0] Epoch 00100 | Loss 4.1594
21:50:22.482331 [0] Epoch: 100, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:22.516476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:22.573336 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:23.374013 [0] Epoch 00101 | Loss 4.1594
21:50:23.412368 [0] Epoch: 101, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:24.024799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.031815 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:24.451457 [0] Epoch 00102 | Loss 4.1594
21:50:24.489182 [0] Epoch: 102, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:24.523333 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:24.580138 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:25.379359 [0] Epoch 00103 | Loss 4.1594
21:50:25.417306 [0] Epoch: 103, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:26.029727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.037431 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:26.456353 [0] Epoch 00104 | Loss 4.1594
21:50:26.494148 [0] Epoch: 104, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:26.528275 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:26.585188 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:27.384802 [0] Epoch 00105 | Loss 4.1594
21:50:27.423195 [0] Epoch: 105, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:28.035605 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.042679 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:28.461847 [0] Epoch 00106 | Loss 4.1594
21:50:28.499587 [0] Epoch: 106, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:28.533767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:28.590466 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:29.390069 [0] Epoch 00107 | Loss 4.1594
21:50:29.427834 [0] Epoch: 107, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:30.040032 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.047066 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:30.465805 [0] Epoch 00108 | Loss 4.1594
21:50:30.503676 [0] Epoch: 108, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:30.538510 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:30.594543 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:31.395743 [0] Epoch 00109 | Loss 4.1594
21:50:31.433786 [0] Epoch: 109, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:32.045359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.052505 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:32.471395 [0] Epoch 00110 | Loss 4.1594
21:50:32.509111 [0] Epoch: 110, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:32.543173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:32.600170 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:33.400630 [0] Epoch 00111 | Loss 4.1594
21:50:33.438353 [0] Epoch: 111, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:34.051326 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.058494 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:34.477952 [0] Epoch 00112 | Loss 4.1594
21:50:34.515738 [0] Epoch: 112, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:34.549842 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:34.606634 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:35.405697 [0] Epoch 00113 | Loss 4.1594
21:50:35.443648 [0] Epoch: 113, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:36.055893 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.062796 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:36.481782 [0] Epoch 00114 | Loss 4.1594
21:50:36.519582 [0] Epoch: 114, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:36.553721 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:36.610548 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:37.410027 [0] Epoch 00115 | Loss 4.1594
21:50:37.447818 [0] Epoch: 115, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:38.059542 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.066785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:38.485636 [0] Epoch 00116 | Loss 4.1594
21:50:38.523448 [0] Epoch: 116, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:38.557477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:38.614368 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:39.412977 [0] Epoch 00117 | Loss 4.1594
21:50:39.452130 [0] Epoch: 117, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:40.067482 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.074813 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:40.493833 [0] Epoch 00118 | Loss 4.1594
21:50:40.531778 [0] Epoch: 118, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:40.565864 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:40.622717 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:41.423057 [0] Epoch 00119 | Loss 4.1594
21:50:41.461194 [0] Epoch: 119, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:42.072805 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.079867 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:42.499318 [0] Epoch 00120 | Loss 4.1594
21:50:42.537070 [0] Epoch: 120, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:42.571154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:42.627794 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:43.428028 [0] Epoch 00121 | Loss 4.1594
21:50:43.466094 [0] Epoch: 121, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:44.078831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.086182 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:44.504638 [0] Epoch 00122 | Loss 4.1594
21:50:44.543713 [0] Epoch: 122, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:44.577720 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:44.634379 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:45.434131 [0] Epoch 00123 | Loss 4.1594
21:50:45.472445 [0] Epoch: 123, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:46.084826 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.092050 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:46.511190 [0] Epoch 00124 | Loss 4.1594
21:50:46.549169 [0] Epoch: 124, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:46.583356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:46.639844 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:47.438937 [0] Epoch 00125 | Loss 4.1594
21:50:47.477131 [0] Epoch: 125, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:48.089241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.096550 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:48.514679 [0] Epoch 00126 | Loss 4.1594
21:50:48.552957 [0] Epoch: 126, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:48.587071 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:48.643943 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:49.443588 [0] Epoch 00127 | Loss 4.1594
21:50:49.482610 [0] Epoch: 127, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:50.098240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.105514 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:50.524874 [0] Epoch 00128 | Loss 4.1594
21:50:50.562811 [0] Epoch: 128, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:50.596910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:50.653736 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:51.454070 [0] Epoch 00129 | Loss 4.1594
21:50:51.492098 [0] Epoch: 129, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:52.105006 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.112031 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:52.531876 [0] Epoch 00130 | Loss 4.1594
21:50:52.569756 [0] Epoch: 130, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:52.603822 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:52.661086 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:53.461180 [0] Epoch 00131 | Loss 4.1594
21:50:53.498943 [0] Epoch: 131, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:54.111278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.118551 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:54.537838 [0] Epoch 00132 | Loss 4.1594
21:50:54.575576 [0] Epoch: 132, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:54.609633 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:54.666476 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:55.466191 [0] Epoch 00133 | Loss 4.1594
21:50:55.504051 [0] Epoch: 133, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:56.116692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.123571 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:56.542493 [0] Epoch 00134 | Loss 4.1594
21:50:56.580384 [0] Epoch: 134, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:56.614443 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:56.671343 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:57.470084 [0] Epoch 00135 | Loss 4.1594
21:50:57.508130 [0] Epoch: 135, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:58.120095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.127346 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:58.546676 [0] Epoch 00136 | Loss 4.1594
21:50:58.584361 [0] Epoch: 136, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:50:58.618427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:50:58.675276 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:50:59.474998 [0] Epoch 00137 | Loss 4.1594
21:50:59.513450 [0] Epoch: 137, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:00.125561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.132947 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:00.552431 [0] Epoch 00138 | Loss 4.1594
21:51:00.590071 [0] Epoch: 138, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:00.623940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:00.681111 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:01.492988 [0] Epoch 00139 | Loss 4.1594
21:51:01.532548 [0] Epoch: 139, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:02.154399 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.162179 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:02.592286 [0] Epoch 00140 | Loss 4.1594
21:51:02.630794 [0] Epoch: 140, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:02.663928 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:02.721216 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:03.523422 [0] Epoch 00141 | Loss 4.1594
21:51:03.561393 [0] Epoch: 141, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:04.173299 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.180540 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:04.600872 [0] Epoch 00142 | Loss 4.1594
21:51:04.638553 [0] Epoch: 142, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:04.672262 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:04.729194 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:05.529887 [0] Epoch 00143 | Loss 4.1594
21:51:05.568023 [0] Epoch: 143, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:06.179804 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.186793 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:06.607247 [0] Epoch 00144 | Loss 4.1594
21:51:06.644839 [0] Epoch: 144, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:06.678726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:06.735706 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:07.536778 [0] Epoch 00145 | Loss 4.1594
21:51:07.574566 [0] Epoch: 145, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:08.187168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.194442 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:08.615313 [0] Epoch 00146 | Loss 4.1594
21:51:08.653359 [0] Epoch: 146, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:08.687111 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:08.743963 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:09.545257 [0] Epoch 00147 | Loss 4.1594
21:51:09.582816 [0] Epoch: 147, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:10.194483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.201656 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:10.619997 [0] Epoch 00148 | Loss 4.1594
21:51:10.658787 [0] Epoch: 148, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:10.692587 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:10.749426 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:11.549873 [0] Epoch 00149 | Loss 4.1594
21:51:11.587889 [0] Epoch: 149, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:12.199352 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.206442 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:12.626412 [0] Epoch 00150 | Loss 4.1594
21:51:12.664274 [0] Epoch: 150, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:12.698259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:12.755171 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:13.556011 [0] Epoch 00151 | Loss 4.1594
21:51:13.593724 [0] Epoch: 151, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:14.205318 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.212642 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:14.631664 [0] Epoch 00152 | Loss 4.1594
21:51:14.669313 [0] Epoch: 152, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:14.703088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:14.759948 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:15.560042 [0] Epoch 00153 | Loss 4.1594
21:51:15.597748 [0] Epoch: 153, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:16.209346 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.216543 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:16.636145 [0] Epoch 00154 | Loss 4.1594
21:51:16.673755 [0] Epoch: 154, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:16.707459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:16.764363 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:17.564934 [0] Epoch 00155 | Loss 4.1594
21:51:17.602525 [0] Epoch: 155, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:18.214250 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.221424 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:18.640927 [0] Epoch 00156 | Loss 4.1594
21:51:18.678752 [0] Epoch: 156, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:18.712421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:18.769490 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:19.569741 [0] Epoch 00157 | Loss 4.1594
21:51:19.608298 [0] Epoch: 157, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:20.220331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.227504 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:20.645647 [0] Epoch 00158 | Loss 4.1594
21:51:20.683571 [0] Epoch: 158, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:20.717360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:20.774234 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:21.574755 [0] Epoch 00159 | Loss 4.1594
21:51:21.612682 [0] Epoch: 159, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:22.223452 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.230616 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:22.650293 [0] Epoch 00160 | Loss 4.1594
21:51:22.688544 [0] Epoch: 160, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:22.722564 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:22.779125 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:23.579551 [0] Epoch 00161 | Loss 4.1594
21:51:23.617734 [0] Epoch: 161, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:24.230455 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.237347 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:24.658086 [0] Epoch 00162 | Loss 4.1594
21:51:24.695911 [0] Epoch: 162, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:24.729739 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:24.786607 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:25.587828 [0] Epoch 00163 | Loss 4.1594
21:51:25.625778 [0] Epoch: 163, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:26.237275 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.244481 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:26.664352 [0] Epoch 00164 | Loss 4.1594
21:51:26.702227 [0] Epoch: 164, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:26.736123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:26.792802 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:27.592451 [0] Epoch 00165 | Loss 4.1594
21:51:27.630596 [0] Epoch: 165, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:28.242617 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.249657 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:28.669280 [0] Epoch 00166 | Loss 4.1594
21:51:28.707572 [0] Epoch: 166, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:28.741634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:28.798410 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:29.598553 [0] Epoch 00167 | Loss 4.1594
21:51:29.636724 [0] Epoch: 167, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:30.249295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.256332 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:30.676753 [0] Epoch 00168 | Loss 4.1594
21:51:30.714539 [0] Epoch: 168, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:30.748647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:30.805214 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:31.605708 [0] Epoch 00169 | Loss 4.1594
21:51:31.643939 [0] Epoch: 169, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:32.256859 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.263615 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:32.683650 [0] Epoch 00170 | Loss 4.1594
21:51:32.721621 [0] Epoch: 170, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:32.755768 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:32.812216 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:33.613104 [0] Epoch 00171 | Loss 4.1594
21:51:33.651044 [0] Epoch: 171, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:34.263771 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.270805 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:34.689917 [0] Epoch 00172 | Loss 4.1594
21:51:34.727722 [0] Epoch: 172, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:34.761757 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:34.818264 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:35.618741 [0] Epoch 00173 | Loss 4.1594
21:51:35.656534 [0] Epoch: 173, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:36.268584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.275725 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:36.695428 [0] Epoch 00174 | Loss 4.1594
21:51:36.733226 [0] Epoch: 174, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:36.767335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:36.823879 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:37.623379 [0] Epoch 00175 | Loss 4.1594
21:51:37.661226 [0] Epoch: 175, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:38.273449 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.280796 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:38.699446 [0] Epoch 00176 | Loss 4.1594
21:51:38.737341 [0] Epoch: 176, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:38.771481 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:38.827956 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:39.627057 [0] Epoch 00177 | Loss 4.1594
21:51:39.665221 [0] Epoch: 177, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:40.277894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.285020 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:40.703418 [0] Epoch 00178 | Loss 4.1594
21:51:40.741213 [0] Epoch: 178, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:40.775236 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:40.831688 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:41.631835 [0] Epoch 00179 | Loss 4.1594
21:51:41.669975 [0] Epoch: 179, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:42.282264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.289325 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:42.708886 [0] Epoch 00180 | Loss 4.1594
21:51:42.746688 [0] Epoch: 180, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:42.780812 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:42.837676 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:43.637781 [0] Epoch 00181 | Loss 4.1594
21:51:43.675628 [0] Epoch: 181, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:44.288159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.295388 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:44.714581 [0] Epoch 00182 | Loss 4.1594
21:51:44.752363 [0] Epoch: 182, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:44.786454 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:44.843037 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:45.642833 [0] Epoch 00183 | Loss 4.1594
21:51:45.681140 [0] Epoch: 183, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:46.293296 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.300414 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:46.718732 [0] Epoch 00184 | Loss 4.1594
21:51:46.756874 [0] Epoch: 184, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:46.791123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:46.847753 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:47.647423 [0] Epoch 00185 | Loss 4.1594
21:51:47.685294 [0] Epoch: 185, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:48.297675 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.304708 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:48.723722 [0] Epoch 00186 | Loss 4.1594
21:51:48.761535 [0] Epoch: 186, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:48.795586 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:48.852471 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:49.652744 [0] Epoch 00187 | Loss 4.1594
21:51:49.690459 [0] Epoch: 187, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:50.302852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.310216 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:50.727982 [0] Epoch 00188 | Loss 4.1594
21:51:50.766178 [0] Epoch: 188, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:50.800106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:50.857111 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:51.657026 [0] Epoch 00189 | Loss 4.1594
21:51:51.694833 [0] Epoch: 189, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:52.307199 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.314217 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:52.732557 [0] Epoch 00190 | Loss 4.1594
21:51:52.770600 [0] Epoch: 190, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:52.804597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:52.861550 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:53.661287 [0] Epoch 00191 | Loss 4.1594
21:51:53.699435 [0] Epoch: 191, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:54.312182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.319106 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:54.738823 [0] Epoch 00192 | Loss 4.1594
21:51:54.776577 [0] Epoch: 192, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:54.810592 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:54.867520 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:55.667465 [0] Epoch 00193 | Loss 4.1594
21:51:55.705300 [0] Epoch: 193, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:56.317545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.324714 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:56.744193 [0] Epoch 00194 | Loss 4.1594
21:51:56.781929 [0] Epoch: 194, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:56.816180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:56.872876 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:57.672391 [0] Epoch 00195 | Loss 4.1594
21:51:57.710224 [0] Epoch: 195, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:58.322744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.329847 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:58.750005 [0] Epoch 00196 | Loss 4.1594
21:51:58.787945 [0] Epoch: 196, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:51:58.822125 [3] Warning: no training nodes in this partition! Backward fake loss.
21:51:58.879268 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:51:59.678508 [0] Epoch 00197 | Loss 4.1594
21:51:59.716333 [0] Epoch: 197, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:52:00.328689 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.335845 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:52:00.754267 [0] Epoch 00198 | Loss 4.1594
21:52:00.791995 [0] Epoch: 198, Train: 0.0156, Val: 0.0155, Test: 0.0154
21:52:00.827227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:52:00.882910 [2] Warning: no training nodes in this partition! Backward fake loss.
0.015430
21:52:01.682932 [0] Epoch 00199 | Loss 4.1594
21:52:01.723546 [0] Epoch: 199, Train: 0.0156, Val: 0.0155, Test: 0.0154
0.015430
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f512_l64_t0.5 nprocs 4
21:52:01.731948 [0] 
timer summary:
 20.61s  17.35s     1 broadcast ForwardL1 0
156.46s  68.77s  3200 broadcast
 48.29s  56.16s  3200 spmm
  0.15s   0.05s     1 broadcast ForwardL1 1
  0.13s   0.04s     1 broadcast ForwardL1 2
  0.12s   0.03s     1 broadcast ForwardL1 3
  4.83s   0.04s   800 mm
 28.89s  16.77s   125 broadcast ForwardL2 0
  8.66s   2.67s   125 broadcast ForwardL2 1
  8.22s   2.48s   125 broadcast ForwardL2 2
  7.71s   2.23s   125 broadcast ForwardL2 3
 27.19s  18.08s   200 broadcast BackwardL2 0
  3.03s   0.88s   200 broadcast BackwardL2 1
  3.09s   0.88s   200 broadcast BackwardL2 2
  2.95s   0.84s   200 broadcast BackwardL2 3
  2.64s   1.85s   400 all_reduce
  7.23s   0.24s   200 broadcast BackwardL1 0
 13.56s   4.28s   200 broadcast BackwardL1 1
 12.83s   3.97s   200 broadcast BackwardL1 2
 12.00s   3.58s   200 broadcast BackwardL1 3
226.30s  17.35s   200 epoch
254.71s   0.09s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
