no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:34:17.830422 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:34:17.993878 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:34:18.009100 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:34:18.016128 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:34:34.172407 [3] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:34:34.186529 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 548605 KiB | 564992 KiB | 599718 KiB |  51113 KiB |
|       from large pool | 548605 KiB | 564989 KiB | 599711 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 548605 KiB | 564992 KiB | 599718 KiB |  51113 KiB |
|       from large pool | 548605 KiB | 564989 KiB | 599711 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 547654 KiB | 563281 KiB | 596487 KiB |  48832 KiB |
|       from large pool | 547654 KiB | 563279 KiB | 596482 KiB |  48828 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 581632 KiB | 581632 KiB | 581632 KiB |      0 B   |
|       from large pool | 579584 KiB | 579584 KiB | 579584 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14594 KiB |  20286 KiB |  58216 KiB |  43621 KiB |
|       from large pool |  14594 KiB |  20286 KiB |  52067 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:34:44.560117 [2] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:34:44.567313 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 652419 KiB | 668806 KiB | 703532 KiB |  51113 KiB |
|       from large pool | 652419 KiB | 668803 KiB | 703525 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 652419 KiB | 668806 KiB | 703532 KiB |  51113 KiB |
|       from large pool | 652419 KiB | 668803 KiB | 703525 KiB |  51105 KiB |
|       from small pool |      0 KiB |      2 KiB |      7 KiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 651468 KiB | 667094 KiB | 700300 KiB |  48832 KiB |
|       from large pool | 651468 KiB | 667093 KiB | 700296 KiB |  48828 KiB |
|       from small pool |      0 KiB |      1 KiB |      4 KiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   | 688128 KiB | 688128 KiB | 688128 KiB |      0 B   |
|       from large pool | 686080 KiB | 686080 KiB | 686080 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  17276 KiB |  20286 KiB |  51719 KiB |  34442 KiB |
|       from large pool |  17276 KiB |  20286 KiB |  45570 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:34:46.976102 [1] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:34:46.981320 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      | 773888 KiB |    771 MiB |    805 MiB |  51113 KiB |
|       from large pool | 773888 KiB |    771 MiB |    805 MiB |  51105 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         | 773888 KiB |    771 MiB |    805 MiB |  51113 KiB |
|       from large pool | 773888 KiB |    771 MiB |    805 MiB |  51105 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      | 771554 KiB |    768 MiB |    801 MiB |  48832 KiB |
|       from large pool | 771554 KiB |    768 MiB |    801 MiB |  48828 KiB |
|       from small pool |      0 KiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    792 MiB |    792 MiB |    792 MiB |      0 B   |
|       from large pool |    790 MiB |    790 MiB |    790 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18687 KiB |  20735 KiB |  53130 KiB |  34442 KiB |
|       from large pool |  18687 KiB |  20286 KiB |  46981 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:35:03.565846 [0] graph loaded <COO Graph: e160M_f256_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 0, |V|: 500000, |E|: 113001879>
21:35:03.581755 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1381 MiB |   1397 MiB |   1431 MiB |  51113 KiB |
|       from large pool |   1381 MiB |   1397 MiB |   1431 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1381 MiB |   1397 MiB |   1431 MiB |  51113 KiB |
|       from large pool |   1381 MiB |   1397 MiB |   1431 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1379 MiB |   1394 MiB |   1426 MiB |  48832 KiB |
|       from large pool |   1379 MiB |   1394 MiB |   1426 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1412 MiB |   1412 MiB |   1412 MiB |      0 B   |
|       from large pool |   1410 MiB |   1410 MiB |   1410 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12472 KiB |  20286 KiB |  46915 KiB |  34442 KiB |
|       from large pool |  12472 KiB |  20286 KiB |  40766 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:35:05.940965 [2] Warning: no training nodes in this partition! Backward fake loss.
21:35:05.948598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:07.345628 [0] Epoch 00000 | Loss 3.4698
21:35:07.392661 [0] Epoch: 000, Train: 0.0311, Val: 0.0307, Test: 0.0310
21:35:07.847698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:07.855048 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030956
21:35:08.234987 [0] Epoch 00001 | Loss 11235.7266
21:35:08.255782 [0] Epoch: 001, Train: 0.0315, Val: 0.0316, Test: 0.0311
21:35:08.711784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:08.719057 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031145
21:35:09.099979 [0] Epoch 00002 | Loss 5658.0210
21:35:09.120845 [0] Epoch: 002, Train: 0.0314, Val: 0.0312, Test: 0.0311
21:35:09.576371 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:09.583392 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031078
21:35:09.961644 [0] Epoch 00003 | Loss 6024.3354
21:35:09.982501 [0] Epoch: 003, Train: 0.0312, Val: 0.0305, Test: 0.0312
21:35:10.437575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:10.444315 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031241
21:35:10.823614 [0] Epoch 00004 | Loss 6242.5581
21:35:10.844334 [0] Epoch: 004, Train: 0.0314, Val: 0.0311, Test: 0.0314
21:35:11.302359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:11.309350 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031394
21:35:11.689569 [0] Epoch 00005 | Loss 5836.5278
21:35:11.710352 [0] Epoch: 005, Train: 0.0313, Val: 0.0309, Test: 0.0315
21:35:12.163358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:12.171514 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031489
21:35:12.550210 [0] Epoch 00006 | Loss 4464.3120
21:35:12.571499 [0] Epoch: 006, Train: 0.0311, Val: 0.0310, Test: 0.0315
21:35:13.023952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.031072 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031484
21:35:13.410626 [0] Epoch 00007 | Loss 3416.8489
21:35:13.431336 [0] Epoch: 007, Train: 0.0314, Val: 0.0315, Test: 0.0309
21:35:13.883917 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:13.891181 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030930
21:35:14.270558 [0] Epoch 00008 | Loss 2106.1973
21:35:14.291348 [0] Epoch: 008, Train: 0.0314, Val: 0.0308, Test: 0.0316
21:35:14.743974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:14.750675 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031567
21:35:15.130264 [0] Epoch 00009 | Loss 910.3784
21:35:15.151006 [0] Epoch: 009, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:15.604400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:15.611085 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031455
21:35:15.990975 [0] Epoch 00010 | Loss 3.4651
21:35:16.011747 [0] Epoch: 010, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:16.465096 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:16.471840 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:16.852877 [0] Epoch 00011 | Loss 3.4651
21:35:16.873733 [0] Epoch: 011, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:17.326377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:17.334004 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:17.712407 [0] Epoch 00012 | Loss 3.4651
21:35:17.733501 [0] Epoch: 012, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:18.186120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:18.193558 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:18.570361 [0] Epoch 00013 | Loss 3.4651
21:35:18.590927 [0] Epoch: 013, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:19.043276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.050496 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:19.427927 [0] Epoch 00014 | Loss 3.4651
21:35:19.448590 [0] Epoch: 014, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:19.901899 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:19.909091 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:20.287242 [0] Epoch 00015 | Loss 3.4651
21:35:20.308071 [0] Epoch: 015, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:20.760731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:20.768427 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:21.145092 [0] Epoch 00016 | Loss 3.4651
21:35:21.166095 [0] Epoch: 016, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:21.619237 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:21.626564 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:22.004875 [0] Epoch 00017 | Loss 3.4651
21:35:22.025663 [0] Epoch: 017, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:22.478261 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:22.485455 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:22.863083 [0] Epoch 00018 | Loss 3.4651
21:35:22.883905 [0] Epoch: 018, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:23.337017 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:23.343976 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:23.721214 [0] Epoch 00019 | Loss 3.4651
21:35:23.741702 [0] Epoch: 019, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:24.194004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:24.201679 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:24.577623 [0] Epoch 00020 | Loss 3.4651
21:35:24.598717 [0] Epoch: 020, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:25.051494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.058470 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:25.436069 [0] Epoch 00021 | Loss 3.4651
21:35:25.456437 [0] Epoch: 021, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:25.909629 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:25.916376 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:26.295008 [0] Epoch 00022 | Loss 3.4651
21:35:26.315493 [0] Epoch: 022, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:26.767829 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:26.775165 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:27.153926 [0] Epoch 00023 | Loss 3.4651
21:35:27.174445 [0] Epoch: 023, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:27.626745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:27.634156 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:28.011384 [0] Epoch 00024 | Loss 3.4651
21:35:28.031865 [0] Epoch: 024, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:28.484086 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:28.491135 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:28.869334 [0] Epoch 00025 | Loss 3.4651
21:35:28.889715 [0] Epoch: 025, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:29.342935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:29.350299 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:29.727651 [0] Epoch 00026 | Loss 3.4651
21:35:29.748021 [0] Epoch: 026, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:30.200215 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:30.207419 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:30.584826 [0] Epoch 00027 | Loss 3.4651
21:35:30.605303 [0] Epoch: 027, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:31.057591 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.064769 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:31.442996 [0] Epoch 00028 | Loss 3.4651
21:35:31.463654 [0] Epoch: 028, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:31.915687 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:31.923868 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:32.300693 [0] Epoch 00029 | Loss 3.4651
21:35:32.321148 [0] Epoch: 029, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:32.773698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:32.780870 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:33.158598 [0] Epoch 00030 | Loss 3.4651
21:35:33.179219 [0] Epoch: 030, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:33.631387 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:33.639335 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:34.015934 [0] Epoch 00031 | Loss 3.4651
21:35:34.036366 [0] Epoch: 031, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:34.488735 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:34.495953 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:34.873443 [0] Epoch 00032 | Loss 3.4651
21:35:34.893762 [0] Epoch: 032, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:35.347010 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:35.353804 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:35.731373 [0] Epoch 00033 | Loss 3.4651
21:35:35.751849 [0] Epoch: 033, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:36.204138 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:36.211285 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:36.588744 [0] Epoch 00034 | Loss 3.4651
21:35:36.609179 [0] Epoch: 034, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:37.062863 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.069788 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:37.447721 [0] Epoch 00035 | Loss 3.4651
21:35:37.468282 [0] Epoch: 035, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:37.921120 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:37.928705 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:38.307052 [0] Epoch 00036 | Loss 3.4651
21:35:38.327532 [0] Epoch: 036, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:38.779880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:38.787037 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:39.164260 [0] Epoch 00037 | Loss 3.4651
21:35:39.185105 [0] Epoch: 037, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:39.638068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:39.645392 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:40.023506 [0] Epoch 00038 | Loss 3.4651
21:35:40.044266 [0] Epoch: 038, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:40.496654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:40.503773 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:40.881268 [0] Epoch 00039 | Loss 3.4651
21:35:40.901820 [0] Epoch: 039, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:41.354644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:41.361905 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:41.738933 [0] Epoch 00040 | Loss 3.4651
21:35:41.759883 [0] Epoch: 040, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:42.212310 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:42.219544 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:42.596515 [0] Epoch 00041 | Loss 3.4651
21:35:42.617377 [0] Epoch: 041, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:43.069693 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.076948 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:43.455406 [0] Epoch 00042 | Loss 3.4651
21:35:43.476635 [0] Epoch: 042, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:43.929143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:43.936570 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:44.315963 [0] Epoch 00043 | Loss 3.4651
21:35:44.336672 [0] Epoch: 043, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:44.789470 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:44.796700 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:45.176751 [0] Epoch 00044 | Loss 3.4651
21:35:45.197442 [0] Epoch: 044, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:45.649994 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:45.657160 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:46.036367 [0] Epoch 00045 | Loss 3.4651
21:35:46.057029 [0] Epoch: 045, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:46.510097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:46.518121 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:46.897185 [0] Epoch 00046 | Loss 3.4651
21:35:46.918070 [0] Epoch: 046, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:47.370428 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:47.377798 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:47.756639 [0] Epoch 00047 | Loss 3.4651
21:35:47.777349 [0] Epoch: 047, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:48.230341 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:48.237425 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:48.616622 [0] Epoch 00048 | Loss 3.4651
21:35:48.637342 [0] Epoch: 048, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:49.089679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.096901 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:49.475640 [0] Epoch 00049 | Loss 3.4651
21:35:49.496724 [0] Epoch: 049, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:49.949422 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:49.956288 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:50.334958 [0] Epoch 00050 | Loss 3.4651
21:35:50.356320 [0] Epoch: 050, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:50.379557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:50.417549 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:51.048193 [0] Epoch 00051 | Loss 3.4651
21:35:51.068837 [0] Epoch: 051, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:51.520786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.527953 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:51.907214 [0] Epoch 00052 | Loss 3.4651
21:35:51.927803 [0] Epoch: 052, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:51.951097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:51.989146 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:52.618610 [0] Epoch 00053 | Loss 3.4651
21:35:52.639381 [0] Epoch: 053, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:53.092013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.099630 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:53.477858 [0] Epoch 00054 | Loss 3.4651
21:35:53.499330 [0] Epoch: 054, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:53.522665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:53.560819 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:54.187287 [0] Epoch 00055 | Loss 3.4651
21:35:54.208074 [0] Epoch: 055, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:54.660345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:54.667539 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:55.043827 [0] Epoch 00056 | Loss 3.4651
21:35:55.064411 [0] Epoch: 056, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:55.087843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:55.126024 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:55.752357 [0] Epoch 00057 | Loss 3.4651
21:35:55.773130 [0] Epoch: 057, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:56.225993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.232848 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:56.609152 [0] Epoch 00058 | Loss 3.4651
21:35:56.629599 [0] Epoch: 058, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:56.653024 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:56.691018 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:57.318006 [0] Epoch 00059 | Loss 3.4651
21:35:57.338352 [0] Epoch: 059, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:57.790567 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:57.797657 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:58.174363 [0] Epoch 00060 | Loss 3.4651
21:35:58.195118 [0] Epoch: 060, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:58.218323 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:58.256374 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:58.882507 [0] Epoch 00061 | Loss 3.4651
21:35:58.903257 [0] Epoch: 061, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:59.355845 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.363049 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:35:59.738972 [0] Epoch 00062 | Loss 3.4651
21:35:59.759675 [0] Epoch: 062, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:35:59.783028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:35:59.821103 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:00.447593 [0] Epoch 00063 | Loss 3.4651
21:36:00.468452 [0] Epoch: 063, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:00.920912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:00.928173 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:01.304569 [0] Epoch 00064 | Loss 3.4651
21:36:01.324858 [0] Epoch: 064, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:01.348238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:01.386213 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:02.015033 [0] Epoch 00065 | Loss 3.4651
21:36:02.036668 [0] Epoch: 065, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:02.498853 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.506290 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:02.893487 [0] Epoch 00066 | Loss 3.4651
21:36:02.914788 [0] Epoch: 066, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:02.938646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:02.976799 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:03.610833 [0] Epoch 00067 | Loss 3.4651
21:36:03.631557 [0] Epoch: 067, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:04.084015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.091907 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:04.470684 [0] Epoch 00068 | Loss 3.4651
21:36:04.491571 [0] Epoch: 068, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:04.515243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:04.552894 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:05.183697 [0] Epoch 00069 | Loss 3.4651
21:36:05.205215 [0] Epoch: 069, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:05.658264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:05.665685 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:06.044831 [0] Epoch 00070 | Loss 3.4651
21:36:06.065386 [0] Epoch: 070, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:06.088388 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:06.126769 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:06.757792 [0] Epoch 00071 | Loss 3.4651
21:36:06.778399 [0] Epoch: 071, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:07.230698 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.237985 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:07.616941 [0] Epoch 00072 | Loss 3.4651
21:36:07.637482 [0] Epoch: 072, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:07.660519 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:07.698871 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:08.330762 [0] Epoch 00073 | Loss 3.4651
21:36:08.351356 [0] Epoch: 073, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:08.803889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:08.811256 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:09.190361 [0] Epoch 00074 | Loss 3.4651
21:36:09.211060 [0] Epoch: 074, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:09.234018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:09.272347 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:09.902869 [0] Epoch 00075 | Loss 3.4651
21:36:09.923290 [0] Epoch: 075, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:10.376242 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.383407 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:10.762140 [0] Epoch 00076 | Loss 3.4651
21:36:10.782599 [0] Epoch: 076, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:10.805561 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:10.843860 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:11.472675 [0] Epoch 00077 | Loss 3.4651
21:36:11.493590 [0] Epoch: 077, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:11.945945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:11.953092 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:12.331122 [0] Epoch 00078 | Loss 3.4651
21:36:12.351641 [0] Epoch: 078, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:12.374556 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:12.412879 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:13.041663 [0] Epoch 00079 | Loss 3.4651
21:36:13.062426 [0] Epoch: 079, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:13.515584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.522795 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:13.900841 [0] Epoch 00080 | Loss 3.4651
21:36:13.921234 [0] Epoch: 080, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:13.944302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:13.982591 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:14.611337 [0] Epoch 00081 | Loss 3.4651
21:36:14.632525 [0] Epoch: 081, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:15.085630 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.092872 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:15.471236 [0] Epoch 00082 | Loss 3.4651
21:36:15.491939 [0] Epoch: 082, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:15.515427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:15.553238 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:16.181732 [0] Epoch 00083 | Loss 3.4651
21:36:16.202125 [0] Epoch: 083, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:16.654306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:16.661568 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:17.039514 [0] Epoch 00084 | Loss 3.4651
21:36:17.059900 [0] Epoch: 084, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:17.082956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:17.121251 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:17.748374 [0] Epoch 00085 | Loss 3.4651
21:36:17.768924 [0] Epoch: 085, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:18.221635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.228917 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:18.605284 [0] Epoch 00086 | Loss 3.4651
21:36:18.626021 [0] Epoch: 086, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:18.648921 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:18.687242 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:19.314904 [0] Epoch 00087 | Loss 3.4651
21:36:19.335360 [0] Epoch: 087, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:19.788211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:19.795476 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:20.172852 [0] Epoch 00088 | Loss 3.4651
21:36:20.193326 [0] Epoch: 088, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:20.216345 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:20.254715 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:20.881051 [0] Epoch 00089 | Loss 3.4651
21:36:20.901768 [0] Epoch: 089, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:21.353171 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.360317 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:21.736568 [0] Epoch 00090 | Loss 3.4651
21:36:21.756786 [0] Epoch: 090, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:21.779819 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:21.818199 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:22.445278 [0] Epoch 00091 | Loss 3.4651
21:36:22.465680 [0] Epoch: 091, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:22.917355 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:22.924785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:23.301775 [0] Epoch 00092 | Loss 3.4651
21:36:23.322442 [0] Epoch: 092, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:23.346118 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:23.383938 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:24.010936 [0] Epoch 00093 | Loss 3.4651
21:36:24.032466 [0] Epoch: 093, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:24.484114 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.491620 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:24.867232 [0] Epoch 00094 | Loss 3.4651
21:36:24.887907 [0] Epoch: 094, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:24.910880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:24.949210 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:25.576929 [0] Epoch 00095 | Loss 3.4651
21:36:25.597293 [0] Epoch: 095, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:26.048313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.055546 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:26.431662 [0] Epoch 00096 | Loss 3.4651
21:36:26.452312 [0] Epoch: 096, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:26.475478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:26.513860 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:27.140750 [0] Epoch 00097 | Loss 3.4651
21:36:27.161208 [0] Epoch: 097, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:27.612707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:27.619938 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:27.995966 [0] Epoch 00098 | Loss 3.4651
21:36:28.016496 [0] Epoch: 098, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:28.039569 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:28.077878 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:28.704490 [0] Epoch 00099 | Loss 3.4651
21:36:28.725218 [0] Epoch: 099, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:29.176895 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.184057 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:29.562173 [0] Epoch 00100 | Loss 3.4651
21:36:29.583045 [0] Epoch: 100, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:29.606018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:29.644353 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:30.271718 [0] Epoch 00101 | Loss 3.4651
21:36:30.292544 [0] Epoch: 101, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:30.743683 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:30.750858 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:31.127422 [0] Epoch 00102 | Loss 3.4651
21:36:31.148213 [0] Epoch: 102, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:31.171834 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:31.209560 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:31.836210 [0] Epoch 00103 | Loss 3.4651
21:36:31.857228 [0] Epoch: 103, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:32.309095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.316386 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:32.693096 [0] Epoch 00104 | Loss 3.4651
21:36:32.713322 [0] Epoch: 104, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:32.736438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:32.774731 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:33.401311 [0] Epoch 00105 | Loss 3.4651
21:36:33.422081 [0] Epoch: 105, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:33.873180 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:33.880496 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:34.256980 [0] Epoch 00106 | Loss 3.4651
21:36:34.277339 [0] Epoch: 106, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:34.300725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:34.338659 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:34.964491 [0] Epoch 00107 | Loss 3.4651
21:36:34.985371 [0] Epoch: 107, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:35.437622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.444884 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:35.820549 [0] Epoch 00108 | Loss 3.4651
21:36:35.841345 [0] Epoch: 108, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:35.865365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:35.902665 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:36.530120 [0] Epoch 00109 | Loss 3.4651
21:36:36.550419 [0] Epoch: 109, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:37.002524 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.009701 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:37.385164 [0] Epoch 00110 | Loss 3.4651
21:36:37.406010 [0] Epoch: 110, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:37.429246 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:37.467267 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:38.094972 [0] Epoch 00111 | Loss 3.4651
21:36:38.115561 [0] Epoch: 111, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:38.568162 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:38.575281 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:38.951457 [0] Epoch 00112 | Loss 3.4651
21:36:38.972146 [0] Epoch: 112, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:38.995596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:39.033537 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:39.660328 [0] Epoch 00113 | Loss 3.4651
21:36:39.680898 [0] Epoch: 113, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:40.133053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.140174 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:40.517376 [0] Epoch 00114 | Loss 3.4651
21:36:40.538148 [0] Epoch: 114, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:40.561405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:40.599442 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:41.226596 [0] Epoch 00115 | Loss 3.4651
21:36:41.246906 [0] Epoch: 115, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:41.699384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:41.706513 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:42.084151 [0] Epoch 00116 | Loss 3.4651
21:36:42.104609 [0] Epoch: 116, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:42.127884 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:42.165941 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:42.792636 [0] Epoch 00117 | Loss 3.4651
21:36:42.812938 [0] Epoch: 117, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:43.266075 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.273074 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:43.650995 [0] Epoch 00118 | Loss 3.4651
21:36:43.671258 [0] Epoch: 118, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:43.694600 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:43.732741 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:44.359306 [0] Epoch 00119 | Loss 3.4651
21:36:44.379727 [0] Epoch: 119, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:44.832079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:44.839117 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:45.216891 [0] Epoch 00120 | Loss 3.4651
21:36:45.237463 [0] Epoch: 120, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:45.260894 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:45.298868 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:45.927797 [0] Epoch 00121 | Loss 3.4651
21:36:45.948551 [0] Epoch: 121, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:46.401274 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.409119 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:46.785774 [0] Epoch 00122 | Loss 3.4651
21:36:46.806449 [0] Epoch: 122, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:46.829856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:46.867923 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:47.495031 [0] Epoch 00123 | Loss 3.4651
21:36:47.515399 [0] Epoch: 123, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:47.967854 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:47.974982 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:48.352796 [0] Epoch 00124 | Loss 3.4651
21:36:48.373225 [0] Epoch: 124, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:48.396538 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:48.434483 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:49.062131 [0] Epoch 00125 | Loss 3.4651
21:36:49.082785 [0] Epoch: 125, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:49.535184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:49.542086 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:49.919105 [0] Epoch 00126 | Loss 3.4651
21:36:49.939532 [0] Epoch: 126, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:49.962737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:50.000771 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:50.627069 [0] Epoch 00127 | Loss 3.4651
21:36:50.647778 [0] Epoch: 127, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:51.099493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.106716 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:51.482862 [0] Epoch 00128 | Loss 3.4651
21:36:51.503379 [0] Epoch: 128, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:51.526810 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:51.564878 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:52.192307 [0] Epoch 00129 | Loss 3.4651
21:36:52.212805 [0] Epoch: 129, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:52.665287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:52.672469 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:53.048206 [0] Epoch 00130 | Loss 3.4651
21:36:53.068801 [0] Epoch: 130, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:53.092172 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:53.130185 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:53.757036 [0] Epoch 00131 | Loss 3.4651
21:36:53.778417 [0] Epoch: 131, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:54.230433 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.237549 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:54.613955 [0] Epoch 00132 | Loss 3.4651
21:36:54.634897 [0] Epoch: 132, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:54.658154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:54.696789 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:55.323248 [0] Epoch 00133 | Loss 3.4651
21:36:55.344423 [0] Epoch: 133, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:55.795514 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:55.802835 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:56.179267 [0] Epoch 00134 | Loss 3.4651
21:36:56.200030 [0] Epoch: 134, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:56.223134 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:56.261561 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:56.888303 [0] Epoch 00135 | Loss 3.4651
21:36:56.908930 [0] Epoch: 135, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:57.360540 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.367745 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:57.744282 [0] Epoch 00136 | Loss 3.4651
21:36:57.765053 [0] Epoch: 136, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:57.788112 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:57.826432 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:58.453307 [0] Epoch 00137 | Loss 3.4651
21:36:58.474366 [0] Epoch: 137, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:58.925644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:58.933002 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:36:59.310472 [0] Epoch 00138 | Loss 3.4651
21:36:59.331721 [0] Epoch: 138, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:36:59.353961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:36:59.392206 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:00.021579 [0] Epoch 00139 | Loss 3.4651
21:37:00.041934 [0] Epoch: 139, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:00.493748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.500982 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:00.878699 [0] Epoch 00140 | Loss 3.4651
21:37:00.899371 [0] Epoch: 140, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:00.922527 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:00.960701 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:01.591748 [0] Epoch 00141 | Loss 3.4651
21:37:01.612933 [0] Epoch: 141, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:02.073444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.080686 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:02.466727 [0] Epoch 00142 | Loss 3.4651
21:37:02.487934 [0] Epoch: 142, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:02.513532 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:02.550136 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:03.184338 [0] Epoch 00143 | Loss 3.4651
21:37:03.204966 [0] Epoch: 143, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:03.660070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:03.667462 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:04.047243 [0] Epoch 00144 | Loss 3.4651
21:37:04.067852 [0] Epoch: 144, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:04.090903 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:04.129229 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:04.761107 [0] Epoch 00145 | Loss 3.4651
21:37:04.781700 [0] Epoch: 145, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:05.233952 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.241286 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:05.620698 [0] Epoch 00146 | Loss 3.4651
21:37:05.641615 [0] Epoch: 146, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:05.664785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:05.703155 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:06.337065 [0] Epoch 00147 | Loss 3.4651
21:37:06.358114 [0] Epoch: 147, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:06.813659 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:06.821190 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:07.203352 [0] Epoch 00148 | Loss 3.4651
21:37:07.224337 [0] Epoch: 148, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:07.247511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:07.285828 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:07.920861 [0] Epoch 00149 | Loss 3.4651
21:37:07.942007 [0] Epoch: 149, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:08.395117 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.402379 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:08.781641 [0] Epoch 00150 | Loss 3.4651
21:37:08.803179 [0] Epoch: 150, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:08.826485 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:08.864471 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:09.492506 [0] Epoch 00151 | Loss 3.4651
21:37:09.513249 [0] Epoch: 151, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:09.968695 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:09.975867 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:10.352718 [0] Epoch 00152 | Loss 3.4651
21:37:10.373503 [0] Epoch: 152, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:10.396650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:10.434692 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:11.063597 [0] Epoch 00153 | Loss 3.4651
21:37:11.084226 [0] Epoch: 153, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:11.539475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:11.547240 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:11.923394 [0] Epoch 00154 | Loss 3.4651
21:37:11.943961 [0] Epoch: 154, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:11.967399 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:12.005330 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:12.632410 [0] Epoch 00155 | Loss 3.4651
21:37:12.652978 [0] Epoch: 155, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:13.108788 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.115949 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:13.492560 [0] Epoch 00156 | Loss 3.4651
21:37:13.513137 [0] Epoch: 156, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:13.536338 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:13.574372 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:14.202069 [0] Epoch 00157 | Loss 3.4651
21:37:14.222507 [0] Epoch: 157, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:14.677946 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:14.685100 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:15.062215 [0] Epoch 00158 | Loss 3.4651
21:37:15.082705 [0] Epoch: 158, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:15.106013 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:15.144013 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:15.771592 [0] Epoch 00159 | Loss 3.4651
21:37:15.792237 [0] Epoch: 159, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:16.246888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.254054 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:16.631535 [0] Epoch 00160 | Loss 3.4651
21:37:16.652184 [0] Epoch: 160, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:16.675358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:16.713582 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:17.341276 [0] Epoch 00161 | Loss 3.4651
21:37:17.362539 [0] Epoch: 161, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:17.818888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:17.825709 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:18.202330 [0] Epoch 00162 | Loss 3.4651
21:37:18.223110 [0] Epoch: 162, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:18.246579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:18.284667 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:18.912423 [0] Epoch 00163 | Loss 3.4651
21:37:18.932882 [0] Epoch: 163, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:19.389283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.396445 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:19.773233 [0] Epoch 00164 | Loss 3.4651
21:37:19.793715 [0] Epoch: 164, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:19.817008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:19.855026 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:20.481631 [0] Epoch 00165 | Loss 3.4651
21:37:20.502638 [0] Epoch: 165, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:20.955477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:20.962417 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:21.340715 [0] Epoch 00166 | Loss 3.4651
21:37:21.361157 [0] Epoch: 166, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:21.384534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:21.422515 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:22.050467 [0] Epoch 00167 | Loss 3.4651
21:37:22.070862 [0] Epoch: 167, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:22.523251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.530367 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:22.906696 [0] Epoch 00168 | Loss 3.4651
21:37:22.927692 [0] Epoch: 168, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:22.951116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:22.989059 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:23.615804 [0] Epoch 00169 | Loss 3.4651
21:37:23.636164 [0] Epoch: 169, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:24.088520 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.095651 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:24.471289 [0] Epoch 00170 | Loss 3.4651
21:37:24.491605 [0] Epoch: 170, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:24.515031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:24.552981 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:25.179959 [0] Epoch 00171 | Loss 3.4651
21:37:25.200661 [0] Epoch: 171, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:25.652968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:25.660133 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:26.036739 [0] Epoch 00172 | Loss 3.4651
21:37:26.056966 [0] Epoch: 172, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:26.080430 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:26.118398 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:26.744912 [0] Epoch 00173 | Loss 3.4651
21:37:26.765329 [0] Epoch: 173, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:27.218050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.224546 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:27.599894 [0] Epoch 00174 | Loss 3.4651
21:37:27.620196 [0] Epoch: 174, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:27.643589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:27.681552 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:28.307886 [0] Epoch 00175 | Loss 3.4651
21:37:28.328998 [0] Epoch: 175, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:28.781290 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:28.788571 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:29.163784 [0] Epoch 00176 | Loss 3.4651
21:37:29.184385 [0] Epoch: 176, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:29.207665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:29.245694 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:29.873021 [0] Epoch 00177 | Loss 3.4651
21:37:29.893470 [0] Epoch: 177, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:30.345772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.352848 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:30.728882 [0] Epoch 00178 | Loss 3.4651
21:37:30.749219 [0] Epoch: 178, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:30.772665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:30.810675 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:31.437392 [0] Epoch 00179 | Loss 3.4651
21:37:31.457709 [0] Epoch: 179, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:31.910676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:31.917824 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:32.293579 [0] Epoch 00180 | Loss 3.4651
21:37:32.314277 [0] Epoch: 180, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:32.337730 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:32.375700 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:33.002368 [0] Epoch 00181 | Loss 3.4651
21:37:33.022779 [0] Epoch: 181, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:33.475086 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.482165 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:33.857176 [0] Epoch 00182 | Loss 3.4651
21:37:33.877778 [0] Epoch: 182, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:33.901841 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:33.939291 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:34.565807 [0] Epoch 00183 | Loss 3.4651
21:37:34.586823 [0] Epoch: 183, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:35.039051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.046104 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:35.422361 [0] Epoch 00184 | Loss 3.4651
21:37:35.442964 [0] Epoch: 184, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:35.466306 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:35.504362 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:36.131020 [0] Epoch 00185 | Loss 3.4651
21:37:36.151905 [0] Epoch: 185, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:36.604220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:36.611619 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:36.987777 [0] Epoch 00186 | Loss 3.4651
21:37:37.008092 [0] Epoch: 186, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:37.031646 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:37.069586 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:37.695316 [0] Epoch 00187 | Loss 3.4651
21:37:37.715935 [0] Epoch: 187, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:38.168340 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.175475 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:38.551620 [0] Epoch 00188 | Loss 3.4651
21:37:38.571941 [0] Epoch: 188, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:38.595375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:38.633322 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:39.259419 [0] Epoch 00189 | Loss 3.4651
21:37:39.280305 [0] Epoch: 189, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:39.732359 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:39.739533 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:40.114841 [0] Epoch 00190 | Loss 3.4651
21:37:40.135520 [0] Epoch: 190, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:40.158941 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:40.196944 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:40.823589 [0] Epoch 00191 | Loss 3.4651
21:37:40.843997 [0] Epoch: 191, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:41.296310 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.303502 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:41.679729 [0] Epoch 00192 | Loss 3.4651
21:37:41.700082 [0] Epoch: 192, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:41.723404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:41.761405 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:42.387403 [0] Epoch 00193 | Loss 3.4651
21:37:42.408159 [0] Epoch: 193, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:42.860373 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:42.867843 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:43.243099 [0] Epoch 00194 | Loss 3.4651
21:37:43.264183 [0] Epoch: 194, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:43.287666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:43.325726 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:43.951364 [0] Epoch 00195 | Loss 3.4651
21:37:43.972641 [0] Epoch: 195, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:44.424711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.431740 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:44.807606 [0] Epoch 00196 | Loss 3.4651
21:37:44.828273 [0] Epoch: 196, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:44.851767 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:44.889716 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:45.515174 [0] Epoch 00197 | Loss 3.4651
21:37:45.535902 [0] Epoch: 197, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:45.988396 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:45.995589 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:46.372015 [0] Epoch 00198 | Loss 3.4651
21:37:46.392492 [0] Epoch: 198, Train: 0.0310, Val: 0.0314, Test: 0.0315
21:37:46.415935 [3] Warning: no training nodes in this partition! Backward fake loss.
21:37:46.454251 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031453
21:37:47.080545 [0] Epoch 00199 | Loss 3.4651
21:37:47.100983 [0] Epoch: 199, Train: 0.0310, Val: 0.0314, Test: 0.0315
0.031453
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f256_l32_t0.5 nprocs 4
21:37:47.105156 [0] 
timer summary:
 17.01s  12.17s     1 broadcast ForwardL1 0
127.98s  53.87s  3200 broadcast
 35.66s  41.49s  3200 spmm
  0.09s   0.03s     1 broadcast ForwardL1 1
  0.06s   0.02s     1 broadcast ForwardL1 2
  0.06s   0.02s     1 broadcast ForwardL1 3
  2.92s   0.02s   800 mm
 16.87s   8.49s   125 broadcast ForwardL2 0
  8.66s   2.68s   125 broadcast ForwardL2 1
  8.24s   2.49s   125 broadcast ForwardL2 2
  7.71s   2.24s   125 broadcast ForwardL2 3
 18.94s  13.01s   200 broadcast BackwardL2 0
  1.43s   0.42s   200 broadcast BackwardL2 1
  1.60s   0.47s   200 broadcast BackwardL2 2
  1.54s   0.47s   200 broadcast BackwardL2 3
  2.45s   1.69s   400 all_reduce
  7.24s   0.24s   200 broadcast BackwardL1 0
 13.56s   4.29s   200 broadcast BackwardL1 1
 12.86s   3.98s   200 broadcast BackwardL1 2
 12.03s   3.58s   200 broadcast BackwardL1 3
178.82s  12.17s   200 epoch
209.14s   0.09s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
