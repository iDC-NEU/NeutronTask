no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:56:22.143705 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:56:22.174348 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:56:22.200207 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:56:22.211354 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:56:51.867711 [3] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:56:51.881486 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:56:54.243465 [2] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:56:54.253511 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:57:13.261373 [1] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:57:13.273222 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:57:23.635477 [0] graph loaded <COO Graph: e160M_f512_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 0, |V|: 500000, |E|: 113001879>
21:57:23.644208 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1867 MiB |   1882 MiB |   1914 MiB |  48832 KiB |
|       from large pool |   1867 MiB |   1882 MiB |   1914 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1900 MiB |   1900 MiB |   1900 MiB |      0 B   |
|       from large pool |   1898 MiB |   1898 MiB |   1898 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12184 KiB |  19998 KiB |  46627 KiB |  34442 KiB |
|       from large pool |  12184 KiB |  19998 KiB |  40478 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:57:25.728956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:25.736372 [2] Warning: no training nodes in this partition! Backward fake loss.
21:57:27.115472 [0] Epoch 00000 | Loss 3.4670
21:57:27.176187 [0] Epoch: 000, Train: 0.0314, Val: 0.0314, Test: 0.0308
21:57:27.788248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:27.795534 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030806
21:57:28.181278 [0] Epoch 00001 | Loss 12845.4121
21:57:28.203593 [0] Epoch: 001, Train: 0.0311, Val: 0.0309, Test: 0.0309
21:57:28.815472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:28.822402 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030889
21:57:29.208129 [0] Epoch 00002 | Loss 4801.0898
21:57:29.229052 [0] Epoch: 002, Train: 0.0312, Val: 0.0311, Test: 0.0314
21:57:29.840682 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:29.847925 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031445
21:57:30.234337 [0] Epoch 00003 | Loss 2513.0146
21:57:30.255305 [0] Epoch: 003, Train: 0.0314, Val: 0.0313, Test: 0.0312
21:57:30.867129 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:30.874007 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031178
21:57:31.259567 [0] Epoch 00004 | Loss 2225.0247
21:57:31.280531 [0] Epoch: 004, Train: 0.0309, Val: 0.0314, Test: 0.0314
21:57:31.893214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:31.900308 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031384
21:57:32.287193 [0] Epoch 00005 | Loss 2076.7546
21:57:32.308155 [0] Epoch: 005, Train: 0.0311, Val: 0.0314, Test: 0.0312
21:57:32.920588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:32.927813 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031170
21:57:33.313831 [0] Epoch 00006 | Loss 1836.7382
21:57:33.335595 [0] Epoch: 006, Train: 0.0314, Val: 0.0311, Test: 0.0310
21:57:33.947780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:33.955198 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030989
21:57:34.341473 [0] Epoch 00007 | Loss 1538.7255
21:57:34.362304 [0] Epoch: 007, Train: 0.0315, Val: 0.0314, Test: 0.0311
21:57:34.974282 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:34.981078 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031105
21:57:35.367152 [0] Epoch 00008 | Loss 992.7660
21:57:35.388041 [0] Epoch: 008, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:57:35.999557 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:36.007088 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031406
21:57:36.392667 [0] Epoch 00009 | Loss 631.5989
21:57:36.414073 [0] Epoch: 009, Train: 0.0310, Val: 0.0308, Test: 0.0313
21:57:37.026589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:37.033826 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031258
21:57:37.420577 [0] Epoch 00010 | Loss 179.2856
21:57:37.441414 [0] Epoch: 010, Train: 0.0312, Val: 0.0317, Test: 0.0310
21:57:38.053670 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:38.060723 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030951
21:57:38.447440 [0] Epoch 00011 | Loss 3.4651
21:57:38.468869 [0] Epoch: 011, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:39.084348 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:39.090972 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:39.478373 [0] Epoch 00012 | Loss 3.4651
21:57:39.499304 [0] Epoch: 012, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:40.110873 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:40.118154 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:40.502851 [0] Epoch 00013 | Loss 3.4651
21:57:40.523628 [0] Epoch: 013, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:41.134966 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:41.142094 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:41.526790 [0] Epoch 00014 | Loss 3.4651
21:57:41.547732 [0] Epoch: 014, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:42.158681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:42.166178 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:42.551372 [0] Epoch 00015 | Loss 3.4651
21:57:42.572120 [0] Epoch: 015, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:43.182432 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:43.189619 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:43.573273 [0] Epoch 00016 | Loss 3.4651
21:57:43.595112 [0] Epoch: 016, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:44.206665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:44.214038 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031435
21:57:44.596150 [0] Epoch 00017 | Loss 3.4651
21:57:44.616560 [0] Epoch: 017, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:45.228252 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:45.235515 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:45.618737 [0] Epoch 00018 | Loss 3.4651
21:57:45.639353 [0] Epoch: 018, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:46.251725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:46.258609 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:46.641327 [0] Epoch 00019 | Loss 3.4651
21:57:46.663250 [0] Epoch: 019, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:47.275353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:47.282927 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:47.663994 [0] Epoch 00020 | Loss 3.4651
21:57:47.684963 [0] Epoch: 020, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:48.297317 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:48.304503 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:48.685941 [0] Epoch 00021 | Loss 3.4651
21:57:48.708077 [0] Epoch: 021, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:49.319459 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:49.327309 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:49.707874 [0] Epoch 00022 | Loss 3.4651
21:57:49.728735 [0] Epoch: 022, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:50.340501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:50.347682 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:50.729772 [0] Epoch 00023 | Loss 3.4651
21:57:50.750572 [0] Epoch: 023, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:51.362157 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:51.369744 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:51.753429 [0] Epoch 00024 | Loss 3.4651
21:57:51.774045 [0] Epoch: 024, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:52.386723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:52.393912 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:52.777660 [0] Epoch 00025 | Loss 3.4651
21:57:52.800206 [0] Epoch: 025, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:53.412271 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:53.419204 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:53.802364 [0] Epoch 00026 | Loss 3.4651
21:57:53.825749 [0] Epoch: 026, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:54.437511 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:54.444854 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:54.827068 [0] Epoch 00027 | Loss 3.4651
21:57:54.847587 [0] Epoch: 027, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:55.459047 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:55.466158 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:55.850837 [0] Epoch 00028 | Loss 3.4651
21:57:55.872231 [0] Epoch: 028, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:56.483297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:56.490424 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:56.876620 [0] Epoch 00029 | Loss 3.4651
21:57:56.897433 [0] Epoch: 029, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:57.508270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:57.515291 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:57.901770 [0] Epoch 00030 | Loss 3.4651
21:57:57.922661 [0] Epoch: 030, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:58.533650 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:58.540844 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:58.926395 [0] Epoch 00031 | Loss 3.4651
21:57:58.947560 [0] Epoch: 031, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:57:59.558603 [3] Warning: no training nodes in this partition! Backward fake loss.
21:57:59.565856 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:57:59.951385 [0] Epoch 00032 | Loss 3.4651
21:57:59.972451 [0] Epoch: 032, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:00.583257 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:00.590473 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:00.974492 [0] Epoch 00033 | Loss 3.4651
21:58:00.995609 [0] Epoch: 033, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:01.616178 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:01.623546 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:02.016983 [0] Epoch 00034 | Loss 3.4651
21:58:02.038161 [0] Epoch: 034, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:02.657731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:02.664897 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:03.048998 [0] Epoch 00035 | Loss 3.4651
21:58:03.069602 [0] Epoch: 035, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:03.682165 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:03.689766 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:04.073846 [0] Epoch 00036 | Loss 3.4651
21:58:04.094526 [0] Epoch: 036, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:04.706411 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:04.713671 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:05.097768 [0] Epoch 00037 | Loss 3.4651
21:58:05.118397 [0] Epoch: 037, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:05.730194 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:05.737312 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:06.121285 [0] Epoch 00038 | Loss 3.4651
21:58:06.141860 [0] Epoch: 038, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:06.752790 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:06.760029 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:07.143437 [0] Epoch 00039 | Loss 3.4651
21:58:07.164498 [0] Epoch: 039, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:07.775707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:07.783154 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:08.167148 [0] Epoch 00040 | Loss 3.4651
21:58:08.188153 [0] Epoch: 040, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:08.798940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:08.805875 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:09.189832 [0] Epoch 00041 | Loss 3.4651
21:58:09.210289 [0] Epoch: 041, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:09.821670 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:09.828920 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:10.213367 [0] Epoch 00042 | Loss 3.4651
21:58:10.234251 [0] Epoch: 042, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:10.845350 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:10.853033 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:11.237115 [0] Epoch 00043 | Loss 3.4651
21:58:11.258201 [0] Epoch: 043, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:11.869319 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:11.876595 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:12.260089 [0] Epoch 00044 | Loss 3.4651
21:58:12.281027 [0] Epoch: 044, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:12.892806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:12.900247 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:13.284363 [0] Epoch 00045 | Loss 3.4651
21:58:13.304837 [0] Epoch: 045, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:13.916220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:13.923574 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:14.307022 [0] Epoch 00046 | Loss 3.4651
21:58:14.328821 [0] Epoch: 046, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:14.939053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:14.946323 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:15.332413 [0] Epoch 00047 | Loss 3.4651
21:58:15.353717 [0] Epoch: 047, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:15.965930 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:15.973373 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:16.358641 [0] Epoch 00048 | Loss 3.4651
21:58:16.379473 [0] Epoch: 048, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:16.990573 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:16.997815 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:17.384427 [0] Epoch 00049 | Loss 3.4651
21:58:17.404993 [0] Epoch: 049, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:18.016676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.023912 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:18.410171 [0] Epoch 00050 | Loss 3.4651
21:58:18.431000 [0] Epoch: 050, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:18.464762 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:18.521704 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:19.287388 [0] Epoch 00051 | Loss 3.4651
21:58:19.308216 [0] Epoch: 051, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:19.919468 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:19.926785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:20.310523 [0] Epoch 00052 | Loss 3.4651
21:58:20.331072 [0] Epoch: 052, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:20.364780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:20.421745 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:21.186855 [0] Epoch 00053 | Loss 3.4651
21:58:21.208601 [0] Epoch: 053, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:21.819365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:21.826737 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:22.210849 [0] Epoch 00054 | Loss 3.4651
21:58:22.231463 [0] Epoch: 054, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:22.265146 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:22.321954 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:23.085701 [0] Epoch 00055 | Loss 3.4651
21:58:23.107861 [0] Epoch: 055, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:23.718529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:23.725885 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:24.108886 [0] Epoch 00056 | Loss 3.4651
21:58:24.129322 [0] Epoch: 056, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:24.163050 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:24.220064 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:24.983594 [0] Epoch 00057 | Loss 3.4651
21:58:25.004567 [0] Epoch: 057, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:25.615755 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:25.623313 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:26.006105 [0] Epoch 00058 | Loss 3.4651
21:58:26.026773 [0] Epoch: 058, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:26.060571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:26.117769 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:26.881189 [0] Epoch 00059 | Loss 3.4651
21:58:26.901555 [0] Epoch: 059, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:27.512193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:27.519466 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:27.902675 [0] Epoch 00060 | Loss 3.4651
21:58:27.923534 [0] Epoch: 060, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:27.957374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:28.014678 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:28.778313 [0] Epoch 00061 | Loss 3.4651
21:58:28.799665 [0] Epoch: 061, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:29.410843 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.418154 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:29.801280 [0] Epoch 00062 | Loss 3.4651
21:58:29.822520 [0] Epoch: 062, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:29.856881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:29.913420 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:30.675855 [0] Epoch 00063 | Loss 3.4651
21:58:30.697587 [0] Epoch: 063, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:31.308287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.315705 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:31.697978 [0] Epoch 00064 | Loss 3.4651
21:58:31.719070 [0] Epoch: 064, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:31.752807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:31.810078 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:32.573389 [0] Epoch 00065 | Loss 3.4651
21:58:32.593914 [0] Epoch: 065, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:33.204874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.212120 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:33.594869 [0] Epoch 00066 | Loss 3.4651
21:58:33.615701 [0] Epoch: 066, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:33.649413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:33.706857 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:34.468791 [0] Epoch 00067 | Loss 3.4651
21:58:34.489422 [0] Epoch: 067, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:35.100015 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.107126 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:35.489976 [0] Epoch 00068 | Loss 3.4651
21:58:35.511549 [0] Epoch: 068, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:35.545211 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:35.602212 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:36.366673 [0] Epoch 00069 | Loss 3.4651
21:58:36.387308 [0] Epoch: 069, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:36.999900 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:37.006170 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:37.389937 [0] Epoch 00070 | Loss 3.4651
21:58:37.410962 [0] Epoch: 070, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:37.444672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:37.501884 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:38.265356 [0] Epoch 00071 | Loss 3.4651
21:58:38.287460 [0] Epoch: 071, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:38.898811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:38.906354 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:39.289624 [0] Epoch 00072 | Loss 3.4651
21:58:39.310274 [0] Epoch: 072, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:39.343823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:39.400844 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:40.165265 [0] Epoch 00073 | Loss 3.4651
21:58:40.185818 [0] Epoch: 073, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:40.796597 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:40.803811 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:41.187024 [0] Epoch 00074 | Loss 3.4651
21:58:41.208721 [0] Epoch: 074, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:41.242258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:41.299624 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:42.063125 [0] Epoch 00075 | Loss 3.4651
21:58:42.084445 [0] Epoch: 075, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:42.696912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:42.704206 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:43.088613 [0] Epoch 00076 | Loss 3.4651
21:58:43.109875 [0] Epoch: 076, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:43.143501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:43.200654 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:43.965091 [0] Epoch 00077 | Loss 3.4651
21:58:43.986063 [0] Epoch: 077, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:44.598219 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:44.605385 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:44.989560 [0] Epoch 00078 | Loss 3.4651
21:58:45.010209 [0] Epoch: 078, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:45.043803 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:45.101402 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:45.865606 [0] Epoch 00079 | Loss 3.4651
21:58:45.886393 [0] Epoch: 079, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:46.498376 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:46.505587 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:46.889441 [0] Epoch 00080 | Loss 3.4651
21:58:46.910005 [0] Epoch: 080, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:46.943649 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:47.000702 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:47.766505 [0] Epoch 00081 | Loss 3.4651
21:58:47.787547 [0] Epoch: 081, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:48.399784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.407028 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:48.792781 [0] Epoch 00082 | Loss 3.4651
21:58:48.813485 [0] Epoch: 082, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:48.847181 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:48.904292 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:49.669440 [0] Epoch 00083 | Loss 3.4651
21:58:49.690411 [0] Epoch: 083, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:50.301945 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.309285 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:50.693362 [0] Epoch 00084 | Loss 3.4651
21:58:50.714568 [0] Epoch: 084, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:50.748801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:50.805326 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:51.568726 [0] Epoch 00085 | Loss 3.4651
21:58:51.589891 [0] Epoch: 085, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:52.201063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.208275 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:52.592274 [0] Epoch 00086 | Loss 3.4651
21:58:52.613036 [0] Epoch: 086, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:52.646744 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:52.704014 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:53.468419 [0] Epoch 00087 | Loss 3.4651
21:58:53.489241 [0] Epoch: 087, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:54.101065 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.108260 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:54.492889 [0] Epoch 00088 | Loss 3.4651
21:58:54.513378 [0] Epoch: 088, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:54.547277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:54.604244 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:55.368607 [0] Epoch 00089 | Loss 3.4651
21:58:55.389786 [0] Epoch: 089, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:56.001718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.009042 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:56.393298 [0] Epoch 00090 | Loss 3.4651
21:58:56.413913 [0] Epoch: 090, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:56.447717 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:56.504853 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:57.269394 [0] Epoch 00091 | Loss 3.4651
21:58:57.290133 [0] Epoch: 091, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:57.903328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:57.909298 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:58.293202 [0] Epoch 00092 | Loss 3.4651
21:58:58.314022 [0] Epoch: 092, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:58.347976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:58.404912 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:58:59.169833 [0] Epoch 00093 | Loss 3.4651
21:58:59.190400 [0] Epoch: 093, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:58:59.802444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:58:59.809578 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:00.194169 [0] Epoch 00094 | Loss 3.4651
21:59:00.214689 [0] Epoch: 094, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:00.248508 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:00.306566 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:01.069469 [0] Epoch 00095 | Loss 3.4651
21:59:01.089974 [0] Epoch: 095, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:01.701852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:01.709113 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:02.102843 [0] Epoch 00096 | Loss 3.4651
21:59:02.124197 [0] Epoch: 096, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:02.160335 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:02.216665 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:02.991950 [0] Epoch 00097 | Loss 3.4651
21:59:03.013481 [0] Epoch: 097, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:03.627491 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:03.634749 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:04.018925 [0] Epoch 00098 | Loss 3.4651
21:59:04.040114 [0] Epoch: 098, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:04.073679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:04.130875 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:04.896156 [0] Epoch 00099 | Loss 3.4651
21:59:04.916978 [0] Epoch: 099, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:05.532394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:05.539496 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:05.924650 [0] Epoch 00100 | Loss 3.4651
21:59:05.945355 [0] Epoch: 100, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:05.979030 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:06.036202 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:06.800647 [0] Epoch 00101 | Loss 3.4651
21:59:06.821734 [0] Epoch: 101, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:07.436147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.443388 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:07.827912 [0] Epoch 00102 | Loss 3.4651
21:59:07.848624 [0] Epoch: 102, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:07.882437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:07.939529 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:08.705914 [0] Epoch 00103 | Loss 3.4651
21:59:08.726611 [0] Epoch: 103, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:09.341244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.348525 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:09.732660 [0] Epoch 00104 | Loss 3.4651
21:59:09.753701 [0] Epoch: 104, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:09.787401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:09.844390 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:10.608641 [0] Epoch 00105 | Loss 3.4651
21:59:10.629765 [0] Epoch: 105, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:11.243640 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.249674 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:11.634124 [0] Epoch 00106 | Loss 3.4651
21:59:11.655834 [0] Epoch: 106, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:11.689588 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:11.746451 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:12.510598 [0] Epoch 00107 | Loss 3.4651
21:59:12.532275 [0] Epoch: 107, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:13.144529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.152043 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:13.534816 [0] Epoch 00108 | Loss 3.4651
21:59:13.555892 [0] Epoch: 108, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:13.589596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:13.646399 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:14.410050 [0] Epoch 00109 | Loss 3.4651
21:59:14.431413 [0] Epoch: 109, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:15.043004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.050129 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:15.433800 [0] Epoch 00110 | Loss 3.4651
21:59:15.455060 [0] Epoch: 110, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:15.489297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:15.545838 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:16.310078 [0] Epoch 00111 | Loss 3.4651
21:59:16.331355 [0] Epoch: 111, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:16.943374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:16.950650 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:17.334228 [0] Epoch 00112 | Loss 3.4651
21:59:17.355372 [0] Epoch: 112, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:17.389040 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:17.446064 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:18.210247 [0] Epoch 00113 | Loss 3.4651
21:59:18.231225 [0] Epoch: 113, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:18.843439 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:18.849943 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:19.232177 [0] Epoch 00114 | Loss 3.4651
21:59:19.253776 [0] Epoch: 114, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:19.287524 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:19.344785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:20.107252 [0] Epoch 00115 | Loss 3.4651
21:59:20.128041 [0] Epoch: 115, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:20.739537 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:20.746635 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:21.128846 [0] Epoch 00116 | Loss 3.4651
21:59:21.149968 [0] Epoch: 116, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:21.183737 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:21.240719 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:22.003829 [0] Epoch 00117 | Loss 3.4651
21:59:22.025290 [0] Epoch: 117, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:22.637174 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:22.644506 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:23.027892 [0] Epoch 00118 | Loss 3.4651
21:59:23.048658 [0] Epoch: 118, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:23.082428 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:23.139507 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:23.902798 [0] Epoch 00119 | Loss 3.4651
21:59:23.923189 [0] Epoch: 119, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:24.534847 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:24.542080 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:24.924614 [0] Epoch 00120 | Loss 3.4651
21:59:24.945467 [0] Epoch: 120, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:24.979293 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:25.036304 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:25.798615 [0] Epoch 00121 | Loss 3.4651
21:59:25.819744 [0] Epoch: 121, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:26.431421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.438438 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:26.820499 [0] Epoch 00122 | Loss 3.4651
21:59:26.842112 [0] Epoch: 122, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:26.875809 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:26.933265 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:27.696544 [0] Epoch 00123 | Loss 3.4651
21:59:27.717106 [0] Epoch: 123, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:28.329213 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.336362 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:28.720340 [0] Epoch 00124 | Loss 3.4651
21:59:28.740891 [0] Epoch: 124, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:28.774733 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:28.831836 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:29.594661 [0] Epoch 00125 | Loss 3.4651
21:59:29.616388 [0] Epoch: 125, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:30.228420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.235836 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:30.620259 [0] Epoch 00126 | Loss 3.4651
21:59:30.640797 [0] Epoch: 126, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:30.674686 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:30.731576 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:31.495393 [0] Epoch 00127 | Loss 3.4651
21:59:31.516441 [0] Epoch: 127, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:32.129358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.135507 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:32.519440 [0] Epoch 00128 | Loss 3.4651
21:59:32.540345 [0] Epoch: 128, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:32.574922 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:32.631185 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:33.395247 [0] Epoch 00129 | Loss 3.4651
21:59:33.416194 [0] Epoch: 129, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:34.028273 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.035486 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:34.419300 [0] Epoch 00130 | Loss 3.4651
21:59:34.439932 [0] Epoch: 130, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:34.473550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:34.530798 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:35.295218 [0] Epoch 00131 | Loss 3.4651
21:59:35.316887 [0] Epoch: 131, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:35.931553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:35.939097 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:36.323192 [0] Epoch 00132 | Loss 3.4651
21:59:36.343951 [0] Epoch: 132, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:36.377560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:36.434623 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:37.200821 [0] Epoch 00133 | Loss 3.4651
21:59:37.221514 [0] Epoch: 133, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:37.836233 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:37.843448 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:38.226734 [0] Epoch 00134 | Loss 3.4651
21:59:38.247812 [0] Epoch: 134, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:38.281427 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:38.338316 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:39.103279 [0] Epoch 00135 | Loss 3.4651
21:59:39.124217 [0] Epoch: 135, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:39.737108 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:39.743715 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:40.127996 [0] Epoch 00136 | Loss 3.4651
21:59:40.148943 [0] Epoch: 136, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:40.182550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:40.239454 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:41.004218 [0] Epoch 00137 | Loss 3.4651
21:59:41.024914 [0] Epoch: 137, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:41.636769 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:41.644023 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:42.027544 [0] Epoch 00138 | Loss 3.4651
21:59:42.048711 [0] Epoch: 138, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:42.082285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:42.139209 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:42.904710 [0] Epoch 00139 | Loss 3.4651
21:59:42.925477 [0] Epoch: 139, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:43.537182 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:43.544276 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:43.928120 [0] Epoch 00140 | Loss 3.4651
21:59:43.949060 [0] Epoch: 140, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:43.982589 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:44.039551 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:44.804166 [0] Epoch 00141 | Loss 3.4651
21:59:44.825935 [0] Epoch: 141, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:45.437963 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.446127 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:45.829027 [0] Epoch 00142 | Loss 3.4651
21:59:45.850125 [0] Epoch: 142, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:45.883700 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:45.941666 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:46.705457 [0] Epoch 00143 | Loss 3.4651
21:59:46.726741 [0] Epoch: 143, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:47.339097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.346286 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:47.729342 [0] Epoch 00144 | Loss 3.4651
21:59:47.750370 [0] Epoch: 144, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:47.783970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:47.841109 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:48.605616 [0] Epoch 00145 | Loss 3.4651
21:59:48.626509 [0] Epoch: 145, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:49.238244 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.245349 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:49.628747 [0] Epoch 00146 | Loss 3.4651
21:59:49.649262 [0] Epoch: 146, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:49.682889 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:49.741234 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:50.503692 [0] Epoch 00147 | Loss 3.4651
21:59:50.524292 [0] Epoch: 147, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:51.136168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.143469 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:51.526234 [0] Epoch 00148 | Loss 3.4651
21:59:51.547182 [0] Epoch: 148, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:51.580725 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:51.637713 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:52.400148 [0] Epoch 00149 | Loss 3.4651
21:59:52.421700 [0] Epoch: 149, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:53.034398 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.041161 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:53.423682 [0] Epoch 00150 | Loss 3.4651
21:59:53.444434 [0] Epoch: 150, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:53.478038 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:53.534876 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:54.298331 [0] Epoch 00151 | Loss 3.4651
21:59:54.318834 [0] Epoch: 151, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:54.930500 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:54.937706 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:55.320028 [0] Epoch 00152 | Loss 3.4651
21:59:55.340669 [0] Epoch: 152, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:55.374287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:55.431362 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:56.195080 [0] Epoch 00153 | Loss 3.4651
21:59:56.215561 [0] Epoch: 153, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:56.827896 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:56.835176 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:57.218272 [0] Epoch 00154 | Loss 3.4651
21:59:57.238813 [0] Epoch: 154, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:57.272441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:57.329501 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:58.092601 [0] Epoch 00155 | Loss 3.4651
21:59:58.113931 [0] Epoch: 155, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:58.726063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:58.733553 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:59.116070 [0] Epoch 00156 | Loss 3.4651
21:59:59.136722 [0] Epoch: 156, Train: 0.0312, Val: 0.0305, Test: 0.0314
21:59:59.170383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:59:59.227610 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
21:59:59.990313 [0] Epoch 00157 | Loss 3.4651
22:00:00.011647 [0] Epoch: 157, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:00.624056 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:00.630633 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:01.013124 [0] Epoch 00158 | Loss 3.4651
22:00:01.034183 [0] Epoch: 158, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:01.067899 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:01.125047 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:01.899938 [0] Epoch 00159 | Loss 3.4651
22:00:01.922117 [0] Epoch: 159, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:02.545124 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:02.552152 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:02.940015 [0] Epoch 00160 | Loss 3.4651
22:00:02.960974 [0] Epoch: 160, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:02.994702 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:03.051803 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:03.814069 [0] Epoch 00161 | Loss 3.4651
22:00:03.835145 [0] Epoch: 161, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:04.446846 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.454010 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:04.836128 [0] Epoch 00162 | Loss 3.4651
22:00:04.857320 [0] Epoch: 162, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:04.890985 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:04.947934 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:05.709762 [0] Epoch 00163 | Loss 3.4651
22:00:05.730543 [0] Epoch: 163, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:06.341293 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.348447 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:06.731852 [0] Epoch 00164 | Loss 3.4651
22:00:06.752279 [0] Epoch: 164, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:06.785983 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:06.843041 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:07.607541 [0] Epoch 00165 | Loss 3.4651
22:00:07.627948 [0] Epoch: 165, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:08.240240 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.246295 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:08.630460 [0] Epoch 00166 | Loss 3.4651
22:00:08.651644 [0] Epoch: 166, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:08.686497 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:08.743027 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:09.505661 [0] Epoch 00167 | Loss 3.4651
22:00:09.526597 [0] Epoch: 167, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:10.137204 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.144804 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:10.527316 [0] Epoch 00168 | Loss 3.4651
22:00:10.547610 [0] Epoch: 168, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:10.581370 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:10.638437 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:11.401398 [0] Epoch 00169 | Loss 3.4651
22:00:11.421947 [0] Epoch: 169, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:12.032268 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.039614 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:12.421820 [0] Epoch 00170 | Loss 3.4651
22:00:12.442401 [0] Epoch: 170, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:12.475988 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:12.533104 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:13.297273 [0] Epoch 00171 | Loss 3.4651
22:00:13.317995 [0] Epoch: 171, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:13.929784 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:13.937011 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:14.320479 [0] Epoch 00172 | Loss 3.4651
22:00:14.341769 [0] Epoch: 172, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:14.375131 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:14.432719 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:15.195062 [0] Epoch 00173 | Loss 3.4651
22:00:15.215956 [0] Epoch: 173, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:15.826982 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:15.834180 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:16.215950 [0] Epoch 00174 | Loss 3.4651
22:00:16.237064 [0] Epoch: 174, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:16.270314 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:16.327544 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:17.090682 [0] Epoch 00175 | Loss 3.4651
22:00:17.111162 [0] Epoch: 175, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:17.721802 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:17.729114 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:18.112212 [0] Epoch 00176 | Loss 3.4651
22:00:18.132825 [0] Epoch: 176, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:18.166244 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:18.223494 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:18.986658 [0] Epoch 00177 | Loss 3.4651
22:00:19.007067 [0] Epoch: 177, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:19.618000 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:19.625414 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:20.008329 [0] Epoch 00178 | Loss 3.4651
22:00:20.028774 [0] Epoch: 178, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:20.062197 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:20.119602 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:20.882636 [0] Epoch 00179 | Loss 3.4651
22:00:20.903042 [0] Epoch: 179, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:21.513816 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:21.521219 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:21.903456 [0] Epoch 00180 | Loss 3.4651
22:00:21.923834 [0] Epoch: 180, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:21.957216 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:22.014560 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:22.777636 [0] Epoch 00181 | Loss 3.4651
22:00:22.798157 [0] Epoch: 181, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:23.408916 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.415740 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:23.798457 [0] Epoch 00182 | Loss 3.4651
22:00:23.819095 [0] Epoch: 182, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:23.852546 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:23.910265 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:24.673937 [0] Epoch 00183 | Loss 3.4651
22:00:24.695776 [0] Epoch: 183, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:25.306779 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.314362 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:25.698089 [0] Epoch 00184 | Loss 3.4651
22:00:25.718595 [0] Epoch: 184, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:25.751963 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:25.809449 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:26.573630 [0] Epoch 00185 | Loss 3.4651
22:00:26.594199 [0] Epoch: 185, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:27.205071 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.213063 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:27.594387 [0] Epoch 00186 | Loss 3.4651
22:00:27.617641 [0] Epoch: 186, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:27.651011 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:27.709053 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:28.474595 [0] Epoch 00187 | Loss 3.4651
22:00:28.495176 [0] Epoch: 187, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:29.105566 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.112772 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:29.494942 [0] Epoch 00188 | Loss 3.4651
22:00:29.515503 [0] Epoch: 188, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:29.548785 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:29.606160 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:30.369703 [0] Epoch 00189 | Loss 3.4651
22:00:30.390424 [0] Epoch: 189, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:31.001226 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.009479 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:31.391434 [0] Epoch 00190 | Loss 3.4651
22:00:31.412666 [0] Epoch: 190, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:31.446263 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:31.503537 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:32.267577 [0] Epoch 00191 | Loss 3.4651
22:00:32.288065 [0] Epoch: 191, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:32.899419 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:32.906849 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:33.290407 [0] Epoch 00192 | Loss 3.4651
22:00:33.311282 [0] Epoch: 192, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:33.344776 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:33.402203 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:34.165328 [0] Epoch 00193 | Loss 3.4651
22:00:34.185844 [0] Epoch: 193, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:34.796871 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:34.805041 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:35.186332 [0] Epoch 00194 | Loss 3.4651
22:00:35.207339 [0] Epoch: 194, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:35.240641 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:35.298166 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:36.061926 [0] Epoch 00195 | Loss 3.4651
22:00:36.082661 [0] Epoch: 195, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:36.693574 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:36.701168 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:37.084412 [0] Epoch 00196 | Loss 3.4651
22:00:37.104921 [0] Epoch: 196, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:37.138868 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:37.195688 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:37.959751 [0] Epoch 00197 | Loss 3.4651
22:00:37.980115 [0] Epoch: 197, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:38.591201 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:38.598734 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:38.981230 [0] Epoch 00198 | Loss 3.4651
22:00:39.001988 [0] Epoch: 198, Train: 0.0312, Val: 0.0305, Test: 0.0314
22:00:39.035375 [3] Warning: no training nodes in this partition! Backward fake loss.
22:00:39.093326 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031438
22:00:39.855320 [0] Epoch 00199 | Loss 3.4651
22:00:39.876662 [0] Epoch: 199, Train: 0.0312, Val: 0.0305, Test: 0.0314
0.031438
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f512_l32_t0.5 nprocs 4
22:00:39.880571 [0] 
timer summary:
 18.16s  15.22s     1 broadcast ForwardL1 0
148.48s  69.34s  3200 broadcast
 47.06s  54.76s  3200 spmm
  0.15s   0.05s     1 broadcast ForwardL1 1
  0.13s   0.04s     1 broadcast ForwardL1 2
  0.12s   0.03s     1 broadcast ForwardL1 3
  4.72s   0.04s   800 mm
 28.85s  16.75s   125 broadcast ForwardL2 0
  8.65s   2.68s   125 broadcast ForwardL2 1
  8.22s   2.48s   125 broadcast ForwardL2 2
  7.69s   2.24s   125 broadcast ForwardL2 3
 26.18s  17.96s   200 broadcast BackwardL2 0
  1.43s   0.42s   200 broadcast BackwardL2 1
  1.60s   0.46s   200 broadcast BackwardL2 2
  1.54s   0.47s   200 broadcast BackwardL2 3
  2.48s   1.71s   400 all_reduce
  7.26s   0.24s   200 broadcast BackwardL1 0
 13.56s   4.29s   200 broadcast BackwardL1 1
 12.85s   3.97s   200 broadcast BackwardL1 2
 12.00s   3.57s   200 broadcast BackwardL1 3
213.19s  15.22s   200 epoch
257.69s   0.03s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
