no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:52:08.164980 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:52:08.213131 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:52:08.227598 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:52:08.231746 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:52:12.509550 [3] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 3, |V|: 500000, |E|: 2349774>
21:52:12.517223 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:52:50.554216 [1] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 1, |V|: 500000, |E|: 31008983>
21:52:50.562323 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:52:54.946496 [2] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 2, |V|: 500000, |E|: 15637930>
21:52:54.954525 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:53:00.893339 [0] graph loaded <COO Graph: e160M_f512_l32_t0.1, |V|: 2000000, |E|: 160000000, masks: 200000,200000,1600000><Local: 0, |V|: 500000, |E|: 113001879>
21:53:00.904797 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1867 MiB |   1882 MiB |   1914 MiB |  48832 KiB |
|       from large pool |   1867 MiB |   1882 MiB |   1914 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1900 MiB |   1900 MiB |   1900 MiB |      0 B   |
|       from large pool |   1898 MiB |   1898 MiB |   1898 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12184 KiB |  19998 KiB |  46627 KiB |  34442 KiB |
|       from large pool |  12184 KiB |  19998 KiB |  40478 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:53:03.437189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.445534 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:03.461581 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:04.774383 [0] Epoch 00000 | Loss 3.4687
21:53:04.818720 [0] Epoch: 000, Train: 0.0313, Val: 0.0310, Test: 0.0313
21:53:05.431506 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.438603 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:05.438619 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031280
21:53:05.812291 [0] Epoch 00001 | Loss 29698.4902
21:53:05.833953 [0] Epoch: 001, Train: 0.0313, Val: 0.0313, Test: 0.0313
21:53:06.446437 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.454016 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:06.454462 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031271
21:53:06.827318 [0] Epoch 00002 | Loss 11757.4746
21:53:06.847996 [0] Epoch: 002, Train: 0.0311, Val: 0.0314, Test: 0.0312
21:53:07.463450 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.470635 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:07.470783 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031246
21:53:07.845435 [0] Epoch 00003 | Loss 9051.0059
21:53:07.867140 [0] Epoch: 003, Train: 0.0317, Val: 0.0312, Test: 0.0313
21:53:08.483642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.490609 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:08.490694 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031316
21:53:08.865134 [0] Epoch 00004 | Loss 8354.0977
21:53:08.885981 [0] Epoch: 004, Train: 0.0309, Val: 0.0309, Test: 0.0314
21:53:09.500879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.507972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:09.508222 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031352
21:53:09.882664 [0] Epoch 00005 | Loss 8106.3496
21:53:09.903772 [0] Epoch: 005, Train: 0.0318, Val: 0.0303, Test: 0.0314
21:53:10.520133 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.526603 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:10.527050 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031395
21:53:10.901714 [0] Epoch 00006 | Loss 7351.2622
21:53:10.923482 [0] Epoch: 006, Train: 0.0308, Val: 0.0320, Test: 0.0310
21:53:11.539732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:11.546328 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:11.546372 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030992
21:53:11.920718 [0] Epoch 00007 | Loss 5163.9819
21:53:11.942463 [0] Epoch: 007, Train: 0.0311, Val: 0.0312, Test: 0.0313
21:53:12.558735 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:12.565490 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:12.565640 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031267
21:53:12.940844 [0] Epoch 00008 | Loss 2972.4634
21:53:12.962661 [0] Epoch: 008, Train: 0.0321, Val: 0.0317, Test: 0.0312
21:53:13.579881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:13.585919 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:13.586602 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031189
21:53:13.961639 [0] Epoch 00009 | Loss 1432.3356
21:53:13.982796 [0] Epoch: 009, Train: 0.0307, Val: 0.0299, Test: 0.0310
21:53:14.599544 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:14.607883 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:14.607915 [1] Warning: no training nodes in this partition! Backward fake loss.
0.030991
21:53:14.982657 [0] Epoch 00010 | Loss 336.0441
21:53:15.003860 [0] Epoch: 010, Train: 0.0312, Val: 0.0317, Test: 0.0312
21:53:15.619384 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:15.626211 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:15.626524 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031223
21:53:16.000204 [0] Epoch 00011 | Loss 3.4656
21:53:16.021150 [0] Epoch: 011, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:16.634438 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:16.641442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:16.641787 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:17.014654 [0] Epoch 00012 | Loss 3.4656
21:53:17.036032 [0] Epoch: 012, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:17.648909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:17.656474 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:17.656719 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:18.028408 [0] Epoch 00013 | Loss 3.4656
21:53:18.049141 [0] Epoch: 013, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:18.662799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:18.669923 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:18.670773 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:19.043520 [0] Epoch 00014 | Loss 3.4656
21:53:19.064556 [0] Epoch: 014, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:19.677798 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:19.684966 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:19.685021 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:20.057607 [0] Epoch 00015 | Loss 3.4656
21:53:20.079614 [0] Epoch: 015, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:20.693278 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:20.700785 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:20.700840 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:21.073666 [0] Epoch 00016 | Loss 3.4656
21:53:21.094515 [0] Epoch: 016, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:21.707708 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:21.714812 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:21.715404 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:22.088024 [0] Epoch 00017 | Loss 3.4656
21:53:22.108955 [0] Epoch: 017, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:22.722207 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:22.729232 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:22.729609 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:23.102055 [0] Epoch 00018 | Loss 3.4656
21:53:23.124051 [0] Epoch: 018, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:23.737724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:23.745155 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:23.745193 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:24.118633 [0] Epoch 00019 | Loss 3.4656
21:53:24.139433 [0] Epoch: 019, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:24.752533 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:24.759868 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:24.759891 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:25.133575 [0] Epoch 00020 | Loss 3.4656
21:53:25.154372 [0] Epoch: 020, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:25.767257 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:25.773860 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:25.774251 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:26.146627 [0] Epoch 00021 | Loss 3.4656
21:53:26.167471 [0] Epoch: 021, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:26.779516 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:26.786697 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:26.787071 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:27.159275 [0] Epoch 00022 | Loss 3.4656
21:53:27.179767 [0] Epoch: 022, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:27.791523 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:27.798698 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:27.798838 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:28.172164 [0] Epoch 00023 | Loss 3.4656
21:53:28.192950 [0] Epoch: 023, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:28.805221 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:28.812294 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:28.812302 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:29.186407 [0] Epoch 00024 | Loss 3.4656
21:53:29.207126 [0] Epoch: 024, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:29.820214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:29.826542 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:29.826577 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:30.199716 [0] Epoch 00025 | Loss 3.4656
21:53:30.220245 [0] Epoch: 025, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:30.832743 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:30.839937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:30.840283 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:31.212652 [0] Epoch 00026 | Loss 3.4656
21:53:31.233433 [0] Epoch: 026, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:31.845978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:31.852824 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:31.852961 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:32.225650 [0] Epoch 00027 | Loss 3.4656
21:53:32.246300 [0] Epoch: 027, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:32.858761 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:32.865939 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:32.866163 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:33.239463 [0] Epoch 00028 | Loss 3.4656
21:53:33.260050 [0] Epoch: 028, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:33.871886 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:33.878924 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:33.879027 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:34.251400 [0] Epoch 00029 | Loss 3.4656
21:53:34.272028 [0] Epoch: 029, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:34.884312 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:34.891528 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:34.891682 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:35.265164 [0] Epoch 00030 | Loss 3.4656
21:53:35.286199 [0] Epoch: 030, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:35.898976 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:35.905883 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:35.906353 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:36.278977 [0] Epoch 00031 | Loss 3.4656
21:53:36.299564 [0] Epoch: 031, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:36.911902 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:36.919269 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:36.919434 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:37.291905 [0] Epoch 00032 | Loss 3.4656
21:53:37.312457 [0] Epoch: 032, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:37.924200 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:37.931399 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:37.931602 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:38.304666 [0] Epoch 00033 | Loss 3.4656
21:53:38.325200 [0] Epoch: 033, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:38.937360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:38.944673 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:38.944808 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:39.318177 [0] Epoch 00034 | Loss 3.4656
21:53:39.338653 [0] Epoch: 034, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:39.950187 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:39.957388 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:39.957425 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:40.330604 [0] Epoch 00035 | Loss 3.4656
21:53:40.351677 [0] Epoch: 035, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:40.964187 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:40.971323 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:40.971613 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:41.344190 [0] Epoch 00036 | Loss 3.4656
21:53:41.364880 [0] Epoch: 036, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:41.977028 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:41.984005 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:41.984029 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:42.355840 [0] Epoch 00037 | Loss 3.4656
21:53:42.376420 [0] Epoch: 037, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:42.988736 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:42.995972 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:42.996031 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:43.369513 [0] Epoch 00038 | Loss 3.4656
21:53:43.390134 [0] Epoch: 038, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:44.002014 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:44.008819 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:44.009173 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:44.380779 [0] Epoch 00039 | Loss 3.4656
21:53:44.401453 [0] Epoch: 039, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:45.013571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:45.020854 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:45.020908 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:45.393456 [0] Epoch 00040 | Loss 3.4656
21:53:45.413871 [0] Epoch: 040, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:46.025212 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:46.032442 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:46.032565 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:46.404642 [0] Epoch 00041 | Loss 3.4656
21:53:46.425285 [0] Epoch: 041, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:47.037575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:47.044851 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:47.044924 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:47.417071 [0] Epoch 00042 | Loss 3.4656
21:53:47.438064 [0] Epoch: 042, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:48.050148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:48.057514 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:48.057709 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:48.430749 [0] Epoch 00043 | Loss 3.4656
21:53:48.451830 [0] Epoch: 043, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:49.064266 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:49.071173 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:49.071311 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:49.444068 [0] Epoch 00044 | Loss 3.4656
21:53:49.464820 [0] Epoch: 044, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:50.076243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:50.083370 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:50.083444 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:50.455865 [0] Epoch 00045 | Loss 3.4656
21:53:50.476383 [0] Epoch: 045, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:51.088185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:51.095464 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:51.095725 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:51.468607 [0] Epoch 00046 | Loss 3.4656
21:53:51.488963 [0] Epoch: 046, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:52.100622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:52.107760 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:52.107912 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:52.481796 [0] Epoch 00047 | Loss 3.4656
21:53:52.503339 [0] Epoch: 047, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:53.115756 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:53.122990 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:53.123051 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:53.496478 [0] Epoch 00048 | Loss 3.4656
21:53:53.516980 [0] Epoch: 048, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:54.128209 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:54.135385 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:54.135515 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:54.507698 [0] Epoch 00049 | Loss 3.4656
21:53:54.528118 [0] Epoch: 049, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:55.140426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.147549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.147930 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:55.521647 [0] Epoch 00050 | Loss 3.4656
21:53:55.542510 [0] Epoch: 050, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:55.576025 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.632652 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:55.698603 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:56.387706 [0] Epoch 00051 | Loss 3.4656
21:53:56.409031 [0] Epoch: 051, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:57.021189 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.028184 [1] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.028448 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:57.401843 [0] Epoch 00052 | Loss 3.4656
21:53:57.422446 [0] Epoch: 052, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:57.455642 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.512629 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:57.578435 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:58.267368 [0] Epoch 00053 | Loss 3.4656
21:53:58.287769 [0] Epoch: 053, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:58.899105 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:58.906322 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:58.906347 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:53:59.279149 [0] Epoch 00054 | Loss 3.4656
21:53:59.299714 [0] Epoch: 054, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:53:59.332877 [3] Warning: no training nodes in this partition! Backward fake loss.
21:53:59.390193 [2] Warning: no training nodes in this partition! Backward fake loss.
21:53:59.455769 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:00.145685 [0] Epoch 00055 | Loss 3.4656
21:54:00.166461 [0] Epoch: 055, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:00.778031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:00.785163 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:00.785326 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:01.157982 [0] Epoch 00056 | Loss 3.4656
21:54:01.179277 [0] Epoch: 056, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:01.211768 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:01.269012 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:01.334635 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:02.034726 [0] Epoch 00057 | Loss 3.4656
21:54:02.056108 [0] Epoch: 057, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:02.678393 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:02.686174 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:02.686278 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:03.069150 [0] Epoch 00058 | Loss 3.4656
21:54:03.090596 [0] Epoch: 058, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:03.125260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:03.182169 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:03.248124 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:03.937856 [0] Epoch 00059 | Loss 3.4656
21:54:03.958701 [0] Epoch: 059, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:04.572079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:04.579409 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:04.579461 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:04.952236 [0] Epoch 00060 | Loss 3.4656
21:54:04.973681 [0] Epoch: 060, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:05.006217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:05.063642 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:05.130258 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:05.819724 [0] Epoch 00061 | Loss 3.4656
21:54:05.840555 [0] Epoch: 061, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:06.454193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.461442 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.461481 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:06.834445 [0] Epoch 00062 | Loss 3.4656
21:54:06.855432 [0] Epoch: 062, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:06.888802 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:06.945950 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:07.011737 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:07.700810 [0] Epoch 00063 | Loss 3.4656
21:54:07.722238 [0] Epoch: 063, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:08.334868 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.342579 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.342726 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:08.716047 [0] Epoch 00064 | Loss 3.4656
21:54:08.736852 [0] Epoch: 064, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:08.769993 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.827302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:08.894111 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:09.583118 [0] Epoch 00065 | Loss 3.4656
21:54:09.603751 [0] Epoch: 065, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:10.217259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.224536 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.224569 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:10.598751 [0] Epoch 00066 | Loss 3.4656
21:54:10.620376 [0] Epoch: 066, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:10.652731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.710061 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:10.776083 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:11.466112 [0] Epoch 00067 | Loss 3.4656
21:54:11.487005 [0] Epoch: 067, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:12.100719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.107628 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.107871 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:12.481104 [0] Epoch 00068 | Loss 3.4656
21:54:12.501850 [0] Epoch: 068, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:12.535288 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.592500 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:12.658383 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:13.347347 [0] Epoch 00069 | Loss 3.4656
21:54:13.368057 [0] Epoch: 069, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:13.981095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:13.988447 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:13.988479 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:14.361797 [0] Epoch 00070 | Loss 3.4656
21:54:14.382603 [0] Epoch: 070, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:14.415806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:14.473233 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:14.539222 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:15.227772 [0] Epoch 00071 | Loss 3.4656
21:54:15.248554 [0] Epoch: 071, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:15.860912 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:15.868349 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:15.868499 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:16.240696 [0] Epoch 00072 | Loss 3.4656
21:54:16.261093 [0] Epoch: 072, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:16.294406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:16.351802 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:16.417964 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:17.106384 [0] Epoch 00073 | Loss 3.4656
21:54:17.126904 [0] Epoch: 073, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:17.738328 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:17.745571 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:17.745664 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:18.117906 [0] Epoch 00074 | Loss 3.4656
21:54:18.138497 [0] Epoch: 074, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:18.171784 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:18.229094 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:18.295038 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:18.984188 [0] Epoch 00075 | Loss 3.4656
21:54:19.004694 [0] Epoch: 075, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:19.616678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:19.624009 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:19.624086 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:19.997112 [0] Epoch 00076 | Loss 3.4656
21:54:20.017676 [0] Epoch: 076, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:20.050880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:20.108363 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:20.174200 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:20.862831 [0] Epoch 00077 | Loss 3.4656
21:54:20.884081 [0] Epoch: 077, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:21.495709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.503039 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.503620 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:21.875303 [0] Epoch 00078 | Loss 3.4656
21:54:21.895863 [0] Epoch: 078, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:21.929119 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:21.986441 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:22.052870 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:22.740948 [0] Epoch 00079 | Loss 3.4656
21:54:22.761386 [0] Epoch: 079, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:23.373068 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.380353 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.380368 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:23.753796 [0] Epoch 00080 | Loss 3.4656
21:54:23.774507 [0] Epoch: 080, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:23.807644 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.864993 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:23.931333 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:24.620331 [0] Epoch 00081 | Loss 3.4656
21:54:24.640754 [0] Epoch: 081, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:25.252821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.259917 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.260080 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:25.633698 [0] Epoch 00082 | Loss 3.4656
21:54:25.654344 [0] Epoch: 082, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:25.687550 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.744931 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:25.810741 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:26.500072 [0] Epoch 00083 | Loss 3.4656
21:54:26.520774 [0] Epoch: 083, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:27.132501 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.139501 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.139692 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:27.512656 [0] Epoch 00084 | Loss 3.4656
21:54:27.533238 [0] Epoch: 084, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:27.566462 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.623698 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:27.690079 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:28.379959 [0] Epoch 00085 | Loss 3.4656
21:54:28.400506 [0] Epoch: 085, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:29.012570 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.019778 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.019841 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:29.393359 [0] Epoch 00086 | Loss 3.4656
21:54:29.413911 [0] Epoch: 086, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:29.447140 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.504525 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:29.570360 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:30.257896 [0] Epoch 00087 | Loss 3.4656
21:54:30.278530 [0] Epoch: 087, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:30.890672 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:30.897783 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:30.897953 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:31.269454 [0] Epoch 00088 | Loss 3.4656
21:54:31.291002 [0] Epoch: 088, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:31.323611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:31.380936 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:31.447037 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:32.135876 [0] Epoch 00089 | Loss 3.4656
21:54:32.156282 [0] Epoch: 089, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:32.767786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:32.775121 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:32.775239 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:33.147398 [0] Epoch 00090 | Loss 3.4656
21:54:33.168042 [0] Epoch: 090, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:33.201320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:33.258549 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:33.324650 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:34.011494 [0] Epoch 00091 | Loss 3.4656
21:54:34.032468 [0] Epoch: 091, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:34.644143 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:34.651426 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:34.651457 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:35.022363 [0] Epoch 00092 | Loss 3.4656
21:54:35.042822 [0] Epoch: 092, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:35.076195 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:35.133526 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:35.199414 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:35.886221 [0] Epoch 00093 | Loss 3.4656
21:54:35.907277 [0] Epoch: 093, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:36.519220 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.526463 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:36.527048 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:36.898592 [0] Epoch 00094 | Loss 3.4656
21:54:36.918991 [0] Epoch: 094, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:36.952441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:37.009836 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:37.075537 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:37.763077 [0] Epoch 00095 | Loss 3.4656
21:54:37.783531 [0] Epoch: 095, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:38.395962 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.403232 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.403306 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:38.775025 [0] Epoch 00096 | Loss 3.4656
21:54:38.795993 [0] Epoch: 096, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:38.828653 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.886102 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:38.952114 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:39.639278 [0] Epoch 00097 | Loss 3.4656
21:54:39.661166 [0] Epoch: 097, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:40.273418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.280390 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.280412 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:40.652435 [0] Epoch 00098 | Loss 3.4656
21:54:40.672800 [0] Epoch: 098, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:40.706287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.763566 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:40.829475 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:41.515918 [0] Epoch 00099 | Loss 3.4656
21:54:41.536649 [0] Epoch: 099, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:42.148173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.155467 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.155932 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:42.526793 [0] Epoch 00100 | Loss 3.4656
21:54:42.547851 [0] Epoch: 100, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:42.581055 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.638424 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:42.704332 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:43.391636 [0] Epoch 00101 | Loss 3.4656
21:54:43.412368 [0] Epoch: 101, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:44.024241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.031320 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.031402 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:44.402235 [0] Epoch 00102 | Loss 3.4656
21:54:44.422873 [0] Epoch: 102, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:44.456217 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.513403 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:44.579328 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:45.266308 [0] Epoch 00103 | Loss 3.4656
21:54:45.287573 [0] Epoch: 103, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:45.899353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:45.906641 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:45.906697 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:46.277946 [0] Epoch 00104 | Loss 3.4656
21:54:46.298820 [0] Epoch: 104, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:46.332231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:46.389577 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:46.455309 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:47.143869 [0] Epoch 00105 | Loss 3.4656
21:54:47.164427 [0] Epoch: 105, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:47.776148 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:47.783487 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:47.783515 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:48.155742 [0] Epoch 00106 | Loss 3.4656
21:54:48.176183 [0] Epoch: 106, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:48.209471 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:48.266960 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:48.332847 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:49.020484 [0] Epoch 00107 | Loss 3.4656
21:54:49.040966 [0] Epoch: 107, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:49.653269 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:49.660521 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:49.660670 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:50.031661 [0] Epoch 00108 | Loss 3.4656
21:54:50.052327 [0] Epoch: 108, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:50.085726 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:50.143105 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:50.209229 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:50.896481 [0] Epoch 00109 | Loss 3.4656
21:54:50.917389 [0] Epoch: 109, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:51.528817 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.535983 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:51.535997 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:51.909290 [0] Epoch 00110 | Loss 3.4656
21:54:51.929707 [0] Epoch: 110, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:51.963413 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:52.020503 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:52.086478 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:52.774112 [0] Epoch 00111 | Loss 3.4656
21:54:52.794966 [0] Epoch: 111, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:53.407903 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.415107 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.415189 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:53.787692 [0] Epoch 00112 | Loss 3.4656
21:54:53.808158 [0] Epoch: 112, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:53.841940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.899951 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:53.965464 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:54.653332 [0] Epoch 00113 | Loss 3.4656
21:54:54.674441 [0] Epoch: 113, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:55.287149 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.294153 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.294289 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:55.667689 [0] Epoch 00114 | Loss 3.4656
21:54:55.688152 [0] Epoch: 114, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:55.721874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.778891 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:55.844791 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:56.532544 [0] Epoch 00115 | Loss 3.4656
21:54:56.554335 [0] Epoch: 115, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:57.167268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.174141 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.174191 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:57.546385 [0] Epoch 00116 | Loss 3.4656
21:54:57.567081 [0] Epoch: 116, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:57.600740 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.657987 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:57.723643 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:58.412441 [0] Epoch 00117 | Loss 3.4656
21:54:58.432851 [0] Epoch: 117, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:59.045173 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.052340 [1] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.052481 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:54:59.425750 [0] Epoch 00118 | Loss 3.4656
21:54:59.446192 [0] Epoch: 118, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:54:59.479475 [3] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.537001 [2] Warning: no training nodes in this partition! Backward fake loss.
21:54:59.602698 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:00.291690 [0] Epoch 00119 | Loss 3.4656
21:55:00.312164 [0] Epoch: 119, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:00.923888 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:00.931148 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:00.931241 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:01.305109 [0] Epoch 00120 | Loss 3.4656
21:55:01.326478 [0] Epoch: 120, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:01.360301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:01.417587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:01.484534 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:02.183260 [0] Epoch 00121 | Loss 3.4656
21:55:02.204518 [0] Epoch: 121, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:02.822571 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:02.829806 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:02.829902 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:03.204645 [0] Epoch 00122 | Loss 3.4656
21:55:03.225331 [0] Epoch: 122, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:03.258925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:03.315967 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:03.381896 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:04.070939 [0] Epoch 00123 | Loss 3.4656
21:55:04.091989 [0] Epoch: 123, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:04.707932 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:04.715166 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:04.715319 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:05.088457 [0] Epoch 00124 | Loss 3.4656
21:55:05.109136 [0] Epoch: 124, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:05.142871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:05.199940 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:05.265766 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:05.954487 [0] Epoch 00125 | Loss 3.4656
21:55:05.975262 [0] Epoch: 125, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:06.591081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:06.598358 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:06.598361 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:06.971745 [0] Epoch 00126 | Loss 3.4656
21:55:06.992488 [0] Epoch: 126, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:07.026123 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:07.083032 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:07.149218 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:07.838058 [0] Epoch 00127 | Loss 3.4656
21:55:07.858920 [0] Epoch: 127, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:08.475037 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.482097 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.482107 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:08.858490 [0] Epoch 00128 | Loss 3.4656
21:55:08.879553 [0] Epoch: 128, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:08.912981 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:08.969998 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:09.036171 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:09.725527 [0] Epoch 00129 | Loss 3.4656
21:55:09.746191 [0] Epoch: 129, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:10.362263 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.369527 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.369550 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:10.742560 [0] Epoch 00130 | Loss 3.4656
21:55:10.763224 [0] Epoch: 130, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:10.796806 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.853934 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:10.919993 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:11.607603 [0] Epoch 00131 | Loss 3.4656
21:55:11.628690 [0] Epoch: 131, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:12.243613 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.250864 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.250894 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:12.623690 [0] Epoch 00132 | Loss 3.4656
21:55:12.644199 [0] Epoch: 132, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:12.677865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.734884 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:12.800779 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:13.489028 [0] Epoch 00133 | Loss 3.4656
21:55:13.509678 [0] Epoch: 133, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:14.122441 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.129601 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.129656 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:14.503163 [0] Epoch 00134 | Loss 3.4656
21:55:14.523743 [0] Epoch: 134, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:14.557404 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.615545 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:14.680319 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:15.368468 [0] Epoch 00135 | Loss 3.4656
21:55:15.389369 [0] Epoch: 135, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:16.002832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.009866 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.010009 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:16.383404 [0] Epoch 00136 | Loss 3.4656
21:55:16.403907 [0] Epoch: 136, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:16.437615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.494743 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:16.560538 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:17.247208 [0] Epoch 00137 | Loss 3.4656
21:55:17.267896 [0] Epoch: 137, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:17.883621 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:17.890825 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:17.890940 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:18.262058 [0] Epoch 00138 | Loss 3.4656
21:55:18.282742 [0] Epoch: 138, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:18.316574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:18.373401 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:18.440068 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:19.127181 [0] Epoch 00139 | Loss 3.4656
21:55:19.148519 [0] Epoch: 139, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:19.764797 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:19.771658 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:19.771671 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:20.144198 [0] Epoch 00140 | Loss 3.4656
21:55:20.164802 [0] Epoch: 140, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:20.198504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:20.255653 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:20.321509 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:21.009060 [0] Epoch 00141 | Loss 3.4656
21:55:21.029741 [0] Epoch: 141, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:21.645377 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:21.652574 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:21.652607 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:22.024542 [0] Epoch 00142 | Loss 3.4656
21:55:22.045101 [0] Epoch: 142, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:22.078776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:22.135879 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:22.201743 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:22.889130 [0] Epoch 00143 | Loss 3.4656
21:55:22.909832 [0] Epoch: 143, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:23.525746 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.532898 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:23.533019 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:23.904692 [0] Epoch 00144 | Loss 3.4656
21:55:23.925366 [0] Epoch: 144, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:23.959027 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:24.015931 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:24.081907 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:24.771067 [0] Epoch 00145 | Loss 3.4656
21:55:24.791827 [0] Epoch: 145, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:25.408367 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.415567 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.415566 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:25.788907 [0] Epoch 00146 | Loss 3.4656
21:55:25.809734 [0] Epoch: 146, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:25.843245 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.900167 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:25.966226 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:26.655840 [0] Epoch 00147 | Loss 3.4656
21:55:26.676481 [0] Epoch: 147, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:27.292336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.299494 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.299613 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:27.672872 [0] Epoch 00148 | Loss 3.4656
21:55:27.693830 [0] Epoch: 148, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:27.727277 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.784625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:27.850312 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:28.544173 [0] Epoch 00149 | Loss 3.4656
21:55:28.565095 [0] Epoch: 149, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:29.181495 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.188714 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.188752 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:29.564057 [0] Epoch 00150 | Loss 3.4656
21:55:29.585126 [0] Epoch: 150, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:29.618972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.675786 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:29.741895 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:30.432520 [0] Epoch 00151 | Loss 3.4656
21:55:30.453398 [0] Epoch: 151, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:31.068504 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.075654 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.075667 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:31.448648 [0] Epoch 00152 | Loss 3.4656
21:55:31.469532 [0] Epoch: 152, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:31.503078 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.560303 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:31.626373 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:32.316827 [0] Epoch 00153 | Loss 3.4656
21:55:32.337530 [0] Epoch: 153, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:32.952910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:32.960302 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:32.960679 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:33.333937 [0] Epoch 00154 | Loss 3.4656
21:55:33.354716 [0] Epoch: 154, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:33.388295 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:33.445329 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:33.511321 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:34.200236 [0] Epoch 00155 | Loss 3.4656
21:55:34.221028 [0] Epoch: 155, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:34.833861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:34.841164 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:34.841247 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:35.214324 [0] Epoch 00156 | Loss 3.4656
21:55:35.235023 [0] Epoch: 156, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:35.268578 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:35.325707 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:35.391549 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:36.080582 [0] Epoch 00157 | Loss 3.4656
21:55:36.101525 [0] Epoch: 157, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:36.714620 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:36.721958 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:36.722001 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:37.095382 [0] Epoch 00158 | Loss 3.4656
21:55:37.116090 [0] Epoch: 158, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:37.149785 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:37.206788 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:37.272773 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:37.962164 [0] Epoch 00159 | Loss 3.4656
21:55:37.983006 [0] Epoch: 159, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:38.598368 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:38.605389 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:38.605513 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:38.978573 [0] Epoch 00160 | Loss 3.4656
21:55:38.999559 [0] Epoch: 160, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:39.033225 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:39.090214 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:39.156468 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:39.844164 [0] Epoch 00161 | Loss 3.4656
21:55:39.865101 [0] Epoch: 161, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:40.477723 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.484848 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.484955 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:40.856479 [0] Epoch 00162 | Loss 3.4656
21:55:40.877279 [0] Epoch: 162, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:40.911056 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:40.968103 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:41.033801 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:41.722984 [0] Epoch 00163 | Loss 3.4656
21:55:41.743753 [0] Epoch: 163, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:42.355539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.362772 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.362869 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:42.735835 [0] Epoch 00164 | Loss 3.4656
21:55:42.756357 [0] Epoch: 164, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:42.790063 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.847170 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:42.913103 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:43.602314 [0] Epoch 00165 | Loss 3.4656
21:55:43.623044 [0] Epoch: 165, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:44.234469 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.241758 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.241813 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:44.614780 [0] Epoch 00166 | Loss 3.4656
21:55:44.635343 [0] Epoch: 166, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:44.668692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.726184 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:44.791908 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:45.480095 [0] Epoch 00167 | Loss 3.4656
21:55:45.500926 [0] Epoch: 167, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:46.112548 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.119796 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.119915 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:46.493103 [0] Epoch 00168 | Loss 3.4656
21:55:46.513676 [0] Epoch: 168, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:46.547081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.604434 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:46.670659 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:47.359246 [0] Epoch 00169 | Loss 3.4656
21:55:47.380148 [0] Epoch: 169, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:47.991939 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:47.999293 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:47.999509 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:48.372045 [0] Epoch 00170 | Loss 3.4656
21:55:48.392537 [0] Epoch: 170, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:48.425878 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:48.483364 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:48.549194 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:49.238175 [0] Epoch 00171 | Loss 3.4656
21:55:49.259276 [0] Epoch: 171, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:49.871859 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:49.878937 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:49.879056 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:50.251993 [0] Epoch 00172 | Loss 3.4656
21:55:50.272401 [0] Epoch: 172, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:50.305796 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:50.363266 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:50.429038 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:51.116429 [0] Epoch 00173 | Loss 3.4656
21:55:51.137310 [0] Epoch: 173, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:51.748984 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:51.756187 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:51.756405 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:52.129201 [0] Epoch 00174 | Loss 3.4656
21:55:52.149645 [0] Epoch: 174, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:52.182934 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:52.240312 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:52.306071 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:52.993079 [0] Epoch 00175 | Loss 3.4656
21:55:53.013731 [0] Epoch: 175, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:53.625940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:53.633104 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:53.633160 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:54.005070 [0] Epoch 00176 | Loss 3.4656
21:55:54.025598 [0] Epoch: 176, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:54.058775 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:54.116208 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:54.181949 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:54.869398 [0] Epoch 00177 | Loss 3.4656
21:55:54.889789 [0] Epoch: 177, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:55.501910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.509275 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.509362 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:55.881522 [0] Epoch 00178 | Loss 3.4656
21:55:55.902397 [0] Epoch: 178, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:55.935639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:55.993019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:56.058685 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:56.747567 [0] Epoch 00179 | Loss 3.4656
21:55:56.768023 [0] Epoch: 179, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:57.379956 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.387228 [1] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.387607 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:57.761002 [0] Epoch 00180 | Loss 3.4656
21:55:57.781647 [0] Epoch: 180, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:57.814836 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.872161 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:57.938159 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:58.626988 [0] Epoch 00181 | Loss 3.4656
21:55:58.647449 [0] Epoch: 181, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:59.259352 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.266625 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.266655 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:55:59.640100 [0] Epoch 00182 | Loss 3.4656
21:55:59.661134 [0] Epoch: 182, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:55:59.694478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.751665 [2] Warning: no training nodes in this partition! Backward fake loss.
21:55:59.817502 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:00.506466 [0] Epoch 00183 | Loss 3.4656
21:56:00.527695 [0] Epoch: 183, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:01.139285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.146538 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.146723 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:01.518762 [0] Epoch 00184 | Loss 3.4656
21:56:01.539347 [0] Epoch: 184, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:01.572619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.630587 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:01.696386 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:02.394405 [0] Epoch 00185 | Loss 3.4656
21:56:02.415649 [0] Epoch: 185, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:03.037831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.045512 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.045656 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:03.423345 [0] Epoch 00186 | Loss 3.4656
21:56:03.444256 [0] Epoch: 186, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:03.477928 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.534997 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:03.601355 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:04.289965 [0] Epoch 00187 | Loss 3.4656
21:56:04.310676 [0] Epoch: 187, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:04.925709 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:04.932764 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:04.932856 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:05.306271 [0] Epoch 00188 | Loss 3.4656
21:56:05.326918 [0] Epoch: 188, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:05.360581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:05.417701 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:05.483498 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:06.173502 [0] Epoch 00189 | Loss 3.4656
21:56:06.194226 [0] Epoch: 189, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:06.810745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:06.817932 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:06.818012 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:07.191168 [0] Epoch 00190 | Loss 3.4656
21:56:07.211895 [0] Epoch: 190, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:07.245595 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:07.302733 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:07.368467 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:08.057059 [0] Epoch 00191 | Loss 3.4656
21:56:08.078169 [0] Epoch: 191, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:08.695175 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:08.702260 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:08.702373 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:09.076357 [0] Epoch 00192 | Loss 3.4656
21:56:09.097236 [0] Epoch: 192, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:09.130807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:09.187717 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:09.254073 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:09.942643 [0] Epoch 00193 | Loss 3.4656
21:56:09.963547 [0] Epoch: 193, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:10.579329 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:10.586401 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:10.586481 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:10.959703 [0] Epoch 00194 | Loss 3.4656
21:56:10.980513 [0] Epoch: 194, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:11.014081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:11.071124 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:11.136827 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:11.825391 [0] Epoch 00195 | Loss 3.4656
21:56:11.846070 [0] Epoch: 195, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:12.458832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.466283 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.466416 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:12.839245 [0] Epoch 00196 | Loss 3.4656
21:56:12.860063 [0] Epoch: 196, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:12.894559 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:12.951485 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:13.016551 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:13.706869 [0] Epoch 00197 | Loss 3.4656
21:56:13.727608 [0] Epoch: 197, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:14.340018 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.347287 [1] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.347403 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:14.721521 [0] Epoch 00198 | Loss 3.4656
21:56:14.742009 [0] Epoch: 198, Train: 0.0313, Val: 0.0313, Test: 0.0314
21:56:14.775691 [3] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.832763 [2] Warning: no training nodes in this partition! Backward fake loss.
21:56:14.898870 [1] Warning: no training nodes in this partition! Backward fake loss.
0.031434
21:56:15.587044 [0] Epoch 00199 | Loss 3.4656
21:56:15.607564 [0] Epoch: 199, Train: 0.0313, Val: 0.0313, Test: 0.0314
0.031434
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f512_l32_t0.1 nprocs 4
21:56:15.612816 [0] 
timer summary:
 16.75s  21.86s     1 broadcast ForwardL1 0
146.79s  70.55s  3200 broadcast
 47.06s  54.77s  3200 spmm
  0.15s   0.05s     1 broadcast ForwardL1 1
  0.13s   0.04s     1 broadcast ForwardL1 2
  0.12s   0.03s     1 broadcast ForwardL1 3
  4.70s   0.04s   800 mm
 28.99s  16.83s   125 broadcast ForwardL2 0
  8.68s   2.67s   125 broadcast ForwardL2 1
  8.21s   2.49s   125 broadcast ForwardL2 2
  7.68s   2.24s   125 broadcast ForwardL2 3
 25.71s  17.11s   200 broadcast BackwardL2 0
  1.43s   0.42s   200 broadcast BackwardL2 1
  1.60s   0.46s   200 broadcast BackwardL2 2
  1.54s   0.47s   200 broadcast BackwardL2 3
  2.47s   1.71s   400 all_reduce
  7.24s   0.24s   200 broadcast BackwardL1 0
 13.58s   4.28s   200 broadcast BackwardL1 1
 12.87s   3.97s   200 broadcast BackwardL1 2
 12.02s   3.58s   200 broadcast BackwardL1 3
209.82s  21.86s   200 epoch
247.40s   0.03s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
