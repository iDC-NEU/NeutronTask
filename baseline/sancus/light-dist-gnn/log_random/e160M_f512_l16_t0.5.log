no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:43:37.463584 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:43:37.606720 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:43:37.636987 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:43:37.646430 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:44:04.223931 [2] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:44:04.231886 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1125 MiB |   1141 MiB |   1175 MiB |  51113 KiB |
|       from large pool |   1125 MiB |   1141 MiB |   1175 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1124 MiB |   1139 MiB |   1172 MiB |  48832 KiB |
|       from large pool |   1124 MiB |   1139 MiB |   1172 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1160 MiB |   1160 MiB |   1160 MiB |      0 B   |
|       from large pool |   1158 MiB |   1158 MiB |   1158 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16988 KiB |  19998 KiB |  51431 KiB |  34442 KiB |
|       from large pool |  16988 KiB |  19998 KiB |  45282 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:44:12.187986 [3] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:44:12.197003 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1024 MiB |   1040 MiB |   1073 MiB |  51113 KiB |
|       from large pool |   1024 MiB |   1040 MiB |   1073 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1023 MiB |   1038 MiB |   1070 MiB |  48832 KiB |
|       from large pool |   1023 MiB |   1038 MiB |   1070 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1056 MiB |   1056 MiB |   1056 MiB |      0 B   |
|       from large pool |   1054 MiB |   1054 MiB |   1054 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  14306 KiB |  19998 KiB |  57928 KiB |  43621 KiB |
|       from large pool |  14306 KiB |  19998 KiB |  51779 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       3    |       3    |       5    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:44:22.989768 [1] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:44:23.000832 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1244 MiB |   1260 MiB |   1293 MiB |  51113 KiB |
|       from large pool |   1244 MiB |   1260 MiB |   1293 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1241 MiB |   1257 MiB |   1289 MiB |  48832 KiB |
|       from large pool |   1241 MiB |   1257 MiB |   1289 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1280 MiB |   1280 MiB |   1280 MiB |      0 B   |
|       from large pool |   1278 MiB |   1278 MiB |   1278 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  18399 KiB |  20447 KiB |  52842 KiB |  34442 KiB |
|       from large pool |  18399 KiB |  19998 KiB |  46693 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       7    |      11    |       5    |
|       from large pool |       6    |       6    |       8    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:44:27.860509 [0] graph loaded <COO Graph: e160M_f512_l16_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 0, |V|: 500000, |E|: 113001879>
21:44:27.872748 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1870 MiB |   1886 MiB |   1920 MiB |  51113 KiB |
|       from large pool |   1870 MiB |   1886 MiB |   1920 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1867 MiB |   1882 MiB |   1914 MiB |  48832 KiB |
|       from large pool |   1867 MiB |   1882 MiB |   1914 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1900 MiB |   1900 MiB |   1900 MiB |      0 B   |
|       from large pool |   1898 MiB |   1898 MiB |   1898 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12184 KiB |  19998 KiB |  46627 KiB |  34442 KiB |
|       from large pool |  12184 KiB |  19998 KiB |  40478 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:44:30.347445 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:30.371019 [2] Warning: no training nodes in this partition! Backward fake loss.
21:44:31.668918 [0] Epoch 00000 | Loss 2.7742
21:44:31.713639 [0] Epoch: 000, Train: 0.0626, Val: 0.0613, Test: 0.0621
21:44:32.327724 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:32.334585 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062081
21:44:32.709702 [0] Epoch 00001 | Loss 15799.9150
21:44:32.721045 [0] Epoch: 001, Train: 0.0628, Val: 0.0627, Test: 0.0629
21:44:33.335496 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:33.342558 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062879
21:44:33.716313 [0] Epoch 00002 | Loss 5695.0112
21:44:33.728954 [0] Epoch: 002, Train: 0.0627, Val: 0.0623, Test: 0.0628
21:44:34.342029 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:34.349840 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062751
21:44:34.723179 [0] Epoch 00003 | Loss 5035.5381
21:44:34.734867 [0] Epoch: 003, Train: 0.0622, Val: 0.0628, Test: 0.0620
21:44:35.345534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:35.352676 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062026
21:44:35.725698 [0] Epoch 00004 | Loss 5869.4751
21:44:35.737065 [0] Epoch: 004, Train: 0.0623, Val: 0.0633, Test: 0.0628
21:44:36.348716 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:36.355593 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062790
21:44:36.729682 [0] Epoch 00005 | Loss 5804.2480
21:44:36.741437 [0] Epoch: 005, Train: 0.0625, Val: 0.0631, Test: 0.0623
21:44:37.352493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:37.360394 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062277
21:44:37.733765 [0] Epoch 00006 | Loss 4616.3604
21:44:37.745222 [0] Epoch: 006, Train: 0.0627, Val: 0.0615, Test: 0.0619
21:44:38.356315 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:38.363806 [2] Warning: no training nodes in this partition! Backward fake loss.
0.061907
21:44:38.739061 [0] Epoch 00007 | Loss 2294.8242
21:44:38.750410 [0] Epoch: 007, Train: 0.0627, Val: 0.0619, Test: 0.0626
21:44:39.360623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:39.368039 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062580
21:44:39.740594 [0] Epoch 00008 | Loss 441.9233
21:44:39.752676 [0] Epoch: 008, Train: 0.0627, Val: 0.0619, Test: 0.0626
21:44:40.362596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:40.369941 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062600
21:44:40.740537 [0] Epoch 00009 | Loss 2.7731
21:44:40.752831 [0] Epoch: 009, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:41.363883 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:41.371190 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:41.742386 [0] Epoch 00010 | Loss 2.7731
21:44:41.753796 [0] Epoch: 010, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:42.365394 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:42.372812 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:42.743782 [0] Epoch 00011 | Loss 2.7731
21:44:42.755051 [0] Epoch: 011, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:43.366554 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:43.373847 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:43.745892 [0] Epoch 00012 | Loss 2.7731
21:44:43.757118 [0] Epoch: 012, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:44.368201 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:44.375705 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:44.746314 [0] Epoch 00013 | Loss 2.7731
21:44:44.757465 [0] Epoch: 013, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:45.368831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:45.375924 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:45.747336 [0] Epoch 00014 | Loss 2.7731
21:44:45.758417 [0] Epoch: 014, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:46.370348 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:46.377133 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:46.748777 [0] Epoch 00015 | Loss 2.7731
21:44:46.760473 [0] Epoch: 015, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:47.371967 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:47.378918 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:47.752192 [0] Epoch 00016 | Loss 2.7731
21:44:47.763516 [0] Epoch: 016, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:48.375358 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:48.382632 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:48.756465 [0] Epoch 00017 | Loss 2.7731
21:44:48.768278 [0] Epoch: 017, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:49.379324 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:49.386342 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:49.761026 [0] Epoch 00018 | Loss 2.7731
21:44:49.772594 [0] Epoch: 018, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:50.383155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:50.390553 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:50.763807 [0] Epoch 00019 | Loss 2.7731
21:44:50.774964 [0] Epoch: 019, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:51.386283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:51.392912 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:51.766521 [0] Epoch 00020 | Loss 2.7731
21:44:51.777845 [0] Epoch: 020, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:52.389406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:52.396605 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:52.769795 [0] Epoch 00021 | Loss 2.7731
21:44:52.781206 [0] Epoch: 021, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:53.393303 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:53.400636 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:53.774574 [0] Epoch 00022 | Loss 2.7731
21:44:53.785938 [0] Epoch: 022, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:54.398276 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:54.405030 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:54.778259 [0] Epoch 00023 | Loss 2.7731
21:44:54.789528 [0] Epoch: 023, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:55.400801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:55.408025 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:55.781576 [0] Epoch 00024 | Loss 2.7731
21:44:55.793049 [0] Epoch: 024, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:56.404071 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:56.411297 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:56.786226 [0] Epoch 00025 | Loss 2.7731
21:44:56.797570 [0] Epoch: 025, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:57.409089 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:57.415744 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:57.789790 [0] Epoch 00026 | Loss 2.7731
21:44:57.801044 [0] Epoch: 026, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:58.412009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:58.419027 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:58.793087 [0] Epoch 00027 | Loss 2.7731
21:44:58.804840 [0] Epoch: 027, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:44:59.415823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:44:59.423062 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:44:59.798333 [0] Epoch 00028 | Loss 2.7731
21:44:59.809763 [0] Epoch: 028, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:00.421456 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:00.428601 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:00.802246 [0] Epoch 00029 | Loss 2.7731
21:45:00.813621 [0] Epoch: 029, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:01.433926 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:01.441525 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:01.821498 [0] Epoch 00030 | Loss 2.7731
21:45:01.832964 [0] Epoch: 030, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:02.454598 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:02.462098 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:02.837709 [0] Epoch 00031 | Loss 2.7731
21:45:02.848999 [0] Epoch: 031, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:03.463634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:03.470234 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:03.843763 [0] Epoch 00032 | Loss 2.7731
21:45:03.854895 [0] Epoch: 032, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:04.468795 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:04.476218 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:04.850142 [0] Epoch 00033 | Loss 2.7731
21:45:04.861420 [0] Epoch: 033, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:05.476353 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:05.483201 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:05.857432 [0] Epoch 00034 | Loss 2.7731
21:45:05.868761 [0] Epoch: 034, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:06.482732 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:06.490369 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:06.863922 [0] Epoch 00035 | Loss 2.7731
21:45:06.875098 [0] Epoch: 035, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:07.489387 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:07.496603 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:07.870317 [0] Epoch 00036 | Loss 2.7731
21:45:07.881588 [0] Epoch: 036, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:08.496731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:08.504028 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:08.877746 [0] Epoch 00037 | Loss 2.7731
21:45:08.889061 [0] Epoch: 037, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:09.503302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:09.510034 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:09.882829 [0] Epoch 00038 | Loss 2.7731
21:45:09.894195 [0] Epoch: 038, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:10.505593 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:10.513031 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:10.886888 [0] Epoch 00039 | Loss 2.7731
21:45:10.898150 [0] Epoch: 039, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:11.509420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:11.516547 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:11.890559 [0] Epoch 00040 | Loss 2.7731
21:45:11.901824 [0] Epoch: 040, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:12.513256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:12.520446 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:12.894297 [0] Epoch 00041 | Loss 2.7731
21:45:12.905755 [0] Epoch: 041, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:13.517534 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:13.524071 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:13.897693 [0] Epoch 00042 | Loss 2.7731
21:45:13.909702 [0] Epoch: 042, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:14.520748 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:14.527824 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:14.902233 [0] Epoch 00043 | Loss 2.7731
21:45:14.913656 [0] Epoch: 043, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:15.525286 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:15.532397 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:15.906441 [0] Epoch 00044 | Loss 2.7731
21:45:15.917684 [0] Epoch: 044, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:16.529151 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:16.536322 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:16.910346 [0] Epoch 00045 | Loss 2.7731
21:45:16.921797 [0] Epoch: 045, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:17.532780 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:17.540204 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:17.913482 [0] Epoch 00046 | Loss 2.7731
21:45:17.924797 [0] Epoch: 046, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:18.535681 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:18.542737 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:18.917125 [0] Epoch 00047 | Loss 2.7731
21:45:18.928768 [0] Epoch: 047, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:19.539408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:19.546603 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:19.920598 [0] Epoch 00048 | Loss 2.7731
21:45:19.931799 [0] Epoch: 048, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:20.543270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:20.550367 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:20.923953 [0] Epoch 00049 | Loss 2.7731
21:45:20.935122 [0] Epoch: 049, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:21.545852 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:21.552997 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:21.926855 [0] Epoch 00050 | Loss 2.7731
21:45:21.938372 [0] Epoch: 050, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:21.971690 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:22.028251 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:22.784488 [0] Epoch 00051 | Loss 2.7731
21:45:22.796817 [0] Epoch: 051, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:23.409159 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.416012 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:23.787660 [0] Epoch 00052 | Loss 2.7731
21:45:23.798838 [0] Epoch: 052, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:23.832365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:23.888856 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:24.641580 [0] Epoch 00053 | Loss 2.7731
21:45:24.652593 [0] Epoch: 053, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:25.264175 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.271276 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:25.643833 [0] Epoch 00054 | Loss 2.7731
21:45:25.654993 [0] Epoch: 054, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:25.688622 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:25.745420 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:26.498173 [0] Epoch 00055 | Loss 2.7731
21:45:26.509430 [0] Epoch: 055, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:27.121229 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.128344 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:27.500120 [0] Epoch 00056 | Loss 2.7731
21:45:27.511152 [0] Epoch: 056, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:27.544858 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:27.601687 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:28.354036 [0] Epoch 00057 | Loss 2.7731
21:45:28.366112 [0] Epoch: 057, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:28.978831 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:28.986143 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:29.358113 [0] Epoch 00058 | Loss 2.7731
21:45:29.369233 [0] Epoch: 058, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:29.402968 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:29.459877 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:30.211366 [0] Epoch 00059 | Loss 2.7731
21:45:30.222478 [0] Epoch: 059, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:30.835210 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:30.842367 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:31.213416 [0] Epoch 00060 | Loss 2.7731
21:45:31.224569 [0] Epoch: 060, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:31.258256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:31.315308 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:32.065859 [0] Epoch 00061 | Loss 2.7731
21:45:32.077067 [0] Epoch: 061, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:32.688083 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:32.695147 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:33.066297 [0] Epoch 00062 | Loss 2.7731
21:45:33.077634 [0] Epoch: 062, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:33.111351 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:33.168584 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:33.918516 [0] Epoch 00063 | Loss 2.7731
21:45:33.929700 [0] Epoch: 063, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:34.540639 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:34.548045 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:34.918312 [0] Epoch 00064 | Loss 2.7731
21:45:34.929829 [0] Epoch: 064, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:34.963543 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:35.020610 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:35.770824 [0] Epoch 00065 | Loss 2.7731
21:45:35.781752 [0] Epoch: 065, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:36.392320 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.399650 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:36.771253 [0] Epoch 00066 | Loss 2.7731
21:45:36.782143 [0] Epoch: 066, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:36.815871 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:36.872851 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:37.623896 [0] Epoch 00067 | Loss 2.7731
21:45:37.635358 [0] Epoch: 067, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:38.246503 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.254535 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:38.624865 [0] Epoch 00068 | Loss 2.7731
21:45:38.635875 [0] Epoch: 068, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:38.669749 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:38.726365 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:39.477405 [0] Epoch 00069 | Loss 2.7731
21:45:39.488655 [0] Epoch: 069, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:40.099827 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.107220 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:40.477370 [0] Epoch 00070 | Loss 2.7731
21:45:40.488638 [0] Epoch: 070, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:40.522547 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:40.579647 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:41.328977 [0] Epoch 00071 | Loss 2.7731
21:45:41.341168 [0] Epoch: 071, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:41.952444 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:41.959394 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:42.329631 [0] Epoch 00072 | Loss 2.7731
21:45:42.340718 [0] Epoch: 072, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:42.374719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:42.431887 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:43.183070 [0] Epoch 00073 | Loss 2.7731
21:45:43.194091 [0] Epoch: 073, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:43.805099 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:43.812427 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:44.183082 [0] Epoch 00074 | Loss 2.7731
21:45:44.194394 [0] Epoch: 074, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:44.228049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:44.285595 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:45.036164 [0] Epoch 00075 | Loss 2.7731
21:45:45.047878 [0] Epoch: 075, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:45.657979 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:45.665392 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:46.035710 [0] Epoch 00076 | Loss 2.7731
21:45:46.046568 [0] Epoch: 076, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:46.080256 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:46.137195 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:46.887749 [0] Epoch 00077 | Loss 2.7731
21:45:46.899989 [0] Epoch: 077, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:47.509982 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.517160 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:47.888805 [0] Epoch 00078 | Loss 2.7731
21:45:47.900151 [0] Epoch: 078, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:47.933879 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:47.990904 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:48.742158 [0] Epoch 00079 | Loss 2.7731
21:45:48.753328 [0] Epoch: 079, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:49.363760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.370931 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:49.742133 [0] Epoch 00080 | Loss 2.7731
21:45:49.753343 [0] Epoch: 080, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:49.786978 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:49.843923 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:50.595010 [0] Epoch 00081 | Loss 2.7731
21:45:50.606628 [0] Epoch: 081, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:51.218654 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.225943 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:51.597968 [0] Epoch 00082 | Loss 2.7731
21:45:51.609352 [0] Epoch: 082, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:51.643076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:51.700000 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:52.450427 [0] Epoch 00083 | Loss 2.7731
21:45:52.461326 [0] Epoch: 083, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:53.071467 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.078607 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:53.448899 [0] Epoch 00084 | Loss 2.7731
21:45:53.460486 [0] Epoch: 084, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:53.493914 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:53.550958 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:54.302079 [0] Epoch 00085 | Loss 2.7731
21:45:54.313166 [0] Epoch: 085, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:54.924000 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:54.931265 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:55.302470 [0] Epoch 00086 | Loss 2.7731
21:45:55.313591 [0] Epoch: 086, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:55.347077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:55.404105 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:56.156467 [0] Epoch 00087 | Loss 2.7731
21:45:56.168129 [0] Epoch: 087, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:56.778406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:56.785831 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:57.158214 [0] Epoch 00088 | Loss 2.7731
21:45:57.169442 [0] Epoch: 088, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:57.203076 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:57.260117 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:58.012150 [0] Epoch 00089 | Loss 2.7731
21:45:58.023289 [0] Epoch: 089, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:58.633627 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:58.641033 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:59.012704 [0] Epoch 00090 | Loss 2.7731
21:45:59.023748 [0] Epoch: 090, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:45:59.057861 [3] Warning: no training nodes in this partition! Backward fake loss.
21:45:59.114608 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:45:59.866501 [0] Epoch 00091 | Loss 2.7731
21:45:59.877450 [0] Epoch: 091, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:00.489360 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.496585 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:00.868996 [0] Epoch 00092 | Loss 2.7731
21:46:00.880340 [0] Epoch: 092, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:00.914478 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:00.971128 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:01.723082 [0] Epoch 00093 | Loss 2.7731
21:46:01.734877 [0] Epoch: 093, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:02.355760 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.363121 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:02.746072 [0] Epoch 00094 | Loss 2.7731
21:46:02.757511 [0] Epoch: 094, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:02.792155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:02.848719 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:03.601986 [0] Epoch 00095 | Loss 2.7731
21:46:03.613157 [0] Epoch: 095, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:04.223925 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.231207 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:04.603085 [0] Epoch 00096 | Loss 2.7731
21:46:04.614163 [0] Epoch: 096, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:04.647774 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:04.704707 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:05.456881 [0] Epoch 00097 | Loss 2.7731
21:46:05.468335 [0] Epoch: 097, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:06.079049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.086164 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:06.459279 [0] Epoch 00098 | Loss 2.7731
21:46:06.470297 [0] Epoch: 098, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:06.503865 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:06.560982 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:07.312889 [0] Epoch 00099 | Loss 2.7731
21:46:07.324813 [0] Epoch: 099, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:07.935436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:07.942391 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:08.314868 [0] Epoch 00100 | Loss 2.7731
21:46:08.326012 [0] Epoch: 100, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:08.359659 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:08.416865 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:09.168777 [0] Epoch 00101 | Loss 2.7731
21:46:09.180018 [0] Epoch: 101, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:09.790224 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:09.797670 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:10.168916 [0] Epoch 00102 | Loss 2.7731
21:46:10.180303 [0] Epoch: 102, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:10.214048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:10.271103 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:11.023060 [0] Epoch 00103 | Loss 2.7731
21:46:11.033976 [0] Epoch: 103, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:11.644486 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:11.651655 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:12.023561 [0] Epoch 00104 | Loss 2.7731
21:46:12.034494 [0] Epoch: 104, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:12.068213 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:12.125290 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:12.876589 [0] Epoch 00105 | Loss 2.7731
21:46:12.888174 [0] Epoch: 105, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:13.498357 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.505590 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:13.877153 [0] Epoch 00106 | Loss 2.7731
21:46:13.888418 [0] Epoch: 106, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:13.922053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:13.979189 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:14.730082 [0] Epoch 00107 | Loss 2.7731
21:46:14.741816 [0] Epoch: 107, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:15.352301 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.359458 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:15.730153 [0] Epoch 00108 | Loss 2.7731
21:46:15.741285 [0] Epoch: 108, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:15.774998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:15.831793 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:16.582757 [0] Epoch 00109 | Loss 2.7731
21:46:16.593704 [0] Epoch: 109, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:17.203581 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.210804 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:17.581851 [0] Epoch 00110 | Loss 2.7731
21:46:17.592816 [0] Epoch: 110, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:17.626515 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:17.683505 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:18.435532 [0] Epoch 00111 | Loss 2.7731
21:46:18.446600 [0] Epoch: 111, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:19.057103 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.064287 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:19.435513 [0] Epoch 00112 | Loss 2.7731
21:46:19.446554 [0] Epoch: 112, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:19.480097 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:19.537323 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:20.288584 [0] Epoch 00113 | Loss 2.7731
21:46:20.300438 [0] Epoch: 113, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:20.911426 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:20.918654 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:21.290180 [0] Epoch 00114 | Loss 2.7731
21:46:21.301342 [0] Epoch: 114, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:21.335008 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:21.391917 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:22.143684 [0] Epoch 00115 | Loss 2.7731
21:46:22.154660 [0] Epoch: 115, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:22.766049 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:22.773134 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:23.144560 [0] Epoch 00116 | Loss 2.7731
21:46:23.155603 [0] Epoch: 116, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:23.189854 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:23.246826 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:23.997394 [0] Epoch 00117 | Loss 2.7731
21:46:24.008601 [0] Epoch: 117, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:24.623085 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:24.630297 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:25.001031 [0] Epoch 00118 | Loss 2.7731
21:46:25.012438 [0] Epoch: 118, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:25.046287 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:25.103389 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:25.854484 [0] Epoch 00119 | Loss 2.7731
21:46:25.865793 [0] Epoch: 119, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:26.479801 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.487346 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:26.857473 [0] Epoch 00120 | Loss 2.7731
21:46:26.868539 [0] Epoch: 120, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:26.902070 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:26.959463 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:27.710525 [0] Epoch 00121 | Loss 2.7731
21:46:27.721644 [0] Epoch: 121, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:28.335434 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.342833 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:28.714071 [0] Epoch 00122 | Loss 2.7731
21:46:28.725371 [0] Epoch: 122, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:28.758909 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:28.816093 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:29.567415 [0] Epoch 00123 | Loss 2.7731
21:46:29.579015 [0] Epoch: 123, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:30.191170 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.198112 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:30.570001 [0] Epoch 00124 | Loss 2.7731
21:46:30.581278 [0] Epoch: 124, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:30.615026 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:30.671851 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:31.423030 [0] Epoch 00125 | Loss 2.7731
21:46:31.434129 [0] Epoch: 125, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:32.046077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.053285 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:32.424740 [0] Epoch 00126 | Loss 2.7731
21:46:32.436437 [0] Epoch: 126, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:32.469566 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:32.527125 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:33.277440 [0] Epoch 00127 | Loss 2.7731
21:46:33.288601 [0] Epoch: 127, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:33.899874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:33.906939 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:34.277607 [0] Epoch 00128 | Loss 2.7731
21:46:34.288726 [0] Epoch: 128, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:34.322449 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:34.379400 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:35.130693 [0] Epoch 00129 | Loss 2.7731
21:46:35.141638 [0] Epoch: 129, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:35.752608 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:35.759864 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:36.131227 [0] Epoch 00130 | Loss 2.7731
21:46:36.142380 [0] Epoch: 130, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:36.175974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:36.233072 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:36.986284 [0] Epoch 00131 | Loss 2.7731
21:46:36.998366 [0] Epoch: 131, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:37.610155 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:37.617332 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:37.988322 [0] Epoch 00132 | Loss 2.7731
21:46:37.999355 [0] Epoch: 132, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:38.033483 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:38.090607 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:38.840805 [0] Epoch 00133 | Loss 2.7731
21:46:38.851826 [0] Epoch: 133, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:39.463248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.470405 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:39.842574 [0] Epoch 00134 | Loss 2.7731
21:46:39.853802 [0] Epoch: 134, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:39.887374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:39.944346 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:40.695996 [0] Epoch 00135 | Loss 2.7731
21:46:40.706961 [0] Epoch: 135, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:41.321231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.328467 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:41.699369 [0] Epoch 00136 | Loss 2.7731
21:46:41.710430 [0] Epoch: 136, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:41.743972 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:41.801113 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:42.552731 [0] Epoch 00137 | Loss 2.7731
21:46:42.563789 [0] Epoch: 137, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:43.177876 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.185249 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:43.556376 [0] Epoch 00138 | Loss 2.7731
21:46:43.567942 [0] Epoch: 138, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:43.601164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:43.658643 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:44.410647 [0] Epoch 00139 | Loss 2.7731
21:46:44.421977 [0] Epoch: 139, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:45.035711 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.042839 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:45.413543 [0] Epoch 00140 | Loss 2.7731
21:46:45.425210 [0] Epoch: 140, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:45.458575 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:45.515623 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:46.267357 [0] Epoch 00141 | Loss 2.7731
21:46:46.279124 [0] Epoch: 141, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:46.893678 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:46.900993 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:47.271938 [0] Epoch 00142 | Loss 2.7731
21:46:47.283467 [0] Epoch: 142, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:47.316609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:47.373586 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:48.125631 [0] Epoch 00143 | Loss 2.7731
21:46:48.136802 [0] Epoch: 143, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:48.748494 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:48.755982 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:49.127529 [0] Epoch 00144 | Loss 2.7731
21:46:49.138491 [0] Epoch: 144, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:49.172090 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:49.229113 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:49.980319 [0] Epoch 00145 | Loss 2.7731
21:46:49.991924 [0] Epoch: 145, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:50.602679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:50.609790 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:50.981569 [0] Epoch 00146 | Loss 2.7731
21:46:50.992865 [0] Epoch: 146, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:51.026507 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:51.083481 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:51.835872 [0] Epoch 00147 | Loss 2.7731
21:46:51.846984 [0] Epoch: 147, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:52.458342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.465575 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:52.835996 [0] Epoch 00148 | Loss 2.7731
21:46:52.847103 [0] Epoch: 148, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:52.881846 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:52.937671 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:53.688688 [0] Epoch 00149 | Loss 2.7731
21:46:53.700974 [0] Epoch: 149, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:54.312830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.319923 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:54.691358 [0] Epoch 00150 | Loss 2.7731
21:46:54.702378 [0] Epoch: 150, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:54.736103 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:54.792995 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:55.545126 [0] Epoch 00151 | Loss 2.7731
21:46:55.556270 [0] Epoch: 151, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:56.168343 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.175467 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:56.546431 [0] Epoch 00152 | Loss 2.7731
21:46:56.557547 [0] Epoch: 152, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:56.591184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:56.648417 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:57.399269 [0] Epoch 00153 | Loss 2.7731
21:46:57.410385 [0] Epoch: 153, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:58.021493 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.029009 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:58.400104 [0] Epoch 00154 | Loss 2.7731
21:46:58.411207 [0] Epoch: 154, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:58.444745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:58.502110 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:46:59.253688 [0] Epoch 00155 | Loss 2.7731
21:46:59.265236 [0] Epoch: 155, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:46:59.877130 [3] Warning: no training nodes in this partition! Backward fake loss.
21:46:59.884434 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:00.256349 [0] Epoch 00156 | Loss 2.7731
21:47:00.268138 [0] Epoch: 156, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:00.301223 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:00.358962 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:01.111386 [0] Epoch 00157 | Loss 2.7731
21:47:01.122432 [0] Epoch: 157, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:01.742665 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:01.750189 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:02.133981 [0] Epoch 00158 | Loss 2.7731
21:47:02.145312 [0] Epoch: 158, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:02.180243 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:02.236370 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:02.990170 [0] Epoch 00159 | Loss 2.7731
21:47:03.002224 [0] Epoch: 159, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:03.614913 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:03.622014 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:03.994015 [0] Epoch 00160 | Loss 2.7731
21:47:04.005301 [0] Epoch: 160, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:04.040062 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:04.095619 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:04.847286 [0] Epoch 00161 | Loss 2.7731
21:47:04.858296 [0] Epoch: 161, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:05.469807 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.476982 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:05.849371 [0] Epoch 00162 | Loss 2.7731
21:47:05.860661 [0] Epoch: 162, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:05.894375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:05.950820 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:06.703027 [0] Epoch 00163 | Loss 2.7731
21:47:06.714444 [0] Epoch: 163, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:07.325778 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.332789 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:07.704217 [0] Epoch 00164 | Loss 2.7731
21:47:07.715220 [0] Epoch: 164, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:07.748990 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:07.806466 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:08.558374 [0] Epoch 00165 | Loss 2.7731
21:47:08.569528 [0] Epoch: 165, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:09.180658 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.187844 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:09.558597 [0] Epoch 00166 | Loss 2.7731
21:47:09.569707 [0] Epoch: 166, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:09.603239 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:09.660099 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:10.411869 [0] Epoch 00167 | Loss 2.7731
21:47:10.423010 [0] Epoch: 167, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:11.034687 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.041899 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:11.413446 [0] Epoch 00168 | Loss 2.7731
21:47:11.424720 [0] Epoch: 168, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:11.459553 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:11.515034 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:12.266452 [0] Epoch 00169 | Loss 2.7731
21:47:12.277504 [0] Epoch: 169, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:12.888839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:12.896369 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:13.266998 [0] Epoch 00170 | Loss 2.7731
21:47:13.278378 [0] Epoch: 170, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:13.312104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:13.369024 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:14.120300 [0] Epoch 00171 | Loss 2.7731
21:47:14.131858 [0] Epoch: 171, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:14.743297 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:14.750371 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:15.120611 [0] Epoch 00172 | Loss 2.7731
21:47:15.132365 [0] Epoch: 172, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:15.165728 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:15.223115 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:15.973460 [0] Epoch 00173 | Loss 2.7731
21:47:15.984924 [0] Epoch: 173, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:16.599077 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:16.606308 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:16.977819 [0] Epoch 00174 | Loss 2.7731
21:47:16.988993 [0] Epoch: 174, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:17.022718 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:17.079735 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:17.831072 [0] Epoch 00175 | Loss 2.7731
21:47:17.842146 [0] Epoch: 175, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:18.456584 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.463781 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:18.834643 [0] Epoch 00176 | Loss 2.7731
21:47:18.845693 [0] Epoch: 176, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:18.879342 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:18.936339 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:19.687776 [0] Epoch 00177 | Loss 2.7731
21:47:19.698841 [0] Epoch: 177, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:20.313611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.320798 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:20.692933 [0] Epoch 00178 | Loss 2.7731
21:47:20.704338 [0] Epoch: 178, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:20.737701 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:20.795356 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:21.547715 [0] Epoch 00179 | Loss 2.7731
21:47:21.558836 [0] Epoch: 179, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:22.170745 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.178114 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:22.550285 [0] Epoch 00180 | Loss 2.7731
21:47:22.561585 [0] Epoch: 180, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:22.594880 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:22.652006 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:23.404903 [0] Epoch 00181 | Loss 2.7731
21:47:23.416413 [0] Epoch: 181, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:24.027361 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.034552 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:24.408356 [0] Epoch 00182 | Loss 2.7731
21:47:24.419480 [0] Epoch: 182, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:24.452754 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:24.510018 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:25.263327 [0] Epoch 00183 | Loss 2.7731
21:47:25.274350 [0] Epoch: 183, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:25.886202 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:25.893417 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:26.265697 [0] Epoch 00184 | Loss 2.7731
21:47:26.276895 [0] Epoch: 184, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:26.310378 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:26.367522 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:27.118792 [0] Epoch 00185 | Loss 2.7731
21:47:27.129815 [0] Epoch: 185, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:27.739679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:27.747172 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:28.119382 [0] Epoch 00186 | Loss 2.7731
21:47:28.130335 [0] Epoch: 186, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:28.163837 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:28.221433 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:28.972391 [0] Epoch 00187 | Loss 2.7731
21:47:28.984152 [0] Epoch: 187, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:29.594240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:29.601439 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:29.972775 [0] Epoch 00188 | Loss 2.7731
21:47:29.984437 [0] Epoch: 188, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:30.017920 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:30.075196 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:30.827507 [0] Epoch 00189 | Loss 2.7731
21:47:30.838755 [0] Epoch: 189, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:31.449635 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.456817 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:31.828422 [0] Epoch 00190 | Loss 2.7731
21:47:31.840114 [0] Epoch: 190, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:31.873240 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:31.930762 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:32.681801 [0] Epoch 00191 | Loss 2.7731
21:47:32.692928 [0] Epoch: 191, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:33.304313 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.311540 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:33.682220 [0] Epoch 00192 | Loss 2.7731
21:47:33.693287 [0] Epoch: 192, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:33.727004 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:33.783795 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:34.535374 [0] Epoch 00193 | Loss 2.7731
21:47:34.546307 [0] Epoch: 193, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:35.158731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.165783 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:35.537087 [0] Epoch 00194 | Loss 2.7731
21:47:35.548070 [0] Epoch: 194, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:35.582075 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:35.639033 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:36.391329 [0] Epoch 00195 | Loss 2.7731
21:47:36.402350 [0] Epoch: 195, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:37.014227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.021697 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:37.392627 [0] Epoch 00196 | Loss 2.7731
21:47:37.404092 [0] Epoch: 196, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:37.437866 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:37.494893 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:38.246163 [0] Epoch 00197 | Loss 2.7731
21:47:38.257389 [0] Epoch: 197, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:38.868874 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:38.876096 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:39.247492 [0] Epoch 00198 | Loss 2.7731
21:47:39.258553 [0] Epoch: 198, Train: 0.0621, Val: 0.0624, Test: 0.0630
21:47:39.292107 [3] Warning: no training nodes in this partition! Backward fake loss.
21:47:39.349065 [2] Warning: no training nodes in this partition! Backward fake loss.
0.062955
21:47:40.101129 [0] Epoch 00199 | Loss 2.7731
21:47:40.112496 [0] Epoch: 199, Train: 0.0621, Val: 0.0624, Test: 0.0630
0.062955
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f512_l16_t0.5 nprocs 4
21:47:40.116756 [0] 
timer summary:
 11.66s  10.59s     1 broadcast ForwardL1 0
139.95s  63.86s  3200 broadcast
 46.92s  54.64s  3200 spmm
  0.15s   0.05s     1 broadcast ForwardL1 1
  0.13s   0.04s     1 broadcast ForwardL1 2
  0.12s   0.03s     1 broadcast ForwardL1 3
  4.67s   0.04s   800 mm
 28.85s  16.74s   125 broadcast ForwardL2 0
  8.66s   2.68s   125 broadcast ForwardL2 1
  8.24s   2.49s   125 broadcast ForwardL2 2
  7.71s   2.24s   125 broadcast ForwardL2 3
 25.62s  17.86s   200 broadcast BackwardL2 0
  0.91s   0.38s   200 broadcast BackwardL2 1
  1.10s   0.43s   200 broadcast BackwardL2 2
  1.05s   0.44s   200 broadcast BackwardL2 3
  2.41s   1.69s   400 all_reduce
  7.25s   0.24s   200 broadcast BackwardL1 0
 13.55s   4.29s   200 broadcast BackwardL1 1
 12.84s   3.97s   200 broadcast BackwardL1 2
 12.01s   3.57s   200 broadcast BackwardL1 3
202.33s  10.58s   200 epoch
242.52s   0.08s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
