no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:37:53.689877 [2] proc begin: <DistEnv 2/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:37:53.823793 [3] proc begin: <DistEnv 3/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:37:53.854355 [1] proc begin: <DistEnv 1/4 nccl>
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
no spmm cpp: cannot import name 'spmm_cusparse_coo' from 'spmm_cpp' (unknown location)
21:37:53.855731 [0] proc begin: <DistEnv 0/4 nccl>
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:38:38.919993 [3] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 3, |V|: 500000, |E|: 2349774>
21:38:38.930885 [3] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2001 MiB |   2017 MiB |   2051 MiB |  51113 KiB |
|       from large pool |   2001 MiB |   2017 MiB |   2051 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2001 MiB |   2017 MiB |   2051 MiB |  51113 KiB |
|       from large pool |   2001 MiB |   2017 MiB |   2051 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1999 MiB |   2014 MiB |   2047 MiB |  48832 KiB |
|       from large pool |   1999 MiB |   2014 MiB |   2047 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2032 MiB |   2032 MiB |   2032 MiB |      0 B   |
|       from large pool |   2030 MiB |   2030 MiB |   2030 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  12834 KiB |  18526 KiB |  56456 KiB |  43621 KiB |
|       from large pool |  12834 KiB |  18526 KiB |  50307 KiB |  37473 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      11    |      15    |      24    |      13    |
|       from large pool |      11    |      12    |      15    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       3    |       7    |       5    |
|       from large pool |       2    |       2    |       4    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:38:49.183775 [1] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 1, |V|: 500000, |E|: 31008983>
21:38:49.189471 [1] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2221 MiB |   2237 MiB |   2271 MiB |  51113 KiB |
|       from large pool |   2221 MiB |   2237 MiB |   2271 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2221 MiB |   2237 MiB |   2271 MiB |  51113 KiB |
|       from large pool |   2221 MiB |   2237 MiB |   2271 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2218 MiB |   2233 MiB |   2266 MiB |  48832 KiB |
|       from large pool |   2218 MiB |   2233 MiB |   2265 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2256 MiB |   2256 MiB |   2256 MiB |      0 B   |
|       from large pool |   2254 MiB |   2254 MiB |   2254 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16927 KiB |  18975 KiB |  51370 KiB |  34442 KiB |
|       from large pool |  16927 KiB |  18526 KiB |  45221 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      15    |      19    |      28    |      13    |
|       from large pool |      15    |      16    |      19    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      11    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       6    |      10    |       5    |
|       from large pool |       5    |       5    |       7    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:38:51.899525 [2] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 2, |V|: 500000, |E|: 15637930>
21:38:51.905683 [2] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2102 MiB |   2118 MiB |   2152 MiB |  51113 KiB |
|       from large pool |   2102 MiB |   2118 MiB |   2152 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2102 MiB |   2118 MiB |   2152 MiB |  51113 KiB |
|       from large pool |   2102 MiB |   2118 MiB |   2152 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2101 MiB |   2116 MiB |   2148 MiB |  48832 KiB |
|       from large pool |   2101 MiB |   2116 MiB |   2148 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2136 MiB |   2136 MiB |   2136 MiB |      0 B   |
|       from large pool |   2134 MiB |   2134 MiB |   2134 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15516 KiB |  18526 KiB |  49959 KiB |  34442 KiB |
|       from large pool |  15516 KiB |  18526 KiB |  43810 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      13    |      17    |      26    |      13    |
|       from large pool |      13    |      14    |      17    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       9    |       9    |       9    |       0    |
|       from large pool |       8    |       8    |       8    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       6    |      10    |       5    |
|       from large pool |       5    |       5    |       7    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
coo torch.Size([500000, 500000])
csr torch.Size([500000, 500000])
small csr torch.Size([500000, 500000])
21:39:08.239643 [0] graph loaded <COO Graph: e160M_f1024_l32_t0.5, |V|: 2000000, |E|: 160000000, masks: 1000000,200000,800000><Local: 0, |V|: 500000, |E|: 113001879>
21:39:08.248421 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2847 MiB |   2863 MiB |   2897 MiB |  51113 KiB |
|       from large pool |   2847 MiB |   2863 MiB |   2897 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   2847 MiB |   2863 MiB |   2897 MiB |  51113 KiB |
|       from large pool |   2847 MiB |   2863 MiB |   2897 MiB |  51105 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      7 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2843 MiB |   2859 MiB |   2891 MiB |  48832 KiB |
|       from large pool |   2843 MiB |   2859 MiB |   2891 MiB |  48828 KiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      4 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2876 MiB |   2876 MiB |   2876 MiB |      0 B   |
|       from large pool |   2874 MiB |   2874 MiB |   2874 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10712 KiB |  18526 KiB |  45155 KiB |  34442 KiB |
|       from large pool |  10712 KiB |  18526 KiB |  39006 KiB |  28294 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |      17    |      21    |      30    |      13    |
|       from large pool |      17    |      18    |      21    |       4    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      13    |      13    |      13    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       6    |       5    |
|       from large pool |       1    |       2    |       3    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:39:10.990057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:10.994809 [2] Warning: no training nodes in this partition! Backward fake loss.
21:39:12.391616 [0] Epoch 00000 | Loss 3.4660
21:39:12.446698 [0] Epoch: 000, Train: 0.0312, Val: 0.0317, Test: 0.0309
21:39:13.375270 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:13.383402 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030937
21:39:13.783531 [0] Epoch 00001 | Loss 12325.9004
21:39:13.805203 [0] Epoch: 001, Train: 0.0316, Val: 0.0309, Test: 0.0311
21:39:14.733789 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:14.741089 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031148
21:39:15.141744 [0] Epoch 00002 | Loss 3760.1006
21:39:15.163333 [0] Epoch: 002, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:16.092799 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:16.100821 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:16.501375 [0] Epoch 00003 | Loss 1443.4078
21:39:16.522462 [0] Epoch: 003, Train: 0.0311, Val: 0.0312, Test: 0.0315
21:39:17.452241 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:17.459379 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031496
21:39:17.857705 [0] Epoch 00004 | Loss 1293.1882
21:39:17.878486 [0] Epoch: 004, Train: 0.0311, Val: 0.0307, Test: 0.0317
21:39:18.806688 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:18.814020 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031734
21:39:19.210646 [0] Epoch 00005 | Loss 867.6371
21:39:19.232739 [0] Epoch: 005, Train: 0.0314, Val: 0.0312, Test: 0.0311
21:39:20.161830 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:20.169479 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031115
21:39:20.568005 [0] Epoch 00006 | Loss 308.8758
21:39:20.589081 [0] Epoch: 006, Train: 0.0312, Val: 0.0317, Test: 0.0309
21:39:21.517331 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:21.525241 [2] Warning: no training nodes in this partition! Backward fake loss.
0.030855
21:39:21.921066 [0] Epoch 00007 | Loss 3.4651
21:39:21.943015 [0] Epoch: 007, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:22.872676 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:22.879687 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:23.278198 [0] Epoch 00008 | Loss 3.4651
21:39:23.298944 [0] Epoch: 008, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:24.228106 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:24.235789 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:24.632741 [0] Epoch 00009 | Loss 3.4651
21:39:24.654177 [0] Epoch: 009, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:25.583374 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:25.590965 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:25.987690 [0] Epoch 00010 | Loss 3.4651
21:39:26.009511 [0] Epoch: 010, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:26.938283 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:26.945633 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:27.342065 [0] Epoch 00011 | Loss 3.4651
21:39:27.363607 [0] Epoch: 011, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:28.292482 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:28.299926 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:28.696980 [0] Epoch 00012 | Loss 3.4651
21:39:28.718041 [0] Epoch: 012, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:29.647998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:29.655074 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:30.051644 [0] Epoch 00013 | Loss 3.4651
21:39:30.073514 [0] Epoch: 013, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:31.002545 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:31.010077 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:31.406943 [0] Epoch 00014 | Loss 3.4651
21:39:31.429136 [0] Epoch: 014, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:32.358418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:32.365125 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:32.761466 [0] Epoch 00015 | Loss 3.4651
21:39:32.783179 [0] Epoch: 015, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:33.711988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:33.719596 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:34.115860 [0] Epoch 00016 | Loss 3.4651
21:39:34.136692 [0] Epoch: 016, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:35.065593 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:35.073066 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:35.468443 [0] Epoch 00017 | Loss 3.4651
21:39:35.489589 [0] Epoch: 017, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:36.418230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:36.425203 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:36.821265 [0] Epoch 00018 | Loss 3.4651
21:39:36.842832 [0] Epoch: 018, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:37.772168 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:37.779766 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:38.175488 [0] Epoch 00019 | Loss 3.4651
21:39:38.196789 [0] Epoch: 019, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:39.126088 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:39.133512 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:39.530431 [0] Epoch 00020 | Loss 3.4651
21:39:39.551670 [0] Epoch: 020, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:40.480183 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:40.487682 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:40.883708 [0] Epoch 00021 | Loss 3.4651
21:39:40.904995 [0] Epoch: 021, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:41.834104 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:41.841445 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:42.238034 [0] Epoch 00022 | Loss 3.4651
21:39:42.259137 [0] Epoch: 022, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:43.188093 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:43.195356 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:43.592078 [0] Epoch 00023 | Loss 3.4651
21:39:43.612720 [0] Epoch: 023, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:44.541666 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:44.549482 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:44.946097 [0] Epoch 00024 | Loss 3.4651
21:39:44.966779 [0] Epoch: 024, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:45.895613 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:45.903029 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:46.298908 [0] Epoch 00025 | Loss 3.4651
21:39:46.319881 [0] Epoch: 025, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:47.249009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:47.255918 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:47.654073 [0] Epoch 00026 | Loss 3.4651
21:39:47.674799 [0] Epoch: 026, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:48.604184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:48.611603 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:49.009284 [0] Epoch 00027 | Loss 3.4651
21:39:49.029978 [0] Epoch: 027, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:49.958268 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:49.965574 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:50.362201 [0] Epoch 00028 | Loss 3.4651
21:39:50.383019 [0] Epoch: 028, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:51.311719 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:51.319116 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:51.715949 [0] Epoch 00029 | Loss 3.4651
21:39:51.736722 [0] Epoch: 029, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:52.665974 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:52.673169 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:53.071101 [0] Epoch 00030 | Loss 3.4651
21:39:53.091811 [0] Epoch: 030, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:54.021407 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:54.028785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:54.425140 [0] Epoch 00031 | Loss 3.4651
21:39:54.446121 [0] Epoch: 031, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:55.375259 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:55.382137 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:55.778361 [0] Epoch 00032 | Loss 3.4651
21:39:55.799209 [0] Epoch: 032, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:56.727989 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:56.735563 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:57.132496 [0] Epoch 00033 | Loss 3.4651
21:39:57.153090 [0] Epoch: 033, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:58.082079 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:58.089438 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:58.485806 [0] Epoch 00034 | Loss 3.4651
21:39:58.506381 [0] Epoch: 034, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:39:59.434881 [3] Warning: no training nodes in this partition! Backward fake loss.
21:39:59.442351 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:39:59.838002 [0] Epoch 00035 | Loss 3.4651
21:39:59.859645 [0] Epoch: 035, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:00.788964 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:00.796337 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:01.192981 [0] Epoch 00036 | Loss 3.4651
21:40:01.214622 [0] Epoch: 036, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:02.146472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:02.153702 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:02.560119 [0] Epoch 00037 | Loss 3.4651
21:40:02.581691 [0] Epoch: 037, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:03.519227 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:03.526991 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:03.924461 [0] Epoch 00038 | Loss 3.4651
21:40:03.945219 [0] Epoch: 038, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:04.876053 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:04.883404 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:05.281311 [0] Epoch 00039 | Loss 3.4651
21:40:05.302177 [0] Epoch: 039, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:06.232285 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:06.240069 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:06.637108 [0] Epoch 00040 | Loss 3.4651
21:40:06.658088 [0] Epoch: 040, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:07.588001 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:07.595368 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:07.994760 [0] Epoch 00041 | Loss 3.4651
21:40:08.015894 [0] Epoch: 041, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:08.945473 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:08.953103 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:09.352166 [0] Epoch 00042 | Loss 3.4651
21:40:09.373350 [0] Epoch: 042, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:10.302904 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:10.310416 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:10.709622 [0] Epoch 00043 | Loss 3.4651
21:40:10.731290 [0] Epoch: 043, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:11.661055 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:11.668210 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:12.067737 [0] Epoch 00044 | Loss 3.4651
21:40:12.088681 [0] Epoch: 044, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:13.018419 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:13.025775 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:13.425174 [0] Epoch 00045 | Loss 3.4651
21:40:13.446361 [0] Epoch: 045, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:14.375470 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:14.383268 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:14.782481 [0] Epoch 00046 | Loss 3.4651
21:40:14.803367 [0] Epoch: 046, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:15.733462 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:15.740887 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:16.140799 [0] Epoch 00047 | Loss 3.4651
21:40:16.161856 [0] Epoch: 047, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:17.091697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:17.099801 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:17.498275 [0] Epoch 00048 | Loss 3.4651
21:40:17.519227 [0] Epoch: 048, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:18.448840 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:18.456278 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:18.855420 [0] Epoch 00049 | Loss 3.4651
21:40:18.876307 [0] Epoch: 049, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:19.805264 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:19.812523 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:20.210853 [0] Epoch 00050 | Loss 3.4651
21:40:20.231689 [0] Epoch: 050, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:20.286610 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:20.382600 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:21.421071 [0] Epoch 00051 | Loss 3.4651
21:40:21.442146 [0] Epoch: 051, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:22.371599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.378801 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:22.775796 [0] Epoch 00052 | Loss 3.4651
21:40:22.796571 [0] Epoch: 052, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:22.851823 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:22.947321 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:23.984423 [0] Epoch 00053 | Loss 3.4651
21:40:24.005607 [0] Epoch: 053, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:24.934560 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:24.941848 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:25.339587 [0] Epoch 00054 | Loss 3.4651
21:40:25.360312 [0] Epoch: 054, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:25.415704 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:25.511177 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:26.547264 [0] Epoch 00055 | Loss 3.4651
21:40:26.568163 [0] Epoch: 055, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:27.497518 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:27.504892 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:27.902344 [0] Epoch 00056 | Loss 3.4651
21:40:27.923110 [0] Epoch: 056, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:27.978599 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:28.073785 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:29.109671 [0] Epoch 00057 | Loss 3.4651
21:40:29.131022 [0] Epoch: 057, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:30.061048 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.068050 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:30.464855 [0] Epoch 00058 | Loss 3.4651
21:40:30.485767 [0] Epoch: 058, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:30.541116 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:30.636266 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:31.671276 [0] Epoch 00059 | Loss 3.4651
21:40:31.692106 [0] Epoch: 059, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:32.620910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:32.628092 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:33.024291 [0] Epoch 00060 | Loss 3.4651
21:40:33.045135 [0] Epoch: 060, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:33.100472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:33.195422 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:34.232351 [0] Epoch 00061 | Loss 3.4651
21:40:34.253190 [0] Epoch: 061, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:35.182207 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.189559 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:35.587599 [0] Epoch 00062 | Loss 3.4651
21:40:35.608449 [0] Epoch: 062, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:35.663856 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:35.758829 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:36.794661 [0] Epoch 00063 | Loss 3.4651
21:40:36.815320 [0] Epoch: 063, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:37.744009 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:37.751286 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:38.147165 [0] Epoch 00064 | Loss 3.4651
21:40:38.167840 [0] Epoch: 064, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:38.222970 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:38.318305 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:39.353272 [0] Epoch 00065 | Loss 3.4651
21:40:39.375045 [0] Epoch: 065, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:40.303786 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.311072 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:40.706985 [0] Epoch 00066 | Loss 3.4651
21:40:40.728463 [0] Epoch: 066, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:40.783708 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:40.879438 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:41.913870 [0] Epoch 00067 | Loss 3.4651
21:40:41.935234 [0] Epoch: 067, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:42.864035 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:42.871387 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:43.268484 [0] Epoch 00068 | Loss 3.4651
21:40:43.289186 [0] Epoch: 068, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:43.344451 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:43.440252 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:44.474985 [0] Epoch 00069 | Loss 3.4651
21:40:44.495772 [0] Epoch: 069, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:45.424781 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.432374 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:45.828167 [0] Epoch 00070 | Loss 3.4651
21:40:45.848928 [0] Epoch: 070, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:45.904235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:45.999587 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:47.036069 [0] Epoch 00071 | Loss 3.4651
21:40:47.056744 [0] Epoch: 071, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:47.985787 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:47.993207 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:48.390024 [0] Epoch 00072 | Loss 3.4651
21:40:48.410548 [0] Epoch: 072, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:48.465776 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:48.561383 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:49.596280 [0] Epoch 00073 | Loss 3.4651
21:40:49.617339 [0] Epoch: 073, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:50.546539 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:50.553867 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:50.950178 [0] Epoch 00074 | Loss 3.4651
21:40:50.970965 [0] Epoch: 074, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:51.027154 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:51.121800 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:52.155887 [0] Epoch 00075 | Loss 3.4651
21:40:52.177843 [0] Epoch: 075, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:53.107163 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.114419 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:53.510769 [0] Epoch 00076 | Loss 3.4651
21:40:53.531662 [0] Epoch: 076, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:53.586919 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:53.682516 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:54.717853 [0] Epoch 00077 | Loss 3.4651
21:40:54.738655 [0] Epoch: 077, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:55.667634 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:55.674935 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:56.071045 [0] Epoch 00078 | Loss 3.4651
21:40:56.092067 [0] Epoch: 078, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:56.147418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:56.243132 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:57.277239 [0] Epoch 00079 | Loss 3.4651
21:40:57.298929 [0] Epoch: 079, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:58.229463 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.236925 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:58.633141 [0] Epoch 00080 | Loss 3.4651
21:40:58.654252 [0] Epoch: 080, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:40:58.709405 [3] Warning: no training nodes in this partition! Backward fake loss.
21:40:58.805360 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:40:59.841132 [0] Epoch 00081 | Loss 3.4651
21:40:59.861915 [0] Epoch: 081, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:00.791238 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:00.798528 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:01.194814 [0] Epoch 00082 | Loss 3.4651
21:41:01.216131 [0] Epoch: 082, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:01.271153 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:01.367827 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:02.413652 [0] Epoch 00083 | Loss 3.4651
21:41:02.435806 [0] Epoch: 083, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:03.366312 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.373383 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:03.770306 [0] Epoch 00084 | Loss 3.4651
21:41:03.791386 [0] Epoch: 084, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:03.846887 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:03.941986 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:04.978230 [0] Epoch 00085 | Loss 3.4651
21:41:04.999906 [0] Epoch: 085, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:05.929254 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:05.936549 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:06.332796 [0] Epoch 00086 | Loss 3.4651
21:41:06.354125 [0] Epoch: 086, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:06.409354 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:06.504476 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:07.543148 [0] Epoch 00087 | Loss 3.4651
21:41:07.563946 [0] Epoch: 087, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:08.492940 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:08.500145 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:08.897324 [0] Epoch 00088 | Loss 3.4651
21:41:08.918194 [0] Epoch: 088, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:08.973579 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:09.069684 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:10.104678 [0] Epoch 00089 | Loss 3.4651
21:41:10.125627 [0] Epoch: 089, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:11.054365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.061724 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:11.459062 [0] Epoch 00090 | Loss 3.4651
21:41:11.480064 [0] Epoch: 090, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:11.535529 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:11.630916 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:12.666528 [0] Epoch 00091 | Loss 3.4651
21:41:12.687545 [0] Epoch: 091, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:13.616333 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:13.623733 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:14.020215 [0] Epoch 00092 | Loss 3.4651
21:41:14.041127 [0] Epoch: 092, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:14.096699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:14.191666 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:15.226136 [0] Epoch 00093 | Loss 3.4651
21:41:15.247916 [0] Epoch: 093, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:16.177014 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.183941 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:16.580892 [0] Epoch 00094 | Loss 3.4651
21:41:16.601781 [0] Epoch: 094, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:16.657356 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:16.752422 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:17.787530 [0] Epoch 00095 | Loss 3.4651
21:41:17.808583 [0] Epoch: 095, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:18.737185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:18.744193 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:19.140993 [0] Epoch 00096 | Loss 3.4651
21:41:19.161588 [0] Epoch: 096, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:19.217436 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:19.312082 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:20.347262 [0] Epoch 00097 | Loss 3.4651
21:41:20.368446 [0] Epoch: 097, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:21.297365 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.305001 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:21.701053 [0] Epoch 00098 | Loss 3.4651
21:41:21.722210 [0] Epoch: 098, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:21.777452 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:21.874397 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:22.908583 [0] Epoch 00099 | Loss 3.4651
21:41:22.929453 [0] Epoch: 099, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:23.859141 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:23.866463 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:24.264937 [0] Epoch 00100 | Loss 3.4651
21:41:24.285726 [0] Epoch: 100, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:24.341140 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:24.436642 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:25.472242 [0] Epoch 00101 | Loss 3.4651
21:41:25.493062 [0] Epoch: 101, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:26.422820 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.430337 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:26.825480 [0] Epoch 00102 | Loss 3.4651
21:41:26.846714 [0] Epoch: 102, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:26.901697 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:26.997445 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:28.032759 [0] Epoch 00103 | Loss 3.4651
21:41:28.053469 [0] Epoch: 103, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:28.983382 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:28.990780 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:29.386680 [0] Epoch 00104 | Loss 3.4651
21:41:29.407635 [0] Epoch: 104, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:29.462647 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:29.558330 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:30.595551 [0] Epoch 00105 | Loss 3.4651
21:41:30.616483 [0] Epoch: 105, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:31.546230 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:31.553644 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:31.949674 [0] Epoch 00106 | Loss 3.4651
21:41:31.970466 [0] Epoch: 106, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:32.026260 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:32.121205 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:33.157078 [0] Epoch 00107 | Loss 3.4651
21:41:33.177800 [0] Epoch: 107, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:34.108302 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.115705 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:34.512129 [0] Epoch 00108 | Loss 3.4651
21:41:34.532804 [0] Epoch: 108, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:34.588418 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:34.683807 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:35.718812 [0] Epoch 00109 | Loss 3.4651
21:41:35.740681 [0] Epoch: 109, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:36.670409 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:36.677816 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:37.074502 [0] Epoch 00110 | Loss 3.4651
21:41:37.095067 [0] Epoch: 110, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:37.150336 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:37.246691 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:38.280987 [0] Epoch 00111 | Loss 3.4651
21:41:38.301799 [0] Epoch: 111, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:39.231375 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.238669 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:39.634673 [0] Epoch 00112 | Loss 3.4651
21:41:39.655332 [0] Epoch: 112, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:39.710521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:39.806206 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:40.841420 [0] Epoch 00113 | Loss 3.4651
21:41:40.862492 [0] Epoch: 113, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:41.791812 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:41.799217 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:42.195021 [0] Epoch 00114 | Loss 3.4651
21:41:42.215691 [0] Epoch: 114, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:42.270961 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:42.366629 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:43.401616 [0] Epoch 00115 | Loss 3.4651
21:41:43.422412 [0] Epoch: 115, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:44.352190 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.359605 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:44.756043 [0] Epoch 00116 | Loss 3.4651
21:41:44.776775 [0] Epoch: 116, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:44.832082 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:44.927695 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:45.962805 [0] Epoch 00117 | Loss 3.4651
21:41:45.984729 [0] Epoch: 117, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:46.914699 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:46.922239 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:47.318292 [0] Epoch 00118 | Loss 3.4651
21:41:47.339688 [0] Epoch: 118, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:47.394231 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:47.489968 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:48.526925 [0] Epoch 00119 | Loss 3.4651
21:41:48.547731 [0] Epoch: 119, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:49.477998 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:49.485428 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:49.881936 [0] Epoch 00120 | Loss 3.4651
21:41:49.902835 [0] Epoch: 120, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:49.957707 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:50.053208 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:51.089865 [0] Epoch 00121 | Loss 3.4651
21:41:51.110594 [0] Epoch: 121, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:52.039835 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.047373 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:52.444072 [0] Epoch 00122 | Loss 3.4651
21:41:52.464780 [0] Epoch: 122, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:52.519910 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:52.615666 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:53.652526 [0] Epoch 00123 | Loss 3.4651
21:41:53.673312 [0] Epoch: 123, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:54.603408 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:54.610865 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:55.008020 [0] Epoch 00124 | Loss 3.4651
21:41:55.028961 [0] Epoch: 124, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:55.084615 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:55.179886 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:56.215166 [0] Epoch 00125 | Loss 3.4651
21:41:56.236309 [0] Epoch: 125, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:57.166184 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.173765 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:57.571985 [0] Epoch 00126 | Loss 3.4651
21:41:57.592697 [0] Epoch: 126, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:57.647985 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:57.743684 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:41:58.777924 [0] Epoch 00127 | Loss 3.4651
21:41:58.798768 [0] Epoch: 127, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:41:59.728832 [3] Warning: no training nodes in this partition! Backward fake loss.
21:41:59.736218 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:00.133503 [0] Epoch 00128 | Loss 3.4651
21:42:00.154411 [0] Epoch: 128, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:00.209671 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:00.305466 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:01.340295 [0] Epoch 00129 | Loss 3.4651
21:42:01.361108 [0] Epoch: 129, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:02.299814 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.307422 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:02.714202 [0] Epoch 00130 | Loss 3.4651
21:42:02.735847 [0] Epoch: 130, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:02.792164 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:02.888589 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:03.924217 [0] Epoch 00131 | Loss 3.4651
21:42:03.945025 [0] Epoch: 131, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:04.874731 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:04.882219 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:05.279850 [0] Epoch 00132 | Loss 3.4651
21:42:05.300798 [0] Epoch: 132, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:05.356021 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:05.451670 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:06.489251 [0] Epoch 00133 | Loss 3.4651
21:42:06.510069 [0] Epoch: 133, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:07.440057 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:07.447441 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:07.845484 [0] Epoch 00134 | Loss 3.4651
21:42:07.866481 [0] Epoch: 134, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:07.921641 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:08.017682 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:09.054375 [0] Epoch 00135 | Loss 3.4651
21:42:09.075517 [0] Epoch: 135, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:10.005472 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.013188 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:10.411521 [0] Epoch 00136 | Loss 3.4651
21:42:10.432563 [0] Epoch: 136, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:10.487477 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:10.583476 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:11.619733 [0] Epoch 00137 | Loss 3.4651
21:42:11.640449 [0] Epoch: 137, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:12.570305 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:12.577848 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:12.974002 [0] Epoch 00138 | Loss 3.4651
21:42:12.994877 [0] Epoch: 138, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:13.049777 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:13.145544 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:14.180436 [0] Epoch 00139 | Loss 3.4651
21:42:14.201456 [0] Epoch: 139, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:15.131258 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.138095 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:15.534498 [0] Epoch 00140 | Loss 3.4651
21:42:15.555409 [0] Epoch: 140, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:15.610521 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:15.706094 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:16.741636 [0] Epoch 00141 | Loss 3.4651
21:42:16.762826 [0] Epoch: 141, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:17.692421 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:17.699911 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:18.096180 [0] Epoch 00142 | Loss 3.4651
21:42:18.117032 [0] Epoch: 142, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:18.172147 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:18.267654 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:19.303050 [0] Epoch 00143 | Loss 3.4651
21:42:19.323896 [0] Epoch: 143, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:20.252815 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.260237 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:20.655946 [0] Epoch 00144 | Loss 3.4651
21:42:20.676853 [0] Epoch: 144, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:20.732069 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:20.827291 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:21.862802 [0] Epoch 00145 | Loss 3.4651
21:42:21.883722 [0] Epoch: 145, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:22.812251 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:22.819478 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:23.216252 [0] Epoch 00146 | Loss 3.4651
21:42:23.236824 [0] Epoch: 146, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:23.291983 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:23.387352 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:24.423731 [0] Epoch 00147 | Loss 3.4651
21:42:24.444549 [0] Epoch: 147, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:25.373623 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.380900 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:25.777548 [0] Epoch 00148 | Loss 3.4651
21:42:25.798305 [0] Epoch: 148, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:25.853609 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:25.948911 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:26.983584 [0] Epoch 00149 | Loss 3.4651
21:42:27.004651 [0] Epoch: 149, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:27.934235 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:27.941715 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:28.338066 [0] Epoch 00150 | Loss 3.4651
21:42:28.359072 [0] Epoch: 150, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:28.414420 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:28.509897 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:29.545287 [0] Epoch 00151 | Loss 3.4651
21:42:29.566089 [0] Epoch: 151, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:30.496245 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:30.503713 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:30.899847 [0] Epoch 00152 | Loss 3.4651
21:42:30.920564 [0] Epoch: 152, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:30.975821 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:31.071529 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:32.106679 [0] Epoch 00153 | Loss 3.4651
21:42:32.127596 [0] Epoch: 153, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:33.057400 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.064784 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:33.461105 [0] Epoch 00154 | Loss 3.4651
21:42:33.482185 [0] Epoch: 154, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:33.537536 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:33.633137 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:34.667453 [0] Epoch 00155 | Loss 3.4651
21:42:34.688365 [0] Epoch: 155, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:35.617818 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:35.625130 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:36.021383 [0] Epoch 00156 | Loss 3.4651
21:42:36.042094 [0] Epoch: 156, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:36.097218 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:36.192485 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:37.227443 [0] Epoch 00157 | Loss 3.4651
21:42:37.248264 [0] Epoch: 157, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:38.177214 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.184559 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:38.580584 [0] Epoch 00158 | Loss 3.4651
21:42:38.601249 [0] Epoch: 158, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:38.656406 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:38.751961 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:39.787418 [0] Epoch 00159 | Loss 3.4651
21:42:39.808493 [0] Epoch: 159, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:40.737509 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:40.745097 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:41.140098 [0] Epoch 00160 | Loss 3.4651
21:42:41.161156 [0] Epoch: 160, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:41.216248 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:41.311880 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:42.346181 [0] Epoch 00161 | Loss 3.4651
21:42:42.367230 [0] Epoch: 161, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:43.296101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.303494 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:43.699235 [0] Epoch 00162 | Loss 3.4651
21:42:43.719938 [0] Epoch: 162, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:43.775101 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:43.870446 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:44.905460 [0] Epoch 00163 | Loss 3.4651
21:42:44.926598 [0] Epoch: 163, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:45.854631 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:45.861885 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:46.258081 [0] Epoch 00164 | Loss 3.4651
21:42:46.278829 [0] Epoch: 164, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:46.333955 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:46.429390 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:47.464474 [0] Epoch 00165 | Loss 3.4651
21:42:47.485553 [0] Epoch: 165, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:48.414206 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.421385 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:48.817004 [0] Epoch 00166 | Loss 3.4651
21:42:48.838069 [0] Epoch: 166, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:48.893294 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:48.988631 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:50.023104 [0] Epoch 00167 | Loss 3.4651
21:42:50.044294 [0] Epoch: 167, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:50.973657 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:50.980985 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:51.377235 [0] Epoch 00168 | Loss 3.4651
21:42:51.398321 [0] Epoch: 168, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:51.453401 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:51.548748 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:52.584887 [0] Epoch 00169 | Loss 3.4651
21:42:52.606010 [0] Epoch: 169, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:53.534772 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:53.542095 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:53.937382 [0] Epoch 00170 | Loss 3.4651
21:42:53.958638 [0] Epoch: 170, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:54.013679 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:54.109211 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:55.144856 [0] Epoch 00171 | Loss 3.4651
21:42:55.165553 [0] Epoch: 171, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:56.094383 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.101566 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:56.497417 [0] Epoch 00172 | Loss 3.4651
21:42:56.518295 [0] Epoch: 172, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:56.573193 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:56.668526 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:57.704864 [0] Epoch 00173 | Loss 3.4651
21:42:57.725322 [0] Epoch: 173, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:58.653684 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:58.661041 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:42:59.056925 [0] Epoch 00174 | Loss 3.4651
21:42:59.077749 [0] Epoch: 174, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:42:59.132839 [3] Warning: no training nodes in this partition! Backward fake loss.
21:42:59.228255 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:00.264783 [0] Epoch 00175 | Loss 3.4651
21:43:00.285471 [0] Epoch: 175, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:01.214081 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.221449 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:01.620547 [0] Epoch 00176 | Loss 3.4651
21:43:01.642036 [0] Epoch: 176, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:01.697692 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:01.793723 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:02.840222 [0] Epoch 00177 | Loss 3.4651
21:43:02.861925 [0] Epoch: 177, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:03.791988 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:03.799153 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:04.196866 [0] Epoch 00178 | Loss 3.4651
21:43:04.217884 [0] Epoch: 178, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:04.273031 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:04.368218 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:05.404354 [0] Epoch 00179 | Loss 3.4651
21:43:05.425393 [0] Epoch: 179, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:06.355095 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.362366 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:06.760324 [0] Epoch 00180 | Loss 3.4651
21:43:06.781106 [0] Epoch: 180, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:06.836611 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:06.931645 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:07.967524 [0] Epoch 00181 | Loss 3.4651
21:43:07.988851 [0] Epoch: 181, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:08.918303 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:08.925684 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:09.322024 [0] Epoch 00182 | Loss 3.4651
21:43:09.343237 [0] Epoch: 182, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:09.398811 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:09.493895 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:10.529929 [0] Epoch 00183 | Loss 3.4651
21:43:10.550601 [0] Epoch: 183, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:11.479619 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:11.486736 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:11.883718 [0] Epoch 00184 | Loss 3.4651
21:43:11.904368 [0] Epoch: 184, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:11.959957 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:12.055093 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:13.090793 [0] Epoch 00185 | Loss 3.4651
21:43:13.111676 [0] Epoch: 185, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:14.042604 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.049953 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:14.446838 [0] Epoch 00186 | Loss 3.4651
21:43:14.467529 [0] Epoch: 186, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:14.523185 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:14.618463 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:15.653661 [0] Epoch 00187 | Loss 3.4651
21:43:15.674529 [0] Epoch: 187, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:16.604574 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:16.611851 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:17.007921 [0] Epoch 00188 | Loss 3.4651
21:43:17.029143 [0] Epoch: 188, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:17.084690 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:17.179991 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:18.215436 [0] Epoch 00189 | Loss 3.4651
21:43:18.236672 [0] Epoch: 189, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:19.166476 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.173357 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:19.569639 [0] Epoch 00190 | Loss 3.4651
21:43:19.590395 [0] Epoch: 190, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:19.645828 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:19.740804 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:20.776572 [0] Epoch 00191 | Loss 3.4651
21:43:20.797431 [0] Epoch: 191, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:21.726127 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:21.733442 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:22.129150 [0] Epoch 00192 | Loss 3.4651
21:43:22.150169 [0] Epoch: 192, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:22.205727 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:22.300494 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:23.337982 [0] Epoch 00193 | Loss 3.4651
21:43:23.358551 [0] Epoch: 193, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:24.288051 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.294992 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:24.691141 [0] Epoch 00194 | Loss 3.4651
21:43:24.712249 [0] Epoch: 194, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:24.767596 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:24.862676 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:25.899525 [0] Epoch 00195 | Loss 3.4651
21:43:25.920433 [0] Epoch: 195, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:26.849660 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:26.857146 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:27.254159 [0] Epoch 00196 | Loss 3.4651
21:43:27.274820 [0] Epoch: 196, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:27.330712 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:27.425800 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:28.463188 [0] Epoch 00197 | Loss 3.4651
21:43:28.483904 [0] Epoch: 197, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:29.413072 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.420281 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:29.816538 [0] Epoch 00198 | Loss 3.4651
21:43:29.837420 [0] Epoch: 198, Train: 0.0313, Val: 0.0319, Test: 0.0310
21:43:29.892962 [3] Warning: no training nodes in this partition! Backward fake loss.
21:43:29.988109 [2] Warning: no training nodes in this partition! Backward fake loss.
0.031000
21:43:31.024361 [0] Epoch 00199 | Loss 3.4651
21:43:31.045064 [0] Epoch: 199, Train: 0.0313, Val: 0.0319, Test: 0.0310
0.031000
Rank: 0, local vtx: 500000, local edge: 113001879
Model: CachedGCN layers: 2 dataset: e160M_f1024_l32_t0.5 nprocs 4
21:43:31.049182 [0] 
timer summary:
 16.71s  12.05s     1 broadcast ForwardL1 0
185.61s  93.20s  3200 broadcast
 69.78s  81.26s  3200 spmm
  0.29s   0.09s     1 broadcast ForwardL1 1
  0.26s   0.08s     1 broadcast ForwardL1 2
  0.24s   0.07s     1 broadcast ForwardL1 3
  8.68s   0.09s   800 mm
 52.73s  33.21s   125 broadcast ForwardL2 0
  8.64s   2.68s   125 broadcast ForwardL2 1
  8.19s   2.48s   125 broadcast ForwardL2 2
  7.67s   2.23s   125 broadcast ForwardL2 3
 40.64s  27.90s   200 broadcast BackwardL2 0
  1.43s   0.42s   200 broadcast BackwardL2 1
  1.59s   0.47s   200 broadcast BackwardL2 2
  1.53s   0.47s   200 broadcast BackwardL2 3
  2.54s   1.75s   400 all_reduce
  7.24s   0.24s   200 broadcast BackwardL1 0
 13.55s   4.29s   200 broadcast BackwardL1 1
 12.82s   3.97s   200 broadcast BackwardL1 2
 11.99s   3.58s   200 broadcast BackwardL1 3
278.19s  12.04s   200 epoch
337.24s   0.08s     1 total
Rank: 3, local vtx: 500000, local edge: 2349774
Rank: 2, local vtx: 500000, local edge: 15637930
Rank: 1, local vtx: 500000, local edge: 31008983
