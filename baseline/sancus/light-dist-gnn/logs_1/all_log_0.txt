19:34:08.186567 [0] proc begin: <DistEnv 0/1 nccl>
19:34:25.603839 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
19:34:25.604836 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:34:27.446188 [0] Epoch 00000 | Loss 3.6996
19:34:27.449257 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
19:34:28.174442 [0] Epoch 00001 | Loss 2.8474
19:34:28.897531 [0] Epoch 00002 | Loss 2.2357
19:34:29.618147 [0] Epoch 00003 | Loss 1.8348
19:34:30.337050 [0] Epoch 00004 | Loss 1.5465
19:34:31.056437 [0] Epoch 00005 | Loss 1.3214
19:34:31.775744 [0] Epoch 00006 | Loss 1.1282
19:34:32.495325 [0] Epoch 00007 | Loss 1.0042
19:34:33.214244 [0] Epoch 00008 | Loss 0.8937
19:34:33.934373 [0] Epoch 00009 | Loss 0.7880
19:34:34.654575 [0] Epoch 00010 | Loss 0.7122
19:34:34.656098 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
19:34:35.381486 [0] Epoch 00011 | Loss 0.6693
19:34:36.106614 [0] Epoch 00012 | Loss 0.6227
19:34:36.828021 [0] Epoch 00013 | Loss 0.5815
19:34:37.553792 [0] Epoch 00014 | Loss 0.5569
19:34:38.276213 [0] Epoch 00015 | Loss 0.5312
19:34:39.001798 [0] Epoch 00016 | Loss 0.5067
19:34:39.723928 [0] Epoch 00017 | Loss 0.4871
19:34:40.449528 [0] Epoch 00018 | Loss 0.4698
19:34:41.171673 [0] Epoch 00019 | Loss 0.4566
19:34:41.173151 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
19:57:29.521872 [0] proc begin: <DistEnv 0/1 nccl>
19:57:48.691194 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
19:57:48.692166 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:57:50.440383 [0] Epoch 00000 | Loss 3.6996
19:57:50.443389 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
19:57:51.168445 [0] Epoch 00001 | Loss 2.8474
19:57:51.891992 [0] Epoch 00002 | Loss 2.2357
19:57:52.614238 [0] Epoch 00003 | Loss 1.8348
19:57:53.334593 [0] Epoch 00004 | Loss 1.5465
19:57:54.055656 [0] Epoch 00005 | Loss 1.3214
19:57:54.776286 [0] Epoch 00006 | Loss 1.1282
19:57:55.496436 [0] Epoch 00007 | Loss 1.0042
19:57:56.216352 [0] Epoch 00008 | Loss 0.8937
19:57:56.936550 [0] Epoch 00009 | Loss 0.7880
19:57:57.656613 [0] Epoch 00010 | Loss 0.7122
19:57:57.658116 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
19:57:58.385172 [0] Epoch 00011 | Loss 0.6693
19:57:59.111578 [0] Epoch 00012 | Loss 0.6227
19:57:59.834753 [0] Epoch 00013 | Loss 0.5815
19:58:00.561398 [0] Epoch 00014 | Loss 0.5569
19:58:01.284003 [0] Epoch 00015 | Loss 0.5312
19:58:02.010081 [0] Epoch 00016 | Loss 0.5067
19:58:02.733040 [0] Epoch 00017 | Loss 0.4871
19:58:03.459028 [0] Epoch 00018 | Loss 0.4698
19:58:04.182865 [0] Epoch 00019 | Loss 0.4566
19:58:04.185838 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
19:59:41.118181 [0] proc begin: <DistEnv 0/1 nccl>
20:00:03.039733 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
20:00:03.045345 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:07.283200 [0] Epoch 00000 | Loss 3.6996
20:00:07.311467 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
20:00:08.888808 [0] Epoch 00001 | Loss 2.8474
20:00:10.462241 [0] Epoch 00002 | Loss 2.2357
20:00:11.673280 [0] Epoch 00003 | Loss 1.8348
20:00:12.887515 [0] Epoch 00004 | Loss 1.5465
20:00:14.454646 [0] Epoch 00005 | Loss 1.3214
20:00:16.022303 [0] Epoch 00006 | Loss 1.1282
20:00:17.594888 [0] Epoch 00007 | Loss 1.0042
20:00:18.637822 [0] Epoch 00008 | Loss 0.8937
20:00:19.759507 [0] Epoch 00009 | Loss 0.7880
20:00:21.315948 [0] Epoch 00010 | Loss 0.7122
20:00:21.343943 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
20:00:22.914761 [0] Epoch 00011 | Loss 0.6693
20:00:24.492003 [0] Epoch 00012 | Loss 0.6227
20:00:25.432665 [0] Epoch 00013 | Loss 0.5815
20:00:26.702062 [0] Epoch 00014 | Loss 0.5569
20:00:28.277561 [0] Epoch 00015 | Loss 0.5312
20:00:29.859216 [0] Epoch 00016 | Loss 0.5067
20:00:31.429764 [0] Epoch 00017 | Loss 0.4871
20:00:32.385912 [0] Epoch 00018 | Loss 0.4698
20:00:33.698274 [0] Epoch 00019 | Loss 0.4566
20:00:33.724419 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
20:04:33.173066 [0] proc begin: <DistEnv 0/1 nccl>
20:04:52.761314 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
20:04:52.762254 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:04:54.526281 [0] Epoch 00000 | Loss 3.6996
20:04:54.529333 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
20:04:55.264833 [0] Epoch 00001 | Loss 2.8474
20:04:55.989442 [0] Epoch 00002 | Loss 2.2357
20:04:56.712171 [0] Epoch 00003 | Loss 1.8348
20:04:57.434315 [0] Epoch 00004 | Loss 1.5465
20:04:58.156953 [0] Epoch 00005 | Loss 1.3214
20:04:58.879305 [0] Epoch 00006 | Loss 1.1282
20:04:59.602779 [0] Epoch 00007 | Loss 1.0042
20:05:00.325689 [0] Epoch 00008 | Loss 0.8937
20:05:01.047584 [0] Epoch 00009 | Loss 0.7880
20:05:01.769484 [0] Epoch 00010 | Loss 0.7122
20:05:01.772012 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
20:05:02.498928 [0] Epoch 00011 | Loss 0.6693
20:05:03.224230 [0] Epoch 00012 | Loss 0.6227
20:05:03.947083 [0] Epoch 00013 | Loss 0.5815
20:05:04.672433 [0] Epoch 00014 | Loss 0.5569
20:05:05.394547 [0] Epoch 00015 | Loss 0.5312
20:05:06.119315 [0] Epoch 00016 | Loss 0.5067
20:05:06.842138 [0] Epoch 00017 | Loss 0.4871
20:05:07.567879 [0] Epoch 00018 | Loss 0.4698
20:05:08.290129 [0] Epoch 00019 | Loss 0.4566
20:05:08.292761 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
20:05:08.294165 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.06s  14.06s    80 spmm
  1.00s   1.00s    80 mm
  0.04s   0.04s    40 all_reduce
 15.52s  15.52s    20 epoch
 35.12s  35.12s     1 total
15:56:47.749910 [0] proc begin: <DistEnv 0/1 nccl>
15:57:40.983653 [0] proc begin: <DistEnv 0/1 nccl>
15:58:00.529845 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:58:00.530823 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:58:02.440380 [0] Epoch 00000 | Loss 3.6996
15:58:02.443542 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
15:58:03.168349 [0] Epoch 00001 | Loss 2.8474
15:58:03.892181 [0] Epoch 00002 | Loss 2.2357
15:58:04.614600 [0] Epoch 00003 | Loss 1.8348
15:58:05.336550 [0] Epoch 00004 | Loss 1.5465
15:58:06.059159 [0] Epoch 00005 | Loss 1.3214
15:58:06.782031 [0] Epoch 00006 | Loss 1.1282
15:58:07.505272 [0] Epoch 00007 | Loss 1.0042
15:58:08.227087 [0] Epoch 00008 | Loss 0.8937
15:58:08.949273 [0] Epoch 00009 | Loss 0.7880
15:58:09.671432 [0] Epoch 00010 | Loss 0.7122
15:58:09.674391 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
15:58:10.401498 [0] Epoch 00011 | Loss 0.6693
15:58:11.127229 [0] Epoch 00012 | Loss 0.6227
15:58:11.850216 [0] Epoch 00013 | Loss 0.5815
15:58:12.576075 [0] Epoch 00014 | Loss 0.5569
15:58:13.298520 [0] Epoch 00015 | Loss 0.5312
15:58:14.023660 [0] Epoch 00016 | Loss 0.5067
15:58:14.746864 [0] Epoch 00017 | Loss 0.4871
15:58:15.472299 [0] Epoch 00018 | Loss 0.4698
15:58:16.194706 [0] Epoch 00019 | Loss 0.4566
15:58:16.196270 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
15:58:16.196812 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.14s  14.14s    80 spmm
  1.06s   1.06s    80 mm
  0.04s   0.04s    40 all_reduce
 15.66s  15.66s    20 epoch
 35.21s  35.21s     1 total
16:00:10.787714 [0] proc begin: <DistEnv 0/1 nccl>
16:00:27.951711 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
16:00:27.952681 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:00:29.740456 [0] Epoch 00000 | Loss 3.6996
16:00:29.741984 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
16:00:30.466465 [0] Epoch 00001 | Loss 2.8474
16:00:31.189951 [0] Epoch 00002 | Loss 2.2357
16:00:31.913234 [0] Epoch 00003 | Loss 1.8348
16:00:32.638318 [0] Epoch 00004 | Loss 1.5465
16:00:33.360625 [0] Epoch 00005 | Loss 1.3214
16:00:34.085839 [0] Epoch 00006 | Loss 1.1282
16:00:34.808118 [0] Epoch 00007 | Loss 1.0042
16:00:35.533704 [0] Epoch 00008 | Loss 0.8937
16:00:36.256701 [0] Epoch 00009 | Loss 0.7880
16:00:36.982154 [0] Epoch 00010 | Loss 0.7122
16:00:36.983661 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
16:00:37.708001 [0] Epoch 00011 | Loss 0.6693
16:00:38.432025 [0] Epoch 00012 | Loss 0.6227
16:00:39.156064 [0] Epoch 00013 | Loss 0.5815
16:00:39.880268 [0] Epoch 00014 | Loss 0.5569
16:00:40.604442 [0] Epoch 00015 | Loss 0.5312
16:00:41.330078 [0] Epoch 00016 | Loss 0.5067
16:00:42.054392 [0] Epoch 00017 | Loss 0.4871
16:00:42.778557 [0] Epoch 00018 | Loss 0.4698
16:00:43.502933 [0] Epoch 00019 | Loss 0.4566
16:00:43.504439 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
16:00:43.504937 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.08s  14.08s    80 spmm
  1.02s   1.02s    80 mm
  0.04s   0.04s    40 all_reduce
 15.55s  15.55s    20 epoch
 32.72s  32.72s     1 total
16:05:19.825960 [0] proc begin: <DistEnv 0/1 nccl>
16:05:40.632227 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
16:05:40.633595 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:05:42.407670 [0] Epoch 00000 | Loss 3.9519
16:05:42.411480 [0] Epoch: 000, Train: 0.0153, Val: 0.0172, Test: 0.0171
16:05:43.144640 [0] Epoch 00001 | Loss 3.4300
16:05:43.875613 [0] Epoch 00002 | Loss 3.2872
16:05:44.605841 [0] Epoch 00003 | Loss 3.1548
16:05:45.334591 [0] Epoch 00004 | Loss 3.0393
16:05:46.102855 [0] Epoch 00005 | Loss 2.9375
16:05:46.892715 [0] Epoch 00006 | Loss 2.8440
16:05:47.682604 [0] Epoch 00007 | Loss 2.7496
16:05:48.439362 [0] Epoch 00008 | Loss 2.6465
16:05:49.169278 [0] Epoch 00009 | Loss 2.5331
16:05:49.900984 [0] Epoch 00010 | Loss 2.4058
16:05:49.904196 [0] Epoch: 010, Train: 0.4937, Val: 0.4934, Test: 0.4972
16:05:50.637153 [0] Epoch 00011 | Loss 2.2638
16:05:51.370991 [0] Epoch 00012 | Loss 2.1029
16:05:52.101861 [0] Epoch 00013 | Loss 1.9344
16:05:52.835123 [0] Epoch 00014 | Loss 1.7727
16:05:53.563945 [0] Epoch 00015 | Loss 1.6292
16:05:54.297043 [0] Epoch 00016 | Loss 1.5056
16:05:55.029270 [0] Epoch 00017 | Loss 1.4025
16:05:55.763380 [0] Epoch 00018 | Loss 1.3029
16:05:56.493262 [0] Epoch 00019 | Loss 1.2051
16:05:56.496299 [0] Epoch: 019, Train: 0.7188, Val: 0.7150, Test: 0.7112
16:05:56.498161 [0] 
timer summary:
  0.08s   0.08s    20 broadcast ForwardL1 0
  0.09s   0.09s    80 broadcast
 14.36s  14.36s    80 spmm
  1.01s   1.01s    80 mm
  0.00s   0.00s    20 broadcast ForwardL2 0
  0.00s   0.00s    20 broadcast BackwardL2 0
  0.03s   0.03s    40 all_reduce
  0.00s   0.00s    20 broadcast BackwardL1 0
 15.85s  15.85s    20 epoch
 36.67s  36.67s     1 total
16:19:11.208431 [0] proc begin: <DistEnv 0/1 nccl>
16:19:29.134833 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
16:19:29.138750 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:23:54.190751 [0] proc begin: <DistEnv 0/1 nccl>
16:24:32.460887 [0] proc begin: <DistEnv 0/1 nccl>
16:25:11.421109 [0] proc begin: <DistEnv 0/1 nccl>
16:25:56.160034 [0] proc begin: <DistEnv 0/1 nccl>
16:26:16.077820 [0] proc begin: <DistEnv 0/1 nccl>
16:26:39.743931 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
16:26:39.745484 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:27:16.557198 [0] proc begin: <DistEnv 0/1 nccl>
16:27:17.972122 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:27:17.972958 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  108544 KB |  108544 KB |  108544 KB |       0 B  |
|       from large pool |  106496 KB |  106496 KB |  106496 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   22053 KB |   28308 KB |   17352 KB |
|       from large pool |   10066 KB |   20501 KB |   24535 KB |   14468 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       3    |       3    |       3    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       3    |       7    |       4    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:32:33.482519 [0] proc begin: <DistEnv 0/1 nccl>
16:34:00.434511 [0] proc begin: <DistEnv 0/1 nccl>
16:34:01.833684 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:34:01.834532 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  108544 KB |  108544 KB |  108544 KB |       0 B  |
|       from large pool |  106496 KB |  106496 KB |  106496 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   22053 KB |   28308 KB |   17352 KB |
|       from large pool |   10066 KB |   20501 KB |   24535 KB |   14468 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       3    |       3    |       3    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       3    |       7    |       4    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:34:37.194787 [0] proc begin: <DistEnv 0/1 nccl>
16:34:38.677437 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:34:38.678412 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |   98933 KB |  101795 KB |    4207 KB |
|       from large pool |   96429 KB |   97774 KB |  100463 KB |    4033 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  108544 KB |  108544 KB |  108544 KB |       0 B  |
|       from large pool |  106496 KB |  106496 KB |  106496 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   22053 KB |   28308 KB |   17352 KB |
|       from large pool |   10066 KB |   20501 KB |   24535 KB |   14468 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       3    |       3    |       3    |       0    |
|       from large pool |       2    |       2    |       2    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       3    |       7    |       4    |
|       from large pool |       2    |       2    |       5    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:37:21.958505 [0] proc begin: <DistEnv 0/1 nccl>
16:37:23.360185 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:37:23.361020 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:37:24.404656 [0] Epoch 00000 | Loss 7.9886
16:37:24.406007 [0] Epoch: 000, Train: 0.0038, Val: 0.0034, Test: 0.0033
16:37:24.426765 [0] Epoch 00001 | Loss 5.0639
16:37:24.449043 [0] Epoch 00002 | Loss 5.0382
16:37:24.470919 [0] Epoch 00003 | Loss 5.6174
16:37:24.491656 [0] Epoch 00004 | Loss 6.3291
16:37:24.512595 [0] Epoch 00005 | Loss 6.6797
16:37:24.535998 [0] Epoch 00006 | Loss 6.9157
16:37:24.558396 [0] Epoch 00007 | Loss 7.4016
16:37:24.579305 [0] Epoch 00008 | Loss 8.0148
16:37:24.601242 [0] Epoch 00009 | Loss 8.3497
16:37:24.625220 [0] Epoch 00010 | Loss 8.6306
16:37:24.627481 [0] Epoch: 010, Train: 0.2779, Val: 0.3248, Test: 0.2978
16:37:24.649506 [0] Epoch 00011 | Loss 8.9669
16:37:24.672323 [0] Epoch 00012 | Loss 9.2917
16:37:24.694412 [0] Epoch 00013 | Loss 9.5622
16:37:24.715162 [0] Epoch 00014 | Loss 9.7409
16:37:24.736980 [0] Epoch 00015 | Loss 9.8547
16:37:24.770535 [0] Epoch 00016 | Loss 9.9943
16:37:24.791903 [0] Epoch 00017 | Loss 10.2060
16:37:24.813225 [0] Epoch 00018 | Loss 10.4067
16:37:24.836340 [0] Epoch 00019 | Loss 10.5208
16:37:24.838565 [0] Epoch: 019, Train: 0.3090, Val: 0.3515, Test: 0.3351
16:37:24.839640 [0] 
timer summary:
  0.08s   0.08s    20 broadcast ForwardL1 0
  0.09s   0.09s    80 broadcast
  0.36s   0.36s    80 spmm
  0.78s   0.78s    80 mm
  0.00s   0.00s    20 broadcast ForwardL2 0
  0.00s   0.00s    20 broadcast BackwardL2 0
  0.04s   0.04s    40 all_reduce
  0.00s   0.00s    20 broadcast BackwardL1 0
  1.47s   1.47s    20 epoch
  2.88s   2.88s     1 total
10:43:25.607860 [0] proc begin: <DistEnv 0/1 nccl>
10:43:28.864622 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
10:43:28.871596 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:44:11.038083 [0] proc begin: <DistEnv 0/1 nccl>
10:44:12.477168 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
10:44:12.478062 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:47:58.493890 [0] proc begin: <DistEnv 0/1 nccl>
10:47:59.753813 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
10:47:59.754648 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:13:15.350425 [0] proc begin: <DistEnv 0/1 nccl>
11:13:18.101722 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:13:18.107247 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:16:47.875778 [0] proc begin: <DistEnv 0/1 nccl>
11:16:49.345156 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:16:49.346004 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:18:17.079055 [0] proc begin: <DistEnv 0/1 nccl>
11:18:18.566059 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:18:18.566986 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:19:09.497094 [0] proc begin: <DistEnv 0/1 nccl>
11:19:10.986726 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:19:10.987616 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:20:05.982715 [0] proc begin: <DistEnv 0/1 nccl>
11:20:07.468434 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:20:07.469326 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:22:43.555022 [0] proc begin: <DistEnv 0/1 nccl>
11:22:45.032769 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:22:45.033594 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:27:26.850677 [0] proc begin: <DistEnv 0/1 nccl>
16:27:28.703371 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:27:28.710757 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:27:28.722118 [0] L1 tensor(16460.7832, device='cuda:0', grad_fn=<SumBackward0>) tensor(253.2845, device='cuda:0', grad_fn=<SumBackward0>)
16:40:31.689214 [0] proc begin: <DistEnv 0/1 nccl>
16:40:33.161116 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:40:33.162206 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:40:33.164571 [0] L1 tensor(16460.7832, device='cuda:0', grad_fn=<SumBackward0>) tensor(253.2845, device='cuda:0', grad_fn=<SumBackward0>)
16:41:04.318031 [0] proc begin: <DistEnv 0/1 nccl>
16:41:05.749885 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:41:05.750753 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:41:05.752627 [0] L1 tensor(16460.7832, device='cuda:0', grad_fn=<SumBackward0>) tensor(253.2845, device='cuda:0', grad_fn=<SumBackward0>)
16:48:57.896907 [0] proc begin: <DistEnv 0/1 nccl>
16:48:59.342733 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:48:59.343622 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:48:59.345308 [0] L1 tensor(16460.7832, device='cuda:0', grad_fn=<SumBackward0>) tensor(253.2845, device='cuda:0', grad_fn=<SumBackward0>)
16:54:17.687523 [0] proc begin: <DistEnv 0/1 nccl>
16:56:28.078075 [0] proc begin: <DistEnv 0/1 nccl>
16:56:29.288466 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
16:56:29.289359 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:56:29.297703 [0] L1 tensor(183542.5938, device='cuda:0', grad_fn=<SumBackward0>) tensor(258.8653, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.458525 [0] Epoch 00000 | Loss 3.8037
16:56:30.459658 [0] Epoch: 000, Train: 0.1429, Val: 0.1540, Test: 0.1430
16:56:30.459888 [0] L1 tensor(183461.1562, device='cuda:0', grad_fn=<SumBackward0>) tensor(260.8840, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.472634 [0] Epoch 00001 | Loss 2.3597
16:56:30.472981 [0] L1 tensor(183267.0312, device='cuda:0', grad_fn=<SumBackward0>) tensor(261.8731, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.485138 [0] Epoch 00002 | Loss 2.0139
16:56:30.485369 [0] L1 tensor(182955.4844, device='cuda:0', grad_fn=<SumBackward0>) tensor(263.0002, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.495018 [0] Epoch 00003 | Loss 1.6815
16:56:30.495259 [0] L1 tensor(182516.9531, device='cuda:0', grad_fn=<SumBackward0>) tensor(264.4781, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.504535 [0] Epoch 00004 | Loss 1.6928
16:56:30.504784 [0] L1 tensor(181995.4531, device='cuda:0', grad_fn=<SumBackward0>) tensor(265.8781, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.514967 [0] Epoch 00005 | Loss 1.5629
16:56:30.515218 [0] L1 tensor(181443.8438, device='cuda:0', grad_fn=<SumBackward0>) tensor(266.8669, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.525222 [0] Epoch 00006 | Loss 1.2371
16:56:30.525436 [0] L1 tensor(180893.6250, device='cuda:0', grad_fn=<SumBackward0>) tensor(268.3044, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.534781 [0] Epoch 00007 | Loss 0.9646
16:56:30.535023 [0] L1 tensor(180345.2812, device='cuda:0', grad_fn=<SumBackward0>) tensor(269.1888, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.544804 [0] Epoch 00008 | Loss 0.7755
16:56:30.545017 [0] L1 tensor(179814.1875, device='cuda:0', grad_fn=<SumBackward0>) tensor(270.9081, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.554521 [0] Epoch 00009 | Loss 0.6426
16:56:30.554777 [0] L1 tensor(179297., device='cuda:0', grad_fn=<SumBackward0>) tensor(272.7711, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.565380 [0] Epoch 00010 | Loss 0.5616
16:56:30.566249 [0] Epoch: 010, Train: 0.9214, Val: 0.5900, Test: 0.5990
16:56:30.566425 [0] L1 tensor(178789.5781, device='cuda:0', grad_fn=<SumBackward0>) tensor(274.7384, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.576271 [0] Epoch 00011 | Loss 0.4984
16:56:30.576501 [0] L1 tensor(178296.6562, device='cuda:0', grad_fn=<SumBackward0>) tensor(276.5874, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.586484 [0] Epoch 00012 | Loss 0.4254
16:56:30.586720 [0] L1 tensor(177820.1562, device='cuda:0', grad_fn=<SumBackward0>) tensor(278.1270, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.596420 [0] Epoch 00013 | Loss 0.3536
16:56:30.596632 [0] L1 tensor(177371.9531, device='cuda:0', grad_fn=<SumBackward0>) tensor(279.4996, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.606282 [0] Epoch 00014 | Loss 0.2965
16:56:30.606524 [0] L1 tensor(176948.7344, device='cuda:0', grad_fn=<SumBackward0>) tensor(280.7299, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.616431 [0] Epoch 00015 | Loss 0.2582
16:56:30.616645 [0] L1 tensor(176548.5625, device='cuda:0', grad_fn=<SumBackward0>) tensor(281.8299, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.626110 [0] Epoch 00016 | Loss 0.2321
16:56:30.626343 [0] L1 tensor(176169.4375, device='cuda:0', grad_fn=<SumBackward0>) tensor(282.9438, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.636669 [0] Epoch 00017 | Loss 0.2091
16:56:30.636920 [0] L1 tensor(175820.3281, device='cuda:0', grad_fn=<SumBackward0>) tensor(284.0013, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.646673 [0] Epoch 00018 | Loss 0.1872
16:56:30.646933 [0] L1 tensor(175504.1250, device='cuda:0', grad_fn=<SumBackward0>) tensor(285.0097, device='cuda:0', grad_fn=<SumBackward0>)
16:56:30.659323 [0] Epoch 00019 | Loss 0.1626
16:56:30.661156 [0] Epoch: 019, Train: 0.9714, Val: 0.7060, Test: 0.6820
16:56:30.662106 [0] 
timer summary:
  1.36s   1.36s    20 epoch
  2.58s   2.58s     1 total
17:00:02.634761 [0] proc begin: <DistEnv 0/1 nccl>
17:00:22.374302 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
17:00:22.375330 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:00:22.378739 [0] L1 tensor(77137.2891, device='cuda:0', grad_fn=<SumBackward0>) tensor(256.2829, device='cuda:0', grad_fn=<SumBackward0>)
09:55:19.466167 [0] proc begin: <DistEnv 0/1 nccl>
09:55:20.846436 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
09:55:20.848231 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:55:20.858931 [0] L1 tensor(183542.5938, device='cuda:0', grad_fn=<SumBackward0>) tensor(258.8653, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.308831 [0] Epoch 00000 | Loss 3.8037
09:55:22.309921 [0] Epoch: 000, Train: 0.1429, Val: 0.1540, Test: 0.1430
09:55:22.310131 [0] L1 tensor(183461.1562, device='cuda:0', grad_fn=<SumBackward0>) tensor(260.8840, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.320145 [0] Epoch 00001 | Loss 2.3597
09:55:22.320368 [0] L1 tensor(183267.0312, device='cuda:0', grad_fn=<SumBackward0>) tensor(261.8731, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.330415 [0] Epoch 00002 | Loss 2.0139
09:55:22.330657 [0] L1 tensor(182955.4844, device='cuda:0', grad_fn=<SumBackward0>) tensor(263.0002, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.341146 [0] Epoch 00003 | Loss 1.6815
09:55:22.341372 [0] L1 tensor(182516.9531, device='cuda:0', grad_fn=<SumBackward0>) tensor(264.4781, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.351708 [0] Epoch 00004 | Loss 1.6928
09:55:22.351927 [0] L1 tensor(181995.4531, device='cuda:0', grad_fn=<SumBackward0>) tensor(265.8781, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.361742 [0] Epoch 00005 | Loss 1.5629
09:55:22.361942 [0] L1 tensor(181443.8438, device='cuda:0', grad_fn=<SumBackward0>) tensor(266.8669, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.371110 [0] Epoch 00006 | Loss 1.2371
09:55:22.371323 [0] L1 tensor(180893.6250, device='cuda:0', grad_fn=<SumBackward0>) tensor(268.3044, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.380838 [0] Epoch 00007 | Loss 0.9646
09:55:22.381042 [0] L1 tensor(180345.2812, device='cuda:0', grad_fn=<SumBackward0>) tensor(269.1888, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.390484 [0] Epoch 00008 | Loss 0.7755
09:55:22.390722 [0] L1 tensor(179814.1875, device='cuda:0', grad_fn=<SumBackward0>) tensor(270.9081, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.401132 [0] Epoch 00009 | Loss 0.6426
09:55:22.401350 [0] L1 tensor(179297., device='cuda:0', grad_fn=<SumBackward0>) tensor(272.7711, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.411991 [0] Epoch 00010 | Loss 0.5616
09:55:22.413037 [0] Epoch: 010, Train: 0.9214, Val: 0.5900, Test: 0.5990
09:55:22.413246 [0] L1 tensor(178789.5781, device='cuda:0', grad_fn=<SumBackward0>) tensor(274.7384, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.423706 [0] Epoch 00011 | Loss 0.4984
09:55:22.423912 [0] L1 tensor(178296.6562, device='cuda:0', grad_fn=<SumBackward0>) tensor(276.5874, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.433426 [0] Epoch 00012 | Loss 0.4254
09:55:22.433630 [0] L1 tensor(177820.1562, device='cuda:0', grad_fn=<SumBackward0>) tensor(278.1270, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.443295 [0] Epoch 00013 | Loss 0.3536
09:55:22.443495 [0] L1 tensor(177371.9531, device='cuda:0', grad_fn=<SumBackward0>) tensor(279.4996, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.452858 [0] Epoch 00014 | Loss 0.2965
09:55:22.453075 [0] L1 tensor(176948.7344, device='cuda:0', grad_fn=<SumBackward0>) tensor(280.7299, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.462790 [0] Epoch 00015 | Loss 0.2582
09:55:22.462999 [0] L1 tensor(176548.5625, device='cuda:0', grad_fn=<SumBackward0>) tensor(281.8299, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.472343 [0] Epoch 00016 | Loss 0.2321
09:55:22.472560 [0] L1 tensor(176169.4375, device='cuda:0', grad_fn=<SumBackward0>) tensor(282.9438, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.485640 [0] Epoch 00017 | Loss 0.2091
09:55:22.485941 [0] L1 tensor(175820.3281, device='cuda:0', grad_fn=<SumBackward0>) tensor(284.0013, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.497601 [0] Epoch 00018 | Loss 0.1872
09:55:22.497819 [0] L1 tensor(175504.1250, device='cuda:0', grad_fn=<SumBackward0>) tensor(285.0097, device='cuda:0', grad_fn=<SumBackward0>)
09:55:22.507161 [0] Epoch 00019 | Loss 0.1626
09:55:22.508189 [0] Epoch: 019, Train: 0.9714, Val: 0.7060, Test: 0.6820
09:55:22.508770 [0] 
timer summary:
  1.65s   1.65s    20 epoch
  3.04s   3.04s     1 total
09:58:42.746593 [0] proc begin: <DistEnv 0/1 nccl>
09:58:43.828455 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
09:58:43.829210 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:59:35.391594 [0] proc begin: <DistEnv 0/1 nccl>
09:59:36.610652 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
09:59:36.611565 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:59:37.958483 [0] Epoch 00000 | Loss 1.9460
09:59:37.959666 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
09:59:37.963460 [0] Epoch 00001 | Loss 1.9093
09:59:37.966773 [0] Epoch 00002 | Loss 1.8631
09:59:37.970107 [0] Epoch 00003 | Loss 1.8043
09:59:37.973345 [0] Epoch 00004 | Loss 1.7353
09:59:37.978662 [0] Epoch 00005 | Loss 1.6568
09:59:37.983153 [0] Epoch 00006 | Loss 1.5686
09:59:37.986552 [0] Epoch 00007 | Loss 1.4710
09:59:37.989718 [0] Epoch 00008 | Loss 1.3651
09:59:37.992794 [0] Epoch 00009 | Loss 1.2527
09:59:37.995783 [0] Epoch 00010 | Loss 1.1357
09:59:37.996630 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
09:59:37.999675 [0] Epoch 00011 | Loss 1.0166
09:59:38.002723 [0] Epoch 00012 | Loss 0.8983
09:59:38.005698 [0] Epoch 00013 | Loss 0.7835
09:59:38.008707 [0] Epoch 00014 | Loss 0.6750
09:59:38.011709 [0] Epoch 00015 | Loss 0.5752
09:59:38.014816 [0] Epoch 00016 | Loss 0.4855
09:59:38.017947 [0] Epoch 00017 | Loss 0.4068
09:59:38.021061 [0] Epoch 00018 | Loss 0.3391
09:59:38.024070 [0] Epoch 00019 | Loss 0.2818
09:59:38.024949 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
09:59:38.025424 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.33s   0.33s    80 spmm
  0.91s   0.91s    80 mm
  0.04s   0.04s    40 all_reduce
  1.41s   1.41s    20 epoch
  2.63s   2.63s     1 total
10:04:29.107879 [0] proc begin: <DistEnv 0/1 nccl>
10:04:30.326040 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:04:30.326993 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:04:31.399915 [0] Epoch 00000 | Loss 1.9460
10:04:31.400975 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:04:31.405396 [0] Epoch 00001 | Loss 1.9093
10:04:31.408750 [0] Epoch 00002 | Loss 1.8631
10:04:31.412249 [0] Epoch 00003 | Loss 1.8043
10:04:31.415415 [0] Epoch 00004 | Loss 1.7353
10:04:31.419226 [0] Epoch 00005 | Loss 1.6568
10:04:31.422938 [0] Epoch 00006 | Loss 1.5686
10:04:31.426900 [0] Epoch 00007 | Loss 1.4710
10:04:31.430077 [0] Epoch 00008 | Loss 1.3651
10:04:31.433305 [0] Epoch 00009 | Loss 1.2527
10:04:31.437242 [0] Epoch 00010 | Loss 1.1357
10:04:31.438226 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:04:31.441959 [0] Epoch 00011 | Loss 1.0166
10:04:31.445139 [0] Epoch 00012 | Loss 0.8983
10:04:31.448741 [0] Epoch 00013 | Loss 0.7835
10:04:31.452196 [0] Epoch 00014 | Loss 0.6750
10:04:31.455359 [0] Epoch 00015 | Loss 0.5752
10:04:31.459027 [0] Epoch 00016 | Loss 0.4855
10:04:31.462883 [0] Epoch 00017 | Loss 0.4068
10:04:31.466831 [0] Epoch 00018 | Loss 0.3391
10:04:31.470259 [0] Epoch 00019 | Loss 0.2818
10:04:31.471181 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:04:31.471678 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.26s   0.26s    80 spmm
  0.72s   0.72s    80 mm
  0.04s   0.04s    40 all_reduce
  1.14s   1.14s    20 epoch
  2.36s   2.36s     1 total
10:12:17.961774 [0] proc begin: <DistEnv 0/1 nccl>
10:12:18.973154 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:12:18.973932 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:12:20.036816 [0] Epoch 00000 | Loss 1.9460
10:12:20.037763 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:12:20.041346 [0] Epoch 00001 | Loss 1.9093
10:12:20.044523 [0] Epoch 00002 | Loss 1.8631
10:12:20.048047 [0] Epoch 00003 | Loss 1.8043
10:12:20.051230 [0] Epoch 00004 | Loss 1.7353
10:12:20.054443 [0] Epoch 00005 | Loss 1.6568
10:12:20.057338 [0] Epoch 00006 | Loss 1.5686
10:12:20.060694 [0] Epoch 00007 | Loss 1.4710
10:12:20.064111 [0] Epoch 00008 | Loss 1.3651
10:12:20.067340 [0] Epoch 00009 | Loss 1.2527
10:12:20.070513 [0] Epoch 00010 | Loss 1.1357
10:12:20.071480 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:12:20.074825 [0] Epoch 00011 | Loss 1.0166
10:12:20.078292 [0] Epoch 00012 | Loss 0.8983
10:12:20.081464 [0] Epoch 00013 | Loss 0.7835
10:12:20.084676 [0] Epoch 00014 | Loss 0.6750
10:12:20.087866 [0] Epoch 00015 | Loss 0.5752
10:12:20.090893 [0] Epoch 00016 | Loss 0.4855
10:12:20.093896 [0] Epoch 00017 | Loss 0.4068
10:12:20.097006 [0] Epoch 00018 | Loss 0.3391
10:12:20.100179 [0] Epoch 00019 | Loss 0.2818
10:12:20.101102 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:12:20.101630 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.25s   0.25s    80 spmm
  0.71s   0.71s    80 mm
  0.04s   0.04s    40 all_reduce
  1.12s   1.12s    20 epoch
  2.14s   2.14s     1 total
10:32:10.775396 [0] proc begin: <DistEnv 0/1 nccl>
10:32:11.936324 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:32:11.937097 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:32:13.290708 [0] Epoch 00000 | Loss 1.9460
10:32:13.292206 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:32:13.296144 [0] Epoch 00001 | Loss 1.9093
10:32:13.299605 [0] Epoch 00002 | Loss 1.8631
10:32:13.303085 [0] Epoch 00003 | Loss 1.8043
10:32:13.306514 [0] Epoch 00004 | Loss 1.7353
10:32:13.309600 [0] Epoch 00005 | Loss 1.6568
10:32:13.312559 [0] Epoch 00006 | Loss 1.5686
10:32:13.315579 [0] Epoch 00007 | Loss 1.4710
10:32:13.318644 [0] Epoch 00008 | Loss 1.3651
10:32:13.321628 [0] Epoch 00009 | Loss 1.2527
10:32:13.324564 [0] Epoch 00010 | Loss 1.1357
10:32:13.325478 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:32:13.328406 [0] Epoch 00011 | Loss 1.0166
10:32:13.331357 [0] Epoch 00012 | Loss 0.8983
10:32:13.334289 [0] Epoch 00013 | Loss 0.7835
10:32:13.337307 [0] Epoch 00014 | Loss 0.6750
10:32:13.340376 [0] Epoch 00015 | Loss 0.5752
10:32:13.343358 [0] Epoch 00016 | Loss 0.4855
10:32:13.346281 [0] Epoch 00017 | Loss 0.4068
10:32:13.349234 [0] Epoch 00018 | Loss 0.3391
10:32:13.352223 [0] Epoch 00019 | Loss 0.2818
10:32:13.353156 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:32:13.353807 [0] 
timer summary:
  0.07s   0.07s    80 broadcast
  0.33s   0.33s    80 spmm
  0.94s   0.94s    80 mm
  0.03s   0.03s    40 all_reduce
  1.41s   1.41s    20 epoch
  2.58s   2.58s     1 total
10:32:54.755163 [0] proc begin: <DistEnv 0/1 nccl>
10:32:55.945230 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:32:55.946127 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:32:57.105077 [0] Epoch 00000 | Loss 1.9460
10:32:57.107768 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:32:57.114833 [0] Epoch 00001 | Loss 1.9093
10:32:57.118790 [0] Epoch 00002 | Loss 1.8631
10:32:57.121896 [0] Epoch 00003 | Loss 1.8043
10:32:57.124874 [0] Epoch 00004 | Loss 1.7353
10:32:57.127796 [0] Epoch 00005 | Loss 1.6568
10:32:57.130726 [0] Epoch 00006 | Loss 1.5686
10:32:57.133560 [0] Epoch 00007 | Loss 1.4710
10:32:57.136393 [0] Epoch 00008 | Loss 1.3651
10:32:57.139222 [0] Epoch 00009 | Loss 1.2527
10:32:57.142019 [0] Epoch 00010 | Loss 1.1357
10:32:57.142906 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:32:57.145746 [0] Epoch 00011 | Loss 1.0166
10:32:57.148567 [0] Epoch 00012 | Loss 0.8983
10:32:57.151321 [0] Epoch 00013 | Loss 0.7835
10:32:57.154059 [0] Epoch 00014 | Loss 0.6750
10:32:57.156862 [0] Epoch 00015 | Loss 0.5752
10:32:57.159732 [0] Epoch 00016 | Loss 0.4855
10:32:57.162546 [0] Epoch 00017 | Loss 0.4068
10:32:57.165430 [0] Epoch 00018 | Loss 0.3391
10:32:57.168280 [0] Epoch 00019 | Loss 0.2818
10:32:57.169169 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:32:57.169666 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.33s   0.33s    80 spmm
  0.72s   0.72s    80 mm
  0.04s   0.04s    40 all_reduce
  1.21s   1.21s    20 epoch
  2.41s   2.41s     1 total
10:33:22.597456 [0] proc begin: <DistEnv 0/1 nccl>
10:33:23.687933 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:33:23.688695 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:33:24.740877 [0] Epoch 00000 | Loss 1.9460
10:33:24.741942 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:33:24.745400 [0] Epoch 00001 | Loss 1.9093
10:33:24.748162 [0] Epoch 00002 | Loss 1.8631
10:33:24.750913 [0] Epoch 00003 | Loss 1.8043
10:33:24.753632 [0] Epoch 00004 | Loss 1.7353
10:33:24.756722 [0] Epoch 00005 | Loss 1.6568
10:33:24.759584 [0] Epoch 00006 | Loss 1.5686
10:33:24.762488 [0] Epoch 00007 | Loss 1.4710
10:33:24.765319 [0] Epoch 00008 | Loss 1.3651
10:33:24.768411 [0] Epoch 00009 | Loss 1.2527
10:33:24.771259 [0] Epoch 00010 | Loss 1.1357
10:33:24.772208 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:33:24.775104 [0] Epoch 00011 | Loss 1.0166
10:33:24.778022 [0] Epoch 00012 | Loss 0.8983
10:33:24.780916 [0] Epoch 00013 | Loss 0.7835
10:33:24.783792 [0] Epoch 00014 | Loss 0.6750
10:33:24.786736 [0] Epoch 00015 | Loss 0.5752
10:33:24.789595 [0] Epoch 00016 | Loss 0.4855
10:33:24.792519 [0] Epoch 00017 | Loss 0.4068
10:33:24.795321 [0] Epoch 00018 | Loss 0.3391
10:33:24.798252 [0] Epoch 00019 | Loss 0.2818
10:33:24.799137 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:33:24.799628 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.25s   0.25s    80 spmm
  0.70s   0.70s    80 mm
  0.03s   0.03s    40 all_reduce
  1.10s   1.10s    20 epoch
  2.20s   2.20s     1 total
10:33:42.986169 [0] proc begin: <DistEnv 0/1 nccl>
10:33:44.194003 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:33:44.194900 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:57:07.082795 [0] proc begin: <DistEnv 0/1 nccl>
10:57:08.233826 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:57:08.234745 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:57:38.625906 [0] proc begin: <DistEnv 0/1 nccl>
10:57:39.798156 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:57:39.799041 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:57:41.094784 [0] Epoch 00000 | Loss 1.9460
10:57:41.095962 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:57:41.099581 [0] Epoch 00001 | Loss 1.9093
10:57:41.102523 [0] Epoch 00002 | Loss 1.8631
10:57:41.105522 [0] Epoch 00003 | Loss 1.8043
10:57:41.108485 [0] Epoch 00004 | Loss 1.7353
10:57:41.111786 [0] Epoch 00005 | Loss 1.6568
10:57:41.114617 [0] Epoch 00006 | Loss 1.5686
10:57:41.117726 [0] Epoch 00007 | Loss 1.4710
10:57:41.120567 [0] Epoch 00008 | Loss 1.3651
10:57:41.123676 [0] Epoch 00009 | Loss 1.2527
10:57:41.126446 [0] Epoch 00010 | Loss 1.1357
10:57:41.127320 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:57:41.130293 [0] Epoch 00011 | Loss 1.0166
10:57:41.133591 [0] Epoch 00012 | Loss 0.8983
10:57:41.136596 [0] Epoch 00013 | Loss 0.7835
10:57:41.139467 [0] Epoch 00014 | Loss 0.6750
10:57:41.142473 [0] Epoch 00015 | Loss 0.5752
10:57:41.145304 [0] Epoch 00016 | Loss 0.4855
10:57:41.148854 [0] Epoch 00017 | Loss 0.4068
10:57:41.153735 [0] Epoch 00018 | Loss 0.3391
10:57:41.158541 [0] Epoch 00019 | Loss 0.2818
10:57:41.159426 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:57:41.159907 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.31s   0.31s    80 spmm
  0.89s   0.89s    80 mm
  0.04s   0.04s    40 all_reduce
  1.35s   1.35s    20 epoch
  2.53s   2.53s     1 total
11:24:29.603372 [0] proc begin: <DistEnv 0/1 nccl>
11:24:30.853401 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
11:24:30.854341 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:25:02.175731 [0] proc begin: <DistEnv 0/1 nccl>
11:25:03.403926 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
11:25:03.404834 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:26:08.239230 [0] proc begin: <DistEnv 0/1 nccl>
11:26:09.442210 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
11:26:09.443137 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:26:10.741967 [0] Epoch 00000 | Loss 1.9460
11:26:10.743129 [0] Epoch: 000, Train: 0.1000, Val: 0.1960, Test: 0.2040
11:26:10.746258 [0] Epoch 00001 | Loss 1.9092
11:26:10.748928 [0] Epoch 00002 | Loss 1.8670
11:26:10.751490 [0] Epoch 00003 | Loss 1.8128
11:26:10.754080 [0] Epoch 00004 | Loss 1.7471
11:26:10.756754 [0] Epoch 00005 | Loss 1.6714
11:26:10.759444 [0] Epoch 00006 | Loss 1.5865
11:26:10.762205 [0] Epoch 00007 | Loss 1.4926
11:26:10.764957 [0] Epoch 00008 | Loss 1.3909
11:26:10.767697 [0] Epoch 00009 | Loss 1.2824
11:26:10.770304 [0] Epoch 00010 | Loss 1.1690
11:26:10.771169 [0] Epoch: 010, Train: 0.9786, Val: 0.7860, Test: 0.8010
11:26:10.773844 [0] Epoch 00011 | Loss 1.0530
11:26:10.776508 [0] Epoch 00012 | Loss 0.9370
11:26:10.779127 [0] Epoch 00013 | Loss 0.8237
11:26:10.781748 [0] Epoch 00014 | Loss 0.7157
11:26:10.784426 [0] Epoch 00015 | Loss 0.6153
11:26:10.787135 [0] Epoch 00016 | Loss 0.5240
11:26:10.789720 [0] Epoch 00017 | Loss 0.4428
11:26:10.792302 [0] Epoch 00018 | Loss 0.3721
11:26:10.795080 [0] Epoch 00019 | Loss 0.3114
11:26:10.796035 [0] Epoch: 019, Train: 0.9857, Val: 0.7920, Test: 0.8130
11:26:10.796649 [0] 
timer summary:
  0.90s   0.90s    80 mm
  0.08s   0.08s    80 broadcast
  0.30s   0.30s    80 spmm
  0.03s   0.03s    40 all_reduce
  1.34s   1.34s    20 epoch
  2.56s   2.56s     1 total
15:06:32.824591 [0] proc begin: <DistEnv 0/1 nccl>
15:07:10.086171 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:07:10.087229 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:07:12.101521 [0] Epoch 00000 | Loss 3.7185
15:07:12.108075 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
15:07:12.236816 [0] Epoch 00001 | Loss 2.8261
15:07:12.366955 [0] Epoch 00002 | Loss 2.4048
15:07:12.497536 [0] Epoch 00003 | Loss 2.0652
15:07:12.627994 [0] Epoch 00004 | Loss 1.7978
15:07:12.758591 [0] Epoch 00005 | Loss 1.5821
15:07:12.889095 [0] Epoch 00006 | Loss 1.4100
15:07:13.019838 [0] Epoch 00007 | Loss 1.2561
15:07:13.151186 [0] Epoch 00008 | Loss 1.1295
15:07:13.281624 [0] Epoch 00009 | Loss 1.0056
15:07:13.412182 [0] Epoch 00010 | Loss 0.9125
15:07:13.413831 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
15:07:13.544441 [0] Epoch 00011 | Loss 0.8407
15:07:13.675356 [0] Epoch 00012 | Loss 0.7934
15:07:13.807266 [0] Epoch 00013 | Loss 0.7497
15:07:13.938540 [0] Epoch 00014 | Loss 0.6850
15:07:14.069488 [0] Epoch 00015 | Loss 0.6420
15:07:14.200584 [0] Epoch 00016 | Loss 0.6202
15:07:14.331582 [0] Epoch 00017 | Loss 0.6034
15:07:14.463032 [0] Epoch 00018 | Loss 0.5762
15:07:14.594407 [0] Epoch 00019 | Loss 0.5484
15:07:14.596049 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
15:07:14.596971 [0] 
timer summary:
  1.74s   1.74s    80 mm
  0.09s   0.09s    80 broadcast
  2.37s   2.37s    80 spmm
  0.04s   0.04s    40 all_reduce
  4.50s   4.50s    20 epoch
 41.77s  41.77s     1 total
15:07:42.778788 [0] proc begin: <DistEnv 0/1 nccl>
15:08:00.962721 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:08:00.963742 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:08:02.763585 [0] Epoch 00000 | Loss 3.6996
15:08:02.766599 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
15:08:03.492457 [0] Epoch 00001 | Loss 2.8474
15:08:04.216427 [0] Epoch 00002 | Loss 2.2357
15:08:04.938157 [0] Epoch 00003 | Loss 1.8348
15:08:05.660534 [0] Epoch 00004 | Loss 1.5465
15:08:06.383075 [0] Epoch 00005 | Loss 1.3214
15:08:07.105295 [0] Epoch 00006 | Loss 1.1282
15:08:07.828310 [0] Epoch 00007 | Loss 1.0042
15:08:08.551198 [0] Epoch 00008 | Loss 0.8937
15:08:09.273781 [0] Epoch 00009 | Loss 0.7880
15:08:09.996737 [0] Epoch 00010 | Loss 0.7122
15:08:09.999458 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
15:08:10.726555 [0] Epoch 00011 | Loss 0.6693
15:08:11.452416 [0] Epoch 00012 | Loss 0.6227
15:08:12.175284 [0] Epoch 00013 | Loss 0.5815
15:08:12.901716 [0] Epoch 00014 | Loss 0.5569
15:08:13.624913 [0] Epoch 00015 | Loss 0.5312
15:08:14.351133 [0] Epoch 00016 | Loss 0.5067
15:08:15.074573 [0] Epoch 00017 | Loss 0.4871
15:08:15.800339 [0] Epoch 00018 | Loss 0.4698
15:08:16.522978 [0] Epoch 00019 | Loss 0.4566
15:08:16.525521 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
15:08:16.526795 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.07s  14.07s    80 spmm
  1.03s   1.03s    80 mm
  0.04s   0.04s    40 all_reduce
 15.55s  15.55s    20 epoch
 33.75s  33.75s     1 total
09:20:08.358267 [0] proc begin: <DistEnv 0/1 nccl>
09:21:12.147206 [0] proc begin: <DistEnv 0/1 nccl>
09:21:13.397678 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
09:21:13.398601 [0] graph loaded |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:21:15.938336 [0] Epoch 00000 | Loss 1.9460
09:21:15.942580 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
09:21:15.947438 [0] Epoch 00001 | Loss 1.9093
09:21:15.952416 [0] Epoch 00002 | Loss 1.8631
09:21:15.955907 [0] Epoch 00003 | Loss 1.8043
09:21:15.959247 [0] Epoch 00004 | Loss 1.7353
09:21:15.962552 [0] Epoch 00005 | Loss 1.6568
09:21:15.965942 [0] Epoch 00006 | Loss 1.5686
09:21:15.969397 [0] Epoch 00007 | Loss 1.4710
09:21:15.972908 [0] Epoch 00008 | Loss 1.3651
09:21:15.976524 [0] Epoch 00009 | Loss 1.2527
09:21:15.980118 [0] Epoch 00010 | Loss 1.1357
09:21:15.981070 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
09:21:15.985107 [0] Epoch 00011 | Loss 1.0166
09:21:15.989816 [0] Epoch 00012 | Loss 0.8983
09:21:15.994278 [0] Epoch 00013 | Loss 0.7835
09:21:16.000054 [0] Epoch 00014 | Loss 0.6750
09:21:16.005228 [0] Epoch 00015 | Loss 0.5752
09:21:16.008785 [0] Epoch 00016 | Loss 0.4855
09:21:16.012414 [0] Epoch 00017 | Loss 0.4068
09:21:16.016237 [0] Epoch 00018 | Loss 0.3391
09:21:16.019826 [0] Epoch 00019 | Loss 0.2818
09:21:16.020848 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
09:21:16.021395 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.72s   0.72s    80 spmm
  1.68s   1.68s    80 mm
  0.04s   0.04s    40 all_reduce
  2.61s   2.61s    20 epoch
  3.87s   3.87s     1 total
11:18:20.935545 [0] proc begin: <DistEnv 0/1 nccl>
11:19:59.717959 [0] proc begin: <DistEnv 0/1 nccl>
11:20:01.025820 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
11:20:01.026790 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:20:02.108318 [0] Epoch 00000 | Loss 1.9460
11:20:02.109181 [0] Epoch: 000, Train: 0.1000, Val: 0.1960, Test: 0.2040
11:20:02.111916 [0] Epoch 00001 | Loss 1.9092
11:20:02.114249 [0] Epoch 00002 | Loss 1.8670
11:20:02.116664 [0] Epoch 00003 | Loss 1.8128
11:20:02.119255 [0] Epoch 00004 | Loss 1.7471
11:20:02.121883 [0] Epoch 00005 | Loss 1.6714
11:20:02.124692 [0] Epoch 00006 | Loss 1.5865
11:20:02.127537 [0] Epoch 00007 | Loss 1.4926
11:20:02.130319 [0] Epoch 00008 | Loss 1.3909
11:20:02.133035 [0] Epoch 00009 | Loss 1.2824
11:20:02.135729 [0] Epoch 00010 | Loss 1.1690
11:20:02.136607 [0] Epoch: 010, Train: 0.9786, Val: 0.7860, Test: 0.8010
11:20:02.139356 [0] Epoch 00011 | Loss 1.0530
11:20:02.142110 [0] Epoch 00012 | Loss 0.9370
11:20:02.144858 [0] Epoch 00013 | Loss 0.8237
11:20:02.147640 [0] Epoch 00014 | Loss 0.7157
11:20:02.150348 [0] Epoch 00015 | Loss 0.6153
11:20:02.153089 [0] Epoch 00016 | Loss 0.5240
11:20:02.155728 [0] Epoch 00017 | Loss 0.4428
11:20:02.158398 [0] Epoch 00018 | Loss 0.3721
11:20:02.161294 [0] Epoch 00019 | Loss 0.3114
11:20:02.162199 [0] Epoch: 019, Train: 0.9857, Val: 0.7920, Test: 0.8130
11:20:02.162711 [0] 
timer summary:
  0.73s   0.73s    80 mm
  0.08s   0.08s    80 broadcast
  0.24s   0.24s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.13s   1.13s    20 epoch
  2.44s   2.44s     1 total
18:48:59.494731 [0] proc begin: <DistEnv 0/1 nccl>
18:49:00.706280 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
18:49:00.707231 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

18:49:02.062347 [0] Epoch 00000 | Loss 1.9460
18:49:02.063539 [0] Epoch: 000, Train: 0.1000, Val: 0.1960, Test: 0.2040
18:49:02.066653 [0] Epoch 00001 | Loss 1.9092
18:49:02.069498 [0] Epoch 00002 | Loss 1.8670
18:49:02.072576 [0] Epoch 00003 | Loss 1.8128
18:49:02.075994 [0] Epoch 00004 | Loss 1.7471
18:49:02.079463 [0] Epoch 00005 | Loss 1.6714
18:49:02.082603 [0] Epoch 00006 | Loss 1.5865
18:49:02.085830 [0] Epoch 00007 | Loss 1.4926
18:49:02.089231 [0] Epoch 00008 | Loss 1.3909
18:49:02.092557 [0] Epoch 00009 | Loss 1.2824
18:49:02.095900 [0] Epoch 00010 | Loss 1.1690
18:49:02.096893 [0] Epoch: 010, Train: 0.9786, Val: 0.7860, Test: 0.8010
18:49:02.100329 [0] Epoch 00011 | Loss 1.0530
18:49:02.103751 [0] Epoch 00012 | Loss 0.9370
18:49:02.106926 [0] Epoch 00013 | Loss 0.8237
18:49:02.109841 [0] Epoch 00014 | Loss 0.7157
18:49:02.112837 [0] Epoch 00015 | Loss 0.6153
18:49:02.115878 [0] Epoch 00016 | Loss 0.5240
18:49:02.119236 [0] Epoch 00017 | Loss 0.4428
18:49:02.122522 [0] Epoch 00018 | Loss 0.3721
18:49:02.125954 [0] Epoch 00019 | Loss 0.3114
18:49:02.126951 [0] Epoch: 019, Train: 0.9857, Val: 0.7920, Test: 0.8130
18:49:02.127481 [0] 
timer summary:
  0.92s   0.92s    80 mm
  0.09s   0.09s    80 broadcast
  0.33s   0.33s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.41s   1.41s    20 epoch
  2.63s   2.63s     1 total
15:06:02.581128 [0] proc begin: <DistEnv 0/1 nccl>
15:06:06.940919 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:06:06.941772 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:06:08.287090 [0] Epoch 00000 | Loss 3.6790
15:06:08.288556 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
15:06:08.307497 [0] Epoch 00001 | Loss 3.4899
15:06:08.327828 [0] Epoch 00002 | Loss 3.4467
15:06:08.347844 [0] Epoch 00003 | Loss 3.5920
15:06:08.369962 [0] Epoch 00004 | Loss 3.9286
15:06:08.388779 [0] Epoch 00005 | Loss 4.4289
15:06:08.407840 [0] Epoch 00006 | Loss 5.0451
15:06:08.427222 [0] Epoch 00007 | Loss 5.7289
15:06:08.446039 [0] Epoch 00008 | Loss 6.4915
15:06:08.464343 [0] Epoch 00009 | Loss 7.3353
15:06:08.483267 [0] Epoch 00010 | Loss 8.1901
15:06:08.484569 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
15:06:08.502682 [0] Epoch 00011 | Loss 8.9962
15:06:08.520857 [0] Epoch 00012 | Loss 9.7551
15:06:08.539581 [0] Epoch 00013 | Loss 10.4471
15:06:08.559253 [0] Epoch 00014 | Loss 11.0453
15:06:08.579356 [0] Epoch 00015 | Loss 11.5694
15:06:08.599186 [0] Epoch 00016 | Loss 11.9875
15:06:08.619516 [0] Epoch 00017 | Loss 12.3576
15:06:08.639319 [0] Epoch 00018 | Loss 12.6982
15:06:08.659497 [0] Epoch 00019 | Loss 12.9855
15:06:08.679715 [0] Epoch 00020 | Loss 13.1974
15:06:08.681875 [0] Epoch: 020, Train: 0.2542, Val: 0.2913, Test: 0.2606
15:06:08.701257 [0] Epoch 00021 | Loss 13.2993
15:06:08.720163 [0] Epoch 00022 | Loss 13.3335
15:06:08.738929 [0] Epoch 00023 | Loss 13.3362
15:06:08.757694 [0] Epoch 00024 | Loss 13.3349
15:06:08.776148 [0] Epoch 00025 | Loss 13.3442
15:06:08.794871 [0] Epoch 00026 | Loss 13.3489
15:06:08.813472 [0] Epoch 00027 | Loss 13.3456
15:06:08.832429 [0] Epoch 00028 | Loss 13.3375
15:06:08.851915 [0] Epoch 00029 | Loss 13.3326
15:06:08.871694 [0] Epoch 00030 | Loss 13.3279
15:06:08.873709 [0] Epoch: 030, Train: 0.2720, Val: 0.3024, Test: 0.2726
15:06:08.892941 [0] Epoch 00031 | Loss 13.3046
15:06:08.911777 [0] Epoch 00032 | Loss 13.2425
15:06:08.930800 [0] Epoch 00033 | Loss 13.1455
15:06:08.949438 [0] Epoch 00034 | Loss 13.0283
15:06:08.967890 [0] Epoch 00035 | Loss 12.8993
15:06:08.986827 [0] Epoch 00036 | Loss 12.7659
15:06:09.005615 [0] Epoch 00037 | Loss 12.6366
15:06:09.024219 [0] Epoch 00038 | Loss 12.5185
15:06:09.042553 [0] Epoch 00039 | Loss 12.4118
15:06:09.060597 [0] Epoch 00040 | Loss 12.3168
15:06:09.062022 [0] Epoch: 040, Train: 0.3039, Val: 0.3260, Test: 0.2978
15:06:09.080465 [0] Epoch 00041 | Loss 12.2403
15:06:09.099690 [0] Epoch 00042 | Loss 12.1928
15:06:09.119281 [0] Epoch 00043 | Loss 12.1788
15:06:09.137996 [0] Epoch 00044 | Loss 12.1917
15:06:09.156915 [0] Epoch 00045 | Loss 12.2220
15:06:09.175597 [0] Epoch 00046 | Loss 12.2618
15:06:09.194318 [0] Epoch 00047 | Loss 12.3082
15:06:09.213021 [0] Epoch 00048 | Loss 12.3603
15:06:09.231533 [0] Epoch 00049 | Loss 12.4183
15:06:09.249756 [0] Epoch 00050 | Loss 12.4799
15:06:09.251245 [0] Epoch: 050, Train: 0.3422, Val: 0.3567, Test: 0.3331
15:06:09.269343 [0] Epoch 00051 | Loss 12.5381
15:06:09.288531 [0] Epoch 00052 | Loss 12.5854
15:06:09.307204 [0] Epoch 00053 | Loss 12.6207
15:06:09.326017 [0] Epoch 00054 | Loss 12.6504
15:06:09.344368 [0] Epoch 00055 | Loss 12.6832
15:06:09.363366 [0] Epoch 00056 | Loss 12.7239
15:06:09.382708 [0] Epoch 00057 | Loss 12.7748
15:06:09.403070 [0] Epoch 00058 | Loss 12.8385
15:06:09.422709 [0] Epoch 00059 | Loss 12.9137
15:06:09.442983 [0] Epoch 00060 | Loss 12.9924
15:06:09.445191 [0] Epoch: 060, Train: 0.3687, Val: 0.3826, Test: 0.3652
15:06:09.464859 [0] Epoch 00061 | Loss 13.0665
15:06:09.484501 [0] Epoch 00062 | Loss 13.1342
15:06:09.504268 [0] Epoch 00063 | Loss 13.1985
15:06:09.523900 [0] Epoch 00064 | Loss 13.2637
15:06:09.543130 [0] Epoch 00065 | Loss 13.3352
15:06:09.561827 [0] Epoch 00066 | Loss 13.4171
15:06:09.581259 [0] Epoch 00067 | Loss 13.5085
15:06:09.600247 [0] Epoch 00068 | Loss 13.6024
15:06:09.619090 [0] Epoch 00069 | Loss 13.6930
15:06:09.637647 [0] Epoch 00070 | Loss 13.7801
15:06:09.639062 [0] Epoch: 070, Train: 0.3902, Val: 0.4066, Test: 0.3959
15:06:09.657594 [0] Epoch 00071 | Loss 13.8662
15:06:09.676421 [0] Epoch 00072 | Loss 13.9534
15:06:09.694671 [0] Epoch 00073 | Loss 14.0451
15:06:09.712670 [0] Epoch 00074 | Loss 14.1453
15:06:09.731218 [0] Epoch 00075 | Loss 14.2545
15:06:09.749867 [0] Epoch 00076 | Loss 14.3688
15:06:09.768797 [0] Epoch 00077 | Loss 14.4833
15:06:09.789281 [0] Epoch 00078 | Loss 14.5959
15:06:09.808497 [0] Epoch 00079 | Loss 14.7051
15:06:09.827584 [0] Epoch 00080 | Loss 14.8115
15:06:09.829564 [0] Epoch: 080, Train: 0.4065, Val: 0.4307, Test: 0.4265
15:06:09.848893 [0] Epoch 00081 | Loss 14.9181
15:06:09.867804 [0] Epoch 00082 | Loss 15.0282
15:06:09.887664 [0] Epoch 00083 | Loss 15.1409
15:06:09.906861 [0] Epoch 00084 | Loss 15.2533
15:06:09.925596 [0] Epoch 00085 | Loss 15.3635
15:06:09.944056 [0] Epoch 00086 | Loss 15.4707
15:06:09.962506 [0] Epoch 00087 | Loss 15.5747
15:06:09.980879 [0] Epoch 00088 | Loss 15.6760
15:06:09.999025 [0] Epoch 00089 | Loss 15.7784
15:06:10.017515 [0] Epoch 00090 | Loss 15.8852
15:06:10.018885 [0] Epoch: 090, Train: 0.4230, Val: 0.4536, Test: 0.4555
15:06:10.037706 [0] Epoch 00091 | Loss 15.9976
15:06:10.057662 [0] Epoch 00092 | Loss 16.1155
15:06:10.076791 [0] Epoch 00093 | Loss 16.2376
15:06:10.095726 [0] Epoch 00094 | Loss 16.3602
15:06:10.114168 [0] Epoch 00095 | Loss 16.4792
15:06:10.132335 [0] Epoch 00096 | Loss 16.5924
15:06:10.151699 [0] Epoch 00097 | Loss 16.7010
15:06:10.171405 [0] Epoch 00098 | Loss 16.8073
15:06:10.191662 [0] Epoch 00099 | Loss 16.9143
15:06:10.193644 [0] Epoch: 099, Train: 0.4366, Val: 0.4739, Test: 0.4793
15:06:10.194765 [0] 
timer summary:
  0.13s   0.13s   400 broadcast
  0.80s   0.80s   400 spmm
  1.31s   1.31s   400 mm
  0.05s   0.05s   200 all_reduce
  3.23s   3.23s   100 epoch
  7.61s   7.61s     1 total
15:09:11.486507 [0] proc begin: <DistEnv 0/1 nccl>
15:09:47.300578 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:09:47.301854 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:09:49.072949 [0] Epoch 00000 | Loss 3.6996
15:09:49.075966 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
15:09:49.800978 [0] Epoch 00001 | Loss 2.8474
15:09:50.525156 [0] Epoch 00002 | Loss 2.2357
15:09:51.248608 [0] Epoch 00003 | Loss 1.8348
15:09:51.971429 [0] Epoch 00004 | Loss 1.5465
15:09:52.694577 [0] Epoch 00005 | Loss 1.3214
15:09:53.417836 [0] Epoch 00006 | Loss 1.1282
15:09:54.140192 [0] Epoch 00007 | Loss 1.0042
15:09:54.863409 [0] Epoch 00008 | Loss 0.8937
15:09:55.586850 [0] Epoch 00009 | Loss 0.7880
15:09:56.310037 [0] Epoch 00010 | Loss 0.7122
15:09:56.312703 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
15:09:57.040338 [0] Epoch 00011 | Loss 0.6693
15:09:57.766517 [0] Epoch 00012 | Loss 0.6227
15:09:58.489782 [0] Epoch 00013 | Loss 0.5815
15:09:59.216364 [0] Epoch 00014 | Loss 0.5569
15:09:59.940318 [0] Epoch 00015 | Loss 0.5312
15:10:00.666858 [0] Epoch 00016 | Loss 0.5067
15:10:01.389966 [0] Epoch 00017 | Loss 0.4871
15:10:02.115808 [0] Epoch 00018 | Loss 0.4698
15:10:02.839136 [0] Epoch 00019 | Loss 0.4566
15:10:02.841840 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
15:10:02.843119 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.07s  14.07s    80 spmm
  1.01s   1.01s    80 mm
  0.04s   0.04s    40 all_reduce
 15.53s  15.53s    20 epoch
 51.36s  51.36s     1 total
15:10:26.013713 [0] proc begin: <DistEnv 0/1 nccl>
15:10:27.212921 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
15:10:27.213832 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:10:28.368484 [0] Epoch 00000 | Loss 1.9460
15:10:28.369548 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
15:10:28.372882 [0] Epoch 00001 | Loss 1.9093
15:10:28.375625 [0] Epoch 00002 | Loss 1.8631
15:10:28.378500 [0] Epoch 00003 | Loss 1.8043
15:10:28.381407 [0] Epoch 00004 | Loss 1.7353
15:10:28.384259 [0] Epoch 00005 | Loss 1.6568
15:10:28.386999 [0] Epoch 00006 | Loss 1.5686
15:10:28.389808 [0] Epoch 00007 | Loss 1.4710
15:10:28.392624 [0] Epoch 00008 | Loss 1.3651
15:10:28.395558 [0] Epoch 00009 | Loss 1.2527
15:10:28.398414 [0] Epoch 00010 | Loss 1.1357
15:10:28.399280 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
15:10:28.402228 [0] Epoch 00011 | Loss 1.0166
15:10:28.405265 [0] Epoch 00012 | Loss 0.8983
15:10:28.408301 [0] Epoch 00013 | Loss 0.7835
15:10:28.411295 [0] Epoch 00014 | Loss 0.6750
15:10:28.414168 [0] Epoch 00015 | Loss 0.5752
15:10:28.417054 [0] Epoch 00016 | Loss 0.4855
15:10:28.419947 [0] Epoch 00017 | Loss 0.4068
15:10:28.422737 [0] Epoch 00018 | Loss 0.3391
15:10:28.425545 [0] Epoch 00019 | Loss 0.2818
15:10:28.426365 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
15:10:28.426856 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.31s   0.31s    80 spmm
  0.74s   0.74s    80 mm
  0.03s   0.03s    40 all_reduce
  1.20s   1.20s    20 epoch
  2.41s   2.41s     1 total
15:16:37.722579 [0] proc begin: <DistEnv 0/1 nccl>
15:16:58.090967 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:16:58.092005 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:16:59.892554 [0] Epoch 00000 | Loss 3.6996
15:16:59.894193 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
15:17:00.618943 [0] Epoch 00001 | Loss 2.8474
15:17:01.343746 [0] Epoch 00002 | Loss 2.2357
15:17:02.069145 [0] Epoch 00003 | Loss 1.8348
15:17:02.795095 [0] Epoch 00004 | Loss 1.5465
15:17:03.519200 [0] Epoch 00005 | Loss 1.3214
15:17:04.245850 [0] Epoch 00006 | Loss 1.1282
15:17:04.968859 [0] Epoch 00007 | Loss 1.0042
15:17:05.695846 [0] Epoch 00008 | Loss 0.8937
15:17:06.419698 [0] Epoch 00009 | Loss 0.7880
15:17:07.146230 [0] Epoch 00010 | Loss 0.7122
15:17:07.148047 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
15:17:07.872698 [0] Epoch 00011 | Loss 0.6693
15:17:08.596900 [0] Epoch 00012 | Loss 0.6227
15:17:09.321245 [0] Epoch 00013 | Loss 0.5815
15:17:10.046122 [0] Epoch 00014 | Loss 0.5569
15:17:10.770733 [0] Epoch 00015 | Loss 0.5312
15:17:11.495497 [0] Epoch 00016 | Loss 0.5067
15:17:12.220353 [0] Epoch 00017 | Loss 0.4871
15:17:12.945715 [0] Epoch 00018 | Loss 0.4698
15:17:13.670918 [0] Epoch 00019 | Loss 0.4566
15:17:13.673549 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
15:17:13.674827 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
 14.10s  14.10s    80 spmm
  1.03s   1.03s    80 mm
  0.03s   0.03s    40 all_reduce
 15.57s  15.57s    20 epoch
 35.95s  35.95s     1 total
15:20:44.639103 [0] proc begin: <DistEnv 0/1 nccl>
15:21:03.332600 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
15:21:03.334161 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:21:04.617764 [0] Epoch 00000 | Loss 3.7185
15:21:04.619413 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
15:21:04.748922 [0] Epoch 00001 | Loss 2.8261
15:21:04.879654 [0] Epoch 00002 | Loss 2.4048
15:21:05.010744 [0] Epoch 00003 | Loss 2.0652
15:21:05.141175 [0] Epoch 00004 | Loss 1.7978
15:21:05.272044 [0] Epoch 00005 | Loss 1.5821
15:21:05.402828 [0] Epoch 00006 | Loss 1.4100
15:21:05.533213 [0] Epoch 00007 | Loss 1.2561
15:21:05.663899 [0] Epoch 00008 | Loss 1.1295
15:21:05.795317 [0] Epoch 00009 | Loss 1.0056
15:21:05.926446 [0] Epoch 00010 | Loss 0.9125
15:21:05.929098 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
15:21:06.060058 [0] Epoch 00011 | Loss 0.8407
15:21:06.190756 [0] Epoch 00012 | Loss 0.7934
15:21:06.321094 [0] Epoch 00013 | Loss 0.7497
15:21:06.451789 [0] Epoch 00014 | Loss 0.6850
15:21:06.582806 [0] Epoch 00015 | Loss 0.6420
15:21:06.713290 [0] Epoch 00016 | Loss 0.6202
15:21:06.843969 [0] Epoch 00017 | Loss 0.6034
15:21:06.975102 [0] Epoch 00018 | Loss 0.5762
15:21:07.106803 [0] Epoch 00019 | Loss 0.5484
15:21:07.108397 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
15:21:07.109044 [0] 
timer summary:
  1.12s   1.12s    80 mm
  0.08s   0.08s    80 broadcast
  2.30s   2.30s    80 spmm
  0.03s   0.03s    40 all_reduce
  3.76s   3.76s    20 epoch
 22.47s  22.47s     1 total
15:22:14.873326 [0] proc begin: <DistEnv 0/1 nccl>
15:23:16.902656 [0] proc begin: <DistEnv 0/1 nccl>
15:24:15.810676 [0] proc begin: <DistEnv 0/1 nccl>
15:24:45.330720 [0] proc begin: <DistEnv 0/1 nccl>
15:24:46.760912 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:24:46.761888 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:24:47.819165 [0] Epoch 00000 | Loss 3.6792
15:24:47.821860 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
15:24:47.836649 [0] Epoch 00001 | Loss 3.4872
15:24:47.850524 [0] Epoch 00002 | Loss 3.4445
15:24:47.864423 [0] Epoch 00003 | Loss 3.5919
15:24:47.879984 [0] Epoch 00004 | Loss 3.9270
15:24:47.895198 [0] Epoch 00005 | Loss 4.4153
15:24:47.908793 [0] Epoch 00006 | Loss 5.0240
15:24:47.922710 [0] Epoch 00007 | Loss 5.7201
15:24:47.936692 [0] Epoch 00008 | Loss 6.4735
15:24:47.950633 [0] Epoch 00009 | Loss 7.2819
15:24:47.964733 [0] Epoch 00010 | Loss 8.1136
15:24:47.966141 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
15:24:47.980394 [0] Epoch 00011 | Loss 8.8971
15:24:47.995956 [0] Epoch 00012 | Loss 9.6118
15:24:48.011203 [0] Epoch 00013 | Loss 10.2092
15:24:48.024662 [0] Epoch 00014 | Loss 10.6998
15:24:48.040029 [0] Epoch 00015 | Loss 11.0928
15:24:48.054047 [0] Epoch 00016 | Loss 11.4191
15:24:48.069869 [0] Epoch 00017 | Loss 11.6880
15:24:48.082731 [0] Epoch 00018 | Loss 11.8933
15:24:48.097106 [0] Epoch 00019 | Loss 12.0301
15:24:48.098387 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
15:24:48.098906 [0] 
timer summary:
  0.80s   0.80s    80 mm
  0.03s   0.03s    80 broadcast
  0.26s   0.26s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.33s   1.33s    20 epoch
  2.77s   2.77s     1 total
15:50:02.192294 [0] proc begin: <DistEnv 0/1 nccl>
15:50:03.581694 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:50:03.582529 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:51:37.274014 [0] proc begin: <DistEnv 0/1 nccl>
15:51:39.333016 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:51:39.333750 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:51:40.348165 [0] Epoch 00000 | Loss 3.6790
15:51:40.349449 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
15:51:40.368881 [0] Epoch 00001 | Loss 3.4899
15:51:40.388440 [0] Epoch 00002 | Loss 3.4467
15:51:40.407118 [0] Epoch 00003 | Loss 3.5920
15:51:40.426235 [0] Epoch 00004 | Loss 3.9286
15:51:40.445873 [0] Epoch 00005 | Loss 4.4289
15:51:40.465106 [0] Epoch 00006 | Loss 5.0451
15:51:40.484003 [0] Epoch 00007 | Loss 5.7289
15:51:40.502593 [0] Epoch 00008 | Loss 6.4915
15:51:40.520905 [0] Epoch 00009 | Loss 7.3353
15:51:40.539640 [0] Epoch 00010 | Loss 8.1901
15:51:40.540999 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
15:51:40.559886 [0] Epoch 00011 | Loss 8.9962
15:51:40.579580 [0] Epoch 00012 | Loss 9.7551
15:51:40.599274 [0] Epoch 00013 | Loss 10.4471
15:51:40.618168 [0] Epoch 00014 | Loss 11.0453
15:51:40.636075 [0] Epoch 00015 | Loss 11.5694
15:51:40.654299 [0] Epoch 00016 | Loss 11.9875
15:51:40.672332 [0] Epoch 00017 | Loss 12.3576
15:51:40.692145 [0] Epoch 00018 | Loss 12.6982
15:51:40.712272 [0] Epoch 00019 | Loss 12.9855
15:51:40.714828 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
15:51:40.716187 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.32s   0.32s    80 spmm
  0.74s   0.74s    80 mm
  0.04s   0.04s    40 all_reduce
  1.38s   1.38s    20 epoch
  3.44s   3.44s     1 total
15:52:03.519795 [0] proc begin: <DistEnv 0/1 nccl>
15:52:04.853231 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:52:04.854065 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:52:05.847816 [0] Epoch 00000 | Loss 3.6792
15:52:05.850584 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
15:52:05.866951 [0] Epoch 00001 | Loss 3.4872
15:52:05.881759 [0] Epoch 00002 | Loss 3.4445
15:52:05.894885 [0] Epoch 00003 | Loss 3.5919
15:52:05.910446 [0] Epoch 00004 | Loss 3.9270
15:52:05.924002 [0] Epoch 00005 | Loss 4.4153
15:52:05.940394 [0] Epoch 00006 | Loss 5.0240
15:52:05.955626 [0] Epoch 00007 | Loss 5.7201
15:52:05.969073 [0] Epoch 00008 | Loss 6.4735
15:52:05.983392 [0] Epoch 00009 | Loss 7.2819
15:52:05.996684 [0] Epoch 00010 | Loss 8.1136
15:52:05.998480 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
15:52:06.013256 [0] Epoch 00011 | Loss 8.8971
15:52:06.026727 [0] Epoch 00012 | Loss 9.6118
15:52:06.042313 [0] Epoch 00013 | Loss 10.2092
15:52:06.055665 [0] Epoch 00014 | Loss 10.6998
15:52:06.071485 [0] Epoch 00015 | Loss 11.0928
15:52:06.086651 [0] Epoch 00016 | Loss 11.4191
15:52:06.099954 [0] Epoch 00017 | Loss 11.6880
15:52:06.113395 [0] Epoch 00018 | Loss 11.8933
15:52:06.127060 [0] Epoch 00019 | Loss 12.0301
15:52:06.128340 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
15:52:06.128798 [0] 
timer summary:
  0.74s   0.74s    80 mm
  0.03s   0.03s    80 broadcast
  0.26s   0.26s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.27s   1.27s    20 epoch
  2.61s   2.61s     1 total
16:43:09.890425 [0] proc begin: <DistEnv 0/1 nccl>
16:43:11.128508 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:43:11.129334 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:43:12.130947 [0] Epoch 00000 | Loss 3.6792
16:43:12.133228 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
16:43:12.148280 [0] Epoch 00001 | Loss 3.4872
16:43:12.163320 [0] Epoch 00002 | Loss 3.4445
16:43:12.176965 [0] Epoch 00003 | Loss 3.5919
16:43:12.190869 [0] Epoch 00004 | Loss 3.9270
16:43:12.205044 [0] Epoch 00005 | Loss 4.4153
16:43:12.218385 [0] Epoch 00006 | Loss 5.0240
16:43:12.234002 [0] Epoch 00007 | Loss 5.7201
16:43:12.247819 [0] Epoch 00008 | Loss 6.4735
16:43:12.263393 [0] Epoch 00009 | Loss 7.2819
16:43:12.277040 [0] Epoch 00010 | Loss 8.1136
16:43:12.279043 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
16:43:12.293861 [0] Epoch 00011 | Loss 8.8971
16:43:12.309391 [0] Epoch 00012 | Loss 9.6118
16:43:12.322743 [0] Epoch 00013 | Loss 10.2092
16:43:12.338471 [0] Epoch 00014 | Loss 10.6998
16:43:12.352224 [0] Epoch 00015 | Loss 11.0928
16:43:12.368855 [0] Epoch 00016 | Loss 11.4191
16:43:12.382152 [0] Epoch 00017 | Loss 11.6880
16:43:12.395751 [0] Epoch 00018 | Loss 11.8933
16:43:12.409124 [0] Epoch 00019 | Loss 12.0301
16:43:12.410393 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
16:43:12.410855 [0] 
timer summary:
  0.74s   0.74s    80 mm
  0.03s   0.03s    80 broadcast
  0.27s   0.27s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.27s   1.27s    20 epoch
  2.52s   2.52s     1 total
16:45:15.893831 [0] proc begin: <DistEnv 0/1 nccl>
16:45:17.379484 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:45:17.380464 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:45:18.603628 [0] Epoch 00000 | Loss 3.6792
16:45:18.605093 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
16:45:18.618273 [0] Epoch 00001 | Loss 3.4872
16:45:18.631046 [0] Epoch 00002 | Loss 3.4445
16:45:18.644510 [0] Epoch 00003 | Loss 3.5919
16:45:18.657390 [0] Epoch 00004 | Loss 3.9270
16:45:18.671657 [0] Epoch 00005 | Loss 4.4153
16:45:18.684068 [0] Epoch 00006 | Loss 5.0240
16:45:18.698091 [0] Epoch 00007 | Loss 5.7201
16:45:18.710734 [0] Epoch 00008 | Loss 6.4735
16:45:18.723450 [0] Epoch 00009 | Loss 7.2819
16:45:18.736316 [0] Epoch 00010 | Loss 8.1136
16:45:18.737652 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
16:45:18.750136 [0] Epoch 00011 | Loss 8.8971
16:45:18.762531 [0] Epoch 00012 | Loss 9.6118
16:45:18.775306 [0] Epoch 00013 | Loss 10.2092
16:45:18.787631 [0] Epoch 00014 | Loss 10.6998
16:45:18.800790 [0] Epoch 00015 | Loss 11.0928
16:45:18.813040 [0] Epoch 00016 | Loss 11.4191
16:45:18.825446 [0] Epoch 00017 | Loss 11.6880
16:45:18.837570 [0] Epoch 00018 | Loss 11.8933
16:45:18.851102 [0] Epoch 00019 | Loss 12.0301
16:45:18.852460 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
16:45:18.852950 [0] 
timer summary:
  0.85s   0.85s    80 mm
  0.08s   0.08s    80 broadcast
  0.34s   0.34s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.47s   1.47s    20 epoch
  2.96s   2.96s     1 total
16:45:31.709663 [0] proc begin: <DistEnv 0/1 nccl>
16:45:33.143346 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
16:45:33.144356 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

16:45:34.457190 [0] Epoch 00000 | Loss 3.6790
16:45:34.460321 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
16:45:34.480470 [0] Epoch 00001 | Loss 3.4899
16:45:34.499948 [0] Epoch 00002 | Loss 3.4467
16:45:34.519888 [0] Epoch 00003 | Loss 3.5920
16:45:34.539980 [0] Epoch 00004 | Loss 3.9286
16:45:34.559878 [0] Epoch 00005 | Loss 4.4289
16:45:34.579755 [0] Epoch 00006 | Loss 5.0451
16:45:34.599841 [0] Epoch 00007 | Loss 5.7289
16:45:34.619633 [0] Epoch 00008 | Loss 6.4915
16:45:34.639918 [0] Epoch 00009 | Loss 7.3353
16:45:34.659821 [0] Epoch 00010 | Loss 8.1901
16:45:34.662001 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
16:45:34.681566 [0] Epoch 00011 | Loss 8.9962
16:45:34.701023 [0] Epoch 00012 | Loss 9.7551
16:45:34.720650 [0] Epoch 00013 | Loss 10.4471
16:45:34.739694 [0] Epoch 00014 | Loss 11.0453
16:45:34.758679 [0] Epoch 00015 | Loss 11.5694
16:45:34.777381 [0] Epoch 00016 | Loss 11.9875
16:45:34.796254 [0] Epoch 00017 | Loss 12.3576
16:45:34.816540 [0] Epoch 00018 | Loss 12.6982
16:45:34.837187 [0] Epoch 00019 | Loss 12.9855
16:45:34.839746 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
16:45:34.841170 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.36s   0.36s    80 spmm
  1.00s   1.00s    80 mm
  0.04s   0.04s    40 all_reduce
  1.69s   1.69s    20 epoch
  3.13s   3.13s     1 total
10:43:20.554144 [0] proc begin: <DistEnv 0/1 nccl>
10:43:22.085944 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:43:22.086955 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:43:23.244439 [0] Epoch 00000 | Loss 1.9460
10:43:23.246671 [0] Epoch: 000, Train: 0.1000, Val: 0.1960, Test: 0.2040
10:43:23.250484 [0] Epoch 00001 | Loss 1.9092
10:43:23.254046 [0] Epoch 00002 | Loss 1.8670
10:43:23.256896 [0] Epoch 00003 | Loss 1.8128
10:43:23.259760 [0] Epoch 00004 | Loss 1.7471
10:43:23.262643 [0] Epoch 00005 | Loss 1.6714
10:43:23.265733 [0] Epoch 00006 | Loss 1.5865
10:43:23.268671 [0] Epoch 00007 | Loss 1.4926
10:43:23.271494 [0] Epoch 00008 | Loss 1.3909
10:43:23.274274 [0] Epoch 00009 | Loss 1.2824
10:43:23.277061 [0] Epoch 00010 | Loss 1.1690
10:43:23.277954 [0] Epoch: 010, Train: 0.9786, Val: 0.7860, Test: 0.8010
10:43:23.280743 [0] Epoch 00011 | Loss 1.0530
10:43:23.283597 [0] Epoch 00012 | Loss 0.9370
10:43:23.286544 [0] Epoch 00013 | Loss 0.8237
10:43:23.289379 [0] Epoch 00014 | Loss 0.7157
10:43:23.292224 [0] Epoch 00015 | Loss 0.6153
10:43:23.294960 [0] Epoch 00016 | Loss 0.5240
10:43:23.297821 [0] Epoch 00017 | Loss 0.4428
10:43:23.300631 [0] Epoch 00018 | Loss 0.3721
10:43:23.303444 [0] Epoch 00019 | Loss 0.3114
10:43:23.304315 [0] Epoch: 019, Train: 0.9857, Val: 0.7920, Test: 0.8130
10:43:23.304799 [0] 
timer summary:
  0.80s   0.80s    80 mm
  0.09s   0.09s    80 broadcast
  0.24s   0.24s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.21s   1.21s    20 epoch
  2.75s   2.75s     1 total
10:44:55.288288 [0] proc begin: <DistEnv 0/1 nccl>
10:44:56.709336 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:44:56.710035 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:44:57.738302 [0] Epoch 00000 | Loss 1.9460
10:44:57.739173 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:44:57.742487 [0] Epoch 00001 | Loss 1.9093
10:44:57.745155 [0] Epoch 00002 | Loss 1.8631
10:44:57.747812 [0] Epoch 00003 | Loss 1.8043
10:44:57.750647 [0] Epoch 00004 | Loss 1.7353
10:44:57.753581 [0] Epoch 00005 | Loss 1.6568
10:44:57.756422 [0] Epoch 00006 | Loss 1.5686
10:44:57.759294 [0] Epoch 00007 | Loss 1.4710
10:44:57.762220 [0] Epoch 00008 | Loss 1.3651
10:44:57.765184 [0] Epoch 00009 | Loss 1.2527
10:44:57.768077 [0] Epoch 00010 | Loss 1.1357
10:44:57.768930 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:44:57.772364 [0] Epoch 00011 | Loss 1.0166
10:44:57.775624 [0] Epoch 00012 | Loss 0.8983
10:44:57.778852 [0] Epoch 00013 | Loss 0.7835
10:44:57.782094 [0] Epoch 00014 | Loss 0.6750
10:44:57.785154 [0] Epoch 00015 | Loss 0.5752
10:44:57.788455 [0] Epoch 00016 | Loss 0.4855
10:44:57.791674 [0] Epoch 00017 | Loss 0.4068
10:44:57.795191 [0] Epoch 00018 | Loss 0.3391
10:44:57.798583 [0] Epoch 00019 | Loss 0.2818
10:44:57.799459 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:44:57.799979 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.25s   0.25s    80 spmm
  0.68s   0.68s    80 mm
  0.04s   0.04s    40 all_reduce
  1.08s   1.08s    20 epoch
  2.51s   2.51s     1 total
10:46:32.469042 [0] proc begin: <DistEnv 0/1 nccl>
10:46:33.655663 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:46:33.656507 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:46:34.835335 [0] Epoch 00000 | Loss 1.9460
10:46:34.836361 [0] Epoch: 000, Train: 0.1000, Val: 0.1960, Test: 0.2040
10:46:34.839152 [0] Epoch 00001 | Loss 1.9092
10:46:34.841735 [0] Epoch 00002 | Loss 1.8670
10:46:34.844405 [0] Epoch 00003 | Loss 1.8128
10:46:34.847059 [0] Epoch 00004 | Loss 1.7471
10:46:34.850102 [0] Epoch 00005 | Loss 1.6714
10:46:34.853119 [0] Epoch 00006 | Loss 1.5865
10:46:34.856251 [0] Epoch 00007 | Loss 1.4926
10:46:34.859327 [0] Epoch 00008 | Loss 1.3909
10:46:34.862528 [0] Epoch 00009 | Loss 1.2824
10:46:34.866019 [0] Epoch 00010 | Loss 1.1690
10:46:34.866964 [0] Epoch: 010, Train: 0.9786, Val: 0.7860, Test: 0.8010
10:46:34.870187 [0] Epoch 00011 | Loss 1.0530
10:46:34.873253 [0] Epoch 00012 | Loss 0.9370
10:46:34.876411 [0] Epoch 00013 | Loss 0.8237
10:46:34.879871 [0] Epoch 00014 | Loss 0.7157
10:46:34.884744 [0] Epoch 00015 | Loss 0.6153
10:46:34.887880 [0] Epoch 00016 | Loss 0.5240
10:46:34.891005 [0] Epoch 00017 | Loss 0.4428
10:46:34.893920 [0] Epoch 00018 | Loss 0.3721
10:46:34.896728 [0] Epoch 00019 | Loss 0.3114
10:46:34.897575 [0] Epoch: 019, Train: 0.9857, Val: 0.7920, Test: 0.8130
10:46:34.898050 [0] 
timer summary:
  0.72s   0.72s    80 mm
  0.09s   0.09s    80 broadcast
  0.35s   0.35s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.23s   1.23s    20 epoch
  2.43s   2.43s     1 total
10:49:12.177475 [0] proc begin: <DistEnv 0/1 nccl>
10:49:13.581423 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
10:49:13.582677 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:49:14.831845 [0] Epoch 00000 | Loss 3.6790
10:49:14.833392 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
10:49:14.853563 [0] Epoch 00001 | Loss 3.4899
10:49:14.874340 [0] Epoch 00002 | Loss 3.4467
10:49:14.894781 [0] Epoch 00003 | Loss 3.5920
10:49:14.915374 [0] Epoch 00004 | Loss 3.9286
10:49:14.935958 [0] Epoch 00005 | Loss 4.4289
10:49:14.955605 [0] Epoch 00006 | Loss 5.0451
10:49:14.974772 [0] Epoch 00007 | Loss 5.7289
10:49:14.994280 [0] Epoch 00008 | Loss 6.4915
10:49:15.013822 [0] Epoch 00009 | Loss 7.3353
10:49:15.033872 [0] Epoch 00010 | Loss 8.1901
10:49:15.036085 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
10:49:15.055770 [0] Epoch 00011 | Loss 8.9962
10:49:15.074569 [0] Epoch 00012 | Loss 9.7551
10:49:15.093436 [0] Epoch 00013 | Loss 10.4471
10:49:15.113850 [0] Epoch 00014 | Loss 11.0453
10:49:15.134351 [0] Epoch 00015 | Loss 11.5694
10:49:15.154116 [0] Epoch 00016 | Loss 11.9875
10:49:15.173756 [0] Epoch 00017 | Loss 12.3576
10:49:15.193640 [0] Epoch 00018 | Loss 12.6982
10:49:15.213734 [0] Epoch 00019 | Loss 12.9855
10:49:15.215909 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
10:49:15.217052 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.37s   0.37s    80 spmm
  0.93s   0.93s    80 mm
  0.03s   0.03s    40 all_reduce
  1.62s   1.62s    20 epoch
  3.04s   3.04s     1 total
10:49:30.389195 [0] proc begin: <DistEnv 0/1 nccl>
10:49:31.831174 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
10:49:31.832221 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:49:33.055813 [0] Epoch 00000 | Loss 3.6790
10:49:33.057111 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
10:49:33.076098 [0] Epoch 00001 | Loss 3.4899
10:49:33.095467 [0] Epoch 00002 | Loss 3.4467
10:49:33.115004 [0] Epoch 00003 | Loss 3.5920
10:49:33.134876 [0] Epoch 00004 | Loss 3.9286
10:49:33.154518 [0] Epoch 00005 | Loss 4.4289
10:49:33.174208 [0] Epoch 00006 | Loss 5.0451
10:49:33.193335 [0] Epoch 00007 | Loss 5.7289
10:49:33.212136 [0] Epoch 00008 | Loss 6.4915
10:49:33.230759 [0] Epoch 00009 | Loss 7.3353
10:49:33.249299 [0] Epoch 00010 | Loss 8.1901
10:49:33.250726 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
10:49:33.269348 [0] Epoch 00011 | Loss 8.9962
10:49:33.288286 [0] Epoch 00012 | Loss 9.7551
10:49:33.307720 [0] Epoch 00013 | Loss 10.4471
10:49:33.326984 [0] Epoch 00014 | Loss 11.0453
10:49:33.345747 [0] Epoch 00015 | Loss 11.5694
10:49:33.364566 [0] Epoch 00016 | Loss 11.9875
10:49:33.383019 [0] Epoch 00017 | Loss 12.3576
10:49:33.401815 [0] Epoch 00018 | Loss 12.6982
10:49:33.420606 [0] Epoch 00019 | Loss 12.9855
10:49:33.423037 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
10:49:33.424190 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.36s   0.36s    80 spmm
  0.91s   0.91s    80 mm
  0.03s   0.03s    40 all_reduce
  1.58s   1.58s    20 epoch
  3.03s   3.03s     1 total
10:51:05.748928 [0] proc begin: <DistEnv 0/1 nccl>
10:51:07.272745 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
10:51:07.273774 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| Active memory         |   15304 KB |   15326 KB |   15374 KB |   71680 B  |
|       from large pool |   15158 KB |   15158 KB |   15158 KB |       0 B  |
|       from small pool |     145 KB |     167 KB |     215 KB |   71680 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18432 KB |   18432 KB |   18432 KB |       0 B  |
|       from large pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3128 KB |    3243 KB |    3340 KB |  217600 B  |
|       from large pool |    1225 KB |    1225 KB |    1225 KB |       0 B  |
|       from small pool |    1902 KB |    2045 KB |    2115 KB |  217600 B  |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      17    |       9    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      16    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:51:08.963140 [0] Epoch 00000 | Loss 1.9460
10:51:08.964542 [0] Epoch: 000, Train: 0.1143, Val: 0.2060, Test: 0.2000
10:51:08.969514 [0] Epoch 00001 | Loss 1.9093
10:51:08.973862 [0] Epoch 00002 | Loss 1.8631
10:51:08.978223 [0] Epoch 00003 | Loss 1.8043
10:51:08.982384 [0] Epoch 00004 | Loss 1.7353
10:51:08.986537 [0] Epoch 00005 | Loss 1.6568
10:51:08.990704 [0] Epoch 00006 | Loss 1.5686
10:51:08.994853 [0] Epoch 00007 | Loss 1.4710
10:51:08.998976 [0] Epoch 00008 | Loss 1.3651
10:51:09.003087 [0] Epoch 00009 | Loss 1.2527
10:51:09.007201 [0] Epoch 00010 | Loss 1.1357
10:51:09.008331 [0] Epoch: 010, Train: 0.9786, Val: 0.7900, Test: 0.8070
10:51:09.012352 [0] Epoch 00011 | Loss 1.0166
10:51:09.017760 [0] Epoch 00012 | Loss 0.8983
10:51:09.021919 [0] Epoch 00013 | Loss 0.7835
10:51:09.026352 [0] Epoch 00014 | Loss 0.6750
10:51:09.031312 [0] Epoch 00015 | Loss 0.5752
10:51:09.037834 [0] Epoch 00016 | Loss 0.4855
10:51:09.043346 [0] Epoch 00017 | Loss 0.4068
10:51:09.047643 [0] Epoch 00018 | Loss 0.3391
10:51:09.052126 [0] Epoch 00019 | Loss 0.2818
10:51:09.053222 [0] Epoch: 019, Train: 0.9857, Val: 0.7860, Test: 0.8100
10:51:09.053860 [0] 
timer summary:
  0.10s   0.10s    80 broadcast
  0.40s   0.40s    80 spmm
  1.17s   1.17s    80 mm
  0.04s   0.04s    40 all_reduce
  1.77s   1.77s    20 epoch
  3.30s   3.30s     1 total
11:17:56.225248 [0] proc begin: <DistEnv 0/1 nccl>
11:18:20.968603 [0] proc begin: <DistEnv 0/1 nccl>
11:18:40.856286 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
11:18:40.857271 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:18:42.607780 [0] Epoch 00000 | Loss 3.6996
11:18:42.610931 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
11:18:43.337076 [0] Epoch 00001 | Loss 2.8474
11:18:44.061648 [0] Epoch 00002 | Loss 2.2357
11:18:44.784860 [0] Epoch 00003 | Loss 1.8348
11:18:45.508146 [0] Epoch 00004 | Loss 1.5465
11:18:46.231338 [0] Epoch 00005 | Loss 1.3214
11:18:46.955725 [0] Epoch 00006 | Loss 1.1282
11:18:47.678108 [0] Epoch 00007 | Loss 1.0042
11:18:48.400814 [0] Epoch 00008 | Loss 0.8937
11:18:49.123960 [0] Epoch 00009 | Loss 0.7880
11:18:49.847195 [0] Epoch 00010 | Loss 0.7122
11:18:49.850175 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
11:18:50.578029 [0] Epoch 00011 | Loss 0.6693
11:18:51.303826 [0] Epoch 00012 | Loss 0.6227
11:18:52.026842 [0] Epoch 00013 | Loss 0.5815
11:18:52.753422 [0] Epoch 00014 | Loss 0.5569
11:18:53.476037 [0] Epoch 00015 | Loss 0.5312
11:18:54.202654 [0] Epoch 00016 | Loss 0.5067
11:18:54.926104 [0] Epoch 00017 | Loss 0.4871
11:18:55.652313 [0] Epoch 00018 | Loss 0.4698
11:18:56.374827 [0] Epoch 00019 | Loss 0.4566
11:18:56.377581 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
11:18:56.379013 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.06s  14.06s    80 spmm
  0.99s   0.99s    80 mm
  0.04s   0.04s    40 all_reduce
 15.51s  15.51s    20 epoch
 35.41s  35.41s     1 total
11:24:07.716447 [0] proc begin: <DistEnv 0/1 nccl>
11:24:26.116339 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
11:24:26.117344 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:24:27.284812 [0] Epoch 00000 | Loss 3.7185
11:24:27.288399 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
11:24:27.417968 [0] Epoch 00001 | Loss 2.8261
11:24:27.548711 [0] Epoch 00002 | Loss 2.4048
11:24:27.678612 [0] Epoch 00003 | Loss 2.0652
11:24:27.809660 [0] Epoch 00004 | Loss 1.7978
11:24:27.940807 [0] Epoch 00005 | Loss 1.5821
11:24:28.072182 [0] Epoch 00006 | Loss 1.4100
11:24:28.202988 [0] Epoch 00007 | Loss 1.2561
11:24:28.333964 [0] Epoch 00008 | Loss 1.1295
11:24:28.464007 [0] Epoch 00009 | Loss 1.0056
11:24:28.594344 [0] Epoch 00010 | Loss 0.9125
11:24:28.596665 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
11:24:28.726977 [0] Epoch 00011 | Loss 0.8407
11:24:28.857423 [0] Epoch 00012 | Loss 0.7934
11:24:28.987885 [0] Epoch 00013 | Loss 0.7497
11:24:29.118177 [0] Epoch 00014 | Loss 0.6850
11:24:29.249078 [0] Epoch 00015 | Loss 0.6420
11:24:29.379927 [0] Epoch 00016 | Loss 0.6202
11:24:29.511001 [0] Epoch 00017 | Loss 0.6034
11:24:29.641816 [0] Epoch 00018 | Loss 0.5762
11:24:29.773747 [0] Epoch 00019 | Loss 0.5484
11:24:29.776189 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
11:24:29.777395 [0] 
timer summary:
  1.01s   1.01s    80 mm
  0.09s   0.09s    80 broadcast
  2.29s   2.29s    80 spmm
  0.03s   0.03s    40 all_reduce
  3.65s   3.65s    20 epoch
 22.06s  22.06s     1 total
11:25:27.774789 [0] proc begin: <DistEnv 0/1 nccl>
11:25:49.071720 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
11:25:49.073439 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:25:50.843316 [0] Epoch 00000 | Loss 3.6996
11:25:50.846546 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
11:25:51.573154 [0] Epoch 00001 | Loss 2.8474
11:25:52.298409 [0] Epoch 00002 | Loss 2.2357
11:25:53.022491 [0] Epoch 00003 | Loss 1.8348
11:25:53.745941 [0] Epoch 00004 | Loss 1.5465
11:25:54.469800 [0] Epoch 00005 | Loss 1.3214
11:25:55.193066 [0] Epoch 00006 | Loss 1.1282
11:25:55.915422 [0] Epoch 00007 | Loss 1.0042
11:25:56.642656 [0] Epoch 00008 | Loss 0.8937
11:25:57.367892 [0] Epoch 00009 | Loss 0.7880
11:25:58.092136 [0] Epoch 00010 | Loss 0.7122
11:25:58.095593 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
11:25:58.824312 [0] Epoch 00011 | Loss 0.6693
11:25:59.552188 [0] Epoch 00012 | Loss 0.6227
11:26:00.277979 [0] Epoch 00013 | Loss 0.5815
11:26:01.006817 [0] Epoch 00014 | Loss 0.5569
11:26:01.734236 [0] Epoch 00015 | Loss 0.5312
11:26:02.464398 [0] Epoch 00016 | Loss 0.5067
11:26:03.193085 [0] Epoch 00017 | Loss 0.4871
11:26:03.920908 [0] Epoch 00018 | Loss 0.4698
11:26:04.648529 [0] Epoch 00019 | Loss 0.4566
11:26:04.652286 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
11:26:04.654797 [0] 
timer summary:
  0.10s   0.10s    80 broadcast
 14.06s  14.06s    80 spmm
  1.01s   1.01s    80 mm
  0.04s   0.04s    40 all_reduce
 15.56s  15.56s    20 epoch
 36.88s  36.88s     1 total
11:29:57.044422 [0] proc begin: <DistEnv 0/1 nccl>
11:29:58.923530 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:29:58.925031 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:30:00.317510 [0] Epoch 00000 | Loss 3.6790
11:30:00.318984 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
11:30:00.338086 [0] Epoch 00001 | Loss 3.4899
11:30:00.361292 [0] Epoch 00002 | Loss 3.4467
11:30:00.380676 [0] Epoch 00003 | Loss 3.5920
11:30:00.399770 [0] Epoch 00004 | Loss 3.9286
11:30:00.419019 [0] Epoch 00005 | Loss 4.4289
11:30:00.438774 [0] Epoch 00006 | Loss 5.0451
11:30:00.458637 [0] Epoch 00007 | Loss 5.7289
11:30:00.478149 [0] Epoch 00008 | Loss 6.4915
11:30:00.497860 [0] Epoch 00009 | Loss 7.3353
11:30:00.517491 [0] Epoch 00010 | Loss 8.1901
11:30:00.518776 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
11:30:00.538129 [0] Epoch 00011 | Loss 8.9962
11:30:00.558226 [0] Epoch 00012 | Loss 9.7551
11:30:00.578022 [0] Epoch 00013 | Loss 10.4471
11:30:00.597941 [0] Epoch 00014 | Loss 11.0453
11:30:00.618105 [0] Epoch 00015 | Loss 11.5694
11:30:00.638223 [0] Epoch 00016 | Loss 11.9875
11:30:00.658297 [0] Epoch 00017 | Loss 12.3576
11:30:00.678959 [0] Epoch 00018 | Loss 12.6982
11:30:00.699892 [0] Epoch 00019 | Loss 12.9855
11:30:00.702565 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
11:30:00.704029 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.48s   0.48s    80 spmm
  0.96s   0.96s    80 mm
  0.03s   0.03s    40 all_reduce
  1.77s   1.77s    20 epoch
  3.66s   3.66s     1 total
11:32:02.748869 [0] proc begin: <DistEnv 0/1 nccl>
11:32:04.383342 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:32:04.384420 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:32:06.044226 [0] Epoch 00000 | Loss 3.6792
11:32:06.046171 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
11:32:06.060870 [0] Epoch 00001 | Loss 3.4872
11:32:06.075130 [0] Epoch 00002 | Loss 3.4445
11:32:06.088466 [0] Epoch 00003 | Loss 3.5919
11:32:06.103054 [0] Epoch 00004 | Loss 3.9270
11:32:06.116283 [0] Epoch 00005 | Loss 4.4153
11:32:06.130723 [0] Epoch 00006 | Loss 5.0240
11:32:06.143936 [0] Epoch 00007 | Loss 5.7201
11:32:06.158363 [0] Epoch 00008 | Loss 6.4735
11:32:06.171489 [0] Epoch 00009 | Loss 7.2819
11:32:06.184771 [0] Epoch 00010 | Loss 8.1136
11:32:06.186544 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
11:32:06.199841 [0] Epoch 00011 | Loss 8.8971
11:32:06.213901 [0] Epoch 00012 | Loss 9.6118
11:32:06.227087 [0] Epoch 00013 | Loss 10.2092
11:32:06.240744 [0] Epoch 00014 | Loss 10.6998
11:32:06.253714 [0] Epoch 00015 | Loss 11.0928
11:32:06.266568 [0] Epoch 00016 | Loss 11.4191
11:32:06.279607 [0] Epoch 00017 | Loss 11.6880
11:32:06.293011 [0] Epoch 00018 | Loss 11.8933
11:32:06.305814 [0] Epoch 00019 | Loss 12.0301
11:32:06.308525 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
11:32:06.309800 [0] 
timer summary:
  1.18s   1.18s    80 mm
  0.10s   0.10s    80 broadcast
  0.44s   0.44s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.92s   1.92s    20 epoch
  3.56s   3.56s     1 total
20:45:01.578679 [0] proc begin: <DistEnv 0/1 nccl>
20:45:03.810094 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:45:03.811690 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:45:05.251420 [0] Epoch 00000 | Loss 3.6792
20:45:05.253781 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
20:45:05.269077 [0] Epoch 00001 | Loss 3.4872
20:45:05.288723 [0] Epoch 00002 | Loss 3.4445
20:45:05.304010 [0] Epoch 00003 | Loss 3.5919
20:45:05.320562 [0] Epoch 00004 | Loss 3.9270
20:45:05.338212 [0] Epoch 00005 | Loss 4.4153
20:45:05.360186 [0] Epoch 00006 | Loss 5.0240
20:45:05.374974 [0] Epoch 00007 | Loss 5.7201
20:45:05.390712 [0] Epoch 00008 | Loss 6.4735
20:45:05.406128 [0] Epoch 00009 | Loss 7.2819
20:45:05.422862 [0] Epoch 00010 | Loss 8.1136
20:45:05.425388 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
20:45:05.440229 [0] Epoch 00011 | Loss 8.8971
20:45:05.458570 [0] Epoch 00012 | Loss 9.6118
20:45:05.482417 [0] Epoch 00013 | Loss 10.2092
20:45:05.498412 [0] Epoch 00014 | Loss 10.6998
20:45:05.514384 [0] Epoch 00015 | Loss 11.0928
20:45:05.529256 [0] Epoch 00016 | Loss 11.4191
20:45:05.546320 [0] Epoch 00017 | Loss 11.6880
20:45:05.563833 [0] Epoch 00018 | Loss 11.8933
20:45:05.577631 [0] Epoch 00019 | Loss 12.0301
20:45:05.580226 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
20:45:05.581669 [0] 
timer summary:
  1.06s   1.06s    80 mm
  0.12s   0.12s    80 broadcast
  0.39s   0.39s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.76s   1.76s    20 epoch
  4.00s   4.00s     1 total
21:00:17.960179 [0] proc begin: <DistEnv 0/1 nccl>
21:00:40.463152 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
21:00:40.465034 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1932 MB |    1951 MB |    2925 MB |     992 MB |
|       from large pool |    1932 MB |    1951 MB |    2925 MB |     992 MB |
|       from small pool |       0 MB |       0 MB |       0 MB |       0 MB |
|---------------------------------------------------------------------------|
| Active memory         |    1932 MB |    1951 MB |    2925 MB |     992 MB |
|       from large pool |    1932 MB |    1951 MB |    2925 MB |     992 MB |
|       from small pool |       0 MB |       0 MB |       0 MB |       0 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2396 MB |    2396 MB |    2396 MB |       0 B  |
|       from large pool |    2394 MB |    2394 MB |    2394 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  472519 KB |  482086 KB |  574423 KB |  101904 KB |
|       from large pool |  472519 KB |  482086 KB |  564170 KB |   91651 KB |
|       from small pool |       0 KB |    2047 KB |   10252 KB |   10252 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |      15    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |      15    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |      11    |       7    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       5    |       5    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:04:46.652528 [0] proc begin: <DistEnv 0/1 nccl>
21:04:53.414620 [0] proc begin: <DistEnv 0/1 nccl>
21:04:54.980015 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
21:04:54.981106 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:04:56.255572 [0] Epoch 00000 | Loss 3.6792
21:04:56.258775 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
21:04:56.277182 [0] Epoch 00001 | Loss 3.4872
21:04:56.295230 [0] Epoch 00002 | Loss 3.4445
21:04:56.312404 [0] Epoch 00003 | Loss 3.5919
21:04:56.329918 [0] Epoch 00004 | Loss 3.9270
21:04:56.346046 [0] Epoch 00005 | Loss 4.4153
21:04:56.361661 [0] Epoch 00006 | Loss 5.0240
21:04:56.377940 [0] Epoch 00007 | Loss 5.7201
21:04:56.394303 [0] Epoch 00008 | Loss 6.4735
21:04:56.410313 [0] Epoch 00009 | Loss 7.2819
21:04:56.426411 [0] Epoch 00010 | Loss 8.1136
21:04:56.428063 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
21:04:56.445231 [0] Epoch 00011 | Loss 8.8971
21:04:56.461610 [0] Epoch 00012 | Loss 9.6118
21:04:56.478175 [0] Epoch 00013 | Loss 10.2092
21:04:56.494067 [0] Epoch 00014 | Loss 10.6998
21:04:56.510472 [0] Epoch 00015 | Loss 11.0928
21:04:56.526762 [0] Epoch 00016 | Loss 11.4191
21:04:56.543105 [0] Epoch 00017 | Loss 11.6880
21:04:56.559887 [0] Epoch 00018 | Loss 11.8933
21:04:56.576613 [0] Epoch 00019 | Loss 12.0301
21:04:56.578228 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
21:04:56.578819 [0] 
timer summary:
  0.93s   0.93s    80 mm
  0.11s   0.11s    80 broadcast
  0.36s   0.36s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.59s   1.59s    20 epoch
  3.16s   3.16s     1 total
19:55:50.326980 [0] proc begin: <DistEnv 0/1 nccl>
19:55:53.054181 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
19:55:53.062130 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:55:54.449244 [0] Epoch 00000 | Loss 3.6792
19:55:54.453441 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
19:55:54.468357 [0] Epoch 00001 | Loss 3.4872
19:55:54.482553 [0] Epoch 00002 | Loss 3.4445
19:55:54.497752 [0] Epoch 00003 | Loss 3.5919
19:55:54.511912 [0] Epoch 00004 | Loss 3.9270
19:55:54.526421 [0] Epoch 00005 | Loss 4.4153
19:55:54.540373 [0] Epoch 00006 | Loss 5.0240
19:55:54.555604 [0] Epoch 00007 | Loss 5.7201
19:55:54.569411 [0] Epoch 00008 | Loss 6.4735
19:55:54.584778 [0] Epoch 00009 | Loss 7.2819
19:55:54.598851 [0] Epoch 00010 | Loss 8.1136
19:55:54.601455 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
19:55:54.615864 [0] Epoch 00011 | Loss 8.8971
19:55:54.629456 [0] Epoch 00012 | Loss 9.6118
19:55:54.643077 [0] Epoch 00013 | Loss 10.2092
19:55:54.657962 [0] Epoch 00014 | Loss 10.6998
19:55:54.671528 [0] Epoch 00015 | Loss 11.0928
19:55:54.686411 [0] Epoch 00016 | Loss 11.4191
19:55:54.699847 [0] Epoch 00017 | Loss 11.6880
19:55:54.714805 [0] Epoch 00018 | Loss 11.8933
19:55:54.728286 [0] Epoch 00019 | Loss 12.0301
19:55:54.730785 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
19:55:54.732103 [0] 
timer summary:
  0.93s   0.93s    80 mm
  0.11s   0.11s    80 broadcast
  0.38s   0.38s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.65s   1.65s    20 epoch
  4.40s   4.40s     1 total
19:57:05.235562 [0] proc begin: <DistEnv 0/1 nccl>
19:57:38.138943 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
19:57:38.140669 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

19:57:39.841639 [0] Epoch 00000 | Loss 3.7185
19:57:39.843636 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
19:57:39.974805 [0] Epoch 00001 | Loss 2.8261
19:57:40.106766 [0] Epoch 00002 | Loss 2.4048
19:57:40.239403 [0] Epoch 00003 | Loss 2.0652
19:57:40.371857 [0] Epoch 00004 | Loss 1.7978
19:57:40.504894 [0] Epoch 00005 | Loss 1.5821
19:57:40.637233 [0] Epoch 00006 | Loss 1.4100
19:57:40.769385 [0] Epoch 00007 | Loss 1.2561
19:57:40.901980 [0] Epoch 00008 | Loss 1.1295
19:57:41.034009 [0] Epoch 00009 | Loss 1.0056
19:57:41.165874 [0] Epoch 00010 | Loss 0.9125
19:57:41.167560 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
19:57:41.299146 [0] Epoch 00011 | Loss 0.8407
19:57:41.431209 [0] Epoch 00012 | Loss 0.7934
19:57:41.565158 [0] Epoch 00013 | Loss 0.7497
19:57:41.697915 [0] Epoch 00014 | Loss 0.6850
19:57:41.830259 [0] Epoch 00015 | Loss 0.6420
19:57:41.962495 [0] Epoch 00016 | Loss 0.6202
19:57:42.095139 [0] Epoch 00017 | Loss 0.6034
19:57:42.228021 [0] Epoch 00018 | Loss 0.5762
19:57:42.360932 [0] Epoch 00019 | Loss 0.5484
19:57:42.363672 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
19:57:42.365504 [0] 
timer summary:
  1.41s   1.41s    80 mm
  0.12s   0.12s    80 broadcast
  2.43s   2.43s    80 spmm
  0.00s   0.00s    40 all_reduce
  4.21s   4.21s    20 epoch
 37.13s  37.13s     1 total
20:00:40.371113 [0] proc begin: <DistEnv 0/1 nccl>
20:00:41.650975 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:00:41.651626 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:00:42.650165 [0] Epoch 00000 | Loss 3.6792
20:00:42.652971 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
20:00:42.667857 [0] Epoch 00001 | Loss 3.4872
20:00:42.682245 [0] Epoch 00002 | Loss 3.4445
20:00:42.695535 [0] Epoch 00003 | Loss 3.5919
20:00:42.710527 [0] Epoch 00004 | Loss 3.9270
20:00:42.723783 [0] Epoch 00005 | Loss 4.4153
20:00:42.738431 [0] Epoch 00006 | Loss 5.0240
20:00:42.751735 [0] Epoch 00007 | Loss 5.7201
20:00:42.766282 [0] Epoch 00008 | Loss 6.4735
20:00:42.779630 [0] Epoch 00009 | Loss 7.2819
20:00:42.794216 [0] Epoch 00010 | Loss 8.1136
20:00:42.795527 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
20:00:42.808858 [0] Epoch 00011 | Loss 8.8971
20:00:42.823501 [0] Epoch 00012 | Loss 9.6118
20:00:42.836814 [0] Epoch 00013 | Loss 10.2092
20:00:42.851631 [0] Epoch 00014 | Loss 10.6998
20:00:42.864988 [0] Epoch 00015 | Loss 11.0928
20:00:42.879835 [0] Epoch 00016 | Loss 11.4191
20:00:42.893157 [0] Epoch 00017 | Loss 11.6880
20:00:42.908018 [0] Epoch 00018 | Loss 11.8933
20:00:42.921355 [0] Epoch 00019 | Loss 12.0301
20:00:42.923748 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
20:00:42.924942 [0] 
timer summary:
  0.73s   0.73s    80 mm
  0.10s   0.10s    80 broadcast
  0.27s   0.27s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.26s   1.26s    20 epoch
  2.55s   2.55s     1 total
20:03:25.621455 [0] proc begin: <DistEnv 0/1 nccl>
20:03:28.124030 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:03:28.125005 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:04:40.463188 [0] proc begin: <DistEnv 0/1 nccl>
20:04:42.298508 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:04:42.299462 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:06:39.809761 [0] proc begin: <DistEnv 0/1 nccl>
20:06:41.541520 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:06:41.542324 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:08:07.128299 [0] proc begin: <DistEnv 0/1 nccl>
20:08:08.993574 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:08:08.995608 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:08:46.278843 [0] proc begin: <DistEnv 0/1 nccl>
20:08:47.990089 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:08:47.991452 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:09:11.386997 [0] proc begin: <DistEnv 0/1 nccl>
20:09:13.079023 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:09:13.079842 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:37:10.137132 [0] proc begin: <DistEnv 0/1 nccl>
20:37:11.623634 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:37:11.624382 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:44:40.479020 [0] proc begin: <DistEnv 0/1 nccl>
20:44:42.078650 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:44:42.079648 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:50:06.473358 [0] proc begin: <DistEnv 0/1 nccl>
20:50:08.285996 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:50:08.287179 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:53:19.015279 [0] proc begin: <DistEnv 0/1 nccl>
20:53:20.811381 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:53:20.812246 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:54:28.910341 [0] proc begin: <DistEnv 0/1 nccl>
20:54:30.684362 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
20:54:30.685239 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:10:25.522107 [0] proc begin: <DistEnv 0/1 nccl>
21:10:27.286130 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
21:10:27.287020 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:10:41.242967 [0] proc begin: <DistEnv 0/1 nccl>
21:10:42.811915 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
21:10:42.812749 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

21:10:44.347106 [0] Epoch 00000 | Loss 3.6790
21:10:44.348501 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
21:10:44.367770 [0] Epoch 00001 | Loss 3.4899
21:10:44.387907 [0] Epoch 00002 | Loss 3.4467
21:10:44.407829 [0] Epoch 00003 | Loss 3.5920
21:10:44.427923 [0] Epoch 00004 | Loss 3.9286
21:10:44.447968 [0] Epoch 00005 | Loss 4.4289
21:10:44.468347 [0] Epoch 00006 | Loss 5.0451
21:10:44.487945 [0] Epoch 00007 | Loss 5.7289
21:10:44.508246 [0] Epoch 00008 | Loss 6.4915
21:10:44.528517 [0] Epoch 00009 | Loss 7.3353
21:10:44.547820 [0] Epoch 00010 | Loss 8.1901
21:10:44.550402 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
21:10:44.570587 [0] Epoch 00011 | Loss 8.9962
21:10:44.590653 [0] Epoch 00012 | Loss 9.7551
21:10:44.611192 [0] Epoch 00013 | Loss 10.4471
21:10:44.631342 [0] Epoch 00014 | Loss 11.0453
21:10:44.650971 [0] Epoch 00015 | Loss 11.5694
21:10:44.670729 [0] Epoch 00016 | Loss 11.9875
21:10:44.691048 [0] Epoch 00017 | Loss 12.3576
21:10:44.711147 [0] Epoch 00018 | Loss 12.6982
21:10:44.731651 [0] Epoch 00019 | Loss 12.9855
21:10:44.734187 [0] Epoch: 019, Train: 0.2584, Val: 0.2932, Test: 0.2624
21:10:44.735481 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.41s   0.41s    80 spmm
  1.17s   1.17s    80 mm
  0.04s   0.04s    40 all_reduce
  1.91s   1.91s    20 epoch
  3.49s   3.49s     1 total
09:20:28.903302 [0] proc begin: <DistEnv 0/1 nccl>
09:20:30.701564 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:20:30.702913 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:20:31.846785 [0] Epoch 00000 | Loss 3.6792
09:20:31.848097 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
09:20:31.861387 [0] Epoch 00001 | Loss 3.4872
09:20:31.874617 [0] Epoch 00002 | Loss 3.4445
09:20:31.888592 [0] Epoch 00003 | Loss 3.5919
09:20:31.901268 [0] Epoch 00004 | Loss 3.9270
09:20:31.915157 [0] Epoch 00005 | Loss 4.4153
09:20:31.927768 [0] Epoch 00006 | Loss 5.0240
09:20:31.941858 [0] Epoch 00007 | Loss 5.7201
09:20:31.954587 [0] Epoch 00008 | Loss 6.4735
09:20:31.967813 [0] Epoch 00009 | Loss 7.2819
09:20:31.981200 [0] Epoch 00010 | Loss 8.1136
09:20:31.982647 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
09:20:31.995655 [0] Epoch 00011 | Loss 8.8971
09:20:32.009752 [0] Epoch 00012 | Loss 9.6118
09:20:32.022600 [0] Epoch 00013 | Loss 10.2092
09:20:32.035569 [0] Epoch 00014 | Loss 10.6998
09:20:32.049080 [0] Epoch 00015 | Loss 11.0928
09:20:32.062084 [0] Epoch 00016 | Loss 11.4191
09:20:32.075348 [0] Epoch 00017 | Loss 11.6880
09:20:32.088971 [0] Epoch 00018 | Loss 11.8933
09:20:32.102207 [0] Epoch 00019 | Loss 12.0301
09:20:32.103734 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
09:20:32.104426 [0] 
timer summary:
  0.86s   0.86s    80 mm
  0.09s   0.09s    80 broadcast
  0.26s   0.26s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.40s   1.40s    20 epoch
  3.20s   3.20s     1 total
09:20:56.392359 [0] proc begin: <DistEnv 0/1 nccl>
09:21:25.532427 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:21:25.533831 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:21:26.671843 [0] Epoch 00000 | Loss 3.7185
09:21:26.673456 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:21:26.818805 [0] Epoch 00001 | Loss 2.8261
09:21:26.965947 [0] Epoch 00002 | Loss 2.4048
09:21:27.112895 [0] Epoch 00003 | Loss 2.0652
09:21:27.260224 [0] Epoch 00004 | Loss 1.7978
09:21:27.407982 [0] Epoch 00005 | Loss 1.5821
09:21:27.554906 [0] Epoch 00006 | Loss 1.4100
09:21:27.701915 [0] Epoch 00007 | Loss 1.2561
09:21:27.849306 [0] Epoch 00008 | Loss 1.1295
09:21:27.996525 [0] Epoch 00009 | Loss 1.0056
09:21:28.143430 [0] Epoch 00010 | Loss 0.9125
09:21:28.144870 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:21:28.291786 [0] Epoch 00011 | Loss 0.8407
09:21:28.438701 [0] Epoch 00012 | Loss 0.7934
09:21:28.585875 [0] Epoch 00013 | Loss 0.7497
09:21:28.734263 [0] Epoch 00014 | Loss 0.6850
09:21:28.881697 [0] Epoch 00015 | Loss 0.6420
09:21:29.028977 [0] Epoch 00016 | Loss 0.6202
09:21:29.176814 [0] Epoch 00017 | Loss 0.6034
09:21:29.324602 [0] Epoch 00018 | Loss 0.5762
09:21:29.472464 [0] Epoch 00019 | Loss 0.5484
09:21:29.474990 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:21:29.476190 [0] 
timer summary:
  0.98s   0.98s    80 mm
  0.08s   0.08s    80 broadcast
  2.61s   2.61s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.93s   3.93s    20 epoch
 33.08s  33.08s     1 total
09:22:20.376407 [0] proc begin: <DistEnv 0/1 nccl>
09:22:39.513703 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:22:39.515146 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:22:40.770397 [0] Epoch 00000 | Loss 3.7185
09:22:40.774012 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:22:40.921550 [0] Epoch 00001 | Loss 2.8261
09:22:41.068850 [0] Epoch 00002 | Loss 2.4048
09:22:41.216939 [0] Epoch 00003 | Loss 2.0652
09:22:41.365045 [0] Epoch 00004 | Loss 1.7978
09:22:41.512730 [0] Epoch 00005 | Loss 1.5821
09:22:41.660567 [0] Epoch 00006 | Loss 1.4100
09:22:41.808876 [0] Epoch 00007 | Loss 1.2561
09:22:41.956596 [0] Epoch 00008 | Loss 1.1295
09:22:42.104580 [0] Epoch 00009 | Loss 1.0056
09:22:42.252262 [0] Epoch 00010 | Loss 0.9125
09:22:42.253994 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:22:42.401376 [0] Epoch 00011 | Loss 0.8407
09:22:42.548911 [0] Epoch 00012 | Loss 0.7934
09:22:42.696673 [0] Epoch 00013 | Loss 0.7497
09:22:42.844260 [0] Epoch 00014 | Loss 0.6850
09:22:42.992057 [0] Epoch 00015 | Loss 0.6420
09:22:43.139740 [0] Epoch 00016 | Loss 0.6202
09:22:43.287505 [0] Epoch 00017 | Loss 0.6034
09:22:43.435768 [0] Epoch 00018 | Loss 0.5762
09:22:43.584473 [0] Epoch 00019 | Loss 0.5484
09:22:43.586211 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:22:43.586955 [0] 
timer summary:
  0.99s   0.99s    80 mm
  0.09s   0.09s    80 broadcast
  2.71s   2.71s    80 spmm
  0.04s   0.04s    40 all_reduce
  4.06s   4.06s    20 epoch
 23.21s  23.21s     1 total
09:23:53.541859 [0] proc begin: <DistEnv 0/1 nccl>
09:23:55.343613 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:23:55.344939 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:23:56.949581 [0] Epoch 00000 | Loss 3.6792
09:23:56.951263 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
09:23:56.965444 [0] Epoch 00001 | Loss 3.4872
09:23:56.979921 [0] Epoch 00002 | Loss 3.4445
09:23:56.993132 [0] Epoch 00003 | Loss 3.5919
09:23:57.007911 [0] Epoch 00004 | Loss 3.9270
09:23:57.021058 [0] Epoch 00005 | Loss 4.4153
09:23:57.035691 [0] Epoch 00006 | Loss 5.0240
09:23:57.048816 [0] Epoch 00007 | Loss 5.7201
09:23:57.063383 [0] Epoch 00008 | Loss 6.4735
09:23:57.076532 [0] Epoch 00009 | Loss 7.2819
09:23:57.091073 [0] Epoch 00010 | Loss 8.1136
09:23:57.092519 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
09:23:57.105896 [0] Epoch 00011 | Loss 8.8971
09:23:57.120580 [0] Epoch 00012 | Loss 9.6118
09:23:57.133520 [0] Epoch 00013 | Loss 10.2092
09:23:57.146880 [0] Epoch 00014 | Loss 10.6998
09:23:57.159997 [0] Epoch 00015 | Loss 11.0928
09:23:57.174360 [0] Epoch 00016 | Loss 11.4191
09:23:57.187451 [0] Epoch 00017 | Loss 11.6880
09:23:57.201805 [0] Epoch 00018 | Loss 11.8933
09:23:57.215010 [0] Epoch 00019 | Loss 12.0301
09:23:57.216606 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
09:23:57.217379 [0] 
timer summary:
  1.14s   1.14s    80 mm
  0.10s   0.10s    80 broadcast
  0.43s   0.43s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.87s   1.87s    20 epoch
  3.67s   3.67s     1 total
09:24:22.622475 [0] proc begin: <DistEnv 0/1 nccl>
09:24:24.435744 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:24:24.437076 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:24:25.520522 [0] Epoch 00000 | Loss 3.6792
09:24:25.523506 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
09:24:25.539082 [0] Epoch 00001 | Loss 3.4872
09:24:25.553574 [0] Epoch 00002 | Loss 3.4445
09:24:25.566622 [0] Epoch 00003 | Loss 3.5919
09:24:25.580552 [0] Epoch 00004 | Loss 3.9270
09:24:25.593605 [0] Epoch 00005 | Loss 4.4153
09:24:25.607536 [0] Epoch 00006 | Loss 5.0240
09:24:25.620157 [0] Epoch 00007 | Loss 5.7201
09:24:25.634285 [0] Epoch 00008 | Loss 6.4735
09:24:25.646845 [0] Epoch 00009 | Loss 7.2819
09:24:25.660793 [0] Epoch 00010 | Loss 8.1136
09:24:25.662171 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
09:24:25.674902 [0] Epoch 00011 | Loss 8.8971
09:24:25.688523 [0] Epoch 00012 | Loss 9.6118
09:24:25.701330 [0] Epoch 00013 | Loss 10.2092
09:24:25.714173 [0] Epoch 00014 | Loss 10.6998
09:24:25.727250 [0] Epoch 00015 | Loss 11.0928
09:24:25.740805 [0] Epoch 00016 | Loss 11.4191
09:24:25.753603 [0] Epoch 00017 | Loss 11.6880
09:24:25.766601 [0] Epoch 00018 | Loss 11.8933
09:24:25.779462 [0] Epoch 00019 | Loss 12.0301
09:24:25.781232 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
09:24:25.782101 [0] 
timer summary:
  0.80s   0.80s    80 mm
  0.09s   0.09s    80 broadcast
  0.26s   0.26s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.34s   1.34s    20 epoch
  3.16s   3.16s     1 total
09:24:31.533332 [0] proc begin: <DistEnv 0/1 nccl>
09:24:32.607360 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:24:32.608199 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:24:33.599865 [0] Epoch 00000 | Loss 3.6792
09:24:33.602742 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
09:24:33.618658 [0] Epoch 00001 | Loss 3.4872
09:24:33.632936 [0] Epoch 00002 | Loss 3.4445
09:24:33.645510 [0] Epoch 00003 | Loss 3.5919
09:24:33.659573 [0] Epoch 00004 | Loss 3.9270
09:24:33.672207 [0] Epoch 00005 | Loss 4.4153
09:24:33.686200 [0] Epoch 00006 | Loss 5.0240
09:24:33.698862 [0] Epoch 00007 | Loss 5.7201
09:24:33.712795 [0] Epoch 00008 | Loss 6.4735
09:24:33.725608 [0] Epoch 00009 | Loss 7.2819
09:24:33.738847 [0] Epoch 00010 | Loss 8.1136
09:24:33.741000 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
09:24:33.754876 [0] Epoch 00011 | Loss 8.8971
09:24:33.767493 [0] Epoch 00012 | Loss 9.6118
09:24:33.781433 [0] Epoch 00013 | Loss 10.2092
09:24:33.794265 [0] Epoch 00014 | Loss 10.6998
09:24:33.807437 [0] Epoch 00015 | Loss 11.0928
09:24:33.820934 [0] Epoch 00016 | Loss 11.4191
09:24:33.833976 [0] Epoch 00017 | Loss 11.6880
09:24:33.847155 [0] Epoch 00018 | Loss 11.8933
09:24:33.861072 [0] Epoch 00019 | Loss 12.0301
09:24:33.862445 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
09:24:33.863031 [0] 
timer summary:
  0.70s   0.70s    80 mm
  0.09s   0.09s    80 broadcast
  0.26s   0.26s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.25s   1.25s    20 epoch
  2.33s   2.33s     1 total
09:24:37.875553 [0] proc begin: <DistEnv 0/1 nccl>
09:24:38.941619 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:24:38.942420 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| Active memory         |  112577 KB |  171162 KB |  201452 KB |   88875 KB |
|       from large pool |  112081 KB |  170666 KB |  200786 KB |   88705 KB |
|       from small pool |     496 KB |     498 KB |     666 KB |     170 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   81982 KB |   81982 KB |   94796 KB |   12813 KB |
|       from large pool |   80431 KB |   80431 KB |   91026 KB |   10595 KB |
|       from small pool |    1551 KB |    1882 KB |    3769 KB |    2218 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      21    |      14    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       9    |       5    |
|       from large pool |       3    |       3    |       7    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:24:39.926633 [0] Epoch 00000 | Loss 3.6792
09:24:39.929606 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
09:24:39.944078 [0] Epoch 00001 | Loss 3.4872
09:24:39.957937 [0] Epoch 00002 | Loss 3.4445
09:24:39.970664 [0] Epoch 00003 | Loss 3.5919
09:24:39.984587 [0] Epoch 00004 | Loss 3.9270
09:24:39.997512 [0] Epoch 00005 | Loss 4.4153
09:24:40.010467 [0] Epoch 00006 | Loss 5.0240
09:24:40.023575 [0] Epoch 00007 | Loss 5.7201
09:24:40.036878 [0] Epoch 00008 | Loss 6.4735
09:24:40.049775 [0] Epoch 00009 | Loss 7.2819
09:24:40.063777 [0] Epoch 00010 | Loss 8.1136
09:24:40.065201 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
09:24:40.077986 [0] Epoch 00011 | Loss 8.8971
09:24:40.091087 [0] Epoch 00012 | Loss 9.6118
09:24:40.104667 [0] Epoch 00013 | Loss 10.2092
09:24:40.117510 [0] Epoch 00014 | Loss 10.6998
09:24:40.131518 [0] Epoch 00015 | Loss 11.0928
09:24:40.144736 [0] Epoch 00016 | Loss 11.4191
09:24:40.157981 [0] Epoch 00017 | Loss 11.6880
09:24:40.171401 [0] Epoch 00018 | Loss 11.8933
09:24:40.185213 [0] Epoch 00019 | Loss 12.0301
09:24:40.186691 [0] Epoch: 019, Train: 0.2638, Val: 0.2952, Test: 0.2645
09:24:40.187382 [0] 
timer summary:
  0.70s   0.70s    80 mm
  0.08s   0.08s    80 broadcast
  0.26s   0.26s    80 spmm
  0.04s   0.04s    40 all_reduce
  1.24s   1.24s    20 epoch
  2.31s   2.31s     1 total
09:24:47.994908 [0] proc begin: <DistEnv 0/1 nccl>
09:25:05.187414 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:25:05.188835 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:06.365329 [0] Epoch 00000 | Loss 3.7185
09:25:06.368398 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:25:06.514960 [0] Epoch 00001 | Loss 2.8261
09:25:06.661754 [0] Epoch 00002 | Loss 2.4048
09:25:06.809128 [0] Epoch 00003 | Loss 2.0652
09:25:06.956419 [0] Epoch 00004 | Loss 1.7978
09:25:07.103347 [0] Epoch 00005 | Loss 1.5821
09:25:07.250552 [0] Epoch 00006 | Loss 1.4100
09:25:07.398073 [0] Epoch 00007 | Loss 1.2561
09:25:07.545351 [0] Epoch 00008 | Loss 1.1295
09:25:07.692708 [0] Epoch 00009 | Loss 1.0056
09:25:07.840134 [0] Epoch 00010 | Loss 0.9125
09:25:07.841587 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:25:07.988310 [0] Epoch 00011 | Loss 0.8407
09:25:08.135575 [0] Epoch 00012 | Loss 0.7934
09:25:08.282753 [0] Epoch 00013 | Loss 0.7497
09:25:08.429698 [0] Epoch 00014 | Loss 0.6850
09:25:08.577378 [0] Epoch 00015 | Loss 0.6420
09:25:08.724400 [0] Epoch 00016 | Loss 0.6202
09:25:08.872173 [0] Epoch 00017 | Loss 0.6034
09:25:09.019995 [0] Epoch 00018 | Loss 0.5762
09:25:09.167585 [0] Epoch 00019 | Loss 0.5484
09:25:09.169910 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:25:09.171126 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.09s   0.09s    80 broadcast
  2.62s   2.62s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.97s   3.97s    20 epoch
 21.18s  21.18s     1 total
09:25:21.780733 [0] proc begin: <DistEnv 0/1 nccl>
09:25:39.117537 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:25:39.118937 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:25:40.255160 [0] Epoch 00000 | Loss 3.7185
09:25:40.258250 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:25:40.404395 [0] Epoch 00001 | Loss 2.8261
09:25:40.551647 [0] Epoch 00002 | Loss 2.4048
09:25:40.699464 [0] Epoch 00003 | Loss 2.0652
09:25:40.846503 [0] Epoch 00004 | Loss 1.7978
09:25:40.993745 [0] Epoch 00005 | Loss 1.5821
09:25:41.141749 [0] Epoch 00006 | Loss 1.4100
09:25:41.289750 [0] Epoch 00007 | Loss 1.2561
09:25:41.436044 [0] Epoch 00008 | Loss 1.1295
09:25:41.584483 [0] Epoch 00009 | Loss 1.0056
09:25:41.731953 [0] Epoch 00010 | Loss 0.9125
09:25:41.733374 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:25:41.880417 [0] Epoch 00011 | Loss 0.8407
09:25:42.028752 [0] Epoch 00012 | Loss 0.7934
09:25:42.175576 [0] Epoch 00013 | Loss 0.7497
09:25:42.322889 [0] Epoch 00014 | Loss 0.6850
09:25:42.470438 [0] Epoch 00015 | Loss 0.6420
09:25:42.617807 [0] Epoch 00016 | Loss 0.6202
09:25:42.765376 [0] Epoch 00017 | Loss 0.6034
09:25:42.913004 [0] Epoch 00018 | Loss 0.5762
09:25:43.061343 [0] Epoch 00019 | Loss 0.5484
09:25:43.063887 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:25:43.065090 [0] 
timer summary:
  0.98s   0.98s    80 mm
  0.08s   0.08s    80 broadcast
  2.62s   2.62s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.94s   3.94s    20 epoch
 21.28s  21.28s     1 total
09:25:49.680039 [0] proc begin: <DistEnv 0/1 nccl>
09:26:08.347779 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:26:08.349224 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:09.515005 [0] Epoch 00000 | Loss 3.7185
09:26:09.518069 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:26:09.664773 [0] Epoch 00001 | Loss 2.8261
09:26:09.812583 [0] Epoch 00002 | Loss 2.4048
09:26:09.960247 [0] Epoch 00003 | Loss 2.0652
09:26:10.107539 [0] Epoch 00004 | Loss 1.7978
09:26:10.254960 [0] Epoch 00005 | Loss 1.5821
09:26:10.403034 [0] Epoch 00006 | Loss 1.4100
09:26:10.550749 [0] Epoch 00007 | Loss 1.2561
09:26:10.698336 [0] Epoch 00008 | Loss 1.1295
09:26:10.846002 [0] Epoch 00009 | Loss 1.0056
09:26:10.993689 [0] Epoch 00010 | Loss 0.9125
09:26:10.995123 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:26:11.142381 [0] Epoch 00011 | Loss 0.8407
09:26:11.290340 [0] Epoch 00012 | Loss 0.7934
09:26:11.438576 [0] Epoch 00013 | Loss 0.7497
09:26:11.585989 [0] Epoch 00014 | Loss 0.6850
09:26:11.732961 [0] Epoch 00015 | Loss 0.6420
09:26:11.881015 [0] Epoch 00016 | Loss 0.6202
09:26:12.028410 [0] Epoch 00017 | Loss 0.6034
09:26:12.176405 [0] Epoch 00018 | Loss 0.5762
09:26:12.323926 [0] Epoch 00019 | Loss 0.5484
09:26:12.325397 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:26:12.325816 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.09s   0.09s    80 broadcast
  2.63s   2.63s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.97s   3.97s    20 epoch
 22.65s  22.65s     1 total
09:26:40.689130 [0] proc begin: <DistEnv 0/1 nccl>
09:26:57.953864 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:26:57.955021 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:26:59.327258 [0] Epoch 00000 | Loss 3.7185
09:26:59.330285 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:26:59.682342 [0] Epoch 00001 | Loss 2.8261
09:27:00.034124 [0] Epoch 00002 | Loss 2.4048
09:27:00.385936 [0] Epoch 00003 | Loss 2.0652
09:27:00.738302 [0] Epoch 00004 | Loss 1.7978
09:27:01.090184 [0] Epoch 00005 | Loss 1.5821
09:27:01.441724 [0] Epoch 00006 | Loss 1.4100
09:27:01.794947 [0] Epoch 00007 | Loss 1.2561
09:27:02.148616 [0] Epoch 00008 | Loss 1.1295
09:27:02.501963 [0] Epoch 00009 | Loss 1.0056
09:27:02.855690 [0] Epoch 00010 | Loss 0.9125
09:27:02.857747 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:27:03.211945 [0] Epoch 00011 | Loss 0.8407
09:27:03.566020 [0] Epoch 00012 | Loss 0.7934
09:27:03.919689 [0] Epoch 00013 | Loss 0.7497
09:27:04.273622 [0] Epoch 00014 | Loss 0.6850
09:27:04.627219 [0] Epoch 00015 | Loss 0.6420
09:27:04.981587 [0] Epoch 00016 | Loss 0.6202
09:27:05.334824 [0] Epoch 00017 | Loss 0.6034
09:27:05.689196 [0] Epoch 00018 | Loss 0.5762
09:27:06.042351 [0] Epoch 00019 | Loss 0.5484
09:27:06.043984 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:27:06.044681 [0] 
timer summary:
  0.99s   0.99s    80 mm
  0.09s   0.09s    80 broadcast
  6.75s   6.75s    80 spmm
  0.04s   0.04s    40 all_reduce
  8.08s   8.08s    20 epoch
 25.35s  25.35s     1 total
09:27:19.572222 [0] proc begin: <DistEnv 0/1 nccl>
09:27:36.499414 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:27:36.500369 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:27:37.636004 [0] Epoch 00000 | Loss 3.7185
09:27:37.637546 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:27:37.765717 [0] Epoch 00001 | Loss 2.8261
09:27:37.895613 [0] Epoch 00002 | Loss 2.4048
09:27:38.026651 [0] Epoch 00003 | Loss 2.0652
09:27:38.157279 [0] Epoch 00004 | Loss 1.7978
09:27:38.287745 [0] Epoch 00005 | Loss 1.5821
09:27:38.418592 [0] Epoch 00006 | Loss 1.4100
09:27:38.549363 [0] Epoch 00007 | Loss 1.2561
09:27:38.679629 [0] Epoch 00008 | Loss 1.1295
09:27:38.810405 [0] Epoch 00009 | Loss 1.0056
09:27:38.941705 [0] Epoch 00010 | Loss 0.9125
09:27:38.944081 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:27:39.074369 [0] Epoch 00011 | Loss 0.8407
09:27:39.205634 [0] Epoch 00012 | Loss 0.7934
09:27:39.336349 [0] Epoch 00013 | Loss 0.7497
09:27:39.467282 [0] Epoch 00014 | Loss 0.6850
09:27:39.598177 [0] Epoch 00015 | Loss 0.6420
09:27:39.729027 [0] Epoch 00016 | Loss 0.6202
09:27:39.859732 [0] Epoch 00017 | Loss 0.6034
09:27:39.990437 [0] Epoch 00018 | Loss 0.5762
09:27:40.121158 [0] Epoch 00019 | Loss 0.5484
09:27:40.122598 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:27:40.123023 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.08s   0.08s    80 broadcast
  2.28s   2.28s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.61s   3.61s    20 epoch
 20.55s  20.55s     1 total
09:35:22.877037 [0] proc begin: <DistEnv 0/1 nccl>
09:35:24.508078 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:35:24.508733 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:36:29.689090 [0] proc begin: <DistEnv 0/1 nccl>
09:36:31.116597 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
09:36:31.117535 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:36:32.110045 [0] Epoch 00000 | Loss 3.6823
09:36:32.111342 [0] Epoch: 000, Train: 0.0249, Val: 0.0289, Test: 0.0297
09:36:32.131598 [0] Epoch 00001 | Loss 3.5772
09:36:32.154320 [0] Epoch 00002 | Loss 3.5189
09:36:32.176743 [0] Epoch 00003 | Loss 3.5594
09:36:32.197706 [0] Epoch 00004 | Loss 3.7394
09:36:32.219963 [0] Epoch 00005 | Loss 4.0742
09:36:32.242727 [0] Epoch 00006 | Loss 4.5672
09:36:32.270936 [0] Epoch 00007 | Loss 5.2176
09:36:32.294528 [0] Epoch 00008 | Loss 6.0189
09:36:32.315897 [0] Epoch 00009 | Loss 6.9565
09:36:32.339213 [0] Epoch 00010 | Loss 8.0071
09:36:32.341512 [0] Epoch: 010, Train: 0.1793, Val: 0.0769, Test: 0.0589
09:36:32.364035 [0] Epoch 00011 | Loss 9.1417
09:36:32.385955 [0] Epoch 00012 | Loss 10.3307
09:36:32.406896 [0] Epoch 00013 | Loss 11.5634
09:36:32.430817 [0] Epoch 00014 | Loss 12.8575
09:36:32.453812 [0] Epoch 00015 | Loss 14.2119
09:36:32.474892 [0] Epoch 00016 | Loss 15.6002
09:36:32.498499 [0] Epoch 00017 | Loss 17.0121
09:36:32.520884 [0] Epoch 00018 | Loss 18.4340
09:36:32.541599 [0] Epoch 00019 | Loss 19.8466
09:36:32.543016 [0] Epoch: 019, Train: 0.1100, Val: 0.2297, Test: 0.2156
09:36:32.543663 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.36s   0.36s    80 spmm
  0.74s   0.74s    80 mm
  0.04s   0.04s    40 all_reduce
  1.42s   1.42s    20 epoch
  2.85s   2.85s     1 total
09:36:52.056536 [0] proc begin: <DistEnv 0/1 nccl>
09:37:12.400377 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:37:12.401619 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:37:14.145256 [0] Epoch 00000 | Loss 3.6991
09:37:14.148201 [0] Epoch: 000, Train: 0.0139, Val: 0.0110, Test: 0.0119
09:37:14.879039 [0] Epoch 00001 | Loss 2.7906
09:37:15.608265 [0] Epoch 00002 | Loss 2.1701
09:37:16.335047 [0] Epoch 00003 | Loss 1.7998
09:37:17.060145 [0] Epoch 00004 | Loss 1.5236
09:37:17.787263 [0] Epoch 00005 | Loss 1.3030
09:37:18.514735 [0] Epoch 00006 | Loss 1.1615
09:37:19.240808 [0] Epoch 00007 | Loss 1.0303
09:37:19.966131 [0] Epoch 00008 | Loss 0.9125
09:37:20.692275 [0] Epoch 00009 | Loss 0.8381
09:37:21.417065 [0] Epoch 00010 | Loss 0.7888
09:37:21.418503 [0] Epoch: 010, Train: 0.8566, Val: 0.8700, Test: 0.8663
09:37:22.149956 [0] Epoch 00011 | Loss 0.7392
09:37:22.882181 [0] Epoch 00012 | Loss 0.7055
09:37:23.610900 [0] Epoch 00013 | Loss 0.6819
09:37:24.342966 [0] Epoch 00014 | Loss 0.6626
09:37:25.071179 [0] Epoch 00015 | Loss 0.6389
09:37:25.802632 [0] Epoch 00016 | Loss 0.6254
09:37:26.531773 [0] Epoch 00017 | Loss 0.6181
09:37:27.261628 [0] Epoch 00018 | Loss 0.6122
09:37:27.987625 [0] Epoch 00019 | Loss 0.6043
09:37:27.990054 [0] Epoch: 019, Train: 0.8853, Val: 0.8953, Test: 0.8912
09:37:27.991457 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.14s  14.14s    80 spmm
  0.98s   0.98s    80 mm
  0.04s   0.04s    40 all_reduce
 15.58s  15.58s    20 epoch
 35.93s  35.93s     1 total
09:39:25.541475 [0] proc begin: <DistEnv 0/1 nccl>
09:39:45.155883 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:39:45.158088 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:39:46.922867 [0] Epoch 00000 | Loss 3.6991
09:39:46.926081 [0] Epoch: 000, Train: 0.0139, Val: 0.0110, Test: 0.0119
09:39:47.656737 [0] Epoch 00001 | Loss 2.7906
09:39:48.386691 [0] Epoch 00002 | Loss 2.1701
09:39:49.114306 [0] Epoch 00003 | Loss 1.7998
09:39:49.841179 [0] Epoch 00004 | Loss 1.5236
09:39:50.567862 [0] Epoch 00005 | Loss 1.3030
09:39:51.292931 [0] Epoch 00006 | Loss 1.1615
09:39:52.019849 [0] Epoch 00007 | Loss 1.0303
09:39:52.745976 [0] Epoch 00008 | Loss 0.9125
09:39:53.470734 [0] Epoch 00009 | Loss 0.8381
09:39:54.195154 [0] Epoch 00010 | Loss 0.7888
09:39:54.197730 [0] Epoch: 010, Train: 0.8566, Val: 0.8700, Test: 0.8663
09:39:54.928915 [0] Epoch 00011 | Loss 0.7392
09:39:55.658984 [0] Epoch 00012 | Loss 0.7055
09:39:56.387096 [0] Epoch 00013 | Loss 0.6819
09:39:57.117367 [0] Epoch 00014 | Loss 0.6626
09:39:57.845240 [0] Epoch 00015 | Loss 0.6389
09:39:58.577450 [0] Epoch 00016 | Loss 0.6254
09:39:59.304789 [0] Epoch 00017 | Loss 0.6181
09:40:00.034550 [0] Epoch 00018 | Loss 0.6122
09:40:00.761405 [0] Epoch 00019 | Loss 0.6043
09:40:00.763757 [0] Epoch: 019, Train: 0.8853, Val: 0.8953, Test: 0.8912
09:40:00.765478 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.14s  14.14s    80 spmm
  1.00s   1.00s    80 mm
  0.03s   0.03s    40 all_reduce
 15.59s  15.59s    20 epoch
 35.22s  35.22s     1 total
09:42:03.650766 [0] proc begin: <DistEnv 0/1 nccl>
09:42:26.615461 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:42:26.616840 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:42:28.377413 [0] Epoch 00000 | Loss 3.6996
09:42:28.380366 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
09:42:29.115066 [0] Epoch 00001 | Loss 2.8474
09:42:29.838865 [0] Epoch 00002 | Loss 2.2357
09:42:30.562467 [0] Epoch 00003 | Loss 1.8348
09:42:31.285412 [0] Epoch 00004 | Loss 1.5465
09:42:32.007110 [0] Epoch 00005 | Loss 1.3214
09:42:32.730658 [0] Epoch 00006 | Loss 1.1282
09:42:33.454155 [0] Epoch 00007 | Loss 1.0042
09:42:34.177451 [0] Epoch 00008 | Loss 0.8937
09:42:34.899841 [0] Epoch 00009 | Loss 0.7880
09:42:35.622061 [0] Epoch 00010 | Loss 0.7122
09:42:35.624807 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
09:42:36.351817 [0] Epoch 00011 | Loss 0.6693
09:42:37.078463 [0] Epoch 00012 | Loss 0.6227
09:42:37.801369 [0] Epoch 00013 | Loss 0.5815
09:42:38.527426 [0] Epoch 00014 | Loss 0.5569
09:42:39.251133 [0] Epoch 00015 | Loss 0.5312
09:42:39.977901 [0] Epoch 00016 | Loss 0.5067
09:42:40.700860 [0] Epoch 00017 | Loss 0.4871
09:42:41.426823 [0] Epoch 00018 | Loss 0.4698
09:42:42.149703 [0] Epoch 00019 | Loss 0.4566
09:42:42.152219 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
09:42:42.153569 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
 14.06s  14.06s    80 spmm
  1.01s   1.01s    80 mm
  0.03s   0.03s    40 all_reduce
 15.52s  15.52s    20 epoch
 38.50s  38.50s     1 total
09:43:15.287544 [0] proc begin: <DistEnv 0/1 nccl>
09:43:33.260978 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:43:33.262215 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:43:34.986842 [0] Epoch 00000 | Loss 3.6996
09:43:34.989968 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
09:43:35.714734 [0] Epoch 00001 | Loss 2.8474
09:43:36.438578 [0] Epoch 00002 | Loss 2.2357
09:43:37.160910 [0] Epoch 00003 | Loss 1.8348
09:43:37.882385 [0] Epoch 00004 | Loss 1.5465
09:43:38.604003 [0] Epoch 00005 | Loss 1.3214
09:43:39.326231 [0] Epoch 00006 | Loss 1.1282
09:43:40.048479 [0] Epoch 00007 | Loss 1.0042
09:43:40.770878 [0] Epoch 00008 | Loss 0.8937
09:43:41.492816 [0] Epoch 00009 | Loss 0.7880
09:43:42.214200 [0] Epoch 00010 | Loss 0.7122
09:43:42.215617 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
09:43:42.942334 [0] Epoch 00011 | Loss 0.6693
09:43:43.667331 [0] Epoch 00012 | Loss 0.6227
09:43:44.389206 [0] Epoch 00013 | Loss 0.5815
09:43:45.113863 [0] Epoch 00014 | Loss 0.5569
09:43:45.835506 [0] Epoch 00015 | Loss 0.5312
09:43:46.560026 [0] Epoch 00016 | Loss 0.5067
09:43:47.281903 [0] Epoch 00017 | Loss 0.4871
09:43:48.007063 [0] Epoch 00018 | Loss 0.4698
09:43:48.728775 [0] Epoch 00019 | Loss 0.4566
09:43:48.730168 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
09:43:48.730579 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
 14.05s  14.05s    80 spmm
  0.97s   0.97s    80 mm
  0.04s   0.04s    40 all_reduce
 15.46s  15.46s    20 epoch
 33.44s  33.44s     1 total
09:44:02.470675 [0] proc begin: <DistEnv 0/1 nccl>
09:44:23.114647 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:44:23.117896 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:44:25.286130 [0] Epoch 00000 | Loss 3.6996
09:44:25.289540 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
09:44:26.015625 [0] Epoch 00001 | Loss 2.8474
09:44:26.738863 [0] Epoch 00002 | Loss 2.2357
09:44:27.462328 [0] Epoch 00003 | Loss 1.8348
09:44:28.185009 [0] Epoch 00004 | Loss 1.5465
09:44:28.909704 [0] Epoch 00005 | Loss 1.3214
09:44:29.632330 [0] Epoch 00006 | Loss 1.1282
09:44:30.355459 [0] Epoch 00007 | Loss 1.0042
09:44:31.078699 [0] Epoch 00008 | Loss 0.8937
09:44:31.801382 [0] Epoch 00009 | Loss 0.7880
09:44:32.524769 [0] Epoch 00010 | Loss 0.7122
09:44:32.526303 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
09:44:33.253296 [0] Epoch 00011 | Loss 0.6693
09:44:33.979445 [0] Epoch 00012 | Loss 0.6227
09:44:34.702604 [0] Epoch 00013 | Loss 0.5815
09:44:35.428479 [0] Epoch 00014 | Loss 0.5569
09:44:36.151465 [0] Epoch 00015 | Loss 0.5312
09:44:36.877009 [0] Epoch 00016 | Loss 0.5067
09:44:37.598983 [0] Epoch 00017 | Loss 0.4871
09:44:38.324026 [0] Epoch 00018 | Loss 0.4698
09:44:39.046628 [0] Epoch 00019 | Loss 0.4566
09:44:39.049055 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
09:44:39.050897 [0] 
timer summary:
  0.10s   0.10s    80 broadcast
 14.22s  14.22s    80 spmm
  1.23s   1.23s    80 mm
  0.04s   0.04s    40 all_reduce
 15.92s  15.92s    20 epoch
 36.58s  36.58s     1 total
09:45:41.503288 [0] proc begin: <DistEnv 0/1 nccl>
09:46:02.060124 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:46:02.061795 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:46:04.275657 [0] Epoch 00000 | Loss 3.6996
09:46:04.279314 [0] Epoch: 000, Train: 0.0163, Val: 0.0146, Test: 0.0152
09:46:05.005830 [0] Epoch 00001 | Loss 2.8474
09:46:05.731135 [0] Epoch 00002 | Loss 2.2357
09:46:06.453622 [0] Epoch 00003 | Loss 1.8348
09:46:07.177193 [0] Epoch 00004 | Loss 1.5465
09:46:07.901496 [0] Epoch 00005 | Loss 1.3214
09:46:08.624600 [0] Epoch 00006 | Loss 1.1282
09:46:09.347598 [0] Epoch 00007 | Loss 1.0042
09:46:10.071271 [0] Epoch 00008 | Loss 0.8937
09:46:10.793192 [0] Epoch 00009 | Loss 0.7880
09:46:11.515161 [0] Epoch 00010 | Loss 0.7122
09:46:11.517673 [0] Epoch: 010, Train: 0.8793, Val: 0.8920, Test: 0.8904
09:46:12.244241 [0] Epoch 00011 | Loss 0.6693
09:46:12.969020 [0] Epoch 00012 | Loss 0.6227
09:46:13.691907 [0] Epoch 00013 | Loss 0.5815
09:46:14.417764 [0] Epoch 00014 | Loss 0.5569
09:46:15.140751 [0] Epoch 00015 | Loss 0.5312
09:46:15.866574 [0] Epoch 00016 | Loss 0.5067
09:46:16.588866 [0] Epoch 00017 | Loss 0.4871
09:46:17.314501 [0] Epoch 00018 | Loss 0.4698
09:46:18.036890 [0] Epoch 00019 | Loss 0.4566
09:46:18.038386 [0] Epoch: 019, Train: 0.9108, Val: 0.9201, Test: 0.9192
09:46:18.039047 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
 14.15s  14.15s    80 spmm
  1.35s   1.35s    80 mm
  0.04s   0.04s    40 all_reduce
 15.96s  15.96s    20 epoch
 36.54s  36.54s     1 total
09:46:34.330207 [0] proc begin: <DistEnv 0/1 nccl>
09:46:51.797848 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:46:51.798787 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:46:52.946290 [0] Epoch 00000 | Loss 3.7185
09:46:52.949256 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:46:53.078550 [0] Epoch 00001 | Loss 2.8261
09:46:53.210017 [0] Epoch 00002 | Loss 2.4048
09:46:53.341738 [0] Epoch 00003 | Loss 2.0652
09:46:53.473969 [0] Epoch 00004 | Loss 1.7978
09:46:53.605143 [0] Epoch 00005 | Loss 1.5821
09:46:53.736964 [0] Epoch 00006 | Loss 1.4100
09:46:53.867575 [0] Epoch 00007 | Loss 1.2561
09:46:53.998058 [0] Epoch 00008 | Loss 1.1295
09:46:54.129158 [0] Epoch 00009 | Loss 1.0056
09:46:54.260022 [0] Epoch 00010 | Loss 0.9125
09:46:54.261495 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:46:54.391927 [0] Epoch 00011 | Loss 0.8407
09:46:54.522008 [0] Epoch 00012 | Loss 0.7934
09:46:54.652849 [0] Epoch 00013 | Loss 0.7497
09:46:54.782787 [0] Epoch 00014 | Loss 0.6850
09:46:54.913372 [0] Epoch 00015 | Loss 0.6420
09:46:55.043863 [0] Epoch 00016 | Loss 0.6202
09:46:55.174479 [0] Epoch 00017 | Loss 0.6034
09:46:55.304890 [0] Epoch 00018 | Loss 0.5762
09:46:55.435634 [0] Epoch 00019 | Loss 0.5484
09:46:55.437071 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:46:55.437490 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.08s   0.08s    80 broadcast
  2.28s   2.28s    80 spmm
  0.05s   0.05s    40 all_reduce
  3.63s   3.63s    20 epoch
 21.11s  21.11s     1 total
09:47:21.383976 [0] proc begin: <DistEnv 0/1 nccl>
09:47:41.318856 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:47:41.320016 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:47:42.472751 [0] Epoch 00000 | Loss 3.7185
09:47:42.474308 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:47:42.603017 [0] Epoch 00001 | Loss 2.8261
09:47:42.733915 [0] Epoch 00002 | Loss 2.4048
09:47:42.864469 [0] Epoch 00003 | Loss 2.0652
09:47:42.995552 [0] Epoch 00004 | Loss 1.7978
09:47:43.125757 [0] Epoch 00005 | Loss 1.5821
09:47:43.256161 [0] Epoch 00006 | Loss 1.4100
09:47:43.386901 [0] Epoch 00007 | Loss 1.2561
09:47:43.517561 [0] Epoch 00008 | Loss 1.1295
09:47:43.648932 [0] Epoch 00009 | Loss 1.0056
09:47:43.779622 [0] Epoch 00010 | Loss 0.9125
09:47:43.781071 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:47:43.911743 [0] Epoch 00011 | Loss 0.8407
09:47:44.042403 [0] Epoch 00012 | Loss 0.7934
09:47:44.173134 [0] Epoch 00013 | Loss 0.7497
09:47:44.303724 [0] Epoch 00014 | Loss 0.6850
09:47:44.434545 [0] Epoch 00015 | Loss 0.6420
09:47:44.565179 [0] Epoch 00016 | Loss 0.6202
09:47:44.696351 [0] Epoch 00017 | Loss 0.6034
09:47:44.827350 [0] Epoch 00018 | Loss 0.5762
09:47:44.958639 [0] Epoch 00019 | Loss 0.5484
09:47:44.961265 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:47:44.962608 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.09s   0.09s    80 broadcast
  2.29s   2.29s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.63s   3.63s    20 epoch
 23.58s  23.58s     1 total
09:48:24.054489 [0] proc begin: <DistEnv 0/1 nccl>
09:50:44.673382 [0] proc begin: <DistEnv 0/1 nccl>
09:51:01.868066 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:51:01.869125 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:51:03.269865 [0] Epoch 00000 | Loss 3.7185
09:51:03.271414 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:51:03.400420 [0] Epoch 00001 | Loss 2.8261
09:51:03.530421 [0] Epoch 00002 | Loss 2.4048
09:51:03.660335 [0] Epoch 00003 | Loss 2.0652
09:51:03.790591 [0] Epoch 00004 | Loss 1.7978
09:51:03.920504 [0] Epoch 00005 | Loss 1.5821
09:51:04.051022 [0] Epoch 00006 | Loss 1.4100
09:51:04.181213 [0] Epoch 00007 | Loss 1.2561
09:51:04.311368 [0] Epoch 00008 | Loss 1.1295
09:51:04.441014 [0] Epoch 00009 | Loss 1.0056
09:51:04.570839 [0] Epoch 00010 | Loss 0.9125
09:51:04.572278 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:51:04.701949 [0] Epoch 00011 | Loss 0.8407
09:51:04.832236 [0] Epoch 00012 | Loss 0.7934
09:51:04.962332 [0] Epoch 00013 | Loss 0.7497
09:51:05.092814 [0] Epoch 00014 | Loss 0.6850
09:51:05.223266 [0] Epoch 00015 | Loss 0.6420
09:51:05.353833 [0] Epoch 00016 | Loss 0.6202
09:51:05.484309 [0] Epoch 00017 | Loss 0.6034
09:51:05.614298 [0] Epoch 00018 | Loss 0.5762
09:51:05.744384 [0] Epoch 00019 | Loss 0.5484
09:51:05.745839 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:51:05.746269 [0] 
timer summary:
  1.19s   1.19s    80 mm
  0.08s   0.08s    80 broadcast
  2.34s   2.34s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.87s   3.87s    20 epoch
 21.07s  21.07s     1 total
09:51:39.606644 [0] proc begin: <DistEnv 0/1 nccl>
09:51:57.116155 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:51:57.117514 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:51:58.394370 [0] Epoch 00000 | Loss 3.7185
09:51:58.396035 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:51:58.527686 [0] Epoch 00001 | Loss 2.8261
09:51:58.660226 [0] Epoch 00002 | Loss 2.4048
09:51:58.794615 [0] Epoch 00003 | Loss 2.0652
09:51:58.928834 [0] Epoch 00004 | Loss 1.7978
09:51:59.062595 [0] Epoch 00005 | Loss 1.5821
09:51:59.196886 [0] Epoch 00006 | Loss 1.4100
09:51:59.330867 [0] Epoch 00007 | Loss 1.2561
09:51:59.463961 [0] Epoch 00008 | Loss 1.1295
09:51:59.597507 [0] Epoch 00009 | Loss 1.0056
09:51:59.731350 [0] Epoch 00010 | Loss 0.9125
09:51:59.732891 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:51:59.866437 [0] Epoch 00011 | Loss 0.8407
09:52:00.007104 [0] Epoch 00012 | Loss 0.7934
09:52:00.140767 [0] Epoch 00013 | Loss 0.7497
09:52:00.274337 [0] Epoch 00014 | Loss 0.6850
09:52:00.407519 [0] Epoch 00015 | Loss 0.6420
09:52:00.542005 [0] Epoch 00016 | Loss 0.6202
09:52:00.675914 [0] Epoch 00017 | Loss 0.6034
09:52:00.809611 [0] Epoch 00018 | Loss 0.5762
09:52:00.944243 [0] Epoch 00019 | Loss 0.5484
09:52:00.946813 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:52:00.948051 [0] 
timer summary:
  1.02s   1.02s    80 mm
  0.09s   0.09s    80 broadcast
  2.44s   2.44s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.82s   3.82s    20 epoch
 21.34s  21.34s     1 total
09:52:20.884216 [0] proc begin: <DistEnv 0/1 nccl>
09:52:41.073379 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:52:41.074591 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:52:42.227878 [0] Epoch 00000 | Loss 3.7185
09:52:42.229504 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:52:42.363156 [0] Epoch 00001 | Loss 2.8261
09:52:42.497132 [0] Epoch 00002 | Loss 2.4048
09:52:42.630010 [0] Epoch 00003 | Loss 2.0652
09:52:42.764275 [0] Epoch 00004 | Loss 1.7978
09:52:42.898245 [0] Epoch 00005 | Loss 1.5821
09:52:43.031682 [0] Epoch 00006 | Loss 1.4100
09:52:43.165053 [0] Epoch 00007 | Loss 1.2561
09:52:43.298692 [0] Epoch 00008 | Loss 1.1295
09:52:43.432141 [0] Epoch 00009 | Loss 1.0056
09:52:43.566382 [0] Epoch 00010 | Loss 0.9125
09:52:43.567821 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:52:43.701199 [0] Epoch 00011 | Loss 0.8407
09:52:43.835776 [0] Epoch 00012 | Loss 0.7934
09:52:43.970611 [0] Epoch 00013 | Loss 0.7497
09:52:44.104289 [0] Epoch 00014 | Loss 0.6850
09:52:44.238297 [0] Epoch 00015 | Loss 0.6420
09:52:44.373190 [0] Epoch 00016 | Loss 0.6202
09:52:44.507136 [0] Epoch 00017 | Loss 0.6034
09:52:44.641614 [0] Epoch 00018 | Loss 0.5762
09:52:44.776720 [0] Epoch 00019 | Loss 0.5484
09:52:44.779324 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:52:44.780557 [0] 
timer summary:
  0.99s   0.99s    80 mm
  0.09s   0.09s    80 broadcast
  2.36s   2.36s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.70s   3.70s    20 epoch
 23.90s  23.90s     1 total
09:52:59.610938 [0] proc begin: <DistEnv 0/1 nccl>
09:53:20.705441 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
09:53:20.706866 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| Active memory         |    2728 MB |    2729 MB |    2733 MB |    6027 KB |
|       from large pool |    2727 MB |    2729 MB |    2732 MB |    5794 KB |
|       from small pool |       0 MB |       0 MB |       0 MB |     232 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2752 MB |    2752 MB |    2752 MB |       0 B  |
|       from large pool |    2750 MB |    2750 MB |    2750 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24569 KB |   24569 KB |   32644 KB |    8075 KB |
|       from large pool |   23205 KB |   23205 KB |   29000 KB |    5794 KB |
|       from small pool |    1364 KB |    1820 KB |    3644 KB |    2280 KB |
|---------------------------------------------------------------------------|
| Allocations           |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       7    |      11    |      20    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       3    |       6    |      13    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

09:53:21.879476 [0] Epoch 00000 | Loss 3.7185
09:53:21.881323 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
09:53:22.027431 [0] Epoch 00001 | Loss 2.8261
09:53:22.174659 [0] Epoch 00002 | Loss 2.4048
09:53:22.321617 [0] Epoch 00003 | Loss 2.0652
09:53:22.469211 [0] Epoch 00004 | Loss 1.7978
09:53:22.616174 [0] Epoch 00005 | Loss 1.5821
09:53:22.763576 [0] Epoch 00006 | Loss 1.4100
09:53:22.910947 [0] Epoch 00007 | Loss 1.2561
09:53:23.059363 [0] Epoch 00008 | Loss 1.1295
09:53:23.208220 [0] Epoch 00009 | Loss 1.0056
09:53:23.355956 [0] Epoch 00010 | Loss 0.9125
09:53:23.357577 [0] Epoch: 010, Train: 0.8282, Val: 0.8445, Test: 0.8394
09:53:23.505280 [0] Epoch 00011 | Loss 0.8407
09:53:23.653577 [0] Epoch 00012 | Loss 0.7934
09:53:23.802007 [0] Epoch 00013 | Loss 0.7497
09:53:23.950822 [0] Epoch 00014 | Loss 0.6850
09:53:24.099132 [0] Epoch 00015 | Loss 0.6420
09:53:24.247196 [0] Epoch 00016 | Loss 0.6202
09:53:24.395166 [0] Epoch 00017 | Loss 0.6034
09:53:24.544010 [0] Epoch 00018 | Loss 0.5762
09:53:24.692316 [0] Epoch 00019 | Loss 0.5484
09:53:24.693844 [0] Epoch: 019, Train: 0.8974, Val: 0.9097, Test: 0.9068
09:53:24.694370 [0] 
timer summary:
  1.00s   1.00s    80 mm
  0.09s   0.09s    80 broadcast
  2.62s   2.62s    80 spmm
  0.04s   0.04s    40 all_reduce
  3.98s   3.98s    20 epoch
 25.08s  25.08s     1 total
10:29:09.180598 [0] proc begin: <DistEnv 0/1 nccl>
10:29:29.940144 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:29:29.941363 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:29:31.153385 [0] Epoch 00000 | Loss 3.7185
10:29:31.155107 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:29:31.155754 [0] 
timer summary:
  0.72s   0.72s     4 mm
  0.08s   0.08s     4 broadcast
  0.36s   0.36s     4 spmm
  0.04s   0.04s     2 all_reduce
  1.21s   1.21s     1 epoch
 21.97s  21.97s     1 total
10:35:28.341157 [0] proc begin: <DistEnv 0/1 nccl>
10:35:45.441766 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:35:45.443001 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:35:46.577273 [0] Epoch 00000 | Loss 3.7185
10:35:46.580228 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:35:46.711332 [0] Epoch 00001 | Loss 2.8261
10:35:46.712766 [0] Epoch: 001, Train: 0.4536, Val: 0.4768, Test: 0.4713
10:35:46.713237 [0] 
timer summary:
  0.68s   0.68s     8 mm
  0.08s   0.08s     8 broadcast
  0.44s   0.44s     8 spmm
  0.04s   0.04s     4 all_reduce
  1.26s   1.26s     2 epoch
 18.37s  18.37s     1 total
10:37:02.106341 [0] proc begin: <DistEnv 0/1 nccl>
10:37:21.909821 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:37:21.911904 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:37:23.192528 [0] Epoch 00000 | Loss 3.7185
10:37:23.195185 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:37:23.328321 [0] Epoch 00001 | Loss 2.8261
10:37:23.331019 [0] Epoch: 001, Train: 0.4536, Val: 0.4768, Test: 0.4713
10:37:23.332760 [0] 
timer summary:
  0.69s   0.69s     8 mm
  0.08s   0.08s     8 broadcast
  0.57s   0.57s     8 spmm
  0.04s   0.04s     4 all_reduce
  1.41s   1.41s     2 epoch
 21.22s  21.22s     1 total
10:37:47.774980 [0] proc begin: <DistEnv 0/1 nccl>
10:38:05.031632 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:38:05.032836 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:06.173900 [0] Epoch 00000 | Loss 3.7185
10:38:06.175389 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:38:06.306304 [0] Epoch 00001 | Loss 2.8261
10:38:06.307740 [0] Epoch: 001, Train: 0.4536, Val: 0.4768, Test: 0.4713
10:38:06.308147 [0] 
timer summary:
  0.69s   0.69s     8 mm
  0.08s   0.08s     8 broadcast
  0.44s   0.44s     8 spmm
  0.04s   0.04s     4 all_reduce
  1.27s   1.27s     2 epoch
 18.53s  18.53s     1 total
10:38:19.840494 [0] proc begin: <DistEnv 0/1 nccl>
10:38:36.719390 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:38:36.720590 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:38:37.868263 [0] Epoch 00000 | Loss 3.7185
10:38:37.869784 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:38:37.870262 [0] 
timer summary:
  0.67s   0.67s     4 mm
  0.08s   0.08s     4 broadcast
  0.34s   0.34s     4 spmm
  0.04s   0.04s     2 all_reduce
  1.15s   1.15s     1 epoch
 18.03s  18.03s     1 total
10:39:24.043223 [0] proc begin: <DistEnv 0/1 nccl>
10:39:41.761873 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:39:41.763077 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:39:42.866515 [0] Epoch 00000 | Loss 3.7185
10:39:42.868054 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:39:42.868529 [0] 
timer summary:
  0.66s   0.66s     4 mm
  0.09s   0.09s     4 broadcast
  0.33s   0.33s     4 spmm
  0.00s   0.00s     2 all_reduce
  1.10s   1.10s     1 epoch
 18.82s  18.82s     1 total
10:39:52.090286 [0] proc begin: <DistEnv 0/1 nccl>
10:40:08.173732 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:40:08.174992 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:40:09.270727 [0] Epoch 00000 | Loss 3.7185
10:40:09.273577 [0] Epoch: 000, Train: 0.0233, Val: 0.0159, Test: 0.0164
10:40:09.407447 [0] Epoch 00001 | Loss 2.8261
10:40:09.408864 [0] Epoch: 001, Train: 0.4536, Val: 0.4768, Test: 0.4713
10:40:09.409298 [0] 
timer summary:
  0.68s   0.68s     8 mm
  0.08s   0.08s     8 broadcast
  0.44s   0.44s     8 spmm
  0.00s   0.00s     4 all_reduce
  1.23s   1.23s     2 epoch
 17.32s  17.32s     1 total
15:19:08.963296 [0] proc begin: <DistEnv 0/1 nccl>
15:19:12.608329 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:19:12.614679 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:19:15.740629 [0] Epoch 00000 | Loss 3.6790
15:19:15.745024 [0] Epoch: 000, Train: 0.0259, Val: 0.0272, Test: 0.0270
15:19:15.765329 [0] Epoch 00001 | Loss 3.4899
15:19:15.787328 [0] Epoch 00002 | Loss 3.4467
15:19:15.810924 [0] Epoch 00003 | Loss 3.5920
15:19:15.833035 [0] Epoch 00004 | Loss 3.9286
15:19:15.854421 [0] Epoch 00005 | Loss 4.4289
15:19:15.880677 [0] Epoch 00006 | Loss 5.0451
15:19:15.902691 [0] Epoch 00007 | Loss 5.7289
15:19:15.926462 [0] Epoch 00008 | Loss 6.4915
15:19:15.948185 [0] Epoch 00009 | Loss 7.3353
15:19:15.970912 [0] Epoch 00010 | Loss 8.1901
15:19:15.973007 [0] Epoch: 010, Train: 0.2223, Val: 0.2744, Test: 0.2460
15:19:15.996062 [0] Epoch 00011 | Loss 8.9962
15:19:16.018038 [0] Epoch 00012 | Loss 9.7551
15:19:16.040155 [0] Epoch 00013 | Loss 10.4471
15:19:16.062424 [0] Epoch 00014 | Loss 11.0453
15:19:16.085625 [0] Epoch 00015 | Loss 11.5694
15:19:16.107288 [0] Epoch 00016 | Loss 11.9875
15:19:16.128277 [0] Epoch 00017 | Loss 12.3576
15:19:16.149392 [0] Epoch 00018 | Loss 12.6982
15:19:16.170007 [0] Epoch 00019 | Loss 12.9855
15:19:16.193618 [0] Epoch 00020 | Loss 13.1974
15:19:16.196128 [0] Epoch: 020, Train: 0.2542, Val: 0.2913, Test: 0.2606
15:19:16.217339 [0] Epoch 00021 | Loss 13.2993
15:19:16.239810 [0] Epoch 00022 | Loss 13.3335
15:19:16.263155 [0] Epoch 00023 | Loss 13.3362
15:19:16.285002 [0] Epoch 00024 | Loss 13.3349
15:19:16.306084 [0] Epoch 00025 | Loss 13.3442
15:19:16.333231 [0] Epoch 00026 | Loss 13.3489
15:19:16.355604 [0] Epoch 00027 | Loss 13.3456
15:19:16.379127 [0] Epoch 00028 | Loss 13.3375
15:19:16.401643 [0] Epoch 00029 | Loss 13.3326
15:19:16.422744 [0] Epoch 00030 | Loss 13.3279
15:19:16.425214 [0] Epoch: 030, Train: 0.2720, Val: 0.3024, Test: 0.2726
15:19:16.448243 [0] Epoch 00031 | Loss 13.3046
15:19:16.470767 [0] Epoch 00032 | Loss 13.2425
15:19:16.492853 [0] Epoch 00033 | Loss 13.1455
15:19:16.514314 [0] Epoch 00034 | Loss 13.0283
15:19:16.538298 [0] Epoch 00035 | Loss 12.8993
15:19:16.560132 [0] Epoch 00036 | Loss 12.7659
15:19:16.581578 [0] Epoch 00037 | Loss 12.6366
15:19:16.604109 [0] Epoch 00038 | Loss 12.5185
15:19:16.627090 [0] Epoch 00039 | Loss 12.4118
15:19:16.649293 [0] Epoch 00040 | Loss 12.3168
15:19:16.650821 [0] Epoch: 040, Train: 0.3039, Val: 0.3260, Test: 0.2978
15:19:16.672019 [0] Epoch 00041 | Loss 12.2403
15:19:16.693551 [0] Epoch 00042 | Loss 12.1928
15:19:16.714680 [0] Epoch 00043 | Loss 12.1788
15:19:16.738446 [0] Epoch 00044 | Loss 12.1917
15:19:16.760240 [0] Epoch 00045 | Loss 12.2220
15:19:16.781378 [0] Epoch 00046 | Loss 12.2618
15:19:16.803107 [0] Epoch 00047 | Loss 12.3082
15:19:16.826984 [0] Epoch 00048 | Loss 12.3603
15:19:16.850440 [0] Epoch 00049 | Loss 12.4183
15:19:16.873021 [0] Epoch 00050 | Loss 12.4799
15:19:16.880261 [0] Epoch: 050, Train: 0.3422, Val: 0.3567, Test: 0.3331
15:19:16.902402 [0] Epoch 00051 | Loss 12.5381
15:19:16.925419 [0] Epoch 00052 | Loss 12.5854
15:19:16.947559 [0] Epoch 00053 | Loss 12.6207
15:19:16.969963 [0] Epoch 00054 | Loss 12.6504
15:19:16.993396 [0] Epoch 00055 | Loss 12.6832
15:19:17.015663 [0] Epoch 00056 | Loss 12.7239
15:19:17.037486 [0] Epoch 00057 | Loss 12.7748
15:19:17.058905 [0] Epoch 00058 | Loss 12.8385
15:19:17.081233 [0] Epoch 00059 | Loss 12.9137
15:19:17.102122 [0] Epoch 00060 | Loss 12.9924
15:19:17.103556 [0] Epoch: 060, Train: 0.3687, Val: 0.3826, Test: 0.3652
15:19:17.124832 [0] Epoch 00061 | Loss 13.0665
15:19:17.147536 [0] Epoch 00062 | Loss 13.1342
15:19:17.169055 [0] Epoch 00063 | Loss 13.1985
15:19:17.190698 [0] Epoch 00064 | Loss 13.2637
15:19:17.214660 [0] Epoch 00065 | Loss 13.3352
15:19:17.238574 [0] Epoch 00066 | Loss 13.4171
15:19:17.262706 [0] Epoch 00067 | Loss 13.5085
15:19:17.284320 [0] Epoch 00068 | Loss 13.6024
15:19:17.305665 [0] Epoch 00069 | Loss 13.6930
15:19:17.326785 [0] Epoch 00070 | Loss 13.7801
15:19:17.329298 [0] Epoch: 070, Train: 0.3902, Val: 0.4066, Test: 0.3959
15:19:17.352723 [0] Epoch 00071 | Loss 13.8662
15:19:17.374800 [0] Epoch 00072 | Loss 13.9534
15:19:17.397096 [0] Epoch 00073 | Loss 14.0451
15:19:17.419822 [0] Epoch 00074 | Loss 14.1453
15:19:17.443295 [0] Epoch 00075 | Loss 14.2545
15:19:17.465022 [0] Epoch 00076 | Loss 14.3688
15:19:17.486135 [0] Epoch 00077 | Loss 14.4833
15:19:17.508128 [0] Epoch 00078 | Loss 14.5959
15:19:17.531067 [0] Epoch 00079 | Loss 14.7051
15:19:17.553071 [0] Epoch 00080 | Loss 14.8115
15:19:17.555619 [0] Epoch: 080, Train: 0.4065, Val: 0.4307, Test: 0.4265
15:19:17.577625 [0] Epoch 00081 | Loss 14.9181
15:19:17.600768 [0] Epoch 00082 | Loss 15.0282
15:19:17.623592 [0] Epoch 00083 | Loss 15.1409
15:19:17.645411 [0] Epoch 00084 | Loss 15.2533
15:19:17.667496 [0] Epoch 00085 | Loss 15.3635
15:19:17.690636 [0] Epoch 00086 | Loss 15.4707
15:19:17.713092 [0] Epoch 00087 | Loss 15.5747
15:19:17.735474 [0] Epoch 00088 | Loss 15.6760
15:19:17.759108 [0] Epoch 00089 | Loss 15.7784
15:19:17.782703 [0] Epoch 00090 | Loss 15.8852
15:19:17.785268 [0] Epoch: 090, Train: 0.4230, Val: 0.4536, Test: 0.4555
15:19:17.808214 [0] Epoch 00091 | Loss 15.9976
15:19:17.831322 [0] Epoch 00092 | Loss 16.1155
15:19:17.853498 [0] Epoch 00093 | Loss 16.2376
15:19:17.878125 [0] Epoch 00094 | Loss 16.3602
15:19:17.900378 [0] Epoch 00095 | Loss 16.4792
15:19:17.922214 [0] Epoch 00096 | Loss 16.5924
15:19:17.944300 [0] Epoch 00097 | Loss 16.7010
15:19:17.966919 [0] Epoch 00098 | Loss 16.8073
15:19:17.989236 [0] Epoch 00099 | Loss 16.9143
15:19:18.011716 [0] Epoch 00100 | Loss 17.0250
15:19:18.014361 [0] Epoch: 100, Train: 0.4382, Val: 0.4757, Test: 0.4816
15:19:18.038535 [0] Epoch 00101 | Loss 17.1412
15:19:18.061904 [0] Epoch 00102 | Loss 17.2619
15:19:18.084768 [0] Epoch 00103 | Loss 17.3844
15:19:18.107601 [0] Epoch 00104 | Loss 17.5070
15:19:18.129943 [0] Epoch 00105 | Loss 17.6283
15:19:18.153099 [0] Epoch 00106 | Loss 17.7475
15:19:18.175287 [0] Epoch 00107 | Loss 17.8651
15:19:18.196458 [0] Epoch 00108 | Loss 17.9823
15:19:18.217458 [0] Epoch 00109 | Loss 18.1000
15:19:18.241596 [0] Epoch 00110 | Loss 18.2180
15:19:18.244130 [0] Epoch: 110, Train: 0.4503, Val: 0.4930, Test: 0.5015
15:19:18.267576 [0] Epoch 00111 | Loss 18.3367
15:19:18.290813 [0] Epoch 00112 | Loss 18.4566
15:19:18.312010 [0] Epoch 00113 | Loss 18.5777
15:19:18.332776 [0] Epoch 00114 | Loss 18.6999
15:19:18.357396 [0] Epoch 00115 | Loss 18.8224
15:19:18.379362 [0] Epoch 00116 | Loss 18.9443
15:19:18.400426 [0] Epoch 00117 | Loss 19.0662
15:19:18.421991 [0] Epoch 00118 | Loss 19.1888
15:19:18.444835 [0] Epoch 00119 | Loss 19.3119
15:19:18.468357 [0] Epoch 00120 | Loss 19.4358
15:19:18.470313 [0] Epoch: 120, Train: 0.4617, Val: 0.5064, Test: 0.5165
15:19:18.492328 [0] Epoch 00121 | Loss 19.5610
15:19:18.516122 [0] Epoch 00122 | Loss 19.6867
15:19:18.537971 [0] Epoch 00123 | Loss 19.8118
15:19:18.558877 [0] Epoch 00124 | Loss 19.9363
15:19:18.581917 [0] Epoch 00125 | Loss 20.0607
15:19:18.603948 [0] Epoch 00126 | Loss 20.1856
15:19:18.626404 [0] Epoch 00127 | Loss 20.3122
15:19:18.650237 [0] Epoch 00128 | Loss 20.4407
15:19:18.672806 [0] Epoch 00129 | Loss 20.5700
15:19:18.695107 [0] Epoch 00130 | Loss 20.6983
15:19:18.696942 [0] Epoch: 130, Train: 0.4702, Val: 0.5174, Test: 0.5273
15:19:18.720478 [0] Epoch 00131 | Loss 20.8254
15:19:18.743161 [0] Epoch 00132 | Loss 20.9522
15:19:18.764346 [0] Epoch 00133 | Loss 21.0796
15:19:18.786343 [0] Epoch 00134 | Loss 21.2088
15:19:18.809401 [0] Epoch 00135 | Loss 21.3399
15:19:18.831493 [0] Epoch 00136 | Loss 21.4714
15:19:18.852552 [0] Epoch 00137 | Loss 21.6028
15:19:18.874703 [0] Epoch 00138 | Loss 21.7344
15:19:18.900376 [0] Epoch 00139 | Loss 21.8655
15:19:18.922678 [0] Epoch 00140 | Loss 21.9962
15:19:18.925175 [0] Epoch: 140, Train: 0.4767, Val: 0.5253, Test: 0.5359
15:19:18.948426 [0] Epoch 00141 | Loss 22.1276
15:19:18.970559 [0] Epoch 00142 | Loss 22.2589
15:19:18.994497 [0] Epoch 00143 | Loss 22.3898
15:19:19.016319 [0] Epoch 00144 | Loss 22.5207
15:19:19.039528 [0] Epoch 00145 | Loss 22.6509
15:19:19.062827 [0] Epoch 00146 | Loss 22.7807
15:19:19.084007 [0] Epoch 00147 | Loss 22.9111
15:19:19.105502 [0] Epoch 00148 | Loss 23.0424
15:19:19.127985 [0] Epoch 00149 | Loss 23.1747
15:19:19.152159 [0] Epoch 00150 | Loss 23.3078
15:19:19.154083 [0] Epoch: 150, Train: 0.4836, Val: 0.5312, Test: 0.5424
15:19:19.175951 [0] Epoch 00151 | Loss 23.4409
15:19:19.200025 [0] Epoch 00152 | Loss 23.5738
15:19:19.221770 [0] Epoch 00153 | Loss 23.7072
15:19:19.245776 [0] Epoch 00154 | Loss 23.8411
15:19:19.267529 [0] Epoch 00155 | Loss 23.9752
15:19:19.288772 [0] Epoch 00156 | Loss 24.1091
15:19:19.310204 [0] Epoch 00157 | Loss 24.2426
15:19:19.334003 [0] Epoch 00158 | Loss 24.3765
15:19:19.358749 [0] Epoch 00159 | Loss 24.5111
15:19:19.387596 [0] Epoch 00160 | Loss 24.6462
15:19:19.389984 [0] Epoch: 160, Train: 0.4886, Val: 0.5378, Test: 0.5488
15:19:19.411837 [0] Epoch 00161 | Loss 24.7813
15:19:19.433071 [0] Epoch 00162 | Loss 24.9167
15:19:19.455904 [0] Epoch 00163 | Loss 25.0524
15:19:19.479623 [0] Epoch 00164 | Loss 25.1883
15:19:19.501296 [0] Epoch 00165 | Loss 25.3246
15:19:19.522638 [0] Epoch 00166 | Loss 25.4612
15:19:19.545726 [0] Epoch 00167 | Loss 25.5980
15:19:19.567705 [0] Epoch 00168 | Loss 25.7347
15:19:19.589039 [0] Epoch 00169 | Loss 25.8715
15:19:19.610682 [0] Epoch 00170 | Loss 26.0084
15:19:19.613091 [0] Epoch: 170, Train: 0.4936, Val: 0.5441, Test: 0.5535
15:19:19.636491 [0] Epoch 00171 | Loss 26.1453
15:19:19.658249 [0] Epoch 00172 | Loss 26.2820
15:19:19.681081 [0] Epoch 00173 | Loss 26.4187
15:19:19.704447 [0] Epoch 00174 | Loss 26.5551
15:19:19.726655 [0] Epoch 00175 | Loss 26.6912
15:19:19.749797 [0] Epoch 00176 | Loss 26.8267
15:19:19.772819 [0] Epoch 00177 | Loss 26.9613
15:19:19.795034 [0] Epoch 00178 | Loss 27.0955
15:19:19.817485 [0] Epoch 00179 | Loss 27.2296
15:19:19.839547 [0] Epoch 00180 | Loss 27.3651
15:19:19.841413 [0] Epoch: 180, Train: 0.4976, Val: 0.5486, Test: 0.5582
15:19:19.862565 [0] Epoch 00181 | Loss 27.5015
15:19:19.887462 [0] Epoch 00182 | Loss 27.6392
15:19:19.909924 [0] Epoch 00183 | Loss 27.7771
15:19:19.932404 [0] Epoch 00184 | Loss 27.9156
15:19:19.955090 [0] Epoch 00185 | Loss 28.0548
15:19:19.978125 [0] Epoch 00186 | Loss 28.1953
15:19:20.000581 [0] Epoch 00187 | Loss 28.3361
15:19:20.022420 [0] Epoch 00188 | Loss 28.4773
15:19:20.045426 [0] Epoch 00189 | Loss 28.6185
15:19:20.066492 [0] Epoch 00190 | Loss 28.7615
15:19:20.068899 [0] Epoch: 190, Train: 0.5020, Val: 0.5518, Test: 0.5606
15:19:20.091475 [0] Epoch 00191 | Loss 28.9049
15:19:20.114830 [0] Epoch 00192 | Loss 29.0502
15:19:20.136191 [0] Epoch 00193 | Loss 29.1936
15:19:20.157070 [0] Epoch 00194 | Loss 29.3399
15:19:20.178550 [0] Epoch 00195 | Loss 29.4813
15:19:20.199981 [0] Epoch 00196 | Loss 29.6316
15:19:20.221546 [0] Epoch 00197 | Loss 29.7707
15:19:20.243085 [0] Epoch 00198 | Loss 29.9250
15:19:20.265915 [0] Epoch 00199 | Loss 30.0628
15:19:20.268154 [0] Epoch: 199, Train: 0.5050, Val: 0.5552, Test: 0.5642
15:19:20.269366 [0] 
timer summary:
  0.21s   0.21s   800 broadcast
  2.40s   2.40s   800 spmm
  2.89s   2.89s   800 mm
  0.07s   0.07s   400 all_reduce
  7.60s   7.60s   200 epoch
 11.30s  11.30s     1 total
15:20:06.066371 [0] proc begin: <DistEnv 0/1 nccl>
15:20:07.880511 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:20:07.881857 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:20:09.529503 [0] Epoch 00000 | Loss 3.6849
15:20:09.532581 [0] Epoch: 000, Train: 0.0258, Val: 0.0278, Test: 0.0270
15:20:09.556740 [0] Epoch 00001 | Loss 3.5696
15:20:09.581163 [0] Epoch 00002 | Loss 3.6400
15:20:09.604587 [0] Epoch 00003 | Loss 3.8904
15:20:09.628561 [0] Epoch 00004 | Loss 4.3040
15:20:09.654329 [0] Epoch 00005 | Loss 4.8643
15:20:09.682476 [0] Epoch 00006 | Loss 5.5653
15:20:09.708067 [0] Epoch 00007 | Loss 6.4056
15:20:09.732344 [0] Epoch 00008 | Loss 7.3853
15:20:09.758164 [0] Epoch 00009 | Loss 8.5044
15:20:09.781958 [0] Epoch 00010 | Loss 9.7604
15:20:09.784477 [0] Epoch: 010, Train: 0.2313, Val: 0.2114, Test: 0.1986
15:20:09.807074 [0] Epoch 00011 | Loss 11.1485
15:20:09.832254 [0] Epoch 00012 | Loss 12.6620
15:20:09.855945 [0] Epoch 00013 | Loss 14.2937
15:20:09.881451 [0] Epoch 00014 | Loss 16.0377
15:20:09.906186 [0] Epoch 00015 | Loss 17.8891
15:20:09.936226 [0] Epoch 00016 | Loss 19.8443
15:20:09.962150 [0] Epoch 00017 | Loss 21.9008
15:20:09.984974 [0] Epoch 00018 | Loss 24.0578
15:20:10.015584 [0] Epoch 00019 | Loss 26.3171
15:20:10.042502 [0] Epoch 00020 | Loss 28.6828
15:20:10.044952 [0] Epoch: 020, Train: 0.2571, Val: 0.2689, Test: 0.2508
15:20:10.068659 [0] Epoch 00021 | Loss 31.1594
15:20:10.093025 [0] Epoch 00022 | Loss 33.7492
15:20:10.123091 [0] Epoch 00023 | Loss 36.4509
15:20:10.149460 [0] Epoch 00024 | Loss 39.2608
15:20:10.182930 [0] Epoch 00025 | Loss 42.1754
15:20:10.205525 [0] Epoch 00026 | Loss 45.1909
15:20:10.227858 [0] Epoch 00027 | Loss 48.2998
15:20:10.253305 [0] Epoch 00028 | Loss 51.4902
15:20:10.277045 [0] Epoch 00029 | Loss 54.7522
15:20:10.301946 [0] Epoch 00030 | Loss 58.0858
15:20:10.303763 [0] Epoch: 030, Train: 0.2727, Val: 0.2953, Test: 0.2687
15:20:10.326670 [0] Epoch 00031 | Loss 61.4999
15:20:10.349245 [0] Epoch 00032 | Loss 65.0009
15:20:10.371226 [0] Epoch 00033 | Loss 68.5886
15:20:10.397236 [0] Epoch 00034 | Loss 72.2584
15:20:10.423276 [0] Epoch 00035 | Loss 76.0010
15:20:10.445635 [0] Epoch 00036 | Loss 79.8064
15:20:10.467558 [0] Epoch 00037 | Loss 83.6756
15:20:10.494650 [0] Epoch 00038 | Loss 87.6171
15:20:10.519766 [0] Epoch 00039 | Loss 91.6339
15:20:10.542395 [0] Epoch 00040 | Loss 95.7212
15:20:10.544979 [0] Epoch: 040, Train: 0.2729, Val: 0.2983, Test: 0.2685
15:20:10.571189 [0] Epoch 00041 | Loss 99.8690
15:20:10.595887 [0] Epoch 00042 | Loss 104.0712
15:20:10.618948 [0] Epoch 00043 | Loss 108.3354
15:20:10.653722 [0] Epoch 00044 | Loss 112.6680
15:20:10.677597 [0] Epoch 00045 | Loss 117.0652
15:20:10.702348 [0] Epoch 00046 | Loss 121.5165
15:20:10.726847 [0] Epoch 00047 | Loss 126.0193
15:20:10.749073 [0] Epoch 00048 | Loss 130.5797
15:20:10.771524 [0] Epoch 00049 | Loss 135.1970
15:20:10.798129 [0] Epoch 00050 | Loss 139.8612
15:20:10.800301 [0] Epoch: 050, Train: 0.2718, Val: 0.2982, Test: 0.2680
15:20:10.822789 [0] Epoch 00051 | Loss 144.5664
15:20:10.847849 [0] Epoch 00052 | Loss 149.3177
15:20:10.870359 [0] Epoch 00053 | Loss 154.1138
15:20:10.892460 [0] Epoch 00054 | Loss 158.9448
15:20:10.918674 [0] Epoch 00055 | Loss 163.8104
15:20:10.944766 [0] Epoch 00056 | Loss 168.7169
15:20:10.968169 [0] Epoch 00057 | Loss 173.6598
15:20:10.993810 [0] Epoch 00058 | Loss 178.6316
15:20:11.016639 [0] Epoch 00059 | Loss 183.6360
15:20:11.041753 [0] Epoch 00060 | Loss 188.6721
15:20:11.044346 [0] Epoch: 060, Train: 0.2703, Val: 0.2979, Test: 0.2675
15:20:11.069663 [0] Epoch 00061 | Loss 193.7323
15:20:11.092383 [0] Epoch 00062 | Loss 198.8182
15:20:11.116952 [0] Epoch 00063 | Loss 203.9296
15:20:11.140395 [0] Epoch 00064 | Loss 209.0588
15:20:11.172630 [0] Epoch 00065 | Loss 214.2073
15:20:11.197613 [0] Epoch 00066 | Loss 219.3730
15:20:11.222313 [0] Epoch 00067 | Loss 224.5492
15:20:11.248103 [0] Epoch 00068 | Loss 229.7393
15:20:11.274757 [0] Epoch 00069 | Loss 234.9399
15:20:11.302433 [0] Epoch 00070 | Loss 240.1502
15:20:11.304811 [0] Epoch: 070, Train: 0.2670, Val: 0.2967, Test: 0.2660
15:20:11.335507 [0] Epoch 00071 | Loss 245.3732
15:20:11.360330 [0] Epoch 00072 | Loss 250.6032
15:20:11.386896 [0] Epoch 00073 | Loss 255.8428
15:20:11.410510 [0] Epoch 00074 | Loss 261.0871
15:20:11.436225 [0] Epoch 00075 | Loss 266.3345
15:20:11.459370 [0] Epoch 00076 | Loss 271.5848
15:20:11.484705 [0] Epoch 00077 | Loss 276.8347
15:20:11.510192 [0] Epoch 00078 | Loss 282.0890
15:20:11.534199 [0] Epoch 00079 | Loss 287.3433
15:20:11.559571 [0] Epoch 00080 | Loss 292.6071
15:20:11.561973 [0] Epoch: 080, Train: 0.2665, Val: 0.2964, Test: 0.2658
15:20:11.587115 [0] Epoch 00081 | Loss 297.8693
15:20:11.614577 [0] Epoch 00082 | Loss 303.1406
15:20:11.639796 [0] Epoch 00083 | Loss 308.3988
15:20:11.662800 [0] Epoch 00084 | Loss 313.6733
15:20:11.687918 [0] Epoch 00085 | Loss 318.9151
15:20:11.710311 [0] Epoch 00086 | Loss 324.1991
15:20:11.732270 [0] Epoch 00087 | Loss 329.4021
15:20:11.758833 [0] Epoch 00088 | Loss 334.6655
15:20:11.783768 [0] Epoch 00089 | Loss 339.7685
15:20:11.806836 [0] Epoch 00090 | Loss 344.6613
15:20:11.809443 [0] Epoch: 090, Train: 0.2748, Val: 0.2987, Test: 0.2692
15:20:11.835222 [0] Epoch 00091 | Loss 349.5088
15:20:11.859779 [0] Epoch 00092 | Loss 354.5612
15:20:11.882473 [0] Epoch 00093 | Loss 359.7638
15:20:11.906630 [0] Epoch 00094 | Loss 364.9439
15:20:11.938780 [0] Epoch 00095 | Loss 369.9601
15:20:11.961346 [0] Epoch 00096 | Loss 374.9295
15:20:11.983335 [0] Epoch 00097 | Loss 379.9996
15:20:12.010453 [0] Epoch 00098 | Loss 385.1360
15:20:12.033627 [0] Epoch 00099 | Loss 390.2025
15:20:12.058233 [0] Epoch 00100 | Loss 395.1201
15:20:12.060811 [0] Epoch: 100, Train: 0.2688, Val: 0.2970, Test: 0.2665
15:20:12.086901 [0] Epoch 00101 | Loss 400.0463
15:20:12.111637 [0] Epoch 00102 | Loss 405.0598
15:20:12.136190 [0] Epoch 00103 | Loss 410.1133
15:20:12.162107 [0] Epoch 00104 | Loss 415.1870
15:20:12.184838 [0] Epoch 00105 | Loss 420.2506
15:20:12.208361 [0] Epoch 00106 | Loss 425.2816
15:20:12.232718 [0] Epoch 00107 | Loss 430.3044
15:20:12.255757 [0] Epoch 00108 | Loss 435.3297
15:20:12.279843 [0] Epoch 00109 | Loss 440.3697
15:20:12.307084 [0] Epoch 00110 | Loss 445.3704
15:20:12.309898 [0] Epoch: 110, Train: 0.2589, Val: 0.2935, Test: 0.2631
15:20:12.335352 [0] Epoch 00111 | Loss 450.2754
15:20:12.360044 [0] Epoch 00112 | Loss 455.1770
15:20:12.394167 [0] Epoch 00113 | Loss 460.1179
15:20:12.418359 [0] Epoch 00114 | Loss 465.0530
15:20:12.444677 [0] Epoch 00115 | Loss 469.9807
15:20:12.470416 [0] Epoch 00116 | Loss 474.8832
15:20:12.494057 [0] Epoch 00117 | Loss 479.7509
15:20:12.521486 [0] Epoch 00118 | Loss 484.6528
15:20:12.544768 [0] Epoch 00119 | Loss 489.5688
15:20:12.569352 [0] Epoch 00120 | Loss 494.4231
15:20:12.572088 [0] Epoch: 120, Train: 0.2641, Val: 0.2954, Test: 0.2650
15:20:12.596345 [0] Epoch 00121 | Loss 499.2475
15:20:12.621792 [0] Epoch 00122 | Loss 504.0872
15:20:12.644643 [0] Epoch 00123 | Loss 508.9396
15:20:12.669022 [0] Epoch 00124 | Loss 513.7961
15:20:12.694863 [0] Epoch 00125 | Loss 518.6342
15:20:12.720205 [0] Epoch 00126 | Loss 523.4427
15:20:12.743705 [0] Epoch 00127 | Loss 528.2557
15:20:12.769424 [0] Epoch 00128 | Loss 533.0696
15:20:12.792283 [0] Epoch 00129 | Loss 537.8416
15:20:12.817449 [0] Epoch 00130 | Loss 542.5849
15:20:12.819257 [0] Epoch: 130, Train: 0.2640, Val: 0.2954, Test: 0.2651
15:20:12.842300 [0] Epoch 00131 | Loss 547.3552
15:20:12.868135 [0] Epoch 00132 | Loss 552.1299
15:20:12.897296 [0] Epoch 00133 | Loss 556.8629
15:20:12.926078 [0] Epoch 00134 | Loss 561.5840
15:20:12.951984 [0] Epoch 00135 | Loss 566.3168
15:20:12.976921 [0] Epoch 00136 | Loss 571.0251
15:20:13.000770 [0] Epoch 00137 | Loss 575.6970
15:20:13.025437 [0] Epoch 00138 | Loss 580.3629
15:20:13.054040 [0] Epoch 00139 | Loss 585.0262
15:20:13.078420 [0] Epoch 00140 | Loss 589.6645
15:20:13.081085 [0] Epoch: 140, Train: 0.2633, Val: 0.2956, Test: 0.2649
15:20:13.107298 [0] Epoch 00141 | Loss 594.2919
15:20:13.132147 [0] Epoch 00142 | Loss 598.9275
15:20:13.157795 [0] Epoch 00143 | Loss 603.5591
15:20:13.180613 [0] Epoch 00144 | Loss 608.1733
15:20:13.205464 [0] Epoch 00145 | Loss 612.7728
15:20:13.230507 [0] Epoch 00146 | Loss 617.3535
15:20:13.253975 [0] Epoch 00147 | Loss 621.9134
15:20:13.279876 [0] Epoch 00148 | Loss 626.4673
15:20:13.304821 [0] Epoch 00149 | Loss 631.0162
15:20:13.327975 [0] Epoch 00150 | Loss 635.5459
15:20:13.329742 [0] Epoch: 150, Train: 0.2625, Val: 0.2953, Test: 0.2648
15:20:13.354492 [0] Epoch 00151 | Loss 640.0538
15:20:13.379480 [0] Epoch 00152 | Loss 644.5479
15:20:13.404193 [0] Epoch 00153 | Loss 649.0302
15:20:13.428082 [0] Epoch 00154 | Loss 653.5071
15:20:13.452852 [0] Epoch 00155 | Loss 657.9794
15:20:13.477581 [0] Epoch 00156 | Loss 662.4336
15:20:13.501647 [0] Epoch 00157 | Loss 666.8691
15:20:13.524306 [0] Epoch 00158 | Loss 671.2952
15:20:13.549719 [0] Epoch 00159 | Loss 675.7042
15:20:13.572655 [0] Epoch 00160 | Loss 680.0928
15:20:13.575422 [0] Epoch: 160, Train: 0.2626, Val: 0.2955, Test: 0.2650
15:20:13.599129 [0] Epoch 00161 | Loss 684.4699
15:20:13.624350 [0] Epoch 00162 | Loss 688.8345
15:20:13.648722 [0] Epoch 00163 | Loss 693.1888
15:20:13.680684 [0] Epoch 00164 | Loss 697.5443
15:20:13.704871 [0] Epoch 00165 | Loss 701.8929
15:20:13.730767 [0] Epoch 00166 | Loss 706.2231
15:20:13.758098 [0] Epoch 00167 | Loss 710.5422
15:20:13.785819 [0] Epoch 00168 | Loss 714.8505
15:20:13.808415 [0] Epoch 00169 | Loss 719.1456
15:20:13.834767 [0] Epoch 00170 | Loss 723.4375
15:20:13.836867 [0] Epoch: 170, Train: 0.2640, Val: 0.2966, Test: 0.2654
15:20:13.861389 [0] Epoch 00171 | Loss 727.7254
15:20:13.885550 [0] Epoch 00172 | Loss 732.0060
15:20:13.911473 [0] Epoch 00173 | Loss 736.2819
15:20:13.937929 [0] Epoch 00174 | Loss 740.5427
15:20:13.960852 [0] Epoch 00175 | Loss 744.7811
15:20:13.985388 [0] Epoch 00176 | Loss 749.0046
15:20:14.010028 [0] Epoch 00177 | Loss 753.2089
15:20:14.038844 [0] Epoch 00178 | Loss 757.3933
15:20:14.066152 [0] Epoch 00179 | Loss 761.5726
15:20:14.089480 [0] Epoch 00180 | Loss 765.7516
15:20:14.092102 [0] Epoch: 180, Train: 0.2662, Val: 0.2979, Test: 0.2659
15:20:14.115751 [0] Epoch 00181 | Loss 769.9374
15:20:14.141572 [0] Epoch 00182 | Loss 774.1277
15:20:14.165167 [0] Epoch 00183 | Loss 778.3050
15:20:14.190310 [0] Epoch 00184 | Loss 782.4563
15:20:14.215849 [0] Epoch 00185 | Loss 786.5842
15:20:14.240395 [0] Epoch 00186 | Loss 790.6989
15:20:14.262827 [0] Epoch 00187 | Loss 794.7984
15:20:14.290140 [0] Epoch 00188 | Loss 798.8883
15:20:14.312191 [0] Epoch 00189 | Loss 802.9862
15:20:14.333694 [0] Epoch 00190 | Loss 807.0768
15:20:14.336296 [0] Epoch: 190, Train: 0.2689, Val: 0.2992, Test: 0.2669
15:20:14.361064 [0] Epoch 00191 | Loss 811.1462
15:20:14.389225 [0] Epoch 00192 | Loss 815.1954
15:20:14.415322 [0] Epoch 00193 | Loss 819.2273
15:20:14.439894 [0] Epoch 00194 | Loss 823.2427
15:20:14.466457 [0] Epoch 00195 | Loss 827.2455
15:20:14.489803 [0] Epoch 00196 | Loss 831.2382
15:20:14.518737 [0] Epoch 00197 | Loss 835.2189
15:20:14.542026 [0] Epoch 00198 | Loss 839.1985
15:20:14.569382 [0] Epoch 00199 | Loss 843.1722
15:20:14.571621 [0] Epoch: 199, Train: 0.2715, Val: 0.3016, Test: 0.2680
15:20:14.573024 [0] 
timer summary:
  1.88s   1.88s   800 mm
  0.33s   0.33s  2000 broadcast
  2.40s   2.40s  2000 spmm
  0.07s   0.07s   400 all_reduce
  6.63s   6.63s   200 epoch
  8.51s   8.51s     1 total
15:20:26.335753 [0] proc begin: <DistEnv 0/1 nccl>
15:20:27.784526 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:20:27.785469 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:20:28.780071 [0] Epoch 00000 | Loss 3.6792
15:20:28.782946 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
15:20:28.799226 [0] Epoch 00001 | Loss 3.4872
15:20:28.814610 [0] Epoch 00002 | Loss 3.4445
15:20:28.828617 [0] Epoch 00003 | Loss 3.5919
15:20:28.844835 [0] Epoch 00004 | Loss 3.9270
15:20:28.859170 [0] Epoch 00005 | Loss 4.4153
15:20:28.876017 [0] Epoch 00006 | Loss 5.0240
15:20:28.891656 [0] Epoch 00007 | Loss 5.7201
15:20:28.905658 [0] Epoch 00008 | Loss 6.4735
15:20:28.922556 [0] Epoch 00009 | Loss 7.2819
15:20:28.942423 [0] Epoch 00010 | Loss 8.1136
15:20:28.944556 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
15:20:28.961341 [0] Epoch 00011 | Loss 8.8971
15:20:28.977388 [0] Epoch 00012 | Loss 9.6118
15:20:28.991336 [0] Epoch 00013 | Loss 10.2092
15:20:29.011208 [0] Epoch 00014 | Loss 10.6998
15:20:29.026259 [0] Epoch 00015 | Loss 11.0928
15:20:29.041139 [0] Epoch 00016 | Loss 11.4191
15:20:29.058701 [0] Epoch 00017 | Loss 11.6880
15:20:29.074234 [0] Epoch 00018 | Loss 11.8933
15:20:29.088200 [0] Epoch 00019 | Loss 12.0301
15:20:29.103731 [0] Epoch 00020 | Loss 12.1161
15:20:29.104923 [0] Epoch: 020, Train: 0.2688, Val: 0.2972, Test: 0.2668
15:20:29.119261 [0] Epoch 00021 | Loss 12.1920
15:20:29.134698 [0] Epoch 00022 | Loss 12.2800
15:20:29.149010 [0] Epoch 00023 | Loss 12.3868
15:20:29.163304 [0] Epoch 00024 | Loss 12.4894
15:20:29.179531 [0] Epoch 00025 | Loss 12.5821
15:20:29.193310 [0] Epoch 00026 | Loss 12.6727
15:20:29.209220 [0] Epoch 00027 | Loss 12.7716
15:20:29.223291 [0] Epoch 00028 | Loss 12.8526
15:20:29.238956 [0] Epoch 00029 | Loss 12.8811
15:20:29.252783 [0] Epoch 00030 | Loss 12.8576
15:20:29.255081 [0] Epoch: 030, Train: 0.2803, Val: 0.3073, Test: 0.2787
15:20:29.271235 [0] Epoch 00031 | Loss 12.8037
15:20:29.286497 [0] Epoch 00032 | Loss 12.7330
15:20:29.300345 [0] Epoch 00033 | Loss 12.6484
15:20:29.316503 [0] Epoch 00034 | Loss 12.5501
15:20:29.330666 [0] Epoch 00035 | Loss 12.4418
15:20:29.347708 [0] Epoch 00036 | Loss 12.3345
15:20:29.361522 [0] Epoch 00037 | Loss 12.2503
15:20:29.377363 [0] Epoch 00038 | Loss 12.2040
15:20:29.391501 [0] Epoch 00039 | Loss 12.1924
15:20:29.407212 [0] Epoch 00040 | Loss 12.2049
15:20:29.408503 [0] Epoch: 040, Train: 0.3106, Val: 0.3274, Test: 0.3023
15:20:29.423010 [0] Epoch 00041 | Loss 12.2345
15:20:29.439864 [0] Epoch 00042 | Loss 12.2747
15:20:29.455335 [0] Epoch 00043 | Loss 12.3158
15:20:29.469165 [0] Epoch 00044 | Loss 12.3515
15:20:29.485885 [0] Epoch 00045 | Loss 12.3823
15:20:29.502320 [0] Epoch 00046 | Loss 12.4109
15:20:29.516646 [0] Epoch 00047 | Loss 12.4343
15:20:29.533326 [0] Epoch 00048 | Loss 12.4461
15:20:29.548043 [0] Epoch 00049 | Loss 12.4445
15:20:29.564022 [0] Epoch 00050 | Loss 12.4343
15:20:29.565605 [0] Epoch: 050, Train: 0.3485, Val: 0.3594, Test: 0.3366
15:20:29.580863 [0] Epoch 00051 | Loss 12.4256
15:20:29.595627 [0] Epoch 00052 | Loss 12.4313
15:20:29.610033 [0] Epoch 00053 | Loss 12.4605
15:20:29.626271 [0] Epoch 00054 | Loss 12.5126
15:20:29.640660 [0] Epoch 00055 | Loss 12.5801
15:20:29.657462 [0] Epoch 00056 | Loss 12.6534
15:20:29.672039 [0] Epoch 00057 | Loss 12.7209
15:20:29.688727 [0] Epoch 00058 | Loss 12.7732
15:20:29.706016 [0] Epoch 00059 | Loss 12.8101
15:20:29.722609 [0] Epoch 00060 | Loss 12.8427
15:20:29.723944 [0] Epoch: 060, Train: 0.3787, Val: 0.3911, Test: 0.3746
15:20:29.739043 [0] Epoch 00061 | Loss 12.8849
15:20:29.754695 [0] Epoch 00062 | Loss 12.9446
15:20:29.771592 [0] Epoch 00063 | Loss 13.0240
15:20:29.786743 [0] Epoch 00064 | Loss 13.1207
15:20:29.804445 [0] Epoch 00065 | Loss 13.2280
15:20:29.821683 [0] Epoch 00066 | Loss 13.3377
15:20:29.836900 [0] Epoch 00067 | Loss 13.4445
15:20:29.852528 [0] Epoch 00068 | Loss 13.5485
15:20:29.869470 [0] Epoch 00069 | Loss 13.6508
15:20:29.884545 [0] Epoch 00070 | Loss 13.7514
15:20:29.886452 [0] Epoch: 070, Train: 0.4008, Val: 0.4166, Test: 0.4078
15:20:29.902548 [0] Epoch 00071 | Loss 13.8498
15:20:29.919464 [0] Epoch 00072 | Loss 13.9483
15:20:29.934627 [0] Epoch 00073 | Loss 14.0497
15:20:29.951577 [0] Epoch 00074 | Loss 14.1561
15:20:29.966239 [0] Epoch 00075 | Loss 14.2684
15:20:29.989012 [0] Epoch 00076 | Loss 14.3863
15:20:30.005058 [0] Epoch 00077 | Loss 14.5075
15:20:30.021864 [0] Epoch 00078 | Loss 14.6283
15:20:30.038841 [0] Epoch 00079 | Loss 14.7461
15:20:30.057119 [0] Epoch 00080 | Loss 14.8605
15:20:30.058803 [0] Epoch: 080, Train: 0.4172, Val: 0.4410, Test: 0.4380
15:20:30.075387 [0] Epoch 00081 | Loss 14.9731
15:20:30.092429 [0] Epoch 00082 | Loss 15.0856
15:20:30.108952 [0] Epoch 00083 | Loss 15.2001
15:20:30.123817 [0] Epoch 00084 | Loss 15.3189
15:20:30.140489 [0] Epoch 00085 | Loss 15.4417
15:20:30.157197 [0] Epoch 00086 | Loss 15.5668
15:20:30.172041 [0] Epoch 00087 | Loss 15.6930
15:20:30.188364 [0] Epoch 00088 | Loss 15.8214
15:20:30.203690 [0] Epoch 00089 | Loss 15.9530
15:20:30.220517 [0] Epoch 00090 | Loss 16.0876
15:20:30.222900 [0] Epoch: 090, Train: 0.4339, Val: 0.4647, Test: 0.4666
15:20:30.240794 [0] Epoch 00091 | Loss 16.2245
15:20:30.257356 [0] Epoch 00092 | Loss 16.3623
15:20:30.272174 [0] Epoch 00093 | Loss 16.4994
15:20:30.288648 [0] Epoch 00094 | Loss 16.6344
15:20:30.305010 [0] Epoch 00095 | Loss 16.7681
15:20:30.319879 [0] Epoch 00096 | Loss 16.9036
15:20:30.336512 [0] Epoch 00097 | Loss 17.0436
15:20:30.351839 [0] Epoch 00098 | Loss 17.1890
15:20:30.368248 [0] Epoch 00099 | Loss 17.3387
15:20:30.383442 [0] Epoch 00100 | Loss 17.4913
15:20:30.385962 [0] Epoch: 100, Train: 0.4479, Val: 0.4847, Test: 0.4898
15:20:30.403969 [0] Epoch 00101 | Loss 17.6452
15:20:30.419532 [0] Epoch 00102 | Loss 17.7983
15:20:30.436087 [0] Epoch 00103 | Loss 17.9513
15:20:30.451193 [0] Epoch 00104 | Loss 18.1056
15:20:30.467961 [0] Epoch 00105 | Loss 18.2621
15:20:30.484653 [0] Epoch 00106 | Loss 18.4215
15:20:30.500869 [0] Epoch 00107 | Loss 18.5845
15:20:30.517855 [0] Epoch 00108 | Loss 18.7504
15:20:30.534466 [0] Epoch 00109 | Loss 18.9176
15:20:30.549082 [0] Epoch 00110 | Loss 19.0847
15:20:30.551566 [0] Epoch: 110, Train: 0.4601, Val: 0.5018, Test: 0.5095
15:20:30.568255 [0] Epoch 00111 | Loss 19.2509
15:20:30.584330 [0] Epoch 00112 | Loss 19.4157
15:20:30.599357 [0] Epoch 00113 | Loss 19.5798
15:20:30.616945 [0] Epoch 00114 | Loss 19.7450
15:20:30.632381 [0] Epoch 00115 | Loss 19.9131
15:20:30.648195 [0] Epoch 00116 | Loss 20.0849
15:20:30.665343 [0] Epoch 00117 | Loss 20.2598
15:20:30.680347 [0] Epoch 00118 | Loss 20.4368
15:20:30.696853 [0] Epoch 00119 | Loss 20.6143
15:20:30.716726 [0] Epoch 00120 | Loss 20.7912
15:20:30.719243 [0] Epoch: 120, Train: 0.4696, Val: 0.5148, Test: 0.5226
15:20:30.736149 [0] Epoch 00121 | Loss 20.9672
15:20:30.762683 [0] Epoch 00122 | Loss 21.1427
15:20:30.778441 [0] Epoch 00123 | Loss 21.3189
15:20:30.794306 [0] Epoch 00124 | Loss 21.4970
15:20:30.811677 [0] Epoch 00125 | Loss 21.6782
15:20:30.828454 [0] Epoch 00126 | Loss 21.8625
15:20:30.845698 [0] Epoch 00127 | Loss 22.0492
15:20:30.860997 [0] Epoch 00128 | Loss 22.2376
15:20:30.876449 [0] Epoch 00129 | Loss 22.4276
15:20:30.891961 [0] Epoch 00130 | Loss 22.6196
15:20:30.894476 [0] Epoch: 130, Train: 0.4778, Val: 0.5234, Test: 0.5331
15:20:30.912978 [0] Epoch 00131 | Loss 22.8116
15:20:30.932360 [0] Epoch 00132 | Loss 23.0025
15:20:30.949850 [0] Epoch 00133 | Loss 23.1921
15:20:30.966509 [0] Epoch 00134 | Loss 23.3818
15:20:30.982954 [0] Epoch 00135 | Loss 23.5731
15:20:30.997846 [0] Epoch 00136 | Loss 23.7671
15:20:31.014429 [0] Epoch 00137 | Loss 23.9629
15:20:31.029594 [0] Epoch 00138 | Loss 24.1593
15:20:31.045024 [0] Epoch 00139 | Loss 24.3558
15:20:31.061581 [0] Epoch 00140 | Loss 24.5521
15:20:31.062964 [0] Epoch: 140, Train: 0.4845, Val: 0.5317, Test: 0.5419
15:20:31.077905 [0] Epoch 00141 | Loss 24.7488
15:20:31.093343 [0] Epoch 00142 | Loss 24.9468
15:20:31.111703 [0] Epoch 00143 | Loss 25.1464
15:20:31.127950 [0] Epoch 00144 | Loss 25.3472
15:20:31.143244 [0] Epoch 00145 | Loss 25.5495
15:20:31.160885 [0] Epoch 00146 | Loss 25.7534
15:20:31.178007 [0] Epoch 00147 | Loss 25.9583
15:20:31.193464 [0] Epoch 00148 | Loss 26.1630
15:20:31.209351 [0] Epoch 00149 | Loss 26.3673
15:20:31.226842 [0] Epoch 00150 | Loss 26.5714
15:20:31.229290 [0] Epoch: 150, Train: 0.4903, Val: 0.5381, Test: 0.5489
15:20:31.247566 [0] Epoch 00151 | Loss 26.7766
15:20:31.264905 [0] Epoch 00152 | Loss 26.9832
15:20:31.281487 [0] Epoch 00153 | Loss 27.1914
15:20:31.298096 [0] Epoch 00154 | Loss 27.4006
15:20:31.314572 [0] Epoch 00155 | Loss 27.6107
15:20:31.329588 [0] Epoch 00156 | Loss 27.8217
15:20:31.346261 [0] Epoch 00157 | Loss 28.0342
15:20:31.362876 [0] Epoch 00158 | Loss 28.2485
15:20:31.378310 [0] Epoch 00159 | Loss 28.4641
15:20:31.399761 [0] Epoch 00160 | Loss 28.6810
15:20:31.402216 [0] Epoch: 160, Train: 0.4952, Val: 0.5443, Test: 0.5542
15:20:31.419258 [0] Epoch 00161 | Loss 28.8995
15:20:31.436433 [0] Epoch 00162 | Loss 29.1188
15:20:31.451720 [0] Epoch 00163 | Loss 29.3383
15:20:31.468299 [0] Epoch 00164 | Loss 29.5583
15:20:31.485275 [0] Epoch 00165 | Loss 29.7790
15:20:31.501356 [0] Epoch 00166 | Loss 30.0010
15:20:31.518232 [0] Epoch 00167 | Loss 30.2251
15:20:31.535939 [0] Epoch 00168 | Loss 30.4511
15:20:31.551852 [0] Epoch 00169 | Loss 30.6784
15:20:31.567367 [0] Epoch 00170 | Loss 30.9066
15:20:31.569865 [0] Epoch: 170, Train: 0.4997, Val: 0.5499, Test: 0.5596
15:20:31.588214 [0] Epoch 00171 | Loss 31.1357
15:20:31.604755 [0] Epoch 00172 | Loss 31.3657
15:20:31.620809 [0] Epoch 00173 | Loss 31.5969
15:20:31.635797 [0] Epoch 00174 | Loss 31.8293
15:20:31.652109 [0] Epoch 00175 | Loss 32.0622
15:20:31.667280 [0] Epoch 00176 | Loss 32.2959
15:20:31.684775 [0] Epoch 00177 | Loss 32.5309
15:20:31.699995 [0] Epoch 00178 | Loss 32.7672
15:20:31.717863 [0] Epoch 00179 | Loss 33.0048
15:20:31.736231 [0] Epoch 00180 | Loss 33.2436
15:20:31.738674 [0] Epoch: 180, Train: 0.5043, Val: 0.5541, Test: 0.5638
15:20:31.756179 [0] Epoch 00181 | Loss 33.4838
15:20:31.772872 [0] Epoch 00182 | Loss 33.7257
15:20:31.788031 [0] Epoch 00183 | Loss 33.9685
15:20:31.804150 [0] Epoch 00184 | Loss 34.2119
15:20:31.819318 [0] Epoch 00185 | Loss 34.4544
15:20:31.835572 [0] Epoch 00186 | Loss 34.6964
15:20:31.852030 [0] Epoch 00187 | Loss 34.9392
15:20:31.868519 [0] Epoch 00188 | Loss 35.1842
15:20:31.883847 [0] Epoch 00189 | Loss 35.4314
15:20:31.900319 [0] Epoch 00190 | Loss 35.6799
15:20:31.902401 [0] Epoch: 190, Train: 0.5082, Val: 0.5574, Test: 0.5671
15:20:31.920065 [0] Epoch 00191 | Loss 35.9292
15:20:31.938571 [0] Epoch 00192 | Loss 36.1785
15:20:31.956104 [0] Epoch 00193 | Loss 36.4272
15:20:31.971306 [0] Epoch 00194 | Loss 36.6757
15:20:31.988079 [0] Epoch 00195 | Loss 36.9247
15:20:32.003336 [0] Epoch 00196 | Loss 37.1750
15:20:32.020248 [0] Epoch 00197 | Loss 37.4269
15:20:32.035437 [0] Epoch 00198 | Loss 37.6801
15:20:32.054276 [0] Epoch 00199 | Loss 37.9343
15:20:32.056743 [0] Epoch: 199, Train: 0.5115, Val: 0.5598, Test: 0.5704
15:20:32.057981 [0] 
timer summary:
  1.41s   1.41s   800 mm
  0.18s   0.18s   800 broadcast
  1.01s   1.01s   800 spmm
  0.07s   0.07s   400 all_reduce
  4.22s   4.22s   200 epoch
  5.72s   5.72s     1 total
15:20:54.854213 [0] proc begin: <DistEnv 0/1 nccl>
15:20:56.441161 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:20:56.442007 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:20:57.438234 [0] Epoch 00000 | Loss 3.6892
15:20:57.441091 [0] Epoch: 000, Train: 0.0152, Val: 0.0138, Test: 0.0143
15:20:57.461379 [0] Epoch 00001 | Loss 3.6329
15:20:57.479175 [0] Epoch 00002 | Loss 3.5946
15:20:57.499281 [0] Epoch 00003 | Loss 3.5870
15:20:57.520108 [0] Epoch 00004 | Loss 3.6235
15:20:57.538187 [0] Epoch 00005 | Loss 3.7096
15:20:57.557128 [0] Epoch 00006 | Loss 3.8449
15:20:57.577049 [0] Epoch 00007 | Loss 4.0278
15:20:57.596274 [0] Epoch 00008 | Loss 4.2576
15:20:57.615797 [0] Epoch 00009 | Loss 4.5334
15:20:57.639373 [0] Epoch 00010 | Loss 4.8534
15:20:57.641599 [0] Epoch: 010, Train: 0.1801, Val: 0.0774, Test: 0.0590
15:20:57.662951 [0] Epoch 00011 | Loss 5.2161
15:20:57.683457 [0] Epoch 00012 | Loss 5.6213
15:20:57.703475 [0] Epoch 00013 | Loss 6.0696
15:20:57.723485 [0] Epoch 00014 | Loss 6.5616
15:20:57.742190 [0] Epoch 00015 | Loss 7.0975
15:20:57.764400 [0] Epoch 00016 | Loss 7.6778
15:20:57.783666 [0] Epoch 00017 | Loss 8.3030
15:20:57.802235 [0] Epoch 00018 | Loss 8.9736
15:20:57.823146 [0] Epoch 00019 | Loss 9.6900
15:20:57.846561 [0] Epoch 00020 | Loss 10.4520
15:20:57.848852 [0] Epoch: 020, Train: 0.1795, Val: 0.0767, Test: 0.0587
15:20:57.868767 [0] Epoch 00021 | Loss 11.2577
15:20:57.894636 [0] Epoch 00022 | Loss 12.1044
15:20:57.914147 [0] Epoch 00023 | Loss 12.9903
15:20:57.935131 [0] Epoch 00024 | Loss 13.9139
15:20:57.953964 [0] Epoch 00025 | Loss 14.8743
15:20:57.974903 [0] Epoch 00026 | Loss 15.8709
15:20:57.998795 [0] Epoch 00027 | Loss 16.9023
15:20:58.019351 [0] Epoch 00028 | Loss 17.9666
15:20:58.038433 [0] Epoch 00029 | Loss 19.0627
15:20:58.055856 [0] Epoch 00030 | Loss 20.1902
15:20:58.058349 [0] Epoch: 030, Train: 0.1806, Val: 0.0774, Test: 0.0589
15:20:58.076711 [0] Epoch 00031 | Loss 21.3491
15:20:58.094917 [0] Epoch 00032 | Loss 22.5398
15:20:58.116736 [0] Epoch 00033 | Loss 23.7632
15:20:58.138177 [0] Epoch 00034 | Loss 25.0205
15:20:58.158957 [0] Epoch 00035 | Loss 26.3128
15:20:58.182072 [0] Epoch 00036 | Loss 27.6418
15:20:58.202999 [0] Epoch 00037 | Loss 29.0088
15:20:58.222011 [0] Epoch 00038 | Loss 30.4148
15:20:58.247669 [0] Epoch 00039 | Loss 31.8588
15:20:58.269727 [0] Epoch 00040 | Loss 33.3368
15:20:58.271635 [0] Epoch: 040, Train: 0.2374, Val: 0.2266, Test: 0.2135
15:20:58.291662 [0] Epoch 00041 | Loss 34.8454
15:20:58.311653 [0] Epoch 00042 | Loss 36.3846
15:20:58.330733 [0] Epoch 00043 | Loss 37.9541
15:20:58.352741 [0] Epoch 00044 | Loss 39.5526
15:20:58.374282 [0] Epoch 00045 | Loss 41.1801
15:20:58.395252 [0] Epoch 00046 | Loss 42.8390
15:20:58.417419 [0] Epoch 00047 | Loss 44.5317
15:20:58.438646 [0] Epoch 00048 | Loss 46.2598
15:20:58.460152 [0] Epoch 00049 | Loss 48.0244
15:20:58.481891 [0] Epoch 00050 | Loss 49.8240
15:20:58.483972 [0] Epoch: 050, Train: 0.2609, Val: 0.2763, Test: 0.2568
15:20:58.504169 [0] Epoch 00051 | Loss 51.6560
15:20:58.524290 [0] Epoch 00052 | Loss 53.5164
15:20:58.541627 [0] Epoch 00053 | Loss 55.4010
15:20:58.559290 [0] Epoch 00054 | Loss 57.3083
15:20:58.577827 [0] Epoch 00055 | Loss 59.2425
15:20:58.598019 [0] Epoch 00056 | Loss 61.2083
15:20:58.615818 [0] Epoch 00057 | Loss 63.2076
15:20:58.635579 [0] Epoch 00058 | Loss 65.2390
15:20:58.658143 [0] Epoch 00059 | Loss 67.2988
15:20:58.678516 [0] Epoch 00060 | Loss 69.3832
15:20:58.679973 [0] Epoch: 060, Train: 0.2660, Val: 0.2853, Test: 0.2629
15:20:58.699248 [0] Epoch 00061 | Loss 71.4898
15:20:58.718801 [0] Epoch 00062 | Loss 73.6190
15:20:58.739416 [0] Epoch 00063 | Loss 75.7735
15:20:58.758219 [0] Epoch 00064 | Loss 77.9553
15:20:58.778746 [0] Epoch 00065 | Loss 80.1654
15:20:58.798741 [0] Epoch 00066 | Loss 82.4029
15:20:58.822128 [0] Epoch 00067 | Loss 84.6658
15:20:58.843193 [0] Epoch 00068 | Loss 86.9525
15:20:58.863309 [0] Epoch 00069 | Loss 89.2628
15:20:58.883691 [0] Epoch 00070 | Loss 91.5977
15:20:58.885511 [0] Epoch: 070, Train: 0.2686, Val: 0.2906, Test: 0.2655
15:20:58.904642 [0] Epoch 00071 | Loss 93.9582
15:20:58.925251 [0] Epoch 00072 | Loss 96.3438
15:20:58.944744 [0] Epoch 00073 | Loss 98.7527
15:20:58.967581 [0] Epoch 00074 | Loss 101.1821
15:20:58.990675 [0] Epoch 00075 | Loss 103.6300
15:20:59.012092 [0] Epoch 00076 | Loss 106.0955
15:20:59.032235 [0] Epoch 00077 | Loss 108.5791
15:20:59.051092 [0] Epoch 00078 | Loss 111.0809
15:20:59.074981 [0] Epoch 00079 | Loss 113.6005
15:20:59.096942 [0] Epoch 00080 | Loss 116.1366
15:20:59.099238 [0] Epoch: 080, Train: 0.2687, Val: 0.2920, Test: 0.2660
15:20:59.119129 [0] Epoch 00081 | Loss 118.6880
15:20:59.139030 [0] Epoch 00082 | Loss 121.2544
15:20:59.158908 [0] Epoch 00083 | Loss 123.8358
15:20:59.178779 [0] Epoch 00084 | Loss 126.4327
15:20:59.199941 [0] Epoch 00085 | Loss 129.0446
15:20:59.222733 [0] Epoch 00086 | Loss 131.6702
15:20:59.244373 [0] Epoch 00087 | Loss 134.3082
15:20:59.266240 [0] Epoch 00088 | Loss 136.9576
15:20:59.285840 [0] Epoch 00089 | Loss 139.6179
15:20:59.307744 [0] Epoch 00090 | Loss 142.2890
15:20:59.310344 [0] Epoch: 090, Train: 0.2682, Val: 0.2931, Test: 0.2662
15:20:59.332149 [0] Epoch 00091 | Loss 144.9706
15:20:59.352139 [0] Epoch 00092 | Loss 147.6620
15:20:59.374298 [0] Epoch 00093 | Loss 150.3625
15:20:59.397001 [0] Epoch 00094 | Loss 153.0714
15:20:59.422170 [0] Epoch 00095 | Loss 155.7886
15:20:59.441467 [0] Epoch 00096 | Loss 158.5139
15:20:59.463470 [0] Epoch 00097 | Loss 161.2473
15:20:59.483964 [0] Epoch 00098 | Loss 163.9879
15:20:59.503110 [0] Epoch 00099 | Loss 166.7350
15:20:59.523402 [0] Epoch 00100 | Loss 169.4881
15:20:59.525911 [0] Epoch: 100, Train: 0.2682, Val: 0.2942, Test: 0.2667
15:20:59.547858 [0] Epoch 00101 | Loss 172.2471
15:20:59.567885 [0] Epoch 00102 | Loss 175.0119
15:20:59.590777 [0] Epoch 00103 | Loss 177.7824
15:20:59.612064 [0] Epoch 00104 | Loss 180.5585
15:20:59.632601 [0] Epoch 00105 | Loss 183.3397
15:20:59.654984 [0] Epoch 00106 | Loss 186.1257
15:20:59.675648 [0] Epoch 00107 | Loss 188.9164
15:20:59.695739 [0] Epoch 00108 | Loss 191.7115
15:20:59.715600 [0] Epoch 00109 | Loss 194.5107
15:20:59.734957 [0] Epoch 00110 | Loss 197.3135
15:20:59.736752 [0] Epoch: 110, Train: 0.2685, Val: 0.2951, Test: 0.2669
15:20:59.757796 [0] Epoch 00111 | Loss 200.1197
15:20:59.779215 [0] Epoch 00112 | Loss 202.9287
15:20:59.802779 [0] Epoch 00113 | Loss 205.7406
15:20:59.823875 [0] Epoch 00114 | Loss 208.5555
15:20:59.845779 [0] Epoch 00115 | Loss 211.3734
15:20:59.865828 [0] Epoch 00116 | Loss 214.1940
15:20:59.888349 [0] Epoch 00117 | Loss 217.0175
15:20:59.909541 [0] Epoch 00118 | Loss 219.8449
15:20:59.932673 [0] Epoch 00119 | Loss 222.6771
15:20:59.954383 [0] Epoch 00120 | Loss 225.5111
15:20:59.956749 [0] Epoch: 120, Train: 0.2687, Val: 0.2958, Test: 0.2671
15:20:59.980472 [0] Epoch 00121 | Loss 228.3437
15:21:00.000565 [0] Epoch 00122 | Loss 231.1752
15:21:00.019365 [0] Epoch 00123 | Loss 234.0066
15:21:00.039309 [0] Epoch 00124 | Loss 236.8393
15:21:00.059384 [0] Epoch 00125 | Loss 239.6737
15:21:00.080412 [0] Epoch 00126 | Loss 242.5100
15:21:00.101199 [0] Epoch 00127 | Loss 245.3477
15:21:00.122067 [0] Epoch 00128 | Loss 248.1873
15:21:00.145162 [0] Epoch 00129 | Loss 251.0280
15:21:00.166972 [0] Epoch 00130 | Loss 253.8694
15:21:00.168945 [0] Epoch: 130, Train: 0.2692, Val: 0.2965, Test: 0.2673
15:21:00.191350 [0] Epoch 00131 | Loss 256.7110
15:21:00.211602 [0] Epoch 00132 | Loss 259.5523
15:21:00.231663 [0] Epoch 00133 | Loss 262.3937
15:21:00.252186 [0] Epoch 00134 | Loss 265.2358
15:21:00.274860 [0] Epoch 00135 | Loss 268.0785
15:21:00.295856 [0] Epoch 00136 | Loss 270.9217
15:21:00.318833 [0] Epoch 00137 | Loss 273.7650
15:21:00.340992 [0] Epoch 00138 | Loss 276.6076
15:21:00.362297 [0] Epoch 00139 | Loss 279.4498
15:21:00.381477 [0] Epoch 00140 | Loss 282.2917
15:21:00.383034 [0] Epoch: 140, Train: 0.2696, Val: 0.2971, Test: 0.2673
15:21:00.404277 [0] Epoch 00141 | Loss 285.1337
15:21:00.425358 [0] Epoch 00142 | Loss 287.9759
15:21:00.445695 [0] Epoch 00143 | Loss 290.8182
15:21:00.465834 [0] Epoch 00144 | Loss 293.6599
15:21:00.484746 [0] Epoch 00145 | Loss 296.5009
15:21:00.503653 [0] Epoch 00146 | Loss 299.3401
15:21:00.525189 [0] Epoch 00147 | Loss 302.1765
15:21:00.546895 [0] Epoch 00148 | Loss 305.0103
15:21:00.567218 [0] Epoch 00149 | Loss 307.8425
15:21:00.587097 [0] Epoch 00150 | Loss 310.6739
15:21:00.588589 [0] Epoch: 150, Train: 0.2697, Val: 0.2974, Test: 0.2676
15:21:00.608098 [0] Epoch 00151 | Loss 313.5052
15:21:00.627826 [0] Epoch 00152 | Loss 316.3365
15:21:00.646992 [0] Epoch 00153 | Loss 319.1672
15:21:00.667506 [0] Epoch 00154 | Loss 321.9965
15:21:00.686868 [0] Epoch 00155 | Loss 324.8246
15:21:00.707581 [0] Epoch 00156 | Loss 327.6511
15:21:00.727698 [0] Epoch 00157 | Loss 330.4773
15:21:00.750503 [0] Epoch 00158 | Loss 333.3064
15:21:00.771687 [0] Epoch 00159 | Loss 336.1438
15:21:00.791085 [0] Epoch 00160 | Loss 338.9822
15:21:00.792522 [0] Epoch: 160, Train: 0.2700, Val: 0.2979, Test: 0.2673
15:21:00.812005 [0] Epoch 00161 | Loss 341.8139
15:21:00.831562 [0] Epoch 00162 | Loss 344.6398
15:21:00.850896 [0] Epoch 00163 | Loss 347.4634
15:21:00.875887 [0] Epoch 00164 | Loss 350.2866
15:21:00.897765 [0] Epoch 00165 | Loss 353.1083
15:21:00.920228 [0] Epoch 00166 | Loss 355.9264
15:21:00.940423 [0] Epoch 00167 | Loss 358.7411
15:21:00.960839 [0] Epoch 00168 | Loss 361.5545
15:21:00.981819 [0] Epoch 00169 | Loss 364.3685
15:21:01.001813 [0] Epoch 00170 | Loss 367.1823
15:21:01.003277 [0] Epoch: 170, Train: 0.2703, Val: 0.2987, Test: 0.2674
15:21:01.022320 [0] Epoch 00171 | Loss 369.9928
15:21:01.045838 [0] Epoch 00172 | Loss 372.7978
15:21:01.066157 [0] Epoch 00173 | Loss 375.5975
15:21:01.091112 [0] Epoch 00174 | Loss 378.3939
15:21:01.111412 [0] Epoch 00175 | Loss 381.1869
15:21:01.130978 [0] Epoch 00176 | Loss 383.9742
15:21:01.151108 [0] Epoch 00177 | Loss 386.7563
15:21:01.174318 [0] Epoch 00178 | Loss 389.5365
15:21:01.196611 [0] Epoch 00179 | Loss 392.3157
15:21:01.218595 [0] Epoch 00180 | Loss 395.0905
15:21:01.220576 [0] Epoch: 180, Train: 0.2698, Val: 0.2993, Test: 0.2684
15:21:01.241307 [0] Epoch 00181 | Loss 397.8568
15:21:01.261399 [0] Epoch 00182 | Loss 400.6167
15:21:01.284643 [0] Epoch 00183 | Loss 403.3737
15:21:01.305955 [0] Epoch 00184 | Loss 406.1301
15:21:01.325013 [0] Epoch 00185 | Loss 408.8830
15:21:01.346021 [0] Epoch 00186 | Loss 411.6303
15:21:01.367227 [0] Epoch 00187 | Loss 414.3726
15:21:01.387938 [0] Epoch 00188 | Loss 417.1130
15:21:01.409834 [0] Epoch 00189 | Loss 419.8523
15:21:01.432193 [0] Epoch 00190 | Loss 422.5894
15:21:01.433750 [0] Epoch: 190, Train: 0.2686, Val: 0.2997, Test: 0.2687
15:21:01.454374 [0] Epoch 00191 | Loss 425.3224
15:21:01.475527 [0] Epoch 00192 | Loss 428.0500
15:21:01.495949 [0] Epoch 00193 | Loss 430.7729
15:21:01.518059 [0] Epoch 00194 | Loss 433.4919
15:21:01.539844 [0] Epoch 00195 | Loss 436.2075
15:21:01.560118 [0] Epoch 00196 | Loss 438.9196
15:21:01.583515 [0] Epoch 00197 | Loss 441.6280
15:21:01.603490 [0] Epoch 00198 | Loss 444.3330
15:21:01.622513 [0] Epoch 00199 | Loss 447.0346
15:21:01.624103 [0] Epoch: 199, Train: 0.2672, Val: 0.2995, Test: 0.2685
15:21:01.625078 [0] 
timer summary:
  0.95s   0.95s   800 mm
  0.31s   0.31s  2000 broadcast
  2.15s   2.15s  2000 spmm
  0.07s   0.07s   400 all_reduce
  5.14s   5.14s   200 epoch
  6.77s   6.77s     1 total
15:22:01.521231 [0] proc begin: <DistEnv 0/1 nccl>
15:22:03.510518 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:22:03.511828 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:22:05.266474 [0] Epoch 00000 | Loss 3.6792
15:22:05.268158 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
15:22:05.285772 [0] Epoch 00001 | Loss 3.4872
15:22:05.301200 [0] Epoch 00002 | Loss 3.4445
15:22:05.317307 [0] Epoch 00003 | Loss 3.5919
15:22:05.332915 [0] Epoch 00004 | Loss 3.9270
15:22:05.349089 [0] Epoch 00005 | Loss 4.4153
15:22:05.367172 [0] Epoch 00006 | Loss 5.0240
15:22:05.384535 [0] Epoch 00007 | Loss 5.7201
15:22:05.399962 [0] Epoch 00008 | Loss 6.4735
15:22:05.417128 [0] Epoch 00009 | Loss 7.2819
15:22:05.432651 [0] Epoch 00010 | Loss 8.1136
15:22:05.434686 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
15:22:05.451413 [0] Epoch 00011 | Loss 8.8971
15:22:05.469229 [0] Epoch 00012 | Loss 9.6118
15:22:05.486209 [0] Epoch 00013 | Loss 10.2092
15:22:05.501573 [0] Epoch 00014 | Loss 10.6998
15:22:05.517544 [0] Epoch 00015 | Loss 11.0928
15:22:05.535542 [0] Epoch 00016 | Loss 11.4191
15:22:05.554493 [0] Epoch 00017 | Loss 11.6880
15:22:05.572203 [0] Epoch 00018 | Loss 11.8933
15:22:05.587446 [0] Epoch 00019 | Loss 12.0301
15:22:05.604303 [0] Epoch 00020 | Loss 12.1161
15:22:05.606803 [0] Epoch: 020, Train: 0.2688, Val: 0.2972, Test: 0.2668
15:22:05.623722 [0] Epoch 00021 | Loss 12.1920
15:22:05.639974 [0] Epoch 00022 | Loss 12.2800
15:22:05.655208 [0] Epoch 00023 | Loss 12.3868
15:22:05.674348 [0] Epoch 00024 | Loss 12.4894
15:22:05.692408 [0] Epoch 00025 | Loss 12.5821
15:22:05.707894 [0] Epoch 00026 | Loss 12.6727
15:22:05.724612 [0] Epoch 00027 | Loss 12.7716
15:22:05.740759 [0] Epoch 00028 | Loss 12.8526
15:22:05.755878 [0] Epoch 00029 | Loss 12.8811
15:22:05.771543 [0] Epoch 00030 | Loss 12.8576
15:22:05.774077 [0] Epoch: 030, Train: 0.2803, Val: 0.3073, Test: 0.2787
15:22:05.791120 [0] Epoch 00031 | Loss 12.8037
15:22:05.807226 [0] Epoch 00032 | Loss 12.7330
15:22:05.823803 [0] Epoch 00033 | Loss 12.6484
15:22:05.838562 [0] Epoch 00034 | Loss 12.5501
15:22:05.855306 [0] Epoch 00035 | Loss 12.4418
15:22:05.872036 [0] Epoch 00036 | Loss 12.3345
15:22:05.886810 [0] Epoch 00037 | Loss 12.2503
15:22:05.904111 [0] Epoch 00038 | Loss 12.2040
15:22:05.921369 [0] Epoch 00039 | Loss 12.1924
15:22:05.936293 [0] Epoch 00040 | Loss 12.2049
15:22:05.938189 [0] Epoch: 040, Train: 0.3106, Val: 0.3274, Test: 0.3023
15:22:05.955371 [0] Epoch 00041 | Loss 12.2345
15:22:05.972557 [0] Epoch 00042 | Loss 12.2747
15:22:05.987798 [0] Epoch 00043 | Loss 12.3158
15:22:06.004596 [0] Epoch 00044 | Loss 12.3515
15:22:06.022089 [0] Epoch 00045 | Loss 12.3823
15:22:06.039883 [0] Epoch 00046 | Loss 12.4109
15:22:06.055079 [0] Epoch 00047 | Loss 12.4343
15:22:06.072656 [0] Epoch 00048 | Loss 12.4461
15:22:06.087907 [0] Epoch 00049 | Loss 12.4445
15:22:06.104348 [0] Epoch 00050 | Loss 12.4343
15:22:06.106785 [0] Epoch: 050, Train: 0.3485, Val: 0.3594, Test: 0.3366
15:22:06.125574 [0] Epoch 00051 | Loss 12.4256
15:22:06.143332 [0] Epoch 00052 | Loss 12.4313
15:22:06.159928 [0] Epoch 00053 | Loss 12.4605
15:22:06.177044 [0] Epoch 00054 | Loss 12.5126
15:22:06.192084 [0] Epoch 00055 | Loss 12.5801
15:22:06.208211 [0] Epoch 00056 | Loss 12.6534
15:22:06.224854 [0] Epoch 00057 | Loss 12.7209
15:22:06.240186 [0] Epoch 00058 | Loss 12.7732
15:22:06.256240 [0] Epoch 00059 | Loss 12.8101
15:22:06.271543 [0] Epoch 00060 | Loss 12.8427
15:22:06.274129 [0] Epoch: 060, Train: 0.3787, Val: 0.3911, Test: 0.3746
15:22:06.290586 [0] Epoch 00061 | Loss 12.8849
15:22:06.306623 [0] Epoch 00062 | Loss 12.9446
15:22:06.323239 [0] Epoch 00063 | Loss 13.0240
15:22:06.338428 [0] Epoch 00064 | Loss 13.1207
15:22:06.365091 [0] Epoch 00065 | Loss 13.2280
15:22:06.381281 [0] Epoch 00066 | Loss 13.3377
15:22:06.396563 [0] Epoch 00067 | Loss 13.4445
15:22:06.412346 [0] Epoch 00068 | Loss 13.5485
15:22:06.430443 [0] Epoch 00069 | Loss 13.6508
15:22:06.448487 [0] Epoch 00070 | Loss 13.7514
15:22:06.450824 [0] Epoch: 070, Train: 0.4008, Val: 0.4166, Test: 0.4078
15:22:06.470566 [0] Epoch 00071 | Loss 13.8498
15:22:06.488657 [0] Epoch 00072 | Loss 13.9483
15:22:06.503971 [0] Epoch 00073 | Loss 14.0497
15:22:06.520302 [0] Epoch 00074 | Loss 14.1561
15:22:06.536618 [0] Epoch 00075 | Loss 14.2684
15:22:06.556280 [0] Epoch 00076 | Loss 14.3863
15:22:06.573578 [0] Epoch 00077 | Loss 14.5075
15:22:06.588995 [0] Epoch 00078 | Loss 14.6283
15:22:06.604608 [0] Epoch 00079 | Loss 14.7461
15:22:06.620624 [0] Epoch 00080 | Loss 14.8605
15:22:06.623120 [0] Epoch: 080, Train: 0.4172, Val: 0.4410, Test: 0.4380
15:22:06.641814 [0] Epoch 00081 | Loss 14.9731
15:22:06.658796 [0] Epoch 00082 | Loss 15.0856
15:22:06.675377 [0] Epoch 00083 | Loss 15.2001
15:22:06.690126 [0] Epoch 00084 | Loss 15.3189
15:22:06.706725 [0] Epoch 00085 | Loss 15.4417
15:22:06.724716 [0] Epoch 00086 | Loss 15.5668
15:22:06.740858 [0] Epoch 00087 | Loss 15.6930
15:22:06.755981 [0] Epoch 00088 | Loss 15.8214
15:22:06.773065 [0] Epoch 00089 | Loss 15.9530
15:22:06.788155 [0] Epoch 00090 | Loss 16.0876
15:22:06.790153 [0] Epoch: 090, Train: 0.4339, Val: 0.4647, Test: 0.4666
15:22:06.807073 [0] Epoch 00091 | Loss 16.2245
15:22:06.823862 [0] Epoch 00092 | Loss 16.3623
15:22:06.840509 [0] Epoch 00093 | Loss 16.4994
15:22:06.855745 [0] Epoch 00094 | Loss 16.6344
15:22:06.872031 [0] Epoch 00095 | Loss 16.7681
15:22:06.888335 [0] Epoch 00096 | Loss 16.9036
15:22:06.903539 [0] Epoch 00097 | Loss 17.0436
15:22:06.920216 [0] Epoch 00098 | Loss 17.1890
15:22:06.936482 [0] Epoch 00099 | Loss 17.3387
15:22:06.953809 [0] Epoch 00100 | Loss 17.4913
15:22:06.955215 [0] Epoch: 100, Train: 0.4479, Val: 0.4847, Test: 0.4898
15:22:06.971214 [0] Epoch 00101 | Loss 17.6452
15:22:06.986724 [0] Epoch 00102 | Loss 17.7983
15:22:07.003552 [0] Epoch 00103 | Loss 17.9513
15:22:07.022536 [0] Epoch 00104 | Loss 18.1056
15:22:07.040136 [0] Epoch 00105 | Loss 18.2620
15:22:07.055385 [0] Epoch 00106 | Loss 18.4215
15:22:07.071975 [0] Epoch 00107 | Loss 18.5845
15:22:07.088952 [0] Epoch 00108 | Loss 18.7504
15:22:07.104229 [0] Epoch 00109 | Loss 18.9176
15:22:07.120773 [0] Epoch 00110 | Loss 19.0847
15:22:07.123297 [0] Epoch: 110, Train: 0.4601, Val: 0.5018, Test: 0.5095
15:22:07.140933 [0] Epoch 00111 | Loss 19.2509
15:22:07.156732 [0] Epoch 00112 | Loss 19.4157
15:22:07.174037 [0] Epoch 00113 | Loss 19.5798
15:22:07.189433 [0] Epoch 00114 | Loss 19.7450
15:22:07.204707 [0] Epoch 00115 | Loss 19.9131
15:22:07.219688 [0] Epoch 00116 | Loss 20.0849
15:22:07.235665 [0] Epoch 00117 | Loss 20.2598
15:22:07.250992 [0] Epoch 00118 | Loss 20.4369
15:22:07.269072 [0] Epoch 00119 | Loss 20.6143
15:22:07.286580 [0] Epoch 00120 | Loss 20.7912
15:22:07.287992 [0] Epoch: 120, Train: 0.4696, Val: 0.5148, Test: 0.5226
15:22:07.304147 [0] Epoch 00121 | Loss 20.9672
15:22:07.319496 [0] Epoch 00122 | Loss 21.1427
15:22:07.336006 [0] Epoch 00123 | Loss 21.3189
15:22:07.352246 [0] Epoch 00124 | Loss 21.4970
15:22:07.367246 [0] Epoch 00125 | Loss 21.6782
15:22:07.384701 [0] Epoch 00126 | Loss 21.8625
15:22:07.401289 [0] Epoch 00127 | Loss 22.0492
15:22:07.416288 [0] Epoch 00128 | Loss 22.2376
15:22:07.432521 [0] Epoch 00129 | Loss 22.4276
15:22:07.450918 [0] Epoch 00130 | Loss 22.6195
15:22:07.453433 [0] Epoch: 130, Train: 0.4778, Val: 0.5234, Test: 0.5331
15:22:07.472625 [0] Epoch 00131 | Loss 22.8116
15:22:07.489870 [0] Epoch 00132 | Loss 23.0025
15:22:07.505278 [0] Epoch 00133 | Loss 23.1921
15:22:07.520738 [0] Epoch 00134 | Loss 23.3818
15:22:07.536110 [0] Epoch 00135 | Loss 23.5731
15:22:07.552723 [0] Epoch 00136 | Loss 23.7671
15:22:07.568530 [0] Epoch 00137 | Loss 23.9629
15:22:07.584470 [0] Epoch 00138 | Loss 24.1593
15:22:07.600156 [0] Epoch 00139 | Loss 24.3558
15:22:07.616170 [0] Epoch 00140 | Loss 24.5521
15:22:07.618292 [0] Epoch: 140, Train: 0.4845, Val: 0.5317, Test: 0.5419
15:22:07.645001 [0] Epoch 00141 | Loss 24.7488
15:22:07.661921 [0] Epoch 00142 | Loss 24.9468
15:22:07.680135 [0] Epoch 00143 | Loss 25.1464
15:22:07.695416 [0] Epoch 00144 | Loss 25.3472
15:22:07.713156 [0] Epoch 00145 | Loss 25.5495
15:22:07.730852 [0] Epoch 00146 | Loss 25.7534
15:22:07.747835 [0] Epoch 00147 | Loss 25.9583
15:22:07.763155 [0] Epoch 00148 | Loss 26.1630
15:22:07.780868 [0] Epoch 00149 | Loss 26.3673
15:22:07.795993 [0] Epoch 00150 | Loss 26.5714
15:22:07.798244 [0] Epoch: 150, Train: 0.4903, Val: 0.5381, Test: 0.5489
15:22:07.814710 [0] Epoch 00151 | Loss 26.7766
15:22:07.832606 [0] Epoch 00152 | Loss 26.9832
15:22:07.847665 [0] Epoch 00153 | Loss 27.1914
15:22:07.864071 [0] Epoch 00154 | Loss 27.4006
15:22:07.879454 [0] Epoch 00155 | Loss 27.6107
15:22:07.896329 [0] Epoch 00156 | Loss 27.8217
15:22:07.913771 [0] Epoch 00157 | Loss 28.0342
15:22:07.929261 [0] Epoch 00158 | Loss 28.2484
15:22:07.944834 [0] Epoch 00159 | Loss 28.4641
15:22:07.962454 [0] Epoch 00160 | Loss 28.6810
15:22:07.963926 [0] Epoch: 160, Train: 0.4952, Val: 0.5443, Test: 0.5542
15:22:07.980312 [0] Epoch 00161 | Loss 28.8995
15:22:07.995760 [0] Epoch 00162 | Loss 29.1188
15:22:08.013308 [0] Epoch 00163 | Loss 29.3383
15:22:08.033305 [0] Epoch 00164 | Loss 29.5583
15:22:08.050484 [0] Epoch 00165 | Loss 29.7790
15:22:08.068175 [0] Epoch 00166 | Loss 30.0010
15:22:08.084952 [0] Epoch 00167 | Loss 30.2251
15:22:08.100042 [0] Epoch 00168 | Loss 30.4510
15:22:08.116320 [0] Epoch 00169 | Loss 30.6784
15:22:08.136476 [0] Epoch 00170 | Loss 30.9065
15:22:08.139072 [0] Epoch: 170, Train: 0.4997, Val: 0.5499, Test: 0.5596
15:22:08.156787 [0] Epoch 00171 | Loss 31.1357
15:22:08.173645 [0] Epoch 00172 | Loss 31.3656
15:22:08.190919 [0] Epoch 00173 | Loss 31.5969
15:22:08.207912 [0] Epoch 00174 | Loss 31.8293
15:22:08.224717 [0] Epoch 00175 | Loss 32.0622
15:22:08.242145 [0] Epoch 00176 | Loss 32.2959
15:22:08.258709 [0] Epoch 00177 | Loss 32.5309
15:22:08.274126 [0] Epoch 00178 | Loss 32.7672
15:22:08.298024 [0] Epoch 00179 | Loss 33.0048
15:22:08.313733 [0] Epoch 00180 | Loss 33.2437
15:22:08.315475 [0] Epoch: 180, Train: 0.5043, Val: 0.5541, Test: 0.5638
15:22:08.331309 [0] Epoch 00181 | Loss 33.4839
15:22:08.348547 [0] Epoch 00182 | Loss 33.7258
15:22:08.368496 [0] Epoch 00183 | Loss 33.9686
15:22:08.385071 [0] Epoch 00184 | Loss 34.2120
15:22:08.401254 [0] Epoch 00185 | Loss 34.4545
15:22:08.418583 [0] Epoch 00186 | Loss 34.6963
15:22:08.436786 [0] Epoch 00187 | Loss 34.9392
15:22:08.452085 [0] Epoch 00188 | Loss 35.1842
15:22:08.470150 [0] Epoch 00189 | Loss 35.4314
15:22:08.486778 [0] Epoch 00190 | Loss 35.6801
15:22:08.488252 [0] Epoch: 190, Train: 0.5082, Val: 0.5574, Test: 0.5671
15:22:08.504573 [0] Epoch 00191 | Loss 35.9294
15:22:08.523510 [0] Epoch 00192 | Loss 36.1786
15:22:08.540877 [0] Epoch 00193 | Loss 36.4272
15:22:08.557634 [0] Epoch 00194 | Loss 36.6757
15:22:08.572812 [0] Epoch 00195 | Loss 36.9247
15:22:08.588815 [0] Epoch 00196 | Loss 37.1750
15:22:08.604007 [0] Epoch 00197 | Loss 37.4269
15:22:08.620298 [0] Epoch 00198 | Loss 37.6801
15:22:08.637816 [0] Epoch 00199 | Loss 37.9343
15:22:08.640363 [0] Epoch: 199, Train: 0.5115, Val: 0.5598, Test: 0.5704
15:22:08.641868 [0] 
timer summary:
  1.99s   1.99s   800 mm
  0.19s   0.19s   800 broadcast
  1.21s   1.21s   800 spmm
  0.08s   0.08s   400 all_reduce
  5.08s   5.08s   200 epoch
  7.12s   7.12s     1 total
15:22:49.623680 [0] proc begin: <DistEnv 0/1 nccl>
15:22:51.269425 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:22:51.270228 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:22:52.230423 [0] Epoch 00000 | Loss 3.6849
15:22:52.231856 [0] Epoch: 000, Train: 0.0258, Val: 0.0278, Test: 0.0270
15:22:52.254207 [0] Epoch 00001 | Loss 3.5696
15:22:52.275480 [0] Epoch 00002 | Loss 3.6400
15:22:52.296878 [0] Epoch 00003 | Loss 3.8904
15:22:52.320174 [0] Epoch 00004 | Loss 4.3040
15:22:52.342205 [0] Epoch 00005 | Loss 4.8643
15:22:52.363097 [0] Epoch 00006 | Loss 5.5653
15:22:52.384902 [0] Epoch 00007 | Loss 6.4056
15:22:52.410186 [0] Epoch 00008 | Loss 7.3853
15:22:52.432520 [0] Epoch 00009 | Loss 8.5044
15:22:52.455202 [0] Epoch 00010 | Loss 9.7604
15:22:52.457290 [0] Epoch: 010, Train: 0.2313, Val: 0.2114, Test: 0.1986
15:22:52.481592 [0] Epoch 00011 | Loss 11.1485
15:22:52.503764 [0] Epoch 00012 | Loss 12.6620
15:22:52.526849 [0] Epoch 00013 | Loss 14.2937
15:22:52.552059 [0] Epoch 00014 | Loss 16.0377
15:22:52.573514 [0] Epoch 00015 | Loss 17.8891
15:22:52.594437 [0] Epoch 00016 | Loss 19.8443
15:22:52.620219 [0] Epoch 00017 | Loss 21.9008
15:22:52.643754 [0] Epoch 00018 | Loss 24.0578
15:22:52.667052 [0] Epoch 00019 | Loss 26.3172
15:22:52.689591 [0] Epoch 00020 | Loss 28.6828
15:22:52.691086 [0] Epoch: 020, Train: 0.2571, Val: 0.2689, Test: 0.2508
15:22:52.713001 [0] Epoch 00021 | Loss 31.1594
15:22:52.736978 [0] Epoch 00022 | Loss 33.7492
15:22:52.761416 [0] Epoch 00023 | Loss 36.4509
15:22:52.785679 [0] Epoch 00024 | Loss 39.2608
15:22:52.807826 [0] Epoch 00025 | Loss 42.1754
15:22:52.832952 [0] Epoch 00026 | Loss 45.1909
15:22:52.856705 [0] Epoch 00027 | Loss 48.2998
15:22:52.880971 [0] Epoch 00028 | Loss 51.4902
15:22:52.905249 [0] Epoch 00029 | Loss 54.7522
15:22:52.929658 [0] Epoch 00030 | Loss 58.0858
15:22:52.931285 [0] Epoch: 030, Train: 0.2727, Val: 0.2953, Test: 0.2687
15:22:52.954054 [0] Epoch 00031 | Loss 61.4999
15:22:52.981535 [0] Epoch 00032 | Loss 65.0009
15:22:53.006318 [0] Epoch 00033 | Loss 68.5886
15:22:53.028515 [0] Epoch 00034 | Loss 72.2584
15:22:53.051757 [0] Epoch 00035 | Loss 76.0010
15:22:53.078635 [0] Epoch 00036 | Loss 79.8064
15:22:53.103961 [0] Epoch 00037 | Loss 83.6756
15:22:53.127357 [0] Epoch 00038 | Loss 87.6171
15:22:53.154255 [0] Epoch 00039 | Loss 91.6339
15:22:53.178142 [0] Epoch 00040 | Loss 95.7212
15:22:53.180653 [0] Epoch: 040, Train: 0.2729, Val: 0.2983, Test: 0.2685
15:22:53.205031 [0] Epoch 00041 | Loss 99.8690
15:22:53.228585 [0] Epoch 00042 | Loss 104.0712
15:22:53.252174 [0] Epoch 00043 | Loss 108.3354
15:22:53.277496 [0] Epoch 00044 | Loss 112.6680
15:22:53.299662 [0] Epoch 00045 | Loss 117.0652
15:22:53.324058 [0] Epoch 00046 | Loss 121.5166
15:22:53.348310 [0] Epoch 00047 | Loss 126.0193
15:22:53.373085 [0] Epoch 00048 | Loss 130.5797
15:22:53.398184 [0] Epoch 00049 | Loss 135.1970
15:22:53.422780 [0] Epoch 00050 | Loss 139.8613
15:22:53.426338 [0] Epoch: 050, Train: 0.2718, Val: 0.2982, Test: 0.2680
15:22:53.451350 [0] Epoch 00051 | Loss 144.5664
15:22:53.473817 [0] Epoch 00052 | Loss 149.3177
15:22:53.496879 [0] Epoch 00053 | Loss 154.1138
15:22:53.521300 [0] Epoch 00054 | Loss 158.9448
15:22:53.544772 [0] Epoch 00055 | Loss 163.8104
15:22:53.568656 [0] Epoch 00056 | Loss 168.7169
15:22:53.591838 [0] Epoch 00057 | Loss 173.6598
15:22:53.617599 [0] Epoch 00058 | Loss 178.6316
15:22:53.640886 [0] Epoch 00059 | Loss 183.6360
15:22:53.665406 [0] Epoch 00060 | Loss 188.6721
15:22:53.667905 [0] Epoch: 060, Train: 0.2703, Val: 0.2979, Test: 0.2675
15:22:53.691279 [0] Epoch 00061 | Loss 193.7323
15:22:53.713945 [0] Epoch 00062 | Loss 198.8182
15:22:53.739639 [0] Epoch 00063 | Loss 203.9296
15:22:53.763016 [0] Epoch 00064 | Loss 209.0589
15:22:53.787521 [0] Epoch 00065 | Loss 214.2074
15:22:53.810777 [0] Epoch 00066 | Loss 219.3730
15:22:53.835750 [0] Epoch 00067 | Loss 224.5492
15:22:53.858920 [0] Epoch 00068 | Loss 229.7393
15:22:53.886254 [0] Epoch 00069 | Loss 234.9399
15:22:53.913932 [0] Epoch 00070 | Loss 240.1502
15:22:53.916397 [0] Epoch: 070, Train: 0.2670, Val: 0.2967, Test: 0.2660
15:22:53.940572 [0] Epoch 00071 | Loss 245.3732
15:22:53.964747 [0] Epoch 00072 | Loss 250.6032
15:22:53.989377 [0] Epoch 00073 | Loss 255.8428
15:22:54.013504 [0] Epoch 00074 | Loss 261.0871
15:22:54.038570 [0] Epoch 00075 | Loss 266.3345
15:22:54.067025 [0] Epoch 00076 | Loss 271.5848
15:22:54.092176 [0] Epoch 00077 | Loss 276.8347
15:22:54.115926 [0] Epoch 00078 | Loss 282.0890
15:22:54.141201 [0] Epoch 00079 | Loss 287.3433
15:22:54.165450 [0] Epoch 00080 | Loss 292.6071
15:22:54.167554 [0] Epoch: 080, Train: 0.2665, Val: 0.2964, Test: 0.2658
15:22:54.191909 [0] Epoch 00081 | Loss 297.8693
15:22:54.214760 [0] Epoch 00082 | Loss 303.1406
15:22:54.239503 [0] Epoch 00083 | Loss 308.3988
15:22:54.264413 [0] Epoch 00084 | Loss 313.6733
15:22:54.288532 [0] Epoch 00085 | Loss 318.9151
15:22:54.311102 [0] Epoch 00086 | Loss 324.1991
15:22:54.336074 [0] Epoch 00087 | Loss 329.4020
15:22:54.360363 [0] Epoch 00088 | Loss 334.6653
15:22:54.384004 [0] Epoch 00089 | Loss 339.7679
15:22:54.410870 [0] Epoch 00090 | Loss 344.6602
15:22:54.413355 [0] Epoch: 090, Train: 0.2748, Val: 0.2987, Test: 0.2692
15:22:54.439123 [0] Epoch 00091 | Loss 349.5078
15:22:54.461160 [0] Epoch 00092 | Loss 354.5607
15:22:54.482957 [0] Epoch 00093 | Loss 359.7637
15:22:54.508971 [0] Epoch 00094 | Loss 364.9435
15:22:54.534273 [0] Epoch 00095 | Loss 369.9591
15:22:54.559554 [0] Epoch 00096 | Loss 374.9283
15:22:54.583156 [0] Epoch 00097 | Loss 379.9988
15:22:54.608055 [0] Epoch 00098 | Loss 385.1353
15:22:54.632674 [0] Epoch 00099 | Loss 390.2013
15:22:54.657398 [0] Epoch 00100 | Loss 395.1187
15:22:54.659221 [0] Epoch: 100, Train: 0.2688, Val: 0.2970, Test: 0.2664
15:22:54.681991 [0] Epoch 00101 | Loss 400.0452
15:22:54.707718 [0] Epoch 00102 | Loss 405.0588
15:22:54.731909 [0] Epoch 00103 | Loss 410.1121
15:22:54.755522 [0] Epoch 00104 | Loss 415.1858
15:22:54.780148 [0] Epoch 00105 | Loss 420.2496
15:22:54.803582 [0] Epoch 00106 | Loss 425.2803
15:22:54.828914 [0] Epoch 00107 | Loss 430.3030
15:22:54.853045 [0] Epoch 00108 | Loss 435.3285
15:22:54.877655 [0] Epoch 00109 | Loss 440.3686
15:22:54.899807 [0] Epoch 00110 | Loss 445.3689
15:22:54.902197 [0] Epoch: 110, Train: 0.2589, Val: 0.2935, Test: 0.2631
15:22:54.926366 [0] Epoch 00111 | Loss 450.2740
15:22:54.953532 [0] Epoch 00112 | Loss 455.1755
15:22:54.978104 [0] Epoch 00113 | Loss 460.1165
15:22:55.001876 [0] Epoch 00114 | Loss 465.0517
15:22:55.024108 [0] Epoch 00115 | Loss 469.9791
15:22:55.049591 [0] Epoch 00116 | Loss 474.8817
15:22:55.076485 [0] Epoch 00117 | Loss 479.7493
15:22:55.099851 [0] Epoch 00118 | Loss 484.6513
15:22:55.125145 [0] Epoch 00119 | Loss 489.5671
15:22:55.148809 [0] Epoch 00120 | Loss 494.4215
15:22:55.150766 [0] Epoch: 120, Train: 0.2641, Val: 0.2954, Test: 0.2650
15:22:55.179747 [0] Epoch 00121 | Loss 499.2457
15:22:55.203295 [0] Epoch 00122 | Loss 504.0855
15:22:55.226937 [0] Epoch 00123 | Loss 508.9380
15:22:55.252041 [0] Epoch 00124 | Loss 513.7944
15:22:55.276325 [0] Epoch 00125 | Loss 518.6324
15:22:55.301137 [0] Epoch 00126 | Loss 523.4409
15:22:55.324257 [0] Epoch 00127 | Loss 528.2538
15:22:55.349536 [0] Epoch 00128 | Loss 533.0677
15:22:55.374132 [0] Epoch 00129 | Loss 537.8397
15:22:55.399084 [0] Epoch 00130 | Loss 542.5829
15:22:55.401596 [0] Epoch: 130, Train: 0.2640, Val: 0.2954, Test: 0.2651
15:22:55.425920 [0] Epoch 00131 | Loss 547.3531
15:22:55.450588 [0] Epoch 00132 | Loss 552.1279
15:22:55.473149 [0] Epoch 00133 | Loss 556.8610
15:22:55.495641 [0] Epoch 00134 | Loss 561.5822
15:22:55.524085 [0] Epoch 00135 | Loss 566.3149
15:22:55.548378 [0] Epoch 00136 | Loss 571.0231
15:22:55.576946 [0] Epoch 00137 | Loss 575.6949
15:22:55.601613 [0] Epoch 00138 | Loss 580.3610
15:22:55.626283 [0] Epoch 00139 | Loss 585.0242
15:22:55.651967 [0] Epoch 00140 | Loss 589.6624
15:22:55.654010 [0] Epoch: 140, Train: 0.2633, Val: 0.2956, Test: 0.2649
15:22:55.677619 [0] Epoch 00141 | Loss 594.2897
15:22:55.701855 [0] Epoch 00142 | Loss 598.9254
15:22:55.727436 [0] Epoch 00143 | Loss 603.5569
15:22:55.753268 [0] Epoch 00144 | Loss 608.1711
15:22:55.777084 [0] Epoch 00145 | Loss 612.7706
15:22:55.800626 [0] Epoch 00146 | Loss 617.3515
15:22:55.824229 [0] Epoch 00147 | Loss 621.9112
15:22:55.848961 [0] Epoch 00148 | Loss 626.4651
15:22:55.872688 [0] Epoch 00149 | Loss 631.0142
15:22:55.897397 [0] Epoch 00150 | Loss 635.5438
15:22:55.899060 [0] Epoch: 150, Train: 0.2625, Val: 0.2953, Test: 0.2648
15:22:55.921445 [0] Epoch 00151 | Loss 640.0516
15:22:55.945938 [0] Epoch 00152 | Loss 644.5456
15:22:55.970936 [0] Epoch 00153 | Loss 649.0280
15:22:55.994200 [0] Epoch 00154 | Loss 653.5048
15:22:56.017561 [0] Epoch 00155 | Loss 657.9773
15:22:56.041790 [0] Epoch 00156 | Loss 662.4315
15:22:56.067207 [0] Epoch 00157 | Loss 666.8670
15:22:56.092078 [0] Epoch 00158 | Loss 671.2930
15:22:56.116769 [0] Epoch 00159 | Loss 675.7019
15:22:56.140987 [0] Epoch 00160 | Loss 680.0906
15:22:56.143451 [0] Epoch: 160, Train: 0.2626, Val: 0.2955, Test: 0.2650
15:22:56.167908 [0] Epoch 00161 | Loss 684.4677
15:22:56.191820 [0] Epoch 00162 | Loss 688.8322
15:22:56.213983 [0] Epoch 00163 | Loss 693.1865
15:22:56.239420 [0] Epoch 00164 | Loss 697.5420
15:22:56.263066 [0] Epoch 00165 | Loss 701.8906
15:22:56.288023 [0] Epoch 00166 | Loss 706.2209
15:22:56.312735 [0] Epoch 00167 | Loss 710.5400
15:22:56.336482 [0] Epoch 00168 | Loss 714.8482
15:22:56.361245 [0] Epoch 00169 | Loss 719.1436
15:22:56.385187 [0] Epoch 00170 | Loss 723.4352
15:22:56.387526 [0] Epoch: 170, Train: 0.2640, Val: 0.2966, Test: 0.2654
15:22:56.411036 [0] Epoch 00171 | Loss 727.7232
15:22:56.436959 [0] Epoch 00172 | Loss 732.0037
15:22:56.459806 [0] Epoch 00173 | Loss 736.2796
15:22:56.485297 [0] Epoch 00174 | Loss 740.5405
15:22:56.510585 [0] Epoch 00175 | Loss 744.7788
15:22:56.533041 [0] Epoch 00176 | Loss 749.0022
15:22:56.555738 [0] Epoch 00177 | Loss 753.2067
15:22:56.580472 [0] Epoch 00178 | Loss 757.3910
15:22:56.604549 [0] Epoch 00179 | Loss 761.5703
15:22:56.628280 [0] Epoch 00180 | Loss 765.7492
15:22:56.630700 [0] Epoch: 180, Train: 0.2662, Val: 0.2979, Test: 0.2659
15:22:56.655178 [0] Epoch 00181 | Loss 769.9351
15:22:56.680093 [0] Epoch 00182 | Loss 774.1254
15:22:56.704187 [0] Epoch 00183 | Loss 778.3027
15:22:56.729378 [0] Epoch 00184 | Loss 782.4541
15:22:56.754154 [0] Epoch 00185 | Loss 786.5818
15:22:56.776601 [0] Epoch 00186 | Loss 790.6965
15:22:56.799835 [0] Epoch 00187 | Loss 794.7962
15:22:56.826752 [0] Epoch 00188 | Loss 798.8863
15:22:56.852123 [0] Epoch 00189 | Loss 802.9841
15:22:56.875968 [0] Epoch 00190 | Loss 807.0748
15:22:56.878439 [0] Epoch: 190, Train: 0.2689, Val: 0.2992, Test: 0.2669
15:22:56.902660 [0] Epoch 00191 | Loss 811.1439
15:22:56.927961 [0] Epoch 00192 | Loss 815.1932
15:22:56.950877 [0] Epoch 00193 | Loss 819.2252
15:22:56.975902 [0] Epoch 00194 | Loss 823.2405
15:22:57.010886 [0] Epoch 00195 | Loss 827.2434
15:22:57.035721 [0] Epoch 00196 | Loss 831.2359
15:22:57.064265 [0] Epoch 00197 | Loss 835.2167
15:22:57.086704 [0] Epoch 00198 | Loss 839.1962
15:22:57.111539 [0] Epoch 00199 | Loss 843.1702
15:22:57.113607 [0] Epoch: 199, Train: 0.2715, Val: 0.3016, Test: 0.2680
15:22:57.114526 [0] 
timer summary:
  1.40s   1.40s   800 mm
  0.31s   0.31s  2000 broadcast
  2.15s   2.15s  2000 spmm
  0.06s   0.06s   400 all_reduce
  5.79s   5.79s   200 epoch
  7.49s   7.49s     1 total
15:23:58.460179 [0] proc begin: <DistEnv 0/1 nccl>
15:24:00.239258 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:24:00.240077 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:24:01.214484 [0] Epoch 00000 | Loss 3.6817
15:24:01.215833 [0] Epoch: 000, Train: 0.0260, Val: 0.0273, Test: 0.0270
15:24:01.234982 [0] Epoch 00001 | Loss 3.5276
15:24:01.254045 [0] Epoch 00002 | Loss 3.5425
15:24:01.270957 [0] Epoch 00003 | Loss 3.7412
15:24:01.288251 [0] Epoch 00004 | Loss 4.1146
15:24:01.307739 [0] Epoch 00005 | Loss 4.6436
15:24:01.326289 [0] Epoch 00006 | Loss 5.3105
15:24:01.344805 [0] Epoch 00007 | Loss 6.0979
15:24:01.361480 [0] Epoch 00008 | Loss 6.9920
15:24:01.380553 [0] Epoch 00009 | Loss 7.9809
15:24:01.400766 [0] Epoch 00010 | Loss 9.0519
15:24:01.402848 [0] Epoch: 010, Train: 0.2683, Val: 0.2861, Test: 0.2619
15:24:01.421385 [0] Epoch 00011 | Loss 10.1932
15:24:01.440434 [0] Epoch 00012 | Loss 11.4023
15:24:01.460679 [0] Epoch 00013 | Loss 12.6861
15:24:01.480351 [0] Epoch 00014 | Loss 14.0424
15:24:01.498580 [0] Epoch 00015 | Loss 15.4536
15:24:01.517331 [0] Epoch 00016 | Loss 16.9000
15:24:01.534193 [0] Epoch 00017 | Loss 18.3692
15:24:01.551922 [0] Epoch 00018 | Loss 19.8430
15:24:01.570942 [0] Epoch 00019 | Loss 21.2893
15:24:01.599685 [0] Epoch 00020 | Loss 22.7116
15:24:01.601142 [0] Epoch: 020, Train: 0.2604, Val: 0.2948, Test: 0.2640
15:24:01.619460 [0] Epoch 00021 | Loss 24.0998
15:24:01.637671 [0] Epoch 00022 | Loss 25.4555
15:24:01.656236 [0] Epoch 00023 | Loss 26.7885
15:24:01.675207 [0] Epoch 00024 | Loss 28.0987
15:24:01.699077 [0] Epoch 00025 | Loss 29.3971
15:24:01.719251 [0] Epoch 00026 | Loss 30.6620
15:24:01.737831 [0] Epoch 00027 | Loss 31.8794
15:24:01.756507 [0] Epoch 00028 | Loss 33.0405
15:24:01.778322 [0] Epoch 00029 | Loss 34.1444
15:24:01.795989 [0] Epoch 00030 | Loss 35.1940
15:24:01.797665 [0] Epoch: 030, Train: 0.2663, Val: 0.2968, Test: 0.2660
15:24:01.815449 [0] Epoch 00031 | Loss 36.1828
15:24:01.834703 [0] Epoch 00032 | Loss 37.1250
15:24:01.855625 [0] Epoch 00033 | Loss 38.0356
15:24:01.876318 [0] Epoch 00034 | Loss 38.9188
15:24:01.895456 [0] Epoch 00035 | Loss 39.7675
15:24:01.914141 [0] Epoch 00036 | Loss 40.5734
15:24:01.934510 [0] Epoch 00037 | Loss 41.3623
15:24:01.952078 [0] Epoch 00038 | Loss 42.1615
15:24:01.968942 [0] Epoch 00039 | Loss 42.9601
15:24:01.995914 [0] Epoch 00040 | Loss 43.7345
15:24:01.997466 [0] Epoch: 040, Train: 0.2652, Val: 0.2965, Test: 0.2654
15:24:02.015516 [0] Epoch 00041 | Loss 44.4792
15:24:02.032838 [0] Epoch 00042 | Loss 45.1863
15:24:02.051351 [0] Epoch 00043 | Loss 45.8448
15:24:02.070274 [0] Epoch 00044 | Loss 46.4789
15:24:02.088457 [0] Epoch 00045 | Loss 47.0921
15:24:02.105346 [0] Epoch 00046 | Loss 47.6315
15:24:02.123562 [0] Epoch 00047 | Loss 48.0912
15:24:02.140381 [0] Epoch 00048 | Loss 48.4900
15:24:02.160716 [0] Epoch 00049 | Loss 48.8348
15:24:02.178781 [0] Epoch 00050 | Loss 49.1656
15:24:02.180667 [0] Epoch: 050, Train: 0.2674, Val: 0.2978, Test: 0.2670
15:24:02.199007 [0] Epoch 00051 | Loss 49.4832
15:24:02.217411 [0] Epoch 00052 | Loss 49.7778
15:24:02.236863 [0] Epoch 00053 | Loss 50.0827
15:24:02.254507 [0] Epoch 00054 | Loss 50.3906
15:24:02.272814 [0] Epoch 00055 | Loss 50.7074
15:24:02.289974 [0] Epoch 00056 | Loss 51.0367
15:24:02.306519 [0] Epoch 00057 | Loss 51.3518
15:24:02.323572 [0] Epoch 00058 | Loss 51.6699
15:24:02.341538 [0] Epoch 00059 | Loss 51.9918
15:24:02.357387 [0] Epoch 00060 | Loss 52.3103
15:24:02.359588 [0] Epoch: 060, Train: 0.2744, Val: 0.3043, Test: 0.2744
15:24:02.380155 [0] Epoch 00061 | Loss 52.6124
15:24:02.397531 [0] Epoch 00062 | Loss 52.8886
15:24:02.414215 [0] Epoch 00063 | Loss 53.1671
15:24:02.432989 [0] Epoch 00064 | Loss 53.4447
15:24:02.450295 [0] Epoch 00065 | Loss 53.7137
15:24:02.466302 [0] Epoch 00066 | Loss 53.9832
15:24:02.482380 [0] Epoch 00067 | Loss 54.2604
15:24:02.499138 [0] Epoch 00068 | Loss 54.5423
15:24:02.515604 [0] Epoch 00069 | Loss 54.8143
15:24:02.532894 [0] Epoch 00070 | Loss 55.0911
15:24:02.534389 [0] Epoch: 070, Train: 0.2901, Val: 0.3149, Test: 0.2849
15:24:02.552259 [0] Epoch 00071 | Loss 55.3730
15:24:02.568930 [0] Epoch 00072 | Loss 55.6464
15:24:02.585548 [0] Epoch 00073 | Loss 55.9143
15:24:02.602947 [0] Epoch 00074 | Loss 56.1766
15:24:02.620575 [0] Epoch 00075 | Loss 56.4298
15:24:02.637264 [0] Epoch 00076 | Loss 56.6755
15:24:02.653769 [0] Epoch 00077 | Loss 56.9167
15:24:02.670737 [0] Epoch 00078 | Loss 57.1509
15:24:02.687943 [0] Epoch 00079 | Loss 57.3793
15:24:02.705608 [0] Epoch 00080 | Loss 57.6071
15:24:02.707181 [0] Epoch: 080, Train: 0.3077, Val: 0.3277, Test: 0.2959
15:24:02.730774 [0] Epoch 00081 | Loss 57.8377
15:24:02.747155 [0] Epoch 00082 | Loss 58.0711
15:24:02.764415 [0] Epoch 00083 | Loss 58.3053
15:24:02.781926 [0] Epoch 00084 | Loss 58.5406
15:24:02.799184 [0] Epoch 00085 | Loss 58.7752
15:24:02.816106 [0] Epoch 00086 | Loss 59.0011
15:24:02.840829 [0] Epoch 00087 | Loss 59.2175
15:24:02.858334 [0] Epoch 00088 | Loss 59.4345
15:24:02.875463 [0] Epoch 00089 | Loss 59.6599
15:24:02.893588 [0] Epoch 00090 | Loss 59.8942
15:24:02.895180 [0] Epoch: 090, Train: 0.3241, Val: 0.3419, Test: 0.3104
15:24:02.911610 [0] Epoch 00091 | Loss 60.1378
15:24:02.928379 [0] Epoch 00092 | Loss 60.3910
15:24:02.945387 [0] Epoch 00093 | Loss 60.6511
15:24:02.963127 [0] Epoch 00094 | Loss 60.9144
15:24:02.980686 [0] Epoch 00095 | Loss 61.1816
15:24:02.998469 [0] Epoch 00096 | Loss 61.4524
15:24:03.015262 [0] Epoch 00097 | Loss 61.7293
15:24:03.033121 [0] Epoch 00098 | Loss 62.0172
15:24:03.052143 [0] Epoch 00099 | Loss 62.3174
15:24:03.069326 [0] Epoch 00100 | Loss 62.6275
15:24:03.071031 [0] Epoch: 100, Train: 0.3399, Val: 0.3552, Test: 0.3273
15:24:03.088045 [0] Epoch 00101 | Loss 62.9451
15:24:03.105231 [0] Epoch 00102 | Loss 63.2682
15:24:03.131064 [0] Epoch 00103 | Loss 63.5957
15:24:03.148233 [0] Epoch 00104 | Loss 63.9256
15:24:03.165788 [0] Epoch 00105 | Loss 64.2557
15:24:03.184696 [0] Epoch 00106 | Loss 64.5858
15:24:03.201842 [0] Epoch 00107 | Loss 64.9193
15:24:03.220005 [0] Epoch 00108 | Loss 65.2580
15:24:03.237588 [0] Epoch 00109 | Loss 65.6017
15:24:03.254628 [0] Epoch 00110 | Loss 65.9529
15:24:03.256698 [0] Epoch: 110, Train: 0.3541, Val: 0.3674, Test: 0.3427
15:24:03.275743 [0] Epoch 00111 | Loss 66.3117
15:24:03.292863 [0] Epoch 00112 | Loss 66.6745
15:24:03.309953 [0] Epoch 00113 | Loss 67.0396
15:24:03.335675 [0] Epoch 00114 | Loss 67.4087
15:24:03.352751 [0] Epoch 00115 | Loss 67.7831
15:24:03.371550 [0] Epoch 00116 | Loss 68.1618
15:24:03.390791 [0] Epoch 00117 | Loss 68.5414
15:24:03.409787 [0] Epoch 00118 | Loss 68.9188
15:24:03.430261 [0] Epoch 00119 | Loss 69.2966
15:24:03.451853 [0] Epoch 00120 | Loss 69.6794
15:24:03.454340 [0] Epoch: 120, Train: 0.3658, Val: 0.3797, Test: 0.3575
15:24:03.471696 [0] Epoch 00121 | Loss 70.0689
15:24:03.487768 [0] Epoch 00122 | Loss 70.4643
15:24:03.505466 [0] Epoch 00123 | Loss 70.8615
15:24:03.522004 [0] Epoch 00124 | Loss 71.2561
15:24:03.545302 [0] Epoch 00125 | Loss 71.6470
15:24:03.562038 [0] Epoch 00126 | Loss 72.0361
15:24:03.578995 [0] Epoch 00127 | Loss 72.4270
15:24:03.595337 [0] Epoch 00128 | Loss 72.8220
15:24:03.612165 [0] Epoch 00129 | Loss 73.2207
15:24:03.632450 [0] Epoch 00130 | Loss 73.6249
15:24:03.634705 [0] Epoch: 130, Train: 0.3759, Val: 0.3902, Test: 0.3710
15:24:03.651457 [0] Epoch 00131 | Loss 74.0358
15:24:03.668247 [0] Epoch 00132 | Loss 74.4513
15:24:03.686271 [0] Epoch 00133 | Loss 74.8684
15:24:03.703364 [0] Epoch 00134 | Loss 75.2857
15:24:03.719843 [0] Epoch 00135 | Loss 75.7024
15:24:03.736386 [0] Epoch 00136 | Loss 76.1192
15:24:03.753378 [0] Epoch 00137 | Loss 76.5376
15:24:03.771757 [0] Epoch 00138 | Loss 76.9583
15:24:03.792007 [0] Epoch 00139 | Loss 77.3802
15:24:03.812444 [0] Epoch 00140 | Loss 77.8048
15:24:03.815024 [0] Epoch: 140, Train: 0.3854, Val: 0.4008, Test: 0.3842
15:24:03.834553 [0] Epoch 00141 | Loss 78.2329
15:24:03.853343 [0] Epoch 00142 | Loss 78.6661
15:24:03.871180 [0] Epoch 00143 | Loss 79.1045
15:24:03.889408 [0] Epoch 00144 | Loss 79.5450
15:24:03.913355 [0] Epoch 00145 | Loss 79.9873
15:24:03.933359 [0] Epoch 00146 | Loss 80.4340
15:24:03.954327 [0] Epoch 00147 | Loss 80.8849
15:24:03.972109 [0] Epoch 00148 | Loss 81.3391
15:24:03.990429 [0] Epoch 00149 | Loss 81.7947
15:24:04.012020 [0] Epoch 00150 | Loss 82.2518
15:24:04.014665 [0] Epoch: 150, Train: 0.3934, Val: 0.4120, Test: 0.3973
15:24:04.034976 [0] Epoch 00151 | Loss 82.7118
15:24:04.054942 [0] Epoch 00152 | Loss 83.1748
15:24:04.076822 [0] Epoch 00153 | Loss 83.6431
15:24:04.097022 [0] Epoch 00154 | Loss 84.1190
15:24:04.119729 [0] Epoch 00155 | Loss 84.6024
15:24:04.138475 [0] Epoch 00156 | Loss 85.0908
15:24:04.160061 [0] Epoch 00157 | Loss 85.5804
15:24:04.180880 [0] Epoch 00158 | Loss 86.0697
15:24:04.201691 [0] Epoch 00159 | Loss 86.5569
15:24:04.224335 [0] Epoch 00160 | Loss 87.0416
15:24:04.226438 [0] Epoch: 160, Train: 0.4012, Val: 0.4218, Test: 0.4104
15:24:04.245086 [0] Epoch 00161 | Loss 87.5254
15:24:04.265538 [0] Epoch 00162 | Loss 88.0120
15:24:04.284468 [0] Epoch 00163 | Loss 88.5044
15:24:04.301643 [0] Epoch 00164 | Loss 89.0015
15:24:04.331297 [0] Epoch 00165 | Loss 89.5012
15:24:04.353016 [0] Epoch 00166 | Loss 90.0028
15:24:04.370713 [0] Epoch 00167 | Loss 90.5055
15:24:04.389461 [0] Epoch 00168 | Loss 91.0084
15:24:04.408293 [0] Epoch 00169 | Loss 91.5096
15:24:04.428753 [0] Epoch 00170 | Loss 92.0090
15:24:04.431686 [0] Epoch: 170, Train: 0.4083, Val: 0.4329, Test: 0.4243
15:24:04.450691 [0] Epoch 00171 | Loss 92.5104
15:24:04.470149 [0] Epoch 00172 | Loss 93.0167
15:24:04.489968 [0] Epoch 00173 | Loss 93.5271
15:24:04.510898 [0] Epoch 00174 | Loss 94.0386
15:24:04.530068 [0] Epoch 00175 | Loss 94.5506
15:24:04.549182 [0] Epoch 00176 | Loss 95.0643
15:24:04.572352 [0] Epoch 00177 | Loss 95.5805
15:24:04.592459 [0] Epoch 00178 | Loss 96.0962
15:24:04.612554 [0] Epoch 00179 | Loss 96.6115
15:24:04.637011 [0] Epoch 00180 | Loss 97.1313
15:24:04.639113 [0] Epoch: 180, Train: 0.4154, Val: 0.4440, Test: 0.4358
15:24:04.658171 [0] Epoch 00181 | Loss 97.6549
15:24:04.677733 [0] Epoch 00182 | Loss 98.1792
15:24:04.695924 [0] Epoch 00183 | Loss 98.7026
15:24:04.716664 [0] Epoch 00184 | Loss 99.2250
15:24:04.737017 [0] Epoch 00185 | Loss 99.7470
15:24:04.758859 [0] Epoch 00186 | Loss 100.2693
15:24:04.778191 [0] Epoch 00187 | Loss 100.7946
15:24:04.797419 [0] Epoch 00188 | Loss 101.3251
15:24:04.819739 [0] Epoch 00189 | Loss 101.8608
15:24:04.838436 [0] Epoch 00190 | Loss 102.3991
15:24:04.839888 [0] Epoch: 190, Train: 0.4221, Val: 0.4553, Test: 0.4474
15:24:04.859380 [0] Epoch 00191 | Loss 102.9380
15:24:04.879863 [0] Epoch 00192 | Loss 103.4766
15:24:04.900671 [0] Epoch 00193 | Loss 104.0173
15:24:04.920749 [0] Epoch 00194 | Loss 104.5630
15:24:04.940620 [0] Epoch 00195 | Loss 105.1142
15:24:04.960743 [0] Epoch 00196 | Loss 105.6709
15:24:04.980570 [0] Epoch 00197 | Loss 106.2337
15:24:05.000679 [0] Epoch 00198 | Loss 106.8023
15:24:05.021320 [0] Epoch 00199 | Loss 107.3758
15:24:05.023638 [0] Epoch: 199, Train: 0.4272, Val: 0.4619, Test: 0.4567
15:24:05.025145 [0] 
timer summary:
  1.43s   1.43s   800 mm
  0.19s   0.19s  1200 broadcast
  1.36s   1.36s  1200 spmm
  0.07s   0.07s   400 all_reduce
  4.74s   4.74s   200 epoch
  6.56s   6.56s     1 total
15:24:14.116801 [0] proc begin: <DistEnv 0/1 nccl>
15:24:15.874308 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
15:24:15.875134 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

15:24:16.872706 [0] Epoch 00000 | Loss 3.6792
15:24:16.875695 [0] Epoch: 000, Train: 0.0259, Val: 0.0271, Test: 0.0269
15:24:16.891667 [0] Epoch 00001 | Loss 3.4872
15:24:16.906861 [0] Epoch 00002 | Loss 3.4445
15:24:16.920927 [0] Epoch 00003 | Loss 3.5919
15:24:16.938103 [0] Epoch 00004 | Loss 3.9270
15:24:16.954879 [0] Epoch 00005 | Loss 4.4153
15:24:16.968993 [0] Epoch 00006 | Loss 5.0240
15:24:16.985137 [0] Epoch 00007 | Loss 5.7201
15:24:16.999588 [0] Epoch 00008 | Loss 6.4735
15:24:17.015576 [0] Epoch 00009 | Loss 7.2819
15:24:17.030375 [0] Epoch 00010 | Loss 8.1136
15:24:17.032782 [0] Epoch: 010, Train: 0.2481, Val: 0.2887, Test: 0.2587
15:24:17.049698 [0] Epoch 00011 | Loss 8.8971
15:24:17.066869 [0] Epoch 00012 | Loss 9.6118
15:24:17.081362 [0] Epoch 00013 | Loss 10.2092
15:24:17.097355 [0] Epoch 00014 | Loss 10.6998
15:24:17.111644 [0] Epoch 00015 | Loss 11.0928
15:24:17.128961 [0] Epoch 00016 | Loss 11.4191
15:24:17.143334 [0] Epoch 00017 | Loss 11.6880
15:24:17.159158 [0] Epoch 00018 | Loss 11.8933
15:24:17.181240 [0] Epoch 00019 | Loss 12.0301
15:24:17.197163 [0] Epoch 00020 | Loss 12.1161
15:24:17.199012 [0] Epoch: 020, Train: 0.2688, Val: 0.2972, Test: 0.2668
15:24:17.215842 [0] Epoch 00021 | Loss 12.1920
15:24:17.233821 [0] Epoch 00022 | Loss 12.2800
15:24:17.249931 [0] Epoch 00023 | Loss 12.3868
15:24:17.264883 [0] Epoch 00024 | Loss 12.4894
15:24:17.280695 [0] Epoch 00025 | Loss 12.5821
15:24:17.295818 [0] Epoch 00026 | Loss 12.6727
15:24:17.312204 [0] Epoch 00027 | Loss 12.7716
15:24:17.327436 [0] Epoch 00028 | Loss 12.8526
15:24:17.344725 [0] Epoch 00029 | Loss 12.8811
15:24:17.359798 [0] Epoch 00030 | Loss 12.8576
15:24:17.368277 [0] Epoch: 030, Train: 0.2803, Val: 0.3073, Test: 0.2787
15:24:17.384679 [0] Epoch 00031 | Loss 12.8037
15:24:17.400617 [0] Epoch 00032 | Loss 12.7330
15:24:17.415783 [0] Epoch 00033 | Loss 12.6484
15:24:17.432158 [0] Epoch 00034 | Loss 12.5501
15:24:17.447385 [0] Epoch 00035 | Loss 12.4418
15:24:17.464468 [0] Epoch 00036 | Loss 12.3345
15:24:17.479601 [0] Epoch 00037 | Loss 12.2503
15:24:17.497972 [0] Epoch 00038 | Loss 12.2040
15:24:17.512958 [0] Epoch 00039 | Loss 12.1924
15:24:17.529085 [0] Epoch 00040 | Loss 12.2049
15:24:17.531619 [0] Epoch: 040, Train: 0.3106, Val: 0.3274, Test: 0.3023
15:24:17.549767 [0] Epoch 00041 | Loss 12.2345
15:24:17.567237 [0] Epoch 00042 | Loss 12.2747
15:24:17.583995 [0] Epoch 00043 | Loss 12.3158
15:24:17.599912 [0] Epoch 00044 | Loss 12.3515
15:24:17.614977 [0] Epoch 00045 | Loss 12.3823
15:24:17.631733 [0] Epoch 00046 | Loss 12.4109
15:24:17.646879 [0] Epoch 00047 | Loss 12.4343
15:24:17.663538 [0] Epoch 00048 | Loss 12.4461
15:24:17.679979 [0] Epoch 00049 | Loss 12.4445
15:24:17.695079 [0] Epoch 00050 | Loss 12.4343
15:24:17.697627 [0] Epoch: 050, Train: 0.3485, Val: 0.3594, Test: 0.3366
15:24:17.714985 [0] Epoch 00051 | Loss 12.4256
15:24:17.732676 [0] Epoch 00052 | Loss 12.4313
15:24:17.749845 [0] Epoch 00053 | Loss 12.4605
15:24:17.764982 [0] Epoch 00054 | Loss 12.5126
15:24:17.780439 [0] Epoch 00055 | Loss 12.5801
15:24:17.797653 [0] Epoch 00056 | Loss 12.6534
15:24:17.812750 [0] Epoch 00057 | Loss 12.7209
15:24:17.828395 [0] Epoch 00058 | Loss 12.7732
15:24:17.844178 [0] Epoch 00059 | Loss 12.8101
15:24:17.859274 [0] Epoch 00060 | Loss 12.8427
15:24:17.861820 [0] Epoch: 060, Train: 0.3787, Val: 0.3911, Test: 0.3746
15:24:17.879974 [0] Epoch 00061 | Loss 12.8849
15:24:17.897179 [0] Epoch 00062 | Loss 12.9446
15:24:17.912311 [0] Epoch 00063 | Loss 13.0240
15:24:17.928300 [0] Epoch 00064 | Loss 13.1207
15:24:17.942987 [0] Epoch 00065 | Loss 13.2280
15:24:17.960378 [0] Epoch 00066 | Loss 13.3377
15:24:17.975542 [0] Epoch 00067 | Loss 13.4445
15:24:17.992362 [0] Epoch 00068 | Loss 13.5485
15:24:18.008875 [0] Epoch 00069 | Loss 13.6508
15:24:18.023761 [0] Epoch 00070 | Loss 13.7514
15:24:18.025920 [0] Epoch: 070, Train: 0.4008, Val: 0.4166, Test: 0.4078
15:24:18.043112 [0] Epoch 00071 | Loss 13.8498
15:24:18.060684 [0] Epoch 00072 | Loss 13.9483
15:24:18.075873 [0] Epoch 00073 | Loss 14.0497
15:24:18.092221 [0] Epoch 00074 | Loss 14.1561
15:24:18.107472 [0] Epoch 00075 | Loss 14.2684
15:24:18.124163 [0] Epoch 00076 | Loss 14.3863
15:24:18.139401 [0] Epoch 00077 | Loss 14.5075
15:24:18.156298 [0] Epoch 00078 | Loss 14.6283
15:24:18.171497 [0] Epoch 00079 | Loss 14.7461
15:24:18.187977 [0] Epoch 00080 | Loss 14.8605
15:24:18.190400 [0] Epoch: 080, Train: 0.4172, Val: 0.4410, Test: 0.4380
15:24:18.208111 [0] Epoch 00081 | Loss 14.9731
15:24:18.225379 [0] Epoch 00082 | Loss 15.0856
15:24:18.242562 [0] Epoch 00083 | Loss 15.2001
15:24:18.259181 [0] Epoch 00084 | Loss 15.3189
15:24:18.274269 [0] Epoch 00085 | Loss 15.4417
15:24:18.291154 [0] Epoch 00086 | Loss 15.5668
15:24:18.305577 [0] Epoch 00087 | Loss 15.6930
15:24:18.321322 [0] Epoch 00088 | Loss 15.8214
15:24:18.336520 [0] Epoch 00089 | Loss 15.9530
15:24:18.352457 [0] Epoch 00090 | Loss 16.0876
15:24:18.353885 [0] Epoch: 090, Train: 0.4339, Val: 0.4647, Test: 0.4666
15:24:18.368651 [0] Epoch 00091 | Loss 16.2245
15:24:18.384552 [0] Epoch 00092 | Loss 16.3624
15:24:18.399546 [0] Epoch 00093 | Loss 16.4994
15:24:18.415846 [0] Epoch 00094 | Loss 16.6344
15:24:18.430807 [0] Epoch 00095 | Loss 16.7681
15:24:18.447330 [0] Epoch 00096 | Loss 16.9036
15:24:18.463664 [0] Epoch 00097 | Loss 17.0436
15:24:18.478765 [0] Epoch 00098 | Loss 17.1890
15:24:18.495503 [0] Epoch 00099 | Loss 17.3387
15:24:18.511770 [0] Epoch 00100 | Loss 17.4913
15:24:18.513210 [0] Epoch: 100, Train: 0.4479, Val: 0.4847, Test: 0.4898
15:24:18.528159 [0] Epoch 00101 | Loss 17.6452
15:24:18.549090 [0] Epoch 00102 | Loss 17.7983
15:24:18.566586 [0] Epoch 00103 | Loss 17.9513
15:24:18.584669 [0] Epoch 00104 | Loss 18.1056
15:24:18.601526 [0] Epoch 00105 | Loss 18.2620
15:24:18.616432 [0] Epoch 00106 | Loss 18.4215
15:24:18.632599 [0] Epoch 00107 | Loss 18.5845
15:24:18.647714 [0] Epoch 00108 | Loss 18.7504
15:24:18.664269 [0] Epoch 00109 | Loss 18.9176
15:24:18.679370 [0] Epoch 00110 | Loss 19.0847
15:24:18.681965 [0] Epoch: 110, Train: 0.4601, Val: 0.5018, Test: 0.5095
15:24:18.699188 [0] Epoch 00111 | Loss 19.2509
15:24:18.715646 [0] Epoch 00112 | Loss 19.4157
15:24:18.732615 [0] Epoch 00113 | Loss 19.5798
15:24:18.747363 [0] Epoch 00114 | Loss 19.7450
15:24:18.763888 [0] Epoch 00115 | Loss 19.9131
15:24:18.778976 [0] Epoch 00116 | Loss 20.0849
15:24:18.796566 [0] Epoch 00117 | Loss 20.2598
15:24:18.813354 [0] Epoch 00118 | Loss 20.4368
15:24:18.828322 [0] Epoch 00119 | Loss 20.6143
15:24:18.844251 [0] Epoch 00120 | Loss 20.7912
15:24:18.845703 [0] Epoch: 120, Train: 0.4696, Val: 0.5148, Test: 0.5226
15:24:18.860779 [0] Epoch 00121 | Loss 20.9672
15:24:18.876581 [0] Epoch 00122 | Loss 21.1427
15:24:18.893292 [0] Epoch 00123 | Loss 21.3189
15:24:18.908617 [0] Epoch 00124 | Loss 21.4970
15:24:18.930522 [0] Epoch 00125 | Loss 21.6782
15:24:18.946423 [0] Epoch 00126 | Loss 21.8625
15:24:18.962265 [0] Epoch 00127 | Loss 22.0492
15:24:18.977016 [0] Epoch 00128 | Loss 22.2376
15:24:18.994725 [0] Epoch 00129 | Loss 22.4276
15:24:19.010698 [0] Epoch 00130 | Loss 22.6196
15:24:19.012306 [0] Epoch: 130, Train: 0.4778, Val: 0.5234, Test: 0.5331
15:24:19.027829 [0] Epoch 00131 | Loss 22.8116
15:24:19.044915 [0] Epoch 00132 | Loss 23.0025
15:24:19.061298 [0] Epoch 00133 | Loss 23.1921
15:24:19.076255 [0] Epoch 00134 | Loss 23.3818
15:24:19.095653 [0] Epoch 00135 | Loss 23.5731
15:24:19.112739 [0] Epoch 00136 | Loss 23.7671
15:24:19.133743 [0] Epoch 00137 | Loss 23.9629
15:24:19.151188 [0] Epoch 00138 | Loss 24.1593
15:24:19.169001 [0] Epoch 00139 | Loss 24.3558
15:24:19.184522 [0] Epoch 00140 | Loss 24.5521
15:24:19.186762 [0] Epoch: 140, Train: 0.4845, Val: 0.5317, Test: 0.5419
15:24:19.203256 [0] Epoch 00141 | Loss 24.7488
15:24:19.220247 [0] Epoch 00142 | Loss 24.9468
15:24:19.237874 [0] Epoch 00143 | Loss 25.1464
15:24:19.254812 [0] Epoch 00144 | Loss 25.3472
15:24:19.270358 [0] Epoch 00145 | Loss 25.5495
15:24:19.289491 [0] Epoch 00146 | Loss 25.7534
15:24:19.306674 [0] Epoch 00147 | Loss 25.9583
15:24:19.324178 [0] Epoch 00148 | Loss 26.1630
15:24:19.341165 [0] Epoch 00149 | Loss 26.3673
15:24:19.357629 [0] Epoch 00150 | Loss 26.5714
15:24:19.359118 [0] Epoch: 150, Train: 0.4903, Val: 0.5381, Test: 0.5489
15:24:19.374346 [0] Epoch 00151 | Loss 26.7766
15:24:19.390104 [0] Epoch 00152 | Loss 26.9832
15:24:19.406351 [0] Epoch 00153 | Loss 27.1914
15:24:19.422017 [0] Epoch 00154 | Loss 27.4006
15:24:19.437787 [0] Epoch 00155 | Loss 27.6107
15:24:19.455139 [0] Epoch 00156 | Loss 27.8217
15:24:19.475452 [0] Epoch 00157 | Loss 28.0341
15:24:19.492241 [0] Epoch 00158 | Loss 28.2485
15:24:19.509810 [0] Epoch 00159 | Loss 28.4641
15:24:19.525330 [0] Epoch 00160 | Loss 28.6810
15:24:19.527166 [0] Epoch: 160, Train: 0.4952, Val: 0.5443, Test: 0.5542
15:24:19.542884 [0] Epoch 00161 | Loss 28.8995
15:24:19.561095 [0] Epoch 00162 | Loss 29.1188
15:24:19.579702 [0] Epoch 00163 | Loss 29.3383
15:24:19.597033 [0] Epoch 00164 | Loss 29.5583
15:24:19.613845 [0] Epoch 00165 | Loss 29.7790
15:24:19.629075 [0] Epoch 00166 | Loss 30.0010
15:24:19.652310 [0] Epoch 00167 | Loss 30.2251
15:24:19.668669 [0] Epoch 00168 | Loss 30.4510
15:24:19.684180 [0] Epoch 00169 | Loss 30.6784
15:24:19.700729 [0] Epoch 00170 | Loss 30.9065
15:24:19.702180 [0] Epoch: 170, Train: 0.4997, Val: 0.5499, Test: 0.5596
15:24:19.717274 [0] Epoch 00171 | Loss 31.1356
15:24:19.732908 [0] Epoch 00172 | Loss 31.3656
15:24:19.748417 [0] Epoch 00173 | Loss 31.5969
15:24:19.766379 [0] Epoch 00174 | Loss 31.8293
15:24:19.783137 [0] Epoch 00175 | Loss 32.0622
15:24:19.798798 [0] Epoch 00176 | Loss 32.2959
15:24:19.815540 [0] Epoch 00177 | Loss 32.5309
15:24:19.830915 [0] Epoch 00178 | Loss 32.7673
15:24:19.847954 [0] Epoch 00179 | Loss 33.0049
15:24:19.865201 [0] Epoch 00180 | Loss 33.2437
15:24:19.866614 [0] Epoch: 180, Train: 0.5043, Val: 0.5541, Test: 0.5638
15:24:19.881660 [0] Epoch 00181 | Loss 33.4839
15:24:19.897368 [0] Epoch 00182 | Loss 33.7257
15:24:19.914726 [0] Epoch 00183 | Loss 33.9687
15:24:19.932782 [0] Epoch 00184 | Loss 34.2120
15:24:19.949650 [0] Epoch 00185 | Loss 34.4545
15:24:19.967424 [0] Epoch 00186 | Loss 34.6964
15:24:19.983082 [0] Epoch 00187 | Loss 34.9392
15:24:20.001603 [0] Epoch 00188 | Loss 35.1842
15:24:20.017197 [0] Epoch 00189 | Loss 35.4314
15:24:20.032787 [0] Epoch 00190 | Loss 35.6800
15:24:20.035103 [0] Epoch: 190, Train: 0.5082, Val: 0.5574, Test: 0.5671
15:24:20.061351 [0] Epoch 00191 | Loss 35.9293
15:24:20.078691 [0] Epoch 00192 | Loss 36.1785
15:24:20.097075 [0] Epoch 00193 | Loss 36.4272
15:24:20.112298 [0] Epoch 00194 | Loss 36.6757
15:24:20.132108 [0] Epoch 00195 | Loss 36.9248
15:24:20.148534 [0] Epoch 00196 | Loss 37.1750
15:24:20.172018 [0] Epoch 00197 | Loss 37.4269
15:24:20.187501 [0] Epoch 00198 | Loss 37.6801
15:24:20.204679 [0] Epoch 00199 | Loss 37.9343
15:24:20.206109 [0] Epoch: 199, Train: 0.5115, Val: 0.5598, Test: 0.5705
15:24:20.206782 [0] 
timer summary:
  1.41s   1.41s   800 mm
  0.19s   0.19s   800 broadcast
  1.01s   1.01s   800 spmm
  0.07s   0.07s   400 all_reduce
  4.28s   4.28s   200 epoch
  6.09s   6.09s     1 total
10:17:31.949277 [0] proc begin: <DistEnv 0/1 nccl>
10:18:07.913085 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:18:07.921981 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:18:10.477660 [0] Epoch 00000 | Loss 3.7378
10:18:10.484364 [0] Epoch: 000, Train: 0.0188, Val: 0.0167, Test: 0.0176
10:18:10.605871 [0] Epoch 00001 | Loss 3.3019
10:18:10.729258 [0] Epoch 00002 | Loss 2.9654
10:18:10.850902 [0] Epoch 00003 | Loss 2.7097
10:18:10.972134 [0] Epoch 00004 | Loss 2.4923
10:18:11.094238 [0] Epoch 00005 | Loss 2.3055
10:18:11.216209 [0] Epoch 00006 | Loss 2.1307
10:18:11.337776 [0] Epoch 00007 | Loss 1.9692
10:18:11.459147 [0] Epoch 00008 | Loss 1.8326
10:18:11.580704 [0] Epoch 00009 | Loss 1.7122
10:18:11.702951 [0] Epoch 00010 | Loss 1.5938
10:18:11.704553 [0] Epoch: 010, Train: 0.7707, Val: 0.7886, Test: 0.7842
10:18:11.825919 [0] Epoch 00011 | Loss 1.4799
10:18:11.948022 [0] Epoch 00012 | Loss 1.3781
10:18:12.069584 [0] Epoch 00013 | Loss 1.2898
10:18:12.192214 [0] Epoch 00014 | Loss 1.2113
10:18:12.313089 [0] Epoch 00015 | Loss 1.1364
10:18:12.434162 [0] Epoch 00016 | Loss 1.0662
10:18:12.555250 [0] Epoch 00017 | Loss 1.0081
10:18:12.676406 [0] Epoch 00018 | Loss 0.9631
10:18:12.798086 [0] Epoch 00019 | Loss 0.9213
10:18:12.799503 [0] Epoch: 019, Train: 0.8398, Val: 0.8551, Test: 0.8523
10:18:12.799918 [0] 
timer summary:
  1.73s   1.73s    80 mm
  0.12s   0.12s    80 broadcast
  2.74s   2.74s    80 spmm
  0.00s   0.00s    40 all_reduce
  4.86s   4.86s    20 epoch
 40.85s  40.85s     1 total
10:33:01.812530 [0] proc begin: <DistEnv 0/1 nccl>
10:33:18.894709 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:33:18.895953 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

10:33:20.006804 [0] Epoch 00000 | Loss 3.7378
10:33:20.008328 [0] Epoch: 000, Train: 0.0188, Val: 0.0167, Test: 0.0176
10:33:20.128955 [0] Epoch 00001 | Loss 3.3019
10:33:20.250408 [0] Epoch 00002 | Loss 2.9654
10:33:20.371668 [0] Epoch 00003 | Loss 2.7097
10:33:20.493251 [0] Epoch 00004 | Loss 2.4923
10:33:20.614457 [0] Epoch 00005 | Loss 2.3055
10:33:20.739154 [0] Epoch 00006 | Loss 2.1307
10:33:20.864198 [0] Epoch 00007 | Loss 1.9692
10:33:20.986309 [0] Epoch 00008 | Loss 1.8326
10:33:21.107393 [0] Epoch 00009 | Loss 1.7122
10:33:21.228511 [0] Epoch 00010 | Loss 1.5938
10:33:21.230808 [0] Epoch: 010, Train: 0.7707, Val: 0.7886, Test: 0.7842
10:33:21.353615 [0] Epoch 00011 | Loss 1.4799
10:33:21.475903 [0] Epoch 00012 | Loss 1.3781
10:33:21.597440 [0] Epoch 00013 | Loss 1.2898
10:33:21.718869 [0] Epoch 00014 | Loss 1.2113
10:33:21.840670 [0] Epoch 00015 | Loss 1.1364
10:33:21.963088 [0] Epoch 00016 | Loss 1.0662
10:33:22.084584 [0] Epoch 00017 | Loss 1.0081
10:33:22.207517 [0] Epoch 00018 | Loss 0.9631
10:33:22.328829 [0] Epoch 00019 | Loss 0.9213
10:33:22.330255 [0] Epoch: 019, Train: 0.8398, Val: 0.8551, Test: 0.8523
10:33:22.330662 [0] 
timer summary:
  0.77s   0.77s    80 mm
  0.11s   0.11s    80 broadcast
  2.35s   2.35s    80 spmm
  0.00s   0.00s    40 all_reduce
  3.43s   3.43s    20 epoch
 20.52s  20.52s     1 total
10:37:30.086060 [0] proc begin: <DistEnv 0/1 nccl>
10:37:47.410880 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
10:37:47.412122 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:06:24.643642 [0] proc begin: <DistEnv 0/1 nccl>
11:07:16.540878 [0] proc begin: <DistEnv 0/1 nccl>
11:08:29.677022 [0] proc begin: <DistEnv 0/1 nccl>
11:08:32.698680 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:08:32.707029 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:08:36.170066 [0] Epoch 00000 | Loss 3.6878
11:08:36.176154 [0] Epoch: 000, Train: 0.0086, Val: 0.0063, Test: 0.0067
11:08:36.207922 [0] Epoch 00001 | Loss 3.6527
11:08:36.235932 [0] Epoch 00002 | Loss 3.6180
11:08:36.264661 [0] Epoch 00003 | Loss 3.7671
11:08:36.291276 [0] Epoch 00004 | Loss 4.4722
11:08:36.318799 [0] Epoch 00005 | Loss 6.1923
11:08:36.347421 [0] Epoch 00006 | Loss 9.5996
11:08:36.375042 [0] Epoch 00007 | Loss 15.7000
11:08:36.408421 [0] Epoch 00008 | Loss 25.7903
11:08:36.435618 [0] Epoch 00009 | Loss 39.0517
11:08:36.463046 [0] Epoch 00010 | Loss 55.6974
11:08:36.464956 [0] Epoch: 010, Train: 0.1099, Val: 0.2297, Test: 0.2156
11:08:36.493944 [0] Epoch 00011 | Loss 82.8298
11:08:36.522639 [0] Epoch 00012 | Loss 118.6138
11:08:36.551038 [0] Epoch 00013 | Loss 162.4656
11:08:36.578514 [0] Epoch 00014 | Loss 224.6757
11:08:36.606289 [0] Epoch 00015 | Loss 293.9180
11:08:36.634240 [0] Epoch 00016 | Loss 388.5236
11:08:36.660735 [0] Epoch 00017 | Loss 508.4139
11:08:36.687606 [0] Epoch 00018 | Loss 620.5201
11:08:36.715758 [0] Epoch 00019 | Loss 775.8401
11:08:36.717780 [0] Epoch: 019, Train: 0.1099, Val: 0.2297, Test: 0.2156
11:08:36.719666 [0] 
timer summary:
  0.11s   0.11s   200 broadcast
  1.15s   1.15s   200 spmm
  2.39s   2.39s   200 mm
  0.05s   0.05s   100 all_reduce
  3.99s   3.99s    20 epoch
  7.04s   7.04s     1 total
11:09:25.422690 [0] proc begin: <DistEnv 0/1 nccl>
11:09:41.816242 [0] proc begin: <DistEnv 0/1 nccl>
11:09:43.702388 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:09:43.704399 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:09:45.200298 [0] Epoch 00000 | Loss 3.6866
11:09:45.203306 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
11:09:45.219003 [0] Epoch 00001 | Loss 3.6129
11:09:45.232351 [0] Epoch 00002 | Loss 3.5437
11:09:45.244709 [0] Epoch 00003 | Loss 3.4895
11:09:45.259003 [0] Epoch 00004 | Loss 3.4681
11:09:45.271529 [0] Epoch 00005 | Loss 3.4919
11:09:45.285122 [0] Epoch 00006 | Loss 3.5667
11:09:45.297237 [0] Epoch 00007 | Loss 3.6936
11:09:45.309962 [0] Epoch 00008 | Loss 3.8712
11:09:45.322678 [0] Epoch 00009 | Loss 4.0949
11:09:45.335214 [0] Epoch 00010 | Loss 4.3578
11:09:45.336646 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
11:09:45.349191 [0] Epoch 00011 | Loss 4.6524
11:09:45.361952 [0] Epoch 00012 | Loss 4.9699
11:09:45.374790 [0] Epoch 00013 | Loss 5.2999
11:09:45.387402 [0] Epoch 00014 | Loss 5.6309
11:09:45.400204 [0] Epoch 00015 | Loss 5.9635
11:09:45.412948 [0] Epoch 00016 | Loss 6.3158
11:09:45.425847 [0] Epoch 00017 | Loss 6.7074
11:09:45.437828 [0] Epoch 00018 | Loss 7.1246
11:09:45.450895 [0] Epoch 00019 | Loss 7.5391
11:09:45.452276 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
11:09:45.452978 [0] 
timer summary:
  0.10s   0.10s    80 broadcast
  0.42s   0.42s    80 spmm
  1.06s   1.06s    80 mm
  0.03s   0.03s    40 all_reduce
  1.74s   1.74s    20 epoch
  3.64s   3.64s     1 total
11:10:27.607138 [0] proc begin: <DistEnv 0/1 nccl>
11:10:29.650845 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:10:29.651694 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:10:30.683062 [0] Epoch 00000 | Loss 3.6855
11:10:30.684756 [0] Epoch: 000, Train: 0.0151, Val: 0.0139, Test: 0.0146
11:10:30.697569 [0] Epoch 00001 | Loss 3.6119
11:10:30.709356 [0] Epoch 00002 | Loss 3.5413
11:10:30.721731 [0] Epoch 00003 | Loss 3.4868
11:10:30.733153 [0] Epoch 00004 | Loss 3.4680
11:10:30.745193 [0] Epoch 00005 | Loss 3.4961
11:10:30.757479 [0] Epoch 00006 | Loss 3.5754
11:10:30.768990 [0] Epoch 00007 | Loss 3.7081
11:10:30.781083 [0] Epoch 00008 | Loss 3.8934
11:10:30.793721 [0] Epoch 00009 | Loss 4.1267
11:10:30.805120 [0] Epoch 00010 | Loss 4.4012
11:10:30.806533 [0] Epoch: 010, Train: 0.1812, Val: 0.0801, Test: 0.0619
11:10:30.818523 [0] Epoch 00011 | Loss 4.7089
11:10:30.830468 [0] Epoch 00012 | Loss 5.0395
11:10:30.842319 [0] Epoch 00013 | Loss 5.3812
11:10:30.854101 [0] Epoch 00014 | Loss 5.7231
11:10:30.865628 [0] Epoch 00015 | Loss 6.0636
11:10:30.877885 [0] Epoch 00016 | Loss 6.4172
11:10:30.890168 [0] Epoch 00017 | Loss 6.8035
11:10:30.901982 [0] Epoch 00018 | Loss 7.2123
11:10:30.913920 [0] Epoch 00019 | Loss 7.6089
11:10:30.915251 [0] Epoch: 019, Train: 0.2091, Val: 0.2666, Test: 0.2391
11:10:30.915809 [0] 
timer summary:
  0.72s   0.72s    80 mm
  0.10s   0.10s    80 broadcast
  0.30s   0.30s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.26s   1.26s    20 epoch
  3.31s   3.31s     1 total
11:11:35.562006 [0] proc begin: <DistEnv 0/1 nccl>
11:12:10.688929 [0] proc begin: <DistEnv 0/1 nccl>
11:12:51.754435 [0] proc begin: <DistEnv 0/1 nccl>
11:12:53.178437 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:12:53.179570 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:12:54.458547 [0] Epoch 00000 | Loss 3.6855
11:12:54.461440 [0] Epoch: 000, Train: 0.0151, Val: 0.0139, Test: 0.0146
11:12:54.475703 [0] Epoch 00001 | Loss 3.6119
11:12:54.488733 [0] Epoch 00002 | Loss 3.5413
11:12:54.500289 [0] Epoch 00003 | Loss 3.4868
11:12:54.512329 [0] Epoch 00004 | Loss 3.4680
11:12:54.525571 [0] Epoch 00005 | Loss 3.4961
11:12:54.537187 [0] Epoch 00006 | Loss 3.5754
11:12:54.549170 [0] Epoch 00007 | Loss 3.7081
11:12:54.561598 [0] Epoch 00008 | Loss 3.8934
11:12:54.573392 [0] Epoch 00009 | Loss 4.1267
11:12:54.585341 [0] Epoch 00010 | Loss 4.4012
11:12:54.586780 [0] Epoch: 010, Train: 0.1812, Val: 0.0801, Test: 0.0619
11:12:54.598589 [0] Epoch 00011 | Loss 4.7089
11:12:54.610365 [0] Epoch 00012 | Loss 5.0395
11:12:54.627100 [0] Epoch 00013 | Loss 5.3812
11:12:54.639177 [0] Epoch 00014 | Loss 5.7231
11:12:54.650824 [0] Epoch 00015 | Loss 6.0636
11:12:54.662791 [0] Epoch 00016 | Loss 6.4172
11:12:54.674761 [0] Epoch 00017 | Loss 6.8035
11:12:54.686724 [0] Epoch 00018 | Loss 7.2123
11:12:54.698693 [0] Epoch 00019 | Loss 7.6089
11:12:54.700164 [0] Epoch: 019, Train: 0.2091, Val: 0.2666, Test: 0.2391
11:12:54.700788 [0] 
timer summary:
  0.90s   0.90s    80 mm
  0.10s   0.10s    80 broadcast
  0.38s   0.38s    80 spmm
  0.00s   0.00s    40 all_reduce
  1.51s   1.51s    20 epoch
  2.95s   2.95s     1 total
11:13:25.542043 [0] proc begin: <DistEnv 0/1 nccl>
11:13:26.999100 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:13:26.999944 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:13:28.016003 [0] Epoch 00000 | Loss 3.6866
11:13:28.017311 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
11:13:28.029855 [0] Epoch 00001 | Loss 3.6129
11:13:28.042384 [0] Epoch 00002 | Loss 3.5437
11:13:28.054870 [0] Epoch 00003 | Loss 3.4895
11:13:28.067223 [0] Epoch 00004 | Loss 3.4681
11:13:28.079557 [0] Epoch 00005 | Loss 3.4919
11:13:28.091784 [0] Epoch 00006 | Loss 3.5667
11:13:28.105622 [0] Epoch 00007 | Loss 3.6936
11:13:28.117559 [0] Epoch 00008 | Loss 3.8712
11:13:28.130293 [0] Epoch 00009 | Loss 4.0949
11:13:28.143054 [0] Epoch 00010 | Loss 4.3578
11:13:28.144420 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
11:13:28.156448 [0] Epoch 00011 | Loss 4.6524
11:13:28.168520 [0] Epoch 00012 | Loss 4.9699
11:13:28.181269 [0] Epoch 00013 | Loss 5.2999
11:13:28.193238 [0] Epoch 00014 | Loss 5.6309
11:13:28.205200 [0] Epoch 00015 | Loss 5.9635
11:13:28.217239 [0] Epoch 00016 | Loss 6.3158
11:13:28.229573 [0] Epoch 00017 | Loss 6.7074
11:13:28.241226 [0] Epoch 00018 | Loss 7.1246
11:13:28.253575 [0] Epoch 00019 | Loss 7.5391
11:13:28.255054 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
11:13:28.255754 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.29s   0.29s    80 spmm
  0.71s   0.71s    80 mm
  0.03s   0.03s    40 all_reduce
  1.25s   1.25s    20 epoch
  2.71s   2.71s     1 total
11:13:52.396064 [0] proc begin: <DistEnv 0/1 nccl>
11:13:54.306061 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:13:54.307040 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:13:55.355576 [0] Epoch 00000 | Loss 3.6866
11:13:55.356861 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
11:13:55.369613 [0] Epoch 00001 | Loss 3.6129
11:13:55.382218 [0] Epoch 00002 | Loss 3.5437
11:13:55.394882 [0] Epoch 00003 | Loss 3.4895
11:13:55.407438 [0] Epoch 00004 | Loss 3.4681
11:13:55.420196 [0] Epoch 00005 | Loss 3.4919
11:13:55.432954 [0] Epoch 00006 | Loss 3.5667
11:13:55.446232 [0] Epoch 00007 | Loss 3.6936
11:13:55.458633 [0] Epoch 00008 | Loss 3.8712
11:13:55.473243 [0] Epoch 00009 | Loss 4.0949
11:13:55.485601 [0] Epoch 00010 | Loss 4.3578
11:13:55.487008 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
11:13:55.499981 [0] Epoch 00011 | Loss 4.6524
11:13:55.512588 [0] Epoch 00012 | Loss 4.9699
11:13:55.525495 [0] Epoch 00013 | Loss 5.2999
11:13:55.538299 [0] Epoch 00014 | Loss 5.6309
11:13:55.550582 [0] Epoch 00015 | Loss 5.9635
11:13:55.563186 [0] Epoch 00016 | Loss 6.3158
11:13:55.583347 [0] Epoch 00017 | Loss 6.7074
11:13:55.603801 [0] Epoch 00018 | Loss 7.1246
11:13:55.617736 [0] Epoch 00019 | Loss 7.5391
11:13:55.618949 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
11:13:55.619490 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.32s   0.32s    80 spmm
  0.73s   0.73s    80 mm
  0.04s   0.04s    40 all_reduce
  1.31s   1.31s    20 epoch
  3.22s   3.22s     1 total
11:14:15.787502 [0] proc begin: <DistEnv 0/1 nccl>
11:14:17.315224 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:14:17.316297 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:14:18.346510 [0] Epoch 00000 | Loss 3.6866
11:14:18.348066 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
11:14:18.362621 [0] Epoch 00001 | Loss 3.6129
11:14:18.376459 [0] Epoch 00002 | Loss 3.5437
11:14:18.388817 [0] Epoch 00003 | Loss 3.4895
11:14:18.401378 [0] Epoch 00004 | Loss 3.4681
11:14:18.414193 [0] Epoch 00005 | Loss 3.4919
11:14:18.426599 [0] Epoch 00006 | Loss 3.5667
11:14:18.439101 [0] Epoch 00007 | Loss 3.6936
11:14:18.451653 [0] Epoch 00008 | Loss 3.8712
11:14:18.464413 [0] Epoch 00009 | Loss 4.0949
11:14:18.477959 [0] Epoch 00010 | Loss 4.3578
11:14:18.479152 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
11:14:18.491183 [0] Epoch 00011 | Loss 4.6524
11:14:18.504882 [0] Epoch 00012 | Loss 4.9699
11:14:18.517117 [0] Epoch 00013 | Loss 5.2999
11:14:18.530196 [0] Epoch 00014 | Loss 5.6309
11:14:18.542608 [0] Epoch 00015 | Loss 5.9635
11:14:18.554931 [0] Epoch 00016 | Loss 6.3158
11:14:18.566890 [0] Epoch 00017 | Loss 6.7074
11:14:18.580266 [0] Epoch 00018 | Loss 7.1246
11:14:18.592288 [0] Epoch 00019 | Loss 7.5391
11:14:18.593580 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
11:14:18.594252 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.28s   0.28s    80 spmm
  0.74s   0.74s    80 mm
  0.03s   0.03s    40 all_reduce
  1.27s   1.27s    20 epoch
  2.81s   2.81s     1 total
11:14:30.115613 [0] proc begin: <DistEnv 0/1 nccl>
11:14:31.951317 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
11:14:31.952766 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

11:14:33.577200 [0] Epoch 00000 | Loss 3.6866
11:14:33.578892 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
11:14:33.593744 [0] Epoch 00001 | Loss 3.6129
11:14:33.609376 [0] Epoch 00002 | Loss 3.5437
11:14:33.622487 [0] Epoch 00003 | Loss 3.4895
11:14:33.636302 [0] Epoch 00004 | Loss 3.4681
11:14:33.649369 [0] Epoch 00005 | Loss 3.4919
11:14:33.662779 [0] Epoch 00006 | Loss 3.5667
11:14:33.676139 [0] Epoch 00007 | Loss 3.6936
11:14:33.689621 [0] Epoch 00008 | Loss 3.8712
11:14:33.703613 [0] Epoch 00009 | Loss 4.0949
11:14:33.716606 [0] Epoch 00010 | Loss 4.3578
11:14:33.718025 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
11:14:33.731125 [0] Epoch 00011 | Loss 4.6524
11:14:33.743908 [0] Epoch 00012 | Loss 4.9699
11:14:33.756999 [0] Epoch 00013 | Loss 5.2999
11:14:33.770631 [0] Epoch 00014 | Loss 5.6309
11:14:33.783452 [0] Epoch 00015 | Loss 5.9635
11:14:33.796783 [0] Epoch 00016 | Loss 6.3158
11:14:33.811797 [0] Epoch 00017 | Loss 6.7074
11:14:33.827100 [0] Epoch 00018 | Loss 7.1246
11:14:33.840850 [0] Epoch 00019 | Loss 7.5391
11:14:33.842239 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
11:14:33.842901 [0] 
timer summary:
  0.10s   0.10s    80 broadcast
  0.42s   0.42s    80 spmm
  1.18s   1.18s    80 mm
  0.04s   0.04s    40 all_reduce
  1.88s   1.88s    20 epoch
  3.73s   3.73s     1 total
14:28:34.906532 [0] proc begin: <DistEnv 0/1 nccl>
14:28:36.334311 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
14:28:36.335313 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:28:37.621046 [0] Epoch 00000 | Loss 3.6866
14:28:37.622524 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
14:28:37.636126 [0] Epoch 00001 | Loss 3.6129
14:28:37.648364 [0] Epoch 00002 | Loss 3.5437
14:28:37.661385 [0] Epoch 00003 | Loss 3.4895
14:28:37.673911 [0] Epoch 00004 | Loss 3.4681
14:28:37.686583 [0] Epoch 00005 | Loss 3.4919
14:28:37.698825 [0] Epoch 00006 | Loss 3.5667
14:28:37.711284 [0] Epoch 00007 | Loss 3.6936
14:28:37.723734 [0] Epoch 00008 | Loss 3.8712
14:28:37.736052 [0] Epoch 00009 | Loss 4.0949
14:28:37.748514 [0] Epoch 00010 | Loss 4.3578
14:28:37.750392 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
14:28:37.763699 [0] Epoch 00011 | Loss 4.6524
14:28:37.775791 [0] Epoch 00012 | Loss 4.9699
14:28:37.788203 [0] Epoch 00013 | Loss 5.2999
14:28:37.803786 [0] Epoch 00014 | Loss 5.6309
14:28:37.816578 [0] Epoch 00015 | Loss 5.9635
14:28:37.830098 [0] Epoch 00016 | Loss 6.3158
14:28:37.842255 [0] Epoch 00017 | Loss 6.7074
14:28:37.855063 [0] Epoch 00018 | Loss 7.1246
14:28:37.867219 [0] Epoch 00019 | Loss 7.5391
14:28:37.868524 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
14:28:37.869068 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
  0.36s   0.36s    80 spmm
  0.91s   0.91s    80 mm
  0.04s   0.04s    40 all_reduce
  1.53s   1.53s    20 epoch
  2.96s   2.96s     1 total
14:34:04.628488 [0] proc begin: <DistEnv 0/1 nccl>
14:34:06.037188 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
14:34:06.038244 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:34:07.353004 [0] Epoch 00000 | Loss 3.6866
14:34:07.354474 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
14:34:07.367302 [0] Epoch 00001 | Loss 3.6129
14:34:07.379823 [0] Epoch 00002 | Loss 3.5437
14:34:07.392265 [0] Epoch 00003 | Loss 3.4895
14:34:07.404562 [0] Epoch 00004 | Loss 3.4681
14:34:07.417119 [0] Epoch 00005 | Loss 3.4919
14:34:07.429967 [0] Epoch 00006 | Loss 3.5667
14:34:07.442099 [0] Epoch 00007 | Loss 3.6936
14:34:07.454817 [0] Epoch 00008 | Loss 3.8712
14:34:07.467174 [0] Epoch 00009 | Loss 4.0949
14:34:07.479876 [0] Epoch 00010 | Loss 4.3578
14:34:07.481267 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
14:34:07.493678 [0] Epoch 00011 | Loss 4.6524
14:34:07.506337 [0] Epoch 00012 | Loss 4.9699
14:34:07.519062 [0] Epoch 00013 | Loss 5.2999
14:34:07.531488 [0] Epoch 00014 | Loss 5.6309
14:34:07.544130 [0] Epoch 00015 | Loss 5.9635
14:34:07.556532 [0] Epoch 00016 | Loss 6.3158
14:34:07.569099 [0] Epoch 00017 | Loss 6.7074
14:34:07.582302 [0] Epoch 00018 | Loss 7.1246
14:34:07.594558 [0] Epoch 00019 | Loss 7.5391
14:34:07.595990 [0] Epoch: 019, Train: 0.1279, Val: 0.2344, Test: 0.2186
14:34:07.596615 [0] 
timer summary:
  0.09s   0.09s    80 broadcast
  0.38s   0.38s    80 spmm
  0.91s   0.91s    80 mm
  0.04s   0.04s    40 all_reduce
  1.55s   1.55s    20 epoch
  2.97s   2.97s     1 total
14:35:08.843732 [0] proc begin: <DistEnv 0/1 nccl>
14:35:10.779881 [0] graph loaded <COO Graph: ogbn-arxiv, |V|: 169343, |E|: 1166243, masks: 90941,29799,48603><Local: 0, |V|: 169343, |E|: 1335586>
14:35:10.780927 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| Active memory         |   97587 KB |  171162 KB |  186466 KB |   88879 KB |
|       from large pool |   96429 KB |  170666 KB |  185134 KB |   88705 KB |
|       from small pool |    1158 KB |    1160 KB |    1332 KB |     174 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  194560 KB |  194560 KB |  194560 KB |       0 B  |
|       from large pool |  192512 KB |  192512 KB |  192512 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10956 KB |   23397 KB |   29653 KB |   18696 KB |
|       from large pool |   10066 KB |   21846 KB |   25879 KB |   15813 KB |
|       from small pool |     890 KB |    1882 KB |    3773 KB |    2883 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      28    |      20    |
|       from large pool |       4    |       5    |       8    |       4    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       4    |       8    |       5    |
|       from large pool |       2    |       3    |       6    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:35:12.444427 [0] Epoch 00000 | Loss 3.6866
14:35:12.445864 [0] Epoch: 000, Train: 0.0155, Val: 0.0135, Test: 0.0156
14:35:12.458521 [0] Epoch 00001 | Loss 3.6129
14:35:12.471790 [0] Epoch 00002 | Loss 3.5437
14:35:12.483996 [0] Epoch 00003 | Loss 3.4895
14:35:12.496588 [0] Epoch 00004 | Loss 3.4681
14:35:12.510956 [0] Epoch 00005 | Loss 3.4919
14:35:12.523485 [0] Epoch 00006 | Loss 3.5667
14:35:12.536797 [0] Epoch 00007 | Loss 3.6936
14:35:12.549028 [0] Epoch 00008 | Loss 3.8712
14:35:12.563386 [0] Epoch 00009 | Loss 4.0949
14:35:12.576309 [0] Epoch 00010 | Loss 4.3578
14:35:12.577682 [0] Epoch: 010, Train: 0.1795, Val: 0.0769, Test: 0.0589
14:35:12.590119 [0] Epoch 00011 | Loss 4.6524
14:35:12.602994 [0] Epoch 00012 | Loss 4.9699
14:35:12.614899 [0] Epoch 00013 | Loss 5.2999
14:35:12.626846 [0] Epoch 00014 | Loss 5.6309
14:35:12.639417 [0] Epoch 00015 | Loss 5.9635
14:35:12.652502 [0] Epoch 00016 | Loss 6.3158
14:35:12.664520 [0] Epoch 00017 | Loss 6.7074
14:35:12.864602 [0] Epoch 00018 | Loss 7.1246
14:35:12.879547 [0] Epoch 00019 | Loss 7.5391
14:35:12.891572 [0] Epoch 00020 | Loss 7.9312
14:35:12.892983 [0] Epoch: 020, Train: 0.1425, Val: 0.2402, Test: 0.2217
14:35:12.905618 [0] Epoch 00021 | Loss 8.2968
14:35:12.917824 [0] Epoch 00022 | Loss 8.6469
14:35:12.929975 [0] Epoch 00023 | Loss 8.9848
14:35:12.941983 [0] Epoch 00024 | Loss 9.2958
14:35:12.953981 [0] Epoch 00025 | Loss 9.5857
14:35:12.965960 [0] Epoch 00026 | Loss 9.8652
14:35:12.977779 [0] Epoch 00027 | Loss 10.1254
14:35:12.993283 [0] Epoch 00028 | Loss 10.3644
14:35:13.005058 [0] Epoch 00029 | Loss 10.5918
14:35:13.017280 [0] Epoch 00030 | Loss 10.8136
14:35:13.018759 [0] Epoch: 030, Train: 0.2587, Val: 0.2918, Test: 0.2611
14:35:13.030901 [0] Epoch 00031 | Loss 11.0218
14:35:13.042901 [0] Epoch 00032 | Loss 11.2109
14:35:13.054915 [0] Epoch 00033 | Loss 11.3841
14:35:13.066912 [0] Epoch 00034 | Loss 11.5363
14:35:13.078839 [0] Epoch 00035 | Loss 11.6597
14:35:13.092165 [0] Epoch 00036 | Loss 11.7608
14:35:13.104768 [0] Epoch 00037 | Loss 11.8514
14:35:13.116778 [0] Epoch 00038 | Loss 11.9310
14:35:13.129524 [0] Epoch 00039 | Loss 11.9993
14:35:13.141437 [0] Epoch 00040 | Loss 12.0574
14:35:13.142793 [0] Epoch: 040, Train: 0.2530, Val: 0.2889, Test: 0.2586
14:35:13.154853 [0] Epoch 00041 | Loss 12.1021
14:35:13.166662 [0] Epoch 00042 | Loss 12.1321
14:35:13.182178 [0] Epoch 00043 | Loss 12.1521
14:35:13.194648 [0] Epoch 00044 | Loss 12.1657
14:35:13.206637 [0] Epoch 00045 | Loss 12.1724
14:35:13.233583 [0] Epoch 00046 | Loss 12.1734
14:35:13.248513 [0] Epoch 00047 | Loss 12.1696
14:35:13.260282 [0] Epoch 00048 | Loss 12.1582
14:35:13.272265 [0] Epoch 00049 | Loss 12.1365
14:35:13.285078 [0] Epoch 00050 | Loss 12.1047
14:35:13.286372 [0] Epoch: 050, Train: 0.2654, Val: 0.2966, Test: 0.2663
14:35:13.301444 [0] Epoch 00051 | Loss 12.0641
14:35:13.313688 [0] Epoch 00052 | Loss 12.0164
14:35:13.325892 [0] Epoch 00053 | Loss 11.9664
14:35:13.337907 [0] Epoch 00054 | Loss 11.9190
14:35:13.350188 [0] Epoch 00055 | Loss 11.8758
14:35:13.365099 [0] Epoch 00056 | Loss 11.8349
14:35:13.377997 [0] Epoch 00057 | Loss 11.7945
14:35:13.389946 [0] Epoch 00058 | Loss 11.7519
14:35:13.402161 [0] Epoch 00059 | Loss 11.7060
14:35:13.415799 [0] Epoch 00060 | Loss 11.6595
14:35:13.418244 [0] Epoch: 060, Train: 0.2776, Val: 0.3073, Test: 0.2777
14:35:13.615915 [0] Epoch 00061 | Loss 11.6165
14:35:13.628312 [0] Epoch 00062 | Loss 11.5783
14:35:13.640564 [0] Epoch 00063 | Loss 11.5446
14:35:13.653390 [0] Epoch 00064 | Loss 11.5142
14:35:13.665346 [0] Epoch 00065 | Loss 11.4866
14:35:13.677408 [0] Epoch 00066 | Loss 11.4629
14:35:13.689170 [0] Epoch 00067 | Loss 11.4460
14:35:13.701464 [0] Epoch 00068 | Loss 11.4375
14:35:13.715001 [0] Epoch 00069 | Loss 11.4369
14:35:13.727121 [0] Epoch 00070 | Loss 11.4422
14:35:13.728479 [0] Epoch: 070, Train: 0.3014, Val: 0.3237, Test: 0.2967
14:35:13.740762 [0] Epoch 00071 | Loss 11.4510
14:35:13.752947 [0] Epoch 00072 | Loss 11.4611
14:35:13.765368 [0] Epoch 00073 | Loss 11.4722
14:35:13.777176 [0] Epoch 00074 | Loss 11.4849
14:35:13.789381 [0] Epoch 00075 | Loss 11.4998
14:35:13.803508 [0] Epoch 00076 | Loss 11.5173
14:35:13.815725 [0] Epoch 00077 | Loss 11.5376
14:35:13.828375 [0] Epoch 00078 | Loss 11.5606
14:35:13.843757 [0] Epoch 00079 | Loss 11.5862
14:35:13.856347 [0] Epoch 00080 | Loss 11.6146
14:35:13.857597 [0] Epoch: 080, Train: 0.3303, Val: 0.3461, Test: 0.3193
14:35:13.869712 [0] Epoch 00081 | Loss 11.6449
14:35:13.881972 [0] Epoch 00082 | Loss 11.6761
14:35:13.894320 [0] Epoch 00083 | Loss 11.7070
14:35:13.906332 [0] Epoch 00084 | Loss 11.7367
14:35:13.918674 [0] Epoch 00085 | Loss 11.7656
14:35:13.930663 [0] Epoch 00086 | Loss 11.7945
14:35:13.942734 [0] Epoch 00087 | Loss 11.8246
14:35:13.954863 [0] Epoch 00088 | Loss 11.8564
14:35:13.967151 [0] Epoch 00089 | Loss 11.8899
14:35:13.979252 [0] Epoch 00090 | Loss 11.9244
14:35:13.980609 [0] Epoch: 090, Train: 0.3494, Val: 0.3637, Test: 0.3403
14:35:13.992528 [0] Epoch 00091 | Loss 11.9591
14:35:14.004703 [0] Epoch 00092 | Loss 11.9938
14:35:14.017750 [0] Epoch 00093 | Loss 12.0291
14:35:14.029612 [0] Epoch 00094 | Loss 12.0659
14:35:14.041941 [0] Epoch 00095 | Loss 12.1045
14:35:14.054086 [0] Epoch 00096 | Loss 12.1447
14:35:14.066461 [0] Epoch 00097 | Loss 12.1858
14:35:14.078507 [0] Epoch 00098 | Loss 12.2271
14:35:14.090504 [0] Epoch 00099 | Loss 12.2688
14:35:14.102564 [0] Epoch 00100 | Loss 12.3110
14:35:14.103930 [0] Epoch: 100, Train: 0.3642, Val: 0.3785, Test: 0.3589
14:35:14.115932 [0] Epoch 00101 | Loss 12.3540
14:35:14.128381 [0] Epoch 00102 | Loss 12.3976
14:35:14.141576 [0] Epoch 00103 | Loss 12.4418
14:35:14.153425 [0] Epoch 00104 | Loss 12.4870
14:35:14.355214 [0] Epoch 00105 | Loss 12.5334
14:35:14.367694 [0] Epoch 00106 | Loss 12.5813
14:35:14.379804 [0] Epoch 00107 | Loss 12.6304
14:35:14.392964 [0] Epoch 00108 | Loss 12.6805
14:35:14.405079 [0] Epoch 00109 | Loss 12.7312
14:35:14.417297 [0] Epoch 00110 | Loss 12.7824
14:35:14.418771 [0] Epoch: 110, Train: 0.3774, Val: 0.3910, Test: 0.3761
14:35:14.431026 [0] Epoch 00111 | Loss 12.8340
14:35:14.442857 [0] Epoch 00112 | Loss 12.8859
14:35:14.455653 [0] Epoch 00113 | Loss 12.9383
14:35:14.467557 [0] Epoch 00114 | Loss 12.9913
14:35:14.480366 [0] Epoch 00115 | Loss 13.0451
14:35:14.492138 [0] Epoch 00116 | Loss 13.0995
14:35:14.504330 [0] Epoch 00117 | Loss 13.1543
14:35:14.517086 [0] Epoch 00118 | Loss 13.2097
14:35:14.528853 [0] Epoch 00119 | Loss 13.2658
14:35:14.541051 [0] Epoch 00120 | Loss 13.3227
14:35:14.542593 [0] Epoch: 120, Train: 0.3889, Val: 0.4051, Test: 0.3927
14:35:14.554971 [0] Epoch 00121 | Loss 13.3804
14:35:14.567248 [0] Epoch 00122 | Loss 13.4389
14:35:14.580285 [0] Epoch 00123 | Loss 13.4980
14:35:14.592152 [0] Epoch 00124 | Loss 13.5574
14:35:14.604564 [0] Epoch 00125 | Loss 13.6171
14:35:14.617435 [0] Epoch 00126 | Loss 13.6768
14:35:14.629357 [0] Epoch 00127 | Loss 13.7365
14:35:14.641482 [0] Epoch 00128 | Loss 13.7962
14:35:14.653548 [0] Epoch 00129 | Loss 13.8563
14:35:14.665867 [0] Epoch 00130 | Loss 13.9170
14:35:14.667304 [0] Epoch: 130, Train: 0.3996, Val: 0.4192, Test: 0.4098
14:35:14.679799 [0] Epoch 00131 | Loss 13.9781
14:35:14.691780 [0] Epoch 00132 | Loss 14.0398
14:35:14.704119 [0] Epoch 00133 | Loss 14.1018
14:35:14.716396 [0] Epoch 00134 | Loss 14.1641
14:35:14.730526 [0] Epoch 00135 | Loss 14.2268
14:35:14.747553 [0] Epoch 00136 | Loss 14.2895
14:35:14.761247 [0] Epoch 00137 | Loss 14.3521
14:35:14.773808 [0] Epoch 00138 | Loss 14.4143
14:35:14.785711 [0] Epoch 00139 | Loss 14.4760
14:35:14.797831 [0] Epoch 00140 | Loss 14.5372
14:35:14.799217 [0] Epoch: 140, Train: 0.4099, Val: 0.4320, Test: 0.4264
14:35:14.811225 [0] Epoch 00141 | Loss 14.5981
14:35:14.823772 [0] Epoch 00142 | Loss 14.6590
14:35:14.838518 [0] Epoch 00143 | Loss 14.7198
14:35:14.850656 [0] Epoch 00144 | Loss 14.7809
14:35:14.863269 [0] Epoch 00145 | Loss 14.8423
14:35:14.875624 [0] Epoch 00146 | Loss 14.9041
14:35:14.887642 [0] Epoch 00147 | Loss 14.9661
14:35:15.083570 [0] Epoch 00148 | Loss 15.0281
14:35:15.097957 [0] Epoch 00149 | Loss 15.0902
14:35:15.110196 [0] Epoch 00150 | Loss 15.1523
14:35:15.111590 [0] Epoch: 150, Train: 0.4182, Val: 0.4442, Test: 0.4410
14:35:15.124201 [0] Epoch 00151 | Loss 15.2148
14:35:15.136671 [0] Epoch 00152 | Loss 15.2775
14:35:15.149135 [0] Epoch 00153 | Loss 15.3405
14:35:15.161713 [0] Epoch 00154 | Loss 15.4035
14:35:15.175266 [0] Epoch 00155 | Loss 15.4664
14:35:15.193431 [0] Epoch 00156 | Loss 15.5293
14:35:15.206961 [0] Epoch 00157 | Loss 15.5921
14:35:15.219089 [0] Epoch 00158 | Loss 15.6549
14:35:15.231236 [0] Epoch 00159 | Loss 15.7180
14:35:15.243294 [0] Epoch 00160 | Loss 15.7815
14:35:15.244674 [0] Epoch: 160, Train: 0.4259, Val: 0.4551, Test: 0.4556
14:35:15.256935 [0] Epoch 00161 | Loss 15.8453
14:35:15.269144 [0] Epoch 00162 | Loss 15.9096
14:35:15.281532 [0] Epoch 00163 | Loss 15.9742
14:35:15.296786 [0] Epoch 00164 | Loss 16.0392
14:35:15.309284 [0] Epoch 00165 | Loss 16.1045
14:35:15.321932 [0] Epoch 00166 | Loss 16.1702
14:35:15.333881 [0] Epoch 00167 | Loss 16.2358
14:35:15.345881 [0] Epoch 00168 | Loss 16.3014
14:35:15.358475 [0] Epoch 00169 | Loss 16.3672
14:35:15.370500 [0] Epoch 00170 | Loss 16.4334
14:35:15.371842 [0] Epoch: 170, Train: 0.4321, Val: 0.4657, Test: 0.4680
14:35:15.383763 [0] Epoch 00171 | Loss 16.5001
14:35:15.396984 [0] Epoch 00172 | Loss 16.5672
14:35:15.408740 [0] Epoch 00173 | Loss 16.6349
14:35:15.420954 [0] Epoch 00174 | Loss 16.7033
14:35:15.433885 [0] Epoch 00175 | Loss 16.7722
14:35:15.445882 [0] Epoch 00176 | Loss 16.8414
14:35:15.457829 [0] Epoch 00177 | Loss 16.9107
14:35:15.469630 [0] Epoch 00178 | Loss 16.9802
14:35:15.481969 [0] Epoch 00179 | Loss 17.0498
14:35:15.494055 [0] Epoch 00180 | Loss 17.1196
14:35:15.495404 [0] Epoch: 180, Train: 0.4394, Val: 0.4768, Test: 0.4804
14:35:15.507431 [0] Epoch 00181 | Loss 17.1894
14:35:15.519348 [0] Epoch 00182 | Loss 17.2595
14:35:15.531450 [0] Epoch 00183 | Loss 17.3297
14:35:15.543502 [0] Epoch 00184 | Loss 17.4000
14:35:15.555461 [0] Epoch 00185 | Loss 17.4705
14:35:15.567338 [0] Epoch 00186 | Loss 17.5413
14:35:15.580158 [0] Epoch 00187 | Loss 17.6123
14:35:15.591958 [0] Epoch 00188 | Loss 17.6836
14:35:15.606026 [0] Epoch 00189 | Loss 17.7550
14:35:15.618064 [0] Epoch 00190 | Loss 17.8265
14:35:15.619353 [0] Epoch: 190, Train: 0.4454, Val: 0.4863, Test: 0.4910
14:35:15.631217 [0] Epoch 00191 | Loss 17.8980
14:35:15.826193 [0] Epoch 00192 | Loss 17.9694
14:35:15.841741 [0] Epoch 00193 | Loss 18.0406
14:35:15.853857 [0] Epoch 00194 | Loss 18.1115
14:35:15.866294 [0] Epoch 00195 | Loss 18.1821
14:35:15.878336 [0] Epoch 00196 | Loss 18.2524
14:35:15.890420 [0] Epoch 00197 | Loss 18.3225
14:35:15.902819 [0] Epoch 00198 | Loss 18.3923
14:35:15.914843 [0] Epoch 00199 | Loss 18.4622
14:35:15.916196 [0] Epoch: 199, Train: 0.4508, Val: 0.4933, Test: 0.4994
14:35:15.916806 [0] 
timer summary:
  0.15s   0.15s   800 broadcast
  2.03s   2.03s   800 spmm
  1.57s   1.57s   800 mm
  0.06s   0.06s   400 all_reduce
  5.10s   5.10s   200 epoch
  7.07s   7.07s     1 total
14:36:14.087100 [0] proc begin: <DistEnv 0/1 nccl>
14:36:53.854816 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
14:36:53.856075 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1414 MB |    1416 MB |    1420 MB |    6036 KB |
|       from large pool |    1412 MB |    1414 MB |    1418 MB |    5794 KB |
|       from small pool |       1 MB |       1 MB |       1 MB |     241 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1438 MB |    1438 MB |    1438 MB |       0 B  |
|       from large pool |    1436 MB |    1436 MB |    1436 MB |       0 B  |
|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24008 KB |   24008 KB |   33002 KB |    8994 KB |
|       from large pool |   23554 KB |   23554 KB |   29349 KB |    5794 KB |
|       from small pool |     453 KB |    1820 KB |    3653 KB |    3200 KB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      27    |      19    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      20    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:36:56.520148 [0] Epoch 00000 | Loss 3.7335
14:36:56.522008 [0] Epoch: 000, Train: 0.0201, Val: 0.0164, Test: 0.0156
14:36:57.014521 [0] Epoch 00001 | Loss 3.3371
14:36:57.500642 [0] Epoch 00002 | Loss 2.9680
14:36:58.145508 [0] Epoch 00003 | Loss 2.6334
14:36:58.827482 [0] Epoch 00004 | Loss 2.3502
14:36:59.312934 [0] Epoch 00005 | Loss 2.1138
14:36:59.798256 [0] Epoch 00006 | Loss 1.9125
14:37:00.406315 [0] Epoch 00007 | Loss 1.7384
14:37:01.088870 [0] Epoch 00008 | Loss 1.5895
14:37:01.576701 [0] Epoch 00009 | Loss 1.4624
14:37:02.064773 [0] Epoch 00010 | Loss 1.3481
14:37:02.066699 [0] Epoch: 010, Train: 0.7733, Val: 0.7937, Test: 0.7883
14:37:02.676306 [0] Epoch 00011 | Loss 1.2437
14:37:03.386972 [0] Epoch 00012 | Loss 1.1542
14:37:03.874899 [0] Epoch 00013 | Loss 1.0776
14:37:04.364901 [0] Epoch 00014 | Loss 1.0086
14:37:05.042603 [0] Epoch 00015 | Loss 0.9452
14:37:05.712512 [0] Epoch 00016 | Loss 0.8859
14:37:06.198489 [0] Epoch 00017 | Loss 0.8332
14:37:06.684688 [0] Epoch 00018 | Loss 0.7884
14:37:07.337205 [0] Epoch 00019 | Loss 0.7510
14:37:07.340173 [0] Epoch: 019, Train: 0.8713, Val: 0.8878, Test: 0.8833
14:37:07.341715 [0] 
timer summary:
  0.08s   0.08s    80 broadcast
 11.76s  11.76s    80 spmm
  1.32s   1.32s    80 mm
  0.04s   0.04s    40 all_reduce
 13.48s  13.48s    20 epoch
 53.25s  53.25s     1 total
17:00:20.988112 [0] proc begin: <DistEnv 0/1 nccl>
17:02:46.372432 [0] proc begin: <DistEnv 0/1 nccl>
17:07:07.058208 [0] proc begin: <DistEnv 0/1 nccl>
17:12:51.058801 [0] proc begin: <DistEnv 0/1 nccl>
17:12:51.322464 [0] graph loaded <COO Graph: cora, |V|: 2708, |E|: 10556, masks: 140,500,1000><Local: 0, |V|: 2708, |E|: 13264>
17:12:51.339004 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  15304 KiB |  15326 KiB |  15373 KiB |  70656 B   |
|       from large pool |  15158 KiB |  15158 KiB |  15158 KiB |      0 B   |
|       from small pool |    145 KiB |    167 KiB |    214 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Active memory         |  15304 KiB |  15326 KiB |  15373 KiB |  70656 B   |
|       from large pool |  15158 KiB |  15158 KiB |  15158 KiB |      0 B   |
|       from small pool |    145 KiB |    167 KiB |    214 KiB |  70656 B   |
|---------------------------------------------------------------------------|
| Requested memory      |  15301 KiB |  15322 KiB |  15367 KiB |  67724 B   |
|       from large pool |  15158 KiB |  15158 KiB |  15158 KiB |      0 B   |
|       from small pool |    143 KiB |    164 KiB |    209 KiB |  67724 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  18432 KiB |  18432 KiB |  18432 KiB |      0 B   |
|       from large pool |  16384 KiB |  16384 KiB |  16384 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3128 KiB |   3243 KiB |   3339 KiB | 216576 B   |
|       from large pool |   1225 KiB |   1225 KiB |   1225 KiB |      0 B   |
|       from small pool |   1902 KiB |   2045 KiB |   2114 KiB | 216576 B   |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      10    |      15    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      14    |       7    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      10    |      15    |       7    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       7    |       9    |      14    |       7    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       3    |       1    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       2    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

17:12:53.357056 [0] Epoch 00000 | Loss 1.9455
17:12:53.413058 [0] Epoch: 000, Train: 0.1000, Val: 0.1120, Test: 0.1040
17:12:53.456964 [0] Epoch 00001 | Loss 1.9171
17:12:53.502715 [0] Epoch: 001, Train: 0.9786, Val: 0.5640, Test: 0.5620
17:12:53.525640 [0] Epoch 00002 | Loss 1.8858
17:12:53.527013 [0] Epoch: 002, Train: 0.9857, Val: 0.6440, Test: 0.6610
17:12:53.531130 [0] Epoch 00003 | Loss 1.8471
17:12:53.532311 [0] Epoch: 003, Train: 0.9857, Val: 0.6700, Test: 0.6860
17:12:53.536848 [0] Epoch 00004 | Loss 1.8011
17:12:53.541550 [0] Epoch: 004, Train: 0.9857, Val: 0.6860, Test: 0.6960
17:12:53.546156 [0] Epoch 00005 | Loss 1.7490
17:12:53.591812 [0] Epoch: 005, Train: 0.9857, Val: 0.6920, Test: 0.7000
17:12:53.635905 [0] Epoch 00006 | Loss 1.6910
17:12:53.657420 [0] Epoch: 006, Train: 0.9857, Val: 0.7020, Test: 0.7010
17:12:53.664219 [0] Epoch 00007 | Loss 1.6272
17:12:53.667179 [0] Epoch: 007, Train: 0.9857, Val: 0.7020, Test: 0.7050
17:12:53.673540 [0] Epoch 00008 | Loss 1.5577
17:12:53.676904 [0] Epoch: 008, Train: 0.9857, Val: 0.7000, Test: 0.7150
17:12:53.683613 [0] Epoch 00009 | Loss 1.4829
17:12:53.694659 [0] Epoch: 009, Train: 0.9857, Val: 0.7020, Test: 0.7170
17:12:53.710298 [0] Epoch 00010 | Loss 1.4032
17:12:53.713649 [0] Epoch: 010, Train: 0.9929, Val: 0.7040, Test: 0.7200
17:12:53.718819 [0] Epoch 00011 | Loss 1.3192
17:12:53.720855 [0] Epoch: 011, Train: 0.9929, Val: 0.7040, Test: 0.7220
17:12:53.727155 [0] Epoch 00012 | Loss 1.2319
17:12:53.731868 [0] Epoch: 012, Train: 0.9929, Val: 0.7080, Test: 0.7240
17:12:53.740504 [0] Epoch 00013 | Loss 1.1420
17:12:53.743546 [0] Epoch: 013, Train: 0.9929, Val: 0.7060, Test: 0.7260
17:12:53.750032 [0] Epoch 00014 | Loss 1.0506
17:12:53.753158 [0] Epoch: 014, Train: 0.9929, Val: 0.7080, Test: 0.7300
17:12:53.760158 [0] Epoch 00015 | Loss 0.9590
17:12:53.762959 [0] Epoch: 015, Train: 0.9929, Val: 0.7100, Test: 0.7350
17:12:53.770435 [0] Epoch 00016 | Loss 0.8683
17:12:53.791788 [0] Epoch: 016, Train: 0.9929, Val: 0.7120, Test: 0.7360
17:12:53.803486 [0] Epoch 00017 | Loss 0.7798
17:12:53.821513 [0] Epoch: 017, Train: 0.9929, Val: 0.7180, Test: 0.7400
17:12:53.827626 [0] Epoch 00018 | Loss 0.6949
17:12:53.829208 [0] Epoch: 018, Train: 0.9929, Val: 0.7200, Test: 0.7420
17:12:53.856139 [0] Epoch 00019 | Loss 0.6144
17:12:53.883584 [0] Epoch: 019, Train: 0.9929, Val: 0.7260, Test: 0.7540
17:12:53.945557 [0] Epoch 00020 | Loss 0.5394
17:12:53.962281 [0] Epoch: 020, Train: 0.9929, Val: 0.7300, Test: 0.7600
17:12:53.970808 [0] Epoch 00021 | Loss 0.4704
17:12:53.972488 [0] Epoch: 021, Train: 0.9929, Val: 0.7280, Test: 0.7620
17:12:53.977014 [0] Epoch 00022 | Loss 0.4078
17:12:53.978414 [0] Epoch: 022, Train: 0.9929, Val: 0.7240, Test: 0.7610
17:12:53.985774 [0] Epoch 00023 | Loss 0.3518
17:12:53.987092 [0] Epoch: 023, Train: 0.9929, Val: 0.7280, Test: 0.7630
17:12:54.022522 [0] Epoch 00024 | Loss 0.3021
17:12:54.047240 [0] Epoch: 024, Train: 0.9929, Val: 0.7340, Test: 0.7640
17:12:54.055873 [0] Epoch 00025 | Loss 0.2586
17:12:54.059969 [0] Epoch: 025, Train: 0.9929, Val: 0.7380, Test: 0.7620
17:12:54.067175 [0] Epoch 00026 | Loss 0.2207
17:12:54.068758 [0] Epoch: 026, Train: 0.9929, Val: 0.7380, Test: 0.7660
17:12:54.073085 [0] Epoch 00027 | Loss 0.1881
17:12:54.075146 [0] Epoch: 027, Train: 1.0000, Val: 0.7440, Test: 0.7740
17:12:54.081966 [0] Epoch 00028 | Loss 0.1601
17:12:54.083687 [0] Epoch: 028, Train: 1.0000, Val: 0.7440, Test: 0.7770
17:12:54.090266 [0] Epoch 00029 | Loss 0.1362
17:12:54.092474 [0] Epoch: 029, Train: 1.0000, Val: 0.7460, Test: 0.7800
17:12:54.098338 [0] Epoch 00030 | Loss 0.1159
17:12:54.100654 [0] Epoch: 030, Train: 1.0000, Val: 0.7460, Test: 0.7790
17:12:54.106546 [0] Epoch 00031 | Loss 0.0988
17:12:54.108926 [0] Epoch: 031, Train: 1.0000, Val: 0.7480, Test: 0.7780
17:12:54.114519 [0] Epoch 00032 | Loss 0.0843
17:12:54.117079 [0] Epoch: 032, Train: 1.0000, Val: 0.7480, Test: 0.7780
17:12:54.123981 [0] Epoch 00033 | Loss 0.0722
17:12:54.126705 [0] Epoch: 033, Train: 1.0000, Val: 0.7480, Test: 0.7780
17:12:54.132320 [0] Epoch 00034 | Loss 0.0620
17:12:54.134778 [0] Epoch: 034, Train: 1.0000, Val: 0.7460, Test: 0.7750
17:12:54.141663 [0] Epoch 00035 | Loss 0.0535
17:12:54.145532 [0] Epoch: 035, Train: 1.0000, Val: 0.7440, Test: 0.7730
17:12:54.153228 [0] Epoch 00036 | Loss 0.0463
17:12:54.154463 [0] Epoch: 036, Train: 1.0000, Val: 0.7440, Test: 0.7750
17:12:54.159900 [0] Epoch 00037 | Loss 0.0403
17:12:54.161460 [0] Epoch: 037, Train: 1.0000, Val: 0.7420, Test: 0.7740
17:12:54.167890 [0] Epoch 00038 | Loss 0.0352
17:12:54.170248 [0] Epoch: 038, Train: 1.0000, Val: 0.7420, Test: 0.7750
17:12:54.174617 [0] Epoch 00039 | Loss 0.0310
17:12:54.177132 [0] Epoch: 039, Train: 1.0000, Val: 0.7380, Test: 0.7730
17:12:54.183782 [0] Epoch 00040 | Loss 0.0274
17:12:54.187479 [0] Epoch: 040, Train: 1.0000, Val: 0.7400, Test: 0.7710
17:12:54.198007 [0] Epoch 00041 | Loss 0.0243
17:12:54.202265 [0] Epoch: 041, Train: 1.0000, Val: 0.7400, Test: 0.7700
17:12:54.207533 [0] Epoch 00042 | Loss 0.0218
17:12:54.210550 [0] Epoch: 042, Train: 1.0000, Val: 0.7400, Test: 0.7700
17:12:54.216773 [0] Epoch 00043 | Loss 0.0196
17:12:54.220154 [0] Epoch: 043, Train: 1.0000, Val: 0.7400, Test: 0.7660
17:12:54.226543 [0] Epoch 00044 | Loss 0.0177
17:12:54.229696 [0] Epoch: 044, Train: 1.0000, Val: 0.7400, Test: 0.7640
17:12:54.236138 [0] Epoch 00045 | Loss 0.0161
17:12:54.239173 [0] Epoch: 045, Train: 1.0000, Val: 0.7420, Test: 0.7620
17:12:54.246220 [0] Epoch 00046 | Loss 0.0147
17:12:54.249315 [0] Epoch: 046, Train: 1.0000, Val: 0.7420, Test: 0.7620
17:12:54.256509 [0] Epoch 00047 | Loss 0.0135
17:12:54.264386 [0] Epoch: 047, Train: 1.0000, Val: 0.7420, Test: 0.7610
17:12:54.278736 [0] Epoch 00048 | Loss 0.0124
17:12:54.286256 [0] Epoch: 048, Train: 1.0000, Val: 0.7420, Test: 0.7590
17:12:54.318890 [0] Epoch 00049 | Loss 0.0115
17:12:54.345926 [0] Epoch: 049, Train: 1.0000, Val: 0.7400, Test: 0.7580
17:12:54.353086 [0] Epoch 00050 | Loss 0.0107
17:12:54.354295 [0] Epoch: 050, Train: 1.0000, Val: 0.7400, Test: 0.7560
17:12:54.363798 [0] Epoch 00051 | Loss 0.0100
17:12:54.366501 [0] Epoch: 051, Train: 1.0000, Val: 0.7400, Test: 0.7560
17:12:54.372275 [0] Epoch 00052 | Loss 0.0094
17:12:54.375074 [0] Epoch: 052, Train: 1.0000, Val: 0.7400, Test: 0.7560
17:12:54.441287 [0] Epoch 00053 | Loss 0.0088
17:12:54.476402 [0] Epoch: 053, Train: 1.0000, Val: 0.7400, Test: 0.7560
17:12:54.562857 [0] Epoch 00054 | Loss 0.0083
17:12:54.615504 [0] Epoch: 054, Train: 1.0000, Val: 0.7400, Test: 0.7550
17:12:54.665836 [0] Epoch 00055 | Loss 0.0079
17:12:54.694153 [0] Epoch: 055, Train: 1.0000, Val: 0.7380, Test: 0.7550
17:12:54.753039 [0] Epoch 00056 | Loss 0.0075
17:12:54.789938 [0] Epoch: 056, Train: 1.0000, Val: 0.7380, Test: 0.7540
17:12:54.875105 [0] Epoch 00057 | Loss 0.0071
17:12:54.929402 [0] Epoch: 057, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:55.015846 [0] Epoch 00058 | Loss 0.0068
17:12:55.045837 [0] Epoch: 058, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:55.132479 [0] Epoch 00059 | Loss 0.0065
17:12:55.186870 [0] Epoch: 059, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:55.241814 [0] Epoch 00060 | Loss 0.0063
17:12:55.278117 [0] Epoch: 060, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:55.344106 [0] Epoch 00061 | Loss 0.0060
17:12:55.398821 [0] Epoch: 061, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:55.485267 [0] Epoch 00062 | Loss 0.0058
17:12:55.539747 [0] Epoch: 062, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:55.588282 [0] Epoch 00063 | Loss 0.0056
17:12:55.629633 [0] Epoch: 063, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:55.715959 [0] Epoch 00064 | Loss 0.0054
17:12:55.770043 [0] Epoch: 064, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:55.837288 [0] Epoch 00065 | Loss 0.0053
17:12:55.868780 [0] Epoch: 065, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:55.931144 [0] Epoch 00066 | Loss 0.0051
17:12:55.978220 [0] Epoch: 066, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.063840 [0] Epoch 00067 | Loss 0.0050
17:12:56.118343 [0] Epoch: 067, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:56.204111 [0] Epoch 00068 | Loss 0.0048
17:12:56.237002 [0] Epoch: 068, Train: 1.0000, Val: 0.7360, Test: 0.7570
17:12:56.325421 [0] Epoch 00069 | Loss 0.0047
17:12:56.379544 [0] Epoch: 069, Train: 1.0000, Val: 0.7380, Test: 0.7570
17:12:56.447497 [0] Epoch 00070 | Loss 0.0046
17:12:56.451444 [0] Epoch: 070, Train: 1.0000, Val: 0.7380, Test: 0.7570
17:12:56.460639 [0] Epoch 00071 | Loss 0.0045
17:12:56.462445 [0] Epoch: 071, Train: 1.0000, Val: 0.7380, Test: 0.7580
17:12:56.467439 [0] Epoch 00072 | Loss 0.0044
17:12:56.469177 [0] Epoch: 072, Train: 1.0000, Val: 0.7380, Test: 0.7580
17:12:56.473906 [0] Epoch 00073 | Loss 0.0043
17:12:56.475544 [0] Epoch: 073, Train: 1.0000, Val: 0.7380, Test: 0.7580
17:12:56.479684 [0] Epoch 00074 | Loss 0.0042
17:12:56.480713 [0] Epoch: 074, Train: 1.0000, Val: 0.7360, Test: 0.7580
17:12:56.484588 [0] Epoch 00075 | Loss 0.0041
17:12:56.485952 [0] Epoch: 075, Train: 1.0000, Val: 0.7380, Test: 0.7570
17:12:56.489637 [0] Epoch 00076 | Loss 0.0040
17:12:56.490660 [0] Epoch: 076, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.494533 [0] Epoch 00077 | Loss 0.0039
17:12:56.495565 [0] Epoch: 077, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.500035 [0] Epoch 00078 | Loss 0.0039
17:12:56.501194 [0] Epoch: 078, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.506009 [0] Epoch 00079 | Loss 0.0038
17:12:56.508382 [0] Epoch: 079, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.513911 [0] Epoch 00080 | Loss 0.0037
17:12:56.515890 [0] Epoch: 080, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.520748 [0] Epoch 00081 | Loss 0.0037
17:12:56.523826 [0] Epoch: 081, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.529690 [0] Epoch 00082 | Loss 0.0036
17:12:56.530892 [0] Epoch: 082, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.535342 [0] Epoch 00083 | Loss 0.0036
17:12:56.537843 [0] Epoch: 083, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.543529 [0] Epoch 00084 | Loss 0.0035
17:12:56.546060 [0] Epoch: 084, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.552623 [0] Epoch 00085 | Loss 0.0035
17:12:56.556108 [0] Epoch: 085, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.562801 [0] Epoch 00086 | Loss 0.0034
17:12:56.564049 [0] Epoch: 086, Train: 1.0000, Val: 0.7380, Test: 0.7560
17:12:56.569123 [0] Epoch 00087 | Loss 0.0034
17:12:56.571853 [0] Epoch: 087, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.579024 [0] Epoch 00088 | Loss 0.0033
17:12:56.581942 [0] Epoch: 088, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.588983 [0] Epoch 00089 | Loss 0.0033
17:12:56.591374 [0] Epoch: 089, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.597651 [0] Epoch 00090 | Loss 0.0032
17:12:56.600449 [0] Epoch: 090, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.606985 [0] Epoch 00091 | Loss 0.0032
17:12:56.609834 [0] Epoch: 091, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.616272 [0] Epoch 00092 | Loss 0.0031
17:12:56.624491 [0] Epoch: 092, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.645380 [0] Epoch 00093 | Loss 0.0031
17:12:56.671822 [0] Epoch: 093, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:56.715600 [0] Epoch 00094 | Loss 0.0031
17:12:56.742991 [0] Epoch: 094, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:56.786807 [0] Epoch 00095 | Loss 0.0030
17:12:56.813392 [0] Epoch: 095, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:56.823047 [0] Epoch 00096 | Loss 0.0030
17:12:56.824534 [0] Epoch: 096, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:56.828470 [0] Epoch 00097 | Loss 0.0029
17:12:56.830201 [0] Epoch: 097, Train: 1.0000, Val: 0.7340, Test: 0.7560
17:12:56.833768 [0] Epoch 00098 | Loss 0.0029
17:12:56.834932 [0] Epoch: 098, Train: 1.0000, Val: 0.7340, Test: 0.7560
17:12:56.841194 [0] Epoch 00099 | Loss 0.0029
17:12:56.842209 [0] Epoch: 099, Train: 1.0000, Val: 0.7340, Test: 0.7560
17:12:56.849533 [0] Epoch 00100 | Loss 0.0028
17:12:56.877045 [0] Epoch: 100, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.920927 [0] Epoch 00101 | Loss 0.0028
17:12:56.946658 [0] Epoch: 101, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.953219 [0] Epoch 00102 | Loss 0.0028
17:12:56.957314 [0] Epoch: 102, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.963582 [0] Epoch 00103 | Loss 0.0027
17:12:56.965512 [0] Epoch: 103, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:56.970641 [0] Epoch 00104 | Loss 0.0027
17:12:56.974170 [0] Epoch: 104, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:56.981410 [0] Epoch 00105 | Loss 0.0027
17:12:56.984657 [0] Epoch: 105, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.990772 [0] Epoch 00106 | Loss 0.0027
17:12:56.993413 [0] Epoch: 106, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:56.999340 [0] Epoch 00107 | Loss 0.0026
17:12:57.001911 [0] Epoch: 107, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.007633 [0] Epoch 00108 | Loss 0.0026
17:12:57.010115 [0] Epoch: 108, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.016062 [0] Epoch 00109 | Loss 0.0026
17:12:57.018663 [0] Epoch: 109, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.024363 [0] Epoch 00110 | Loss 0.0025
17:12:57.026654 [0] Epoch: 110, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.032193 [0] Epoch 00111 | Loss 0.0025
17:12:57.034920 [0] Epoch: 111, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.041189 [0] Epoch 00112 | Loss 0.0025
17:12:57.044193 [0] Epoch: 112, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.049517 [0] Epoch 00113 | Loss 0.0025
17:12:57.050717 [0] Epoch: 113, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.054835 [0] Epoch 00114 | Loss 0.0024
17:12:57.056041 [0] Epoch: 114, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.059988 [0] Epoch 00115 | Loss 0.0024
17:12:57.061213 [0] Epoch: 115, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.065115 [0] Epoch 00116 | Loss 0.0024
17:12:57.066390 [0] Epoch: 116, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.070393 [0] Epoch 00117 | Loss 0.0024
17:12:57.071574 [0] Epoch: 117, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.076928 [0] Epoch 00118 | Loss 0.0023
17:12:57.078127 [0] Epoch: 118, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.082353 [0] Epoch 00119 | Loss 0.0023
17:12:57.083522 [0] Epoch: 119, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.087935 [0] Epoch 00120 | Loss 0.0023
17:12:57.089195 [0] Epoch: 120, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.094042 [0] Epoch 00121 | Loss 0.0023
17:12:57.096522 [0] Epoch: 121, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.100790 [0] Epoch 00122 | Loss 0.0022
17:12:57.102008 [0] Epoch: 122, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.106746 [0] Epoch 00123 | Loss 0.0022
17:12:57.109842 [0] Epoch: 123, Train: 1.0000, Val: 0.7360, Test: 0.7560
17:12:57.116371 [0] Epoch 00124 | Loss 0.0022
17:12:57.117749 [0] Epoch: 124, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.122883 [0] Epoch 00125 | Loss 0.0022
17:12:57.125092 [0] Epoch: 125, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.132495 [0] Epoch 00126 | Loss 0.0022
17:12:57.136232 [0] Epoch: 126, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.143513 [0] Epoch 00127 | Loss 0.0021
17:12:57.146536 [0] Epoch: 127, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.153654 [0] Epoch 00128 | Loss 0.0021
17:12:57.156776 [0] Epoch: 128, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.163446 [0] Epoch 00129 | Loss 0.0021
17:12:57.166594 [0] Epoch: 129, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.174263 [0] Epoch 00130 | Loss 0.0021
17:12:57.177745 [0] Epoch: 130, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.184416 [0] Epoch 00131 | Loss 0.0021
17:12:57.187492 [0] Epoch: 131, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.202182 [0] Epoch 00132 | Loss 0.0020
17:12:57.214449 [0] Epoch: 132, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.219549 [0] Epoch 00133 | Loss 0.0020
17:12:57.221414 [0] Epoch: 133, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.238245 [0] Epoch 00134 | Loss 0.0020
17:12:57.265747 [0] Epoch: 134, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.309496 [0] Epoch 00135 | Loss 0.0020
17:12:57.336911 [0] Epoch: 135, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.380638 [0] Epoch 00136 | Loss 0.0020
17:12:57.391680 [0] Epoch: 136, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.417005 [0] Epoch 00137 | Loss 0.0020
17:12:57.421850 [0] Epoch: 137, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.429464 [0] Epoch 00138 | Loss 0.0019
17:12:57.433183 [0] Epoch: 138, Train: 1.0000, Val: 0.7360, Test: 0.7550
17:12:57.441379 [0] Epoch 00139 | Loss 0.0019
17:12:57.442919 [0] Epoch: 139, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.447705 [0] Epoch 00140 | Loss 0.0019
17:12:57.449930 [0] Epoch: 140, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.454667 [0] Epoch 00141 | Loss 0.0019
17:12:57.456134 [0] Epoch: 141, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.461068 [0] Epoch 00142 | Loss 0.0019
17:12:57.465258 [0] Epoch: 142, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.469532 [0] Epoch 00143 | Loss 0.0019
17:12:57.496165 [0] Epoch: 143, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.539991 [0] Epoch 00144 | Loss 0.0018
17:12:57.567547 [0] Epoch: 144, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.580020 [0] Epoch 00145 | Loss 0.0018
17:12:57.582530 [0] Epoch: 145, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.587118 [0] Epoch 00146 | Loss 0.0018
17:12:57.588166 [0] Epoch: 146, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.593544 [0] Epoch 00147 | Loss 0.0018
17:12:57.594635 [0] Epoch: 147, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.598796 [0] Epoch 00148 | Loss 0.0018
17:12:57.599883 [0] Epoch: 148, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.603818 [0] Epoch 00149 | Loss 0.0018
17:12:57.604851 [0] Epoch: 149, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.608738 [0] Epoch 00150 | Loss 0.0017
17:12:57.609784 [0] Epoch: 150, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.613561 [0] Epoch 00151 | Loss 0.0017
17:12:57.614607 [0] Epoch: 151, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.618334 [0] Epoch 00152 | Loss 0.0017
17:12:57.619357 [0] Epoch: 152, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.623324 [0] Epoch 00153 | Loss 0.0017
17:12:57.624384 [0] Epoch: 153, Train: 1.0000, Val: 0.7360, Test: 0.7540
17:12:57.628114 [0] Epoch 00154 | Loss 0.0017
17:12:57.629207 [0] Epoch: 154, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.633050 [0] Epoch 00155 | Loss 0.0017
17:12:57.634120 [0] Epoch: 155, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.637820 [0] Epoch 00156 | Loss 0.0017
17:12:57.638856 [0] Epoch: 156, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.642977 [0] Epoch 00157 | Loss 0.0016
17:12:57.644065 [0] Epoch: 157, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.648081 [0] Epoch 00158 | Loss 0.0016
17:12:57.649125 [0] Epoch: 158, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.655064 [0] Epoch 00159 | Loss 0.0016
17:12:57.656708 [0] Epoch: 159, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.662370 [0] Epoch 00160 | Loss 0.0016
17:12:57.663611 [0] Epoch: 160, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.667736 [0] Epoch 00161 | Loss 0.0016
17:12:57.669408 [0] Epoch: 161, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.674883 [0] Epoch 00162 | Loss 0.0016
17:12:57.677620 [0] Epoch: 162, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.683740 [0] Epoch 00163 | Loss 0.0016
17:12:57.686172 [0] Epoch: 163, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.691460 [0] Epoch 00164 | Loss 0.0016
17:12:57.693806 [0] Epoch: 164, Train: 1.0000, Val: 0.7340, Test: 0.7550
17:12:57.699747 [0] Epoch 00165 | Loss 0.0015
17:12:57.702638 [0] Epoch: 165, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.709427 [0] Epoch 00166 | Loss 0.0015
17:12:57.710742 [0] Epoch: 166, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.715845 [0] Epoch 00167 | Loss 0.0015
17:12:57.718493 [0] Epoch: 167, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.722640 [0] Epoch 00168 | Loss 0.0015
17:12:57.723893 [0] Epoch: 168, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.728829 [0] Epoch 00169 | Loss 0.0015
17:12:57.730053 [0] Epoch: 169, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.735419 [0] Epoch 00170 | Loss 0.0015
17:12:57.737000 [0] Epoch: 170, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.742157 [0] Epoch 00171 | Loss 0.0015
17:12:57.743550 [0] Epoch: 171, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.748880 [0] Epoch 00172 | Loss 0.0015
17:12:57.750164 [0] Epoch: 172, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.754436 [0] Epoch 00173 | Loss 0.0014
17:12:57.755658 [0] Epoch: 173, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.759899 [0] Epoch 00174 | Loss 0.0014
17:12:57.761105 [0] Epoch: 174, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.765607 [0] Epoch 00175 | Loss 0.0014
17:12:57.766899 [0] Epoch: 175, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.772535 [0] Epoch 00176 | Loss 0.0014
17:12:57.773941 [0] Epoch: 176, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.778616 [0] Epoch 00177 | Loss 0.0014
17:12:57.781051 [0] Epoch: 177, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.787580 [0] Epoch 00178 | Loss 0.0014
17:12:57.788962 [0] Epoch: 178, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.793958 [0] Epoch 00179 | Loss 0.0014
17:12:57.796585 [0] Epoch: 179, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.802747 [0] Epoch 00180 | Loss 0.0014
17:12:57.805736 [0] Epoch: 180, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.811968 [0] Epoch 00181 | Loss 0.0014
17:12:57.816285 [0] Epoch: 181, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.824901 [0] Epoch 00182 | Loss 0.0013
17:12:57.827148 [0] Epoch: 182, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.833279 [0] Epoch 00183 | Loss 0.0013
17:12:57.836432 [0] Epoch: 183, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.842826 [0] Epoch 00184 | Loss 0.0013
17:12:57.845873 [0] Epoch: 184, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.852664 [0] Epoch 00185 | Loss 0.0013
17:12:57.855827 [0] Epoch: 185, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.862426 [0] Epoch 00186 | Loss 0.0013
17:12:57.877393 [0] Epoch: 186, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.900222 [0] Epoch 00187 | Loss 0.0013
17:12:57.927629 [0] Epoch: 187, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:57.971334 [0] Epoch 00188 | Loss 0.0013
17:12:57.998701 [0] Epoch: 188, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.042409 [0] Epoch 00189 | Loss 0.0013
17:12:58.069808 [0] Epoch: 189, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.113574 [0] Epoch 00190 | Loss 0.0013
17:12:58.140937 [0] Epoch: 190, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.184902 [0] Epoch 00191 | Loss 0.0013
17:12:58.196203 [0] Epoch: 191, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.200944 [0] Epoch 00192 | Loss 0.0013
17:12:58.202035 [0] Epoch: 192, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.205924 [0] Epoch 00193 | Loss 0.0012
17:12:58.206917 [0] Epoch: 193, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.216088 [0] Epoch 00194 | Loss 0.0012
17:12:58.243556 [0] Epoch: 194, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.287283 [0] Epoch 00195 | Loss 0.0012
17:12:58.314766 [0] Epoch: 195, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.354641 [0] Epoch 00196 | Loss 0.0012
17:12:58.358980 [0] Epoch: 196, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.368235 [0] Epoch 00197 | Loss 0.0012
17:12:58.369354 [0] Epoch: 197, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.373327 [0] Epoch 00198 | Loss 0.0012
17:12:58.374565 [0] Epoch: 198, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.378634 [0] Epoch 00199 | Loss 0.0012
17:12:58.379827 [0] Epoch: 199, Train: 1.0000, Val: 0.7340, Test: 0.7540
17:12:58.381177 [0] 
timer summary:
  0.63s   0.63s   800 mm
  0.36s   0.36s   400 broadcast
  0.28s   0.28s   400 spmm
  0.11s   0.11s   400 all_reduce
  6.22s   6.22s   200 epoch
  7.32s   7.32s     1 total
10:24:48.963860 [0] proc begin: <DistEnv 0/1 nccl>
10:27:07.150982 [0] proc begin: <DistEnv 0/1 nccl>
10:28:04.711321 [0] proc begin: <DistEnv 0/1 nccl>
10:31:55.081180 [0] proc begin: <DistEnv 0/1 nccl>
10:36:22.854704 [0] proc begin: <DistEnv 0/1 nccl>
10:36:56.305607 [0] proc begin: <DistEnv 0/1 nccl>
10:37:21.287153 [0] proc begin: <DistEnv 0/1 nccl>
10:37:58.607234 [0] proc begin: <DistEnv 0/1 nccl>
10:38:18.820000 [0] proc begin: <DistEnv 0/1 nccl>
20:10:51.255493 [0] proc begin: <DistEnv 0/1 nccl>
20:10:59.832444 [0] graph loaded <COO Graph: reddit, |V|: 232965, |E|: 114615892, masks: 153431,23831,55703><Local: 0, |V|: 232965, |E|: 114848857>
20:10:59.861299 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1414 MiB |   1416 MiB |   1420 MiB |   6027 KiB |
|       from large pool |   1412 MiB |   1414 MiB |   1418 MiB |   5794 KiB |
|       from small pool |      1 MiB |      1 MiB |      1 MiB |    232 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   1414 MiB |   1416 MiB |   1420 MiB |   6027 KiB |
|       from large pool |   1412 MiB |   1414 MiB |   1418 MiB |   5794 KiB |
|       from small pool |      1 MiB |      1 MiB |      1 MiB |    232 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1414 MiB |   1416 MiB |   1420 MiB |   5688 KiB |
|       from large pool |   1412 MiB |   1414 MiB |   1418 MiB |   5460 KiB |
|       from small pool |      1 MiB |      1 MiB |      1 MiB |    228 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1438 MiB |   1438 MiB |   1438 MiB |      0 B   |
|       from large pool |   1436 MiB |   1436 MiB |   1436 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  24008 KiB |  24008 KiB |  32993 KiB |   8985 KiB |
|       from large pool |  23554 KiB |  23554 KiB |  29349 KiB |   5794 KiB |
|       from small pool |    453 KiB |   1820 KiB |   3644 KiB |   3191 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      21    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      14    |      10    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      21    |      13    |
|       from large pool |       4    |       5    |       7    |       3    |
|       from small pool |       4    |       7    |      14    |      10    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       5    |       5    |       5    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |       9    |       4    |
|       from large pool |       4    |       4    |       7    |       3    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:11:02.274943 [0] Epoch 00000 | Loss 3.8046
20:11:02.309418 [0] Epoch: 000, Train: 0.0177, Val: 0.0172, Test: 0.0172
20:11:03.036709 [0] Epoch 00001 | Loss 3.4151
20:11:03.038750 [0] Epoch: 001, Train: 0.1158, Val: 0.1018, Test: 0.0997
20:11:03.760869 [0] Epoch 00002 | Loss 3.2607
20:11:03.764196 [0] Epoch: 002, Train: 0.2129, Val: 0.2001, Test: 0.1946
20:11:04.488683 [0] Epoch 00003 | Loss 3.1295
20:11:04.490613 [0] Epoch: 003, Train: 0.2557, Val: 0.2371, Test: 0.2363
20:11:05.219942 [0] Epoch 00004 | Loss 3.0229
20:11:05.223748 [0] Epoch: 004, Train: 0.2959, Val: 0.2737, Test: 0.2732
20:11:05.952893 [0] Epoch 00005 | Loss 2.9189
20:11:05.956182 [0] Epoch: 005, Train: 0.3216, Val: 0.2997, Test: 0.2995
20:11:06.685406 [0] Epoch 00006 | Loss 2.8094
20:11:06.687477 [0] Epoch: 006, Train: 0.3549, Val: 0.3402, Test: 0.3409
20:11:07.416515 [0] Epoch 00007 | Loss 2.6895
20:11:07.419275 [0] Epoch: 007, Train: 0.4123, Val: 0.4162, Test: 0.4215
20:11:08.148061 [0] Epoch 00008 | Loss 2.5560
20:11:08.151540 [0] Epoch: 008, Train: 0.4971, Val: 0.5000, Test: 0.5065
20:11:08.880385 [0] Epoch 00009 | Loss 2.4101
20:11:08.883674 [0] Epoch: 009, Train: 0.5323, Val: 0.5318, Test: 0.5378
20:11:09.612626 [0] Epoch 00010 | Loss 2.2561
20:11:09.616650 [0] Epoch: 010, Train: 0.5473, Val: 0.5462, Test: 0.5507
20:11:10.344333 [0] Epoch 00011 | Loss 2.0924
20:11:10.346319 [0] Epoch: 011, Train: 0.5866, Val: 0.5847, Test: 0.5873
20:11:11.075871 [0] Epoch 00012 | Loss 1.9224
20:11:11.077819 [0] Epoch: 012, Train: 0.6027, Val: 0.6004, Test: 0.6021
20:11:11.805229 [0] Epoch 00013 | Loss 1.7500
20:11:11.807341 [0] Epoch: 013, Train: 0.6138, Val: 0.6124, Test: 0.6129
20:11:12.533765 [0] Epoch 00014 | Loss 1.5901
20:11:12.535688 [0] Epoch: 014, Train: 0.6304, Val: 0.6295, Test: 0.6292
20:11:13.262054 [0] Epoch 00015 | Loss 1.4564
20:11:13.265311 [0] Epoch: 015, Train: 0.6723, Val: 0.6708, Test: 0.6711
20:11:13.991887 [0] Epoch 00016 | Loss 1.3460
20:11:13.995113 [0] Epoch: 016, Train: 0.6870, Val: 0.6849, Test: 0.6842
20:11:14.721376 [0] Epoch 00017 | Loss 1.2538
20:11:14.723016 [0] Epoch: 017, Train: 0.7056, Val: 0.7029, Test: 0.7013
20:11:15.447753 [0] Epoch 00018 | Loss 1.1651
20:11:15.449281 [0] Epoch: 018, Train: 0.7196, Val: 0.7164, Test: 0.7144
20:11:16.174840 [0] Epoch 00019 | Loss 1.0715
20:11:16.176402 [0] Epoch: 019, Train: 0.7519, Val: 0.7490, Test: 0.7447
20:11:16.902548 [0] Epoch 00020 | Loss 0.9873
20:11:16.905530 [0] Epoch: 020, Train: 0.7803, Val: 0.7727, Test: 0.7677
20:11:17.631370 [0] Epoch 00021 | Loss 0.9178
20:11:17.632906 [0] Epoch: 021, Train: 0.8002, Val: 0.7885, Test: 0.7843
20:11:18.358774 [0] Epoch 00022 | Loss 0.8600
20:11:18.360346 [0] Epoch: 022, Train: 0.8194, Val: 0.8087, Test: 0.8046
20:11:19.086569 [0] Epoch 00023 | Loss 0.8127
20:11:19.089488 [0] Epoch: 023, Train: 0.8334, Val: 0.8219, Test: 0.8177
20:11:19.815669 [0] Epoch 00024 | Loss 0.7721
20:11:19.818665 [0] Epoch: 024, Train: 0.8421, Val: 0.8299, Test: 0.8269
20:11:20.545011 [0] Epoch 00025 | Loss 0.7348
20:11:20.546562 [0] Epoch: 025, Train: 0.8490, Val: 0.8363, Test: 0.8331
20:11:21.273024 [0] Epoch 00026 | Loss 0.6980
20:11:21.276187 [0] Epoch: 026, Train: 0.8569, Val: 0.8449, Test: 0.8422
20:11:22.001795 [0] Epoch 00027 | Loss 0.6605
20:11:22.003703 [0] Epoch: 027, Train: 0.8797, Val: 0.8741, Test: 0.8729
20:11:22.730688 [0] Epoch 00028 | Loss 0.6209
20:11:22.733855 [0] Epoch: 028, Train: 0.9000, Val: 0.9018, Test: 0.8999
20:11:23.460773 [0] Epoch 00029 | Loss 0.5791
20:11:23.463709 [0] Epoch: 029, Train: 0.9053, Val: 0.9088, Test: 0.9065
20:11:24.190295 [0] Epoch 00030 | Loss 0.5362
20:11:24.193192 [0] Epoch: 030, Train: 0.9091, Val: 0.9126, Test: 0.9106
20:11:24.919812 [0] Epoch 00031 | Loss 0.4961
20:11:24.922576 [0] Epoch: 031, Train: 0.9122, Val: 0.9166, Test: 0.9149
20:11:25.648847 [0] Epoch 00032 | Loss 0.4630
20:11:25.650402 [0] Epoch: 032, Train: 0.9163, Val: 0.9202, Test: 0.9185
20:11:26.375631 [0] Epoch 00033 | Loss 0.4376
20:11:26.377154 [0] Epoch: 033, Train: 0.9209, Val: 0.9250, Test: 0.9227
20:11:27.102453 [0] Epoch 00034 | Loss 0.4171
20:11:27.105631 [0] Epoch: 034, Train: 0.9240, Val: 0.9280, Test: 0.9256
20:11:27.831167 [0] Epoch 00035 | Loss 0.3998
20:11:27.832745 [0] Epoch: 035, Train: 0.9261, Val: 0.9304, Test: 0.9276
20:11:28.558878 [0] Epoch 00036 | Loss 0.3857
20:11:28.560480 [0] Epoch: 036, Train: 0.9274, Val: 0.9320, Test: 0.9292
20:11:29.286298 [0] Epoch 00037 | Loss 0.3743
20:11:29.287806 [0] Epoch: 037, Train: 0.9283, Val: 0.9321, Test: 0.9306
20:11:30.013866 [0] Epoch 00038 | Loss 0.3648
20:11:30.016783 [0] Epoch: 038, Train: 0.9291, Val: 0.9328, Test: 0.9308
20:11:30.743160 [0] Epoch 00039 | Loss 0.3561
20:11:30.744674 [0] Epoch: 039, Train: 0.9300, Val: 0.9337, Test: 0.9317
20:11:31.470268 [0] Epoch 00040 | Loss 0.3477
20:11:31.473215 [0] Epoch: 040, Train: 0.9317, Val: 0.9347, Test: 0.9327
20:11:32.199945 [0] Epoch 00041 | Loss 0.3398
20:11:32.201653 [0] Epoch: 041, Train: 0.9330, Val: 0.9355, Test: 0.9339
20:11:32.926934 [0] Epoch 00042 | Loss 0.3331
20:11:32.929837 [0] Epoch: 042, Train: 0.9338, Val: 0.9359, Test: 0.9351
20:11:33.656088 [0] Epoch 00043 | Loss 0.3281
20:11:33.657683 [0] Epoch: 043, Train: 0.9347, Val: 0.9368, Test: 0.9362
20:11:34.382910 [0] Epoch 00044 | Loss 0.3242
20:11:34.384395 [0] Epoch: 044, Train: 0.9352, Val: 0.9371, Test: 0.9366
20:11:35.110388 [0] Epoch 00045 | Loss 0.3199
20:11:35.113537 [0] Epoch: 045, Train: 0.9357, Val: 0.9371, Test: 0.9369
20:11:35.839484 [0] Epoch 00046 | Loss 0.3151
20:11:35.841020 [0] Epoch: 046, Train: 0.9364, Val: 0.9376, Test: 0.9373
20:11:36.566699 [0] Epoch 00047 | Loss 0.3107
20:11:36.568489 [0] Epoch: 047, Train: 0.9366, Val: 0.9381, Test: 0.9376
20:11:37.289558 [0] Epoch 00048 | Loss 0.3068
20:11:37.291088 [0] Epoch: 048, Train: 0.9371, Val: 0.9382, Test: 0.9378
20:11:38.016690 [0] Epoch 00049 | Loss 0.3029
20:11:38.018224 [0] Epoch: 049, Train: 0.9375, Val: 0.9384, Test: 0.9381
20:11:38.739241 [0] Epoch 00050 | Loss 0.2991
20:11:38.742234 [0] Epoch: 050, Train: 0.9377, Val: 0.9383, Test: 0.9382
20:11:39.467702 [0] Epoch 00051 | Loss 0.2957
20:11:39.470629 [0] Epoch: 051, Train: 0.9382, Val: 0.9382, Test: 0.9381
20:11:40.193014 [0] Epoch 00052 | Loss 0.2926
20:11:40.194566 [0] Epoch: 052, Train: 0.9385, Val: 0.9388, Test: 0.9383
20:11:40.920915 [0] Epoch 00053 | Loss 0.2897
20:11:40.922522 [0] Epoch: 053, Train: 0.9387, Val: 0.9386, Test: 0.9384
20:11:41.643995 [0] Epoch 00054 | Loss 0.2865
20:11:41.645556 [0] Epoch: 054, Train: 0.9394, Val: 0.9388, Test: 0.9387
20:11:42.370968 [0] Epoch 00055 | Loss 0.2836
20:11:42.372454 [0] Epoch: 055, Train: 0.9397, Val: 0.9392, Test: 0.9390
20:11:43.093475 [0] Epoch 00056 | Loss 0.2810
20:11:43.096409 [0] Epoch: 056, Train: 0.9402, Val: 0.9395, Test: 0.9393
20:11:43.823147 [0] Epoch 00057 | Loss 0.2784
20:11:43.826303 [0] Epoch: 057, Train: 0.9406, Val: 0.9398, Test: 0.9396
20:11:44.548063 [0] Epoch 00058 | Loss 0.2759
20:11:44.549590 [0] Epoch: 058, Train: 0.9410, Val: 0.9404, Test: 0.9400
20:11:45.275147 [0] Epoch 00059 | Loss 0.2733
20:11:45.278224 [0] Epoch: 059, Train: 0.9415, Val: 0.9407, Test: 0.9403
20:11:46.000117 [0] Epoch 00060 | Loss 0.2710
20:11:46.001665 [0] Epoch: 060, Train: 0.9421, Val: 0.9405, Test: 0.9404
20:11:46.727574 [0] Epoch 00061 | Loss 0.2689
20:11:46.729092 [0] Epoch: 061, Train: 0.9424, Val: 0.9407, Test: 0.9403
20:11:47.450695 [0] Epoch 00062 | Loss 0.2668
20:11:47.453606 [0] Epoch: 062, Train: 0.9427, Val: 0.9409, Test: 0.9404
20:11:48.180543 [0] Epoch 00063 | Loss 0.2646
20:11:48.183575 [0] Epoch: 063, Train: 0.9431, Val: 0.9412, Test: 0.9408
20:11:48.906485 [0] Epoch 00064 | Loss 0.2626
20:11:48.907982 [0] Epoch: 064, Train: 0.9433, Val: 0.9412, Test: 0.9408
20:11:49.633976 [0] Epoch 00065 | Loss 0.2607
20:11:49.635522 [0] Epoch: 065, Train: 0.9435, Val: 0.9416, Test: 0.9413
20:11:50.360810 [0] Epoch 00066 | Loss 0.2589
20:11:50.362385 [0] Epoch: 066, Train: 0.9438, Val: 0.9417, Test: 0.9413
20:11:51.082480 [0] Epoch 00067 | Loss 0.2571
20:11:51.084022 [0] Epoch: 067, Train: 0.9442, Val: 0.9419, Test: 0.9414
20:11:51.810316 [0] Epoch 00068 | Loss 0.2553
20:11:51.813427 [0] Epoch: 068, Train: 0.9445, Val: 0.9422, Test: 0.9416
20:11:52.533690 [0] Epoch 00069 | Loss 0.2537
20:11:52.536586 [0] Epoch: 069, Train: 0.9449, Val: 0.9424, Test: 0.9417
20:11:53.262811 [0] Epoch 00070 | Loss 0.2520
20:11:53.265735 [0] Epoch: 070, Train: 0.9451, Val: 0.9426, Test: 0.9419
20:11:53.986151 [0] Epoch 00071 | Loss 0.2504
20:11:53.987669 [0] Epoch: 071, Train: 0.9452, Val: 0.9424, Test: 0.9421
20:11:54.714030 [0] Epoch 00072 | Loss 0.2489
20:11:54.715569 [0] Epoch: 072, Train: 0.9454, Val: 0.9423, Test: 0.9421
20:11:55.435654 [0] Epoch 00073 | Loss 0.2474
20:11:55.438674 [0] Epoch: 073, Train: 0.9458, Val: 0.9429, Test: 0.9424
20:11:56.164334 [0] Epoch 00074 | Loss 0.2460
20:11:56.166727 [0] Epoch: 074, Train: 0.9461, Val: 0.9431, Test: 0.9426
20:11:56.886451 [0] Epoch 00075 | Loss 0.2446
20:11:56.887934 [0] Epoch: 075, Train: 0.9463, Val: 0.9432, Test: 0.9424
20:11:57.613178 [0] Epoch 00076 | Loss 0.2432
20:11:57.614644 [0] Epoch: 076, Train: 0.9464, Val: 0.9431, Test: 0.9424
20:11:58.334375 [0] Epoch 00077 | Loss 0.2419
20:11:58.335890 [0] Epoch: 077, Train: 0.9467, Val: 0.9430, Test: 0.9425
20:11:59.062539 [0] Epoch 00078 | Loss 0.2406
20:11:59.065434 [0] Epoch: 078, Train: 0.9468, Val: 0.9430, Test: 0.9425
20:11:59.785601 [0] Epoch 00079 | Loss 0.2394
20:11:59.787061 [0] Epoch: 079, Train: 0.9470, Val: 0.9429, Test: 0.9427
20:12:00.512515 [0] Epoch 00080 | Loss 0.2382
20:12:00.514019 [0] Epoch: 080, Train: 0.9472, Val: 0.9428, Test: 0.9427
20:12:01.234562 [0] Epoch 00081 | Loss 0.2370
20:12:01.236747 [0] Epoch: 081, Train: 0.9474, Val: 0.9427, Test: 0.9427
20:12:01.965604 [0] Epoch 00082 | Loss 0.2359
20:12:01.968266 [0] Epoch: 082, Train: 0.9476, Val: 0.9427, Test: 0.9428
20:12:02.690379 [0] Epoch 00083 | Loss 0.2348
20:12:02.693421 [0] Epoch: 083, Train: 0.9478, Val: 0.9429, Test: 0.9430
20:12:03.420819 [0] Epoch 00084 | Loss 0.2337
20:12:03.423767 [0] Epoch: 084, Train: 0.9479, Val: 0.9431, Test: 0.9431
20:12:04.145317 [0] Epoch 00085 | Loss 0.2326
20:12:04.148443 [0] Epoch: 085, Train: 0.9480, Val: 0.9431, Test: 0.9433
20:12:04.874677 [0] Epoch 00086 | Loss 0.2315
20:12:04.876197 [0] Epoch: 086, Train: 0.9482, Val: 0.9431, Test: 0.9434
20:12:05.596833 [0] Epoch 00087 | Loss 0.2305
20:12:05.599814 [0] Epoch: 087, Train: 0.9484, Val: 0.9431, Test: 0.9435
20:12:06.328774 [0] Epoch 00088 | Loss 0.2295
20:12:06.331810 [0] Epoch: 088, Train: 0.9487, Val: 0.9434, Test: 0.9436
20:12:07.053096 [0] Epoch 00089 | Loss 0.2285
20:12:07.056367 [0] Epoch: 089, Train: 0.9490, Val: 0.9436, Test: 0.9436
20:12:07.782723 [0] Epoch 00090 | Loss 0.2276
20:12:07.784245 [0] Epoch: 090, Train: 0.9492, Val: 0.9437, Test: 0.9437
20:12:08.510340 [0] Epoch 00091 | Loss 0.2266
20:12:08.511895 [0] Epoch: 091, Train: 0.9494, Val: 0.9437, Test: 0.9438
20:12:09.238057 [0] Epoch 00092 | Loss 0.2257
20:12:09.239641 [0] Epoch: 092, Train: 0.9495, Val: 0.9439, Test: 0.9439
20:12:09.965308 [0] Epoch 00093 | Loss 0.2248
20:12:09.966861 [0] Epoch: 093, Train: 0.9497, Val: 0.9439, Test: 0.9440
20:12:10.693818 [0] Epoch 00094 | Loss 0.2239
20:12:10.696811 [0] Epoch: 094, Train: 0.9498, Val: 0.9441, Test: 0.9440
20:12:11.423623 [0] Epoch 00095 | Loss 0.2230
20:12:11.426541 [0] Epoch: 095, Train: 0.9500, Val: 0.9444, Test: 0.9442
20:12:12.153184 [0] Epoch 00096 | Loss 0.2221
20:12:12.156288 [0] Epoch: 096, Train: 0.9501, Val: 0.9443, Test: 0.9444
20:12:12.883267 [0] Epoch 00097 | Loss 0.2213
20:12:12.884849 [0] Epoch: 097, Train: 0.9503, Val: 0.9444, Test: 0.9445
20:12:13.610366 [0] Epoch 00098 | Loss 0.2205
20:12:13.611891 [0] Epoch: 098, Train: 0.9504, Val: 0.9445, Test: 0.9446
20:12:14.331812 [0] Epoch 00099 | Loss 0.2196
20:12:14.334757 [0] Epoch: 099, Train: 0.9506, Val: 0.9447, Test: 0.9448
20:12:15.061294 [0] Epoch 00100 | Loss 0.2188
20:12:15.062812 [0] Epoch: 100, Train: 0.9508, Val: 0.9449, Test: 0.9449
20:12:15.783374 [0] Epoch 00101 | Loss 0.2180
20:12:15.784971 [0] Epoch: 101, Train: 0.9510, Val: 0.9450, Test: 0.9449
20:12:16.510989 [0] Epoch 00102 | Loss 0.2173
20:12:16.513919 [0] Epoch: 102, Train: 0.9511, Val: 0.9452, Test: 0.9450
20:12:17.235181 [0] Epoch 00103 | Loss 0.2165
20:12:17.236894 [0] Epoch: 103, Train: 0.9513, Val: 0.9451, Test: 0.9450
20:12:17.962907 [0] Epoch 00104 | Loss 0.2157
20:12:17.964414 [0] Epoch: 104, Train: 0.9514, Val: 0.9453, Test: 0.9451
20:12:18.684460 [0] Epoch 00105 | Loss 0.2150
20:12:18.686054 [0] Epoch: 105, Train: 0.9516, Val: 0.9454, Test: 0.9451
20:12:19.411770 [0] Epoch 00106 | Loss 0.2143
20:12:19.413265 [0] Epoch: 106, Train: 0.9517, Val: 0.9454, Test: 0.9453
20:12:20.134119 [0] Epoch 00107 | Loss 0.2135
20:12:20.137030 [0] Epoch: 107, Train: 0.9518, Val: 0.9454, Test: 0.9452
20:12:20.863635 [0] Epoch 00108 | Loss 0.2128
20:12:20.865168 [0] Epoch: 108, Train: 0.9520, Val: 0.9456, Test: 0.9453
20:12:21.585278 [0] Epoch 00109 | Loss 0.2121
20:12:21.586837 [0] Epoch: 109, Train: 0.9521, Val: 0.9454, Test: 0.9453
20:12:22.313510 [0] Epoch 00110 | Loss 0.2114
20:12:22.315131 [0] Epoch: 110, Train: 0.9522, Val: 0.9457, Test: 0.9454
20:12:23.035627 [0] Epoch 00111 | Loss 0.2107
20:12:23.038489 [0] Epoch: 111, Train: 0.9524, Val: 0.9458, Test: 0.9454
20:12:23.764373 [0] Epoch 00112 | Loss 0.2101
20:12:23.765901 [0] Epoch: 112, Train: 0.9525, Val: 0.9456, Test: 0.9454
20:12:24.486053 [0] Epoch 00113 | Loss 0.2094
20:12:24.488957 [0] Epoch: 113, Train: 0.9526, Val: 0.9454, Test: 0.9455
20:12:25.215916 [0] Epoch 00114 | Loss 0.2087
20:12:25.217449 [0] Epoch: 114, Train: 0.9527, Val: 0.9454, Test: 0.9455
20:12:25.937607 [0] Epoch 00115 | Loss 0.2081
20:12:25.939150 [0] Epoch: 115, Train: 0.9529, Val: 0.9456, Test: 0.9455
20:12:26.664463 [0] Epoch 00116 | Loss 0.2075
20:12:26.666007 [0] Epoch: 116, Train: 0.9530, Val: 0.9456, Test: 0.9455
20:12:27.386303 [0] Epoch 00117 | Loss 0.2068
20:12:27.387826 [0] Epoch: 117, Train: 0.9531, Val: 0.9457, Test: 0.9456
20:12:28.114420 [0] Epoch 00118 | Loss 0.2062
20:12:28.117418 [0] Epoch: 118, Train: 0.9533, Val: 0.9457, Test: 0.9459
20:12:28.838869 [0] Epoch 00119 | Loss 0.2056
20:12:28.840396 [0] Epoch: 119, Train: 0.9533, Val: 0.9459, Test: 0.9459
20:12:29.565746 [0] Epoch 00120 | Loss 0.2050
20:12:29.568678 [0] Epoch: 120, Train: 0.9534, Val: 0.9460, Test: 0.9459
20:12:30.296034 [0] Epoch 00121 | Loss 0.2044
20:12:30.298997 [0] Epoch: 121, Train: 0.9536, Val: 0.9460, Test: 0.9461
20:12:31.021233 [0] Epoch 00122 | Loss 0.2038
20:12:31.024206 [0] Epoch: 122, Train: 0.9536, Val: 0.9462, Test: 0.9460
20:12:31.750582 [0] Epoch 00123 | Loss 0.2032
20:12:31.752113 [0] Epoch: 123, Train: 0.9537, Val: 0.9462, Test: 0.9460
20:12:32.478205 [0] Epoch 00124 | Loss 0.2026
20:12:32.479765 [0] Epoch: 124, Train: 0.9538, Val: 0.9462, Test: 0.9460
20:12:33.206516 [0] Epoch 00125 | Loss 0.2020
20:12:33.209573 [0] Epoch: 125, Train: 0.9539, Val: 0.9464, Test: 0.9460
20:12:33.935635 [0] Epoch 00126 | Loss 0.2014
20:12:33.937171 [0] Epoch: 126, Train: 0.9540, Val: 0.9466, Test: 0.9460
20:12:34.663272 [0] Epoch 00127 | Loss 0.2009
20:12:34.664790 [0] Epoch: 127, Train: 0.9541, Val: 0.9467, Test: 0.9460
20:12:35.391402 [0] Epoch 00128 | Loss 0.2003
20:12:35.394414 [0] Epoch: 128, Train: 0.9543, Val: 0.9468, Test: 0.9459
20:12:36.121892 [0] Epoch 00129 | Loss 0.1998
20:12:36.124363 [0] Epoch: 129, Train: 0.9544, Val: 0.9467, Test: 0.9459
20:12:36.850568 [0] Epoch 00130 | Loss 0.1992
20:12:36.852116 [0] Epoch: 130, Train: 0.9545, Val: 0.9468, Test: 0.9458
20:12:37.577421 [0] Epoch 00131 | Loss 0.1987
20:12:37.578958 [0] Epoch: 131, Train: 0.9546, Val: 0.9468, Test: 0.9459
20:12:38.305333 [0] Epoch 00132 | Loss 0.1981
20:12:38.308325 [0] Epoch: 132, Train: 0.9547, Val: 0.9468, Test: 0.9459
20:12:39.035571 [0] Epoch 00133 | Loss 0.1976
20:12:39.038552 [0] Epoch: 133, Train: 0.9548, Val: 0.9467, Test: 0.9459
20:12:39.764880 [0] Epoch 00134 | Loss 0.1971
20:12:39.766413 [0] Epoch: 134, Train: 0.9549, Val: 0.9468, Test: 0.9460
20:12:40.494022 [0] Epoch 00135 | Loss 0.1965
20:12:40.497213 [0] Epoch: 135, Train: 0.9550, Val: 0.9468, Test: 0.9460
20:12:41.223901 [0] Epoch 00136 | Loss 0.1960
20:12:41.225464 [0] Epoch: 136, Train: 0.9551, Val: 0.9470, Test: 0.9461
20:12:41.952401 [0] Epoch 00137 | Loss 0.1955
20:12:41.955388 [0] Epoch: 137, Train: 0.9552, Val: 0.9470, Test: 0.9461
20:12:42.681845 [0] Epoch 00138 | Loss 0.1950
20:12:42.683393 [0] Epoch: 138, Train: 0.9553, Val: 0.9470, Test: 0.9462
20:12:43.409436 [0] Epoch 00139 | Loss 0.1945
20:12:43.410994 [0] Epoch: 139, Train: 0.9553, Val: 0.9473, Test: 0.9463
20:12:44.137367 [0] Epoch 00140 | Loss 0.1940
20:12:44.140337 [0] Epoch: 140, Train: 0.9554, Val: 0.9473, Test: 0.9463
20:12:44.866596 [0] Epoch 00141 | Loss 0.1935
20:12:44.868156 [0] Epoch: 141, Train: 0.9555, Val: 0.9474, Test: 0.9464
20:12:45.594289 [0] Epoch 00142 | Loss 0.1930
20:12:45.597267 [0] Epoch: 142, Train: 0.9556, Val: 0.9475, Test: 0.9464
20:12:46.323540 [0] Epoch 00143 | Loss 0.1925
20:12:46.326546 [0] Epoch: 143, Train: 0.9556, Val: 0.9475, Test: 0.9464
20:12:47.053328 [0] Epoch 00144 | Loss 0.1920
20:12:47.056281 [0] Epoch: 144, Train: 0.9556, Val: 0.9474, Test: 0.9464
20:12:47.782531 [0] Epoch 00145 | Loss 0.1916
20:12:47.784089 [0] Epoch: 145, Train: 0.9558, Val: 0.9474, Test: 0.9464
20:12:48.510374 [0] Epoch 00146 | Loss 0.1911
20:12:48.513324 [0] Epoch: 146, Train: 0.9558, Val: 0.9473, Test: 0.9463
20:12:49.240488 [0] Epoch 00147 | Loss 0.1906
20:12:49.242046 [0] Epoch: 147, Train: 0.9559, Val: 0.9472, Test: 0.9463
20:12:49.968290 [0] Epoch 00148 | Loss 0.1902
20:12:49.970052 [0] Epoch: 148, Train: 0.9560, Val: 0.9472, Test: 0.9463
20:12:50.696071 [0] Epoch 00149 | Loss 0.1897
20:12:50.699072 [0] Epoch: 149, Train: 0.9560, Val: 0.9472, Test: 0.9463
20:12:51.425445 [0] Epoch 00150 | Loss 0.1892
20:12:51.428393 [0] Epoch: 150, Train: 0.9561, Val: 0.9472, Test: 0.9462
20:12:52.156035 [0] Epoch 00151 | Loss 0.1888
20:12:52.158852 [0] Epoch: 151, Train: 0.9562, Val: 0.9471, Test: 0.9463
20:12:52.885077 [0] Epoch 00152 | Loss 0.1883
20:12:52.886621 [0] Epoch: 152, Train: 0.9563, Val: 0.9473, Test: 0.9463
20:12:53.612698 [0] Epoch 00153 | Loss 0.1879
20:12:53.615713 [0] Epoch: 153, Train: 0.9564, Val: 0.9472, Test: 0.9463
20:12:54.338720 [0] Epoch 00154 | Loss 0.1874
20:12:54.341681 [0] Epoch: 154, Train: 0.9565, Val: 0.9471, Test: 0.9463
20:12:55.068777 [0] Epoch 00155 | Loss 0.1870
20:12:55.071724 [0] Epoch: 155, Train: 0.9566, Val: 0.9471, Test: 0.9464
20:12:55.794501 [0] Epoch 00156 | Loss 0.1865
20:12:55.796102 [0] Epoch: 156, Train: 0.9567, Val: 0.9472, Test: 0.9464
20:12:56.523227 [0] Epoch 00157 | Loss 0.1861
20:12:56.526216 [0] Epoch: 157, Train: 0.9568, Val: 0.9472, Test: 0.9463
20:12:57.248694 [0] Epoch 00158 | Loss 0.1857
20:12:57.250308 [0] Epoch: 158, Train: 0.9568, Val: 0.9473, Test: 0.9463
20:12:57.976840 [0] Epoch 00159 | Loss 0.1852
20:12:57.979810 [0] Epoch: 159, Train: 0.9569, Val: 0.9471, Test: 0.9463
20:12:58.701960 [0] Epoch 00160 | Loss 0.1848
20:12:58.704878 [0] Epoch: 160, Train: 0.9569, Val: 0.9473, Test: 0.9463
20:12:59.432166 [0] Epoch 00161 | Loss 0.1844
20:12:59.435147 [0] Epoch: 161, Train: 0.9570, Val: 0.9472, Test: 0.9463
20:13:00.157349 [0] Epoch 00162 | Loss 0.1840
20:13:00.159779 [0] Epoch: 162, Train: 0.9571, Val: 0.9472, Test: 0.9464
20:13:00.885649 [0] Epoch 00163 | Loss 0.1835
20:13:00.887180 [0] Epoch: 163, Train: 0.9572, Val: 0.9471, Test: 0.9463
20:13:01.613044 [0] Epoch 00164 | Loss 0.1831
20:13:01.614548 [0] Epoch: 164, Train: 0.9573, Val: 0.9470, Test: 0.9464
20:13:02.342924 [0] Epoch 00165 | Loss 0.1827
20:13:02.345313 [0] Epoch: 165, Train: 0.9573, Val: 0.9470, Test: 0.9464
20:13:03.075587 [0] Epoch 00166 | Loss 0.1823
20:13:03.078768 [0] Epoch: 166, Train: 0.9574, Val: 0.9471, Test: 0.9465
20:13:03.805656 [0] Epoch 00167 | Loss 0.1819
20:13:03.807196 [0] Epoch: 167, Train: 0.9575, Val: 0.9470, Test: 0.9466
20:13:04.532587 [0] Epoch 00168 | Loss 0.1815
20:13:04.535401 [0] Epoch: 168, Train: 0.9577, Val: 0.9471, Test: 0.9466
20:13:05.262310 [0] Epoch 00169 | Loss 0.1811
20:13:05.263845 [0] Epoch: 169, Train: 0.9577, Val: 0.9471, Test: 0.9467
20:13:05.990552 [0] Epoch 00170 | Loss 0.1807
20:13:05.993492 [0] Epoch: 170, Train: 0.9578, Val: 0.9472, Test: 0.9468
20:13:06.720020 [0] Epoch 00171 | Loss 0.1803
20:13:06.721525 [0] Epoch: 171, Train: 0.9579, Val: 0.9473, Test: 0.9468
20:13:07.447034 [0] Epoch 00172 | Loss 0.1799
20:13:07.448542 [0] Epoch: 172, Train: 0.9580, Val: 0.9472, Test: 0.9467
20:13:08.174746 [0] Epoch 00173 | Loss 0.1795
20:13:08.177699 [0] Epoch: 173, Train: 0.9581, Val: 0.9472, Test: 0.9467
20:13:08.904569 [0] Epoch 00174 | Loss 0.1791
20:13:08.907482 [0] Epoch: 174, Train: 0.9581, Val: 0.9473, Test: 0.9468
20:13:09.633542 [0] Epoch 00175 | Loss 0.1787
20:13:09.635134 [0] Epoch: 175, Train: 0.9582, Val: 0.9473, Test: 0.9468
20:13:10.362755 [0] Epoch 00176 | Loss 0.1783
20:13:10.365784 [0] Epoch: 176, Train: 0.9582, Val: 0.9474, Test: 0.9468
20:13:11.093922 [0] Epoch 00177 | Loss 0.1779
20:13:11.096952 [0] Epoch: 177, Train: 0.9583, Val: 0.9474, Test: 0.9467
20:13:11.824008 [0] Epoch 00178 | Loss 0.1775
20:13:11.825546 [0] Epoch: 178, Train: 0.9583, Val: 0.9474, Test: 0.9468
20:13:12.552707 [0] Epoch 00179 | Loss 0.1772
20:13:12.555735 [0] Epoch: 179, Train: 0.9584, Val: 0.9474, Test: 0.9468
20:13:13.283192 [0] Epoch 00180 | Loss 0.1768
20:13:13.286364 [0] Epoch: 180, Train: 0.9585, Val: 0.9475, Test: 0.9468
20:13:14.013925 [0] Epoch 00181 | Loss 0.1764
20:13:14.016920 [0] Epoch: 181, Train: 0.9586, Val: 0.9476, Test: 0.9469
20:13:14.743543 [0] Epoch 00182 | Loss 0.1760
20:13:14.745065 [0] Epoch: 182, Train: 0.9587, Val: 0.9476, Test: 0.9469
20:13:15.470910 [0] Epoch 00183 | Loss 0.1757
20:13:15.473868 [0] Epoch: 183, Train: 0.9588, Val: 0.9475, Test: 0.9469
20:13:16.200806 [0] Epoch 00184 | Loss 0.1753
20:13:16.203753 [0] Epoch: 184, Train: 0.9588, Val: 0.9475, Test: 0.9470
20:13:16.930472 [0] Epoch 00185 | Loss 0.1749
20:13:16.932013 [0] Epoch: 185, Train: 0.9589, Val: 0.9475, Test: 0.9470
20:13:17.658828 [0] Epoch 00186 | Loss 0.1745
20:13:17.660392 [0] Epoch: 186, Train: 0.9590, Val: 0.9474, Test: 0.9471
20:13:18.387270 [0] Epoch 00187 | Loss 0.1742
20:13:18.390312 [0] Epoch: 187, Train: 0.9590, Val: 0.9474, Test: 0.9470
20:13:19.117960 [0] Epoch 00188 | Loss 0.1738
20:13:19.120953 [0] Epoch: 188, Train: 0.9592, Val: 0.9474, Test: 0.9470
20:13:19.846970 [0] Epoch 00189 | Loss 0.1735
20:13:19.848502 [0] Epoch: 189, Train: 0.9591, Val: 0.9475, Test: 0.9469
20:13:20.575382 [0] Epoch 00190 | Loss 0.1732
20:13:20.578339 [0] Epoch: 190, Train: 0.9593, Val: 0.9474, Test: 0.9469
20:13:21.304980 [0] Epoch 00191 | Loss 0.1730
20:13:21.306529 [0] Epoch: 191, Train: 0.9592, Val: 0.9477, Test: 0.9469
20:13:22.032839 [0] Epoch 00192 | Loss 0.1731
20:13:22.034402 [0] Epoch: 192, Train: 0.9591, Val: 0.9469, Test: 0.9464
20:13:22.761756 [0] Epoch 00193 | Loss 0.1729
20:13:22.764945 [0] Epoch: 193, Train: 0.9592, Val: 0.9474, Test: 0.9469
20:13:23.491922 [0] Epoch 00194 | Loss 0.1722
20:13:23.494924 [0] Epoch: 194, Train: 0.9595, Val: 0.9473, Test: 0.9468
20:13:24.222270 [0] Epoch 00195 | Loss 0.1714
20:13:24.224640 [0] Epoch: 195, Train: 0.9596, Val: 0.9475, Test: 0.9471
20:13:24.951996 [0] Epoch 00196 | Loss 0.1716
20:13:24.955013 [0] Epoch: 196, Train: 0.9595, Val: 0.9475, Test: 0.9469
20:13:25.681803 [0] Epoch 00197 | Loss 0.1713
20:13:25.683384 [0] Epoch: 197, Train: 0.9596, Val: 0.9471, Test: 0.9468
20:13:26.410215 [0] Epoch 00198 | Loss 0.1704
20:13:26.413136 [0] Epoch: 198, Train: 0.9598, Val: 0.9474, Test: 0.9471
20:13:27.140796 [0] Epoch 00199 | Loss 0.1705
20:13:27.143945 [0] Epoch: 199, Train: 0.9597, Val: 0.9478, Test: 0.9469
20:13:27.146099 [0] 
timer summary:
  0.28s   0.28s   200 broadcast ForwardL1 0
  0.41s   0.41s   800 broadcast
138.63s 138.63s   800 spmm
  3.05s   3.05s   800 mm
  0.02s   0.02s   200 broadcast ForwardL2 0
  0.05s   0.05s   200 broadcast BackwardL2 0
  0.08s   0.08s   400 all_reduce
  0.02s   0.02s   200 broadcast BackwardL1 0
146.82s 146.82s   200 epoch
155.89s 155.89s     1 total
20:40:17.615654 [0] proc begin: <DistEnv 0/1 nccl>
20:40:28.656730 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
20:40:28.664769 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from large pool |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2396 MiB |   2396 MiB |   2396 MiB |      0 B   |
|       from large pool |   2394 MiB |   2394 MiB |   2394 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 472519 KiB | 482086 KiB | 570319 KiB |  97800 KiB |
|       from large pool | 472519 KiB | 482086 KiB | 564170 KiB |  91651 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:40:30.565677 [0] Epoch 00000 | Loss 4.7410
20:40:30.592715 [0] Epoch: 000, Train: 0.0291, Val: 0.0280, Test: 0.0267
20:40:31.222739 [0] Epoch 00001 | Loss 3.9229
20:40:31.233657 [0] Epoch: 001, Train: 0.0636, Val: 0.0616, Test: 0.0512
20:40:31.862300 [0] Epoch 00002 | Loss 3.2705
20:40:31.873181 [0] Epoch: 002, Train: 0.2419, Val: 0.2230, Test: 0.1489
20:40:32.502705 [0] Epoch 00003 | Loss 2.8071
20:40:32.513705 [0] Epoch: 003, Train: 0.4116, Val: 0.3910, Test: 0.2628
20:40:33.143383 [0] Epoch 00004 | Loss 2.4711
20:40:33.153622 [0] Epoch: 004, Train: 0.5207, Val: 0.5033, Test: 0.3458
20:40:33.783425 [0] Epoch 00005 | Loss 2.2229
20:40:33.794447 [0] Epoch: 005, Train: 0.5857, Val: 0.5718, Test: 0.4060
20:40:34.423787 [0] Epoch 00006 | Loss 2.0293
20:40:34.434765 [0] Epoch: 006, Train: 0.6275, Val: 0.6158, Test: 0.4482
20:40:35.064015 [0] Epoch 00007 | Loss 1.8677
20:40:35.074921 [0] Epoch: 007, Train: 0.6613, Val: 0.6507, Test: 0.4781
20:40:35.704359 [0] Epoch 00008 | Loss 1.7264
20:40:35.715304 [0] Epoch: 008, Train: 0.6909, Val: 0.6792, Test: 0.5022
20:40:36.344553 [0] Epoch 00009 | Loss 1.6003
20:40:36.355553 [0] Epoch: 009, Train: 0.7199, Val: 0.7082, Test: 0.5240
20:40:36.984640 [0] Epoch 00010 | Loss 1.4872
20:40:36.995559 [0] Epoch: 010, Train: 0.7451, Val: 0.7331, Test: 0.5442
20:40:37.627611 [0] Epoch 00011 | Loss 1.3859
20:40:37.638405 [0] Epoch: 011, Train: 0.7662, Val: 0.7539, Test: 0.5626
20:40:38.268280 [0] Epoch 00012 | Loss 1.2961
20:40:38.279762 [0] Epoch: 012, Train: 0.7828, Val: 0.7717, Test: 0.5795
20:40:38.911742 [0] Epoch 00013 | Loss 1.2172
20:40:38.923786 [0] Epoch: 013, Train: 0.7969, Val: 0.7855, Test: 0.5946
20:40:39.554118 [0] Epoch 00014 | Loss 1.1486
20:40:39.565597 [0] Epoch: 014, Train: 0.8083, Val: 0.7969, Test: 0.6085
20:40:40.194944 [0] Epoch 00015 | Loss 1.0889
20:40:40.205471 [0] Epoch: 015, Train: 0.8165, Val: 0.8067, Test: 0.6201
20:40:40.835740 [0] Epoch 00016 | Loss 1.0369
20:40:40.846336 [0] Epoch: 016, Train: 0.8232, Val: 0.8128, Test: 0.6294
20:40:41.474946 [0] Epoch 00017 | Loss 0.9914
20:40:41.485307 [0] Epoch: 017, Train: 0.8291, Val: 0.8191, Test: 0.6373
20:40:42.115121 [0] Epoch 00018 | Loss 0.9510
20:40:42.126532 [0] Epoch: 018, Train: 0.8338, Val: 0.8245, Test: 0.6437
20:40:42.755505 [0] Epoch 00019 | Loss 0.9149
20:40:42.765731 [0] Epoch: 019, Train: 0.8380, Val: 0.8289, Test: 0.6491
20:40:43.393897 [0] Epoch 00020 | Loss 0.8822
20:40:43.404344 [0] Epoch: 020, Train: 0.8418, Val: 0.8331, Test: 0.6536
20:40:44.032557 [0] Epoch 00021 | Loss 0.8523
20:40:44.043800 [0] Epoch: 021, Train: 0.8454, Val: 0.8360, Test: 0.6577
20:40:44.673631 [0] Epoch 00022 | Loss 0.8247
20:40:44.684553 [0] Epoch: 022, Train: 0.8493, Val: 0.8385, Test: 0.6616
20:40:45.314522 [0] Epoch 00023 | Loss 0.7990
20:40:45.325904 [0] Epoch: 023, Train: 0.8524, Val: 0.8415, Test: 0.6650
20:40:45.955077 [0] Epoch 00024 | Loss 0.7752
20:40:45.966133 [0] Epoch: 024, Train: 0.8556, Val: 0.8445, Test: 0.6677
20:40:46.594692 [0] Epoch 00025 | Loss 0.7531
20:40:46.605064 [0] Epoch: 025, Train: 0.8581, Val: 0.8471, Test: 0.6704
20:40:47.235530 [0] Epoch 00026 | Loss 0.7329
20:40:47.246758 [0] Epoch: 026, Train: 0.8606, Val: 0.8497, Test: 0.6726
20:40:47.878354 [0] Epoch 00027 | Loss 0.7146
20:40:47.889547 [0] Epoch: 027, Train: 0.8623, Val: 0.8519, Test: 0.6746
20:40:48.520844 [0] Epoch 00028 | Loss 0.6980
20:40:48.532073 [0] Epoch: 028, Train: 0.8638, Val: 0.8539, Test: 0.6764
20:40:49.161894 [0] Epoch 00029 | Loss 0.6829
20:40:49.173042 [0] Epoch: 029, Train: 0.8647, Val: 0.8551, Test: 0.6779
20:40:49.803293 [0] Epoch 00030 | Loss 0.6688
20:40:49.814685 [0] Epoch: 030, Train: 0.8657, Val: 0.8563, Test: 0.6795
20:40:50.444518 [0] Epoch 00031 | Loss 0.6555
20:40:50.456066 [0] Epoch: 031, Train: 0.8667, Val: 0.8579, Test: 0.6810
20:40:51.086099 [0] Epoch 00032 | Loss 0.6426
20:40:51.097976 [0] Epoch: 032, Train: 0.8682, Val: 0.8587, Test: 0.6829
20:40:51.728015 [0] Epoch 00033 | Loss 0.6303
20:40:51.739572 [0] Epoch: 033, Train: 0.8699, Val: 0.8609, Test: 0.6851
20:40:52.369745 [0] Epoch 00034 | Loss 0.6187
20:40:52.381243 [0] Epoch: 034, Train: 0.8712, Val: 0.8627, Test: 0.6883
20:40:53.010663 [0] Epoch 00035 | Loss 0.6079
20:40:53.021137 [0] Epoch: 035, Train: 0.8728, Val: 0.8634, Test: 0.6917
20:40:53.650551 [0] Epoch 00036 | Loss 0.5981
20:40:53.661785 [0] Epoch: 036, Train: 0.8741, Val: 0.8635, Test: 0.6950
20:40:54.292234 [0] Epoch 00037 | Loss 0.5891
20:40:54.303498 [0] Epoch: 037, Train: 0.8753, Val: 0.8653, Test: 0.6982
20:40:54.933687 [0] Epoch 00038 | Loss 0.5808
20:40:54.944894 [0] Epoch: 038, Train: 0.8765, Val: 0.8669, Test: 0.7014
20:40:55.574190 [0] Epoch 00039 | Loss 0.5730
20:40:55.584484 [0] Epoch: 039, Train: 0.8774, Val: 0.8677, Test: 0.7044
20:40:56.213867 [0] Epoch 00040 | Loss 0.5656
20:40:56.225030 [0] Epoch: 040, Train: 0.8785, Val: 0.8687, Test: 0.7069
20:40:56.855071 [0] Epoch 00041 | Loss 0.5583
20:40:56.866066 [0] Epoch: 041, Train: 0.8797, Val: 0.8698, Test: 0.7092
20:40:57.495943 [0] Epoch 00042 | Loss 0.5511
20:40:57.506797 [0] Epoch: 042, Train: 0.8805, Val: 0.8708, Test: 0.7110
20:40:58.135718 [0] Epoch 00043 | Loss 0.5440
20:40:58.146562 [0] Epoch: 043, Train: 0.8816, Val: 0.8714, Test: 0.7124
20:40:58.775177 [0] Epoch 00044 | Loss 0.5371
20:40:58.785376 [0] Epoch: 044, Train: 0.8827, Val: 0.8733, Test: 0.7135
20:40:59.413833 [0] Epoch 00045 | Loss 0.5305
20:40:59.424004 [0] Epoch: 045, Train: 0.8836, Val: 0.8744, Test: 0.7145
20:41:00.053206 [0] Epoch 00046 | Loss 0.5242
20:41:00.064141 [0] Epoch: 046, Train: 0.8847, Val: 0.8752, Test: 0.7156
20:41:00.693192 [0] Epoch 00047 | Loss 0.5183
20:41:00.704083 [0] Epoch: 047, Train: 0.8854, Val: 0.8760, Test: 0.7163
20:41:01.333535 [0] Epoch 00048 | Loss 0.5127
20:41:01.344563 [0] Epoch: 048, Train: 0.8861, Val: 0.8772, Test: 0.7170
20:41:01.973227 [0] Epoch 00049 | Loss 0.5074
20:41:01.983360 [0] Epoch: 049, Train: 0.8870, Val: 0.8776, Test: 0.7175
20:41:02.616186 [0] Epoch 00050 | Loss 0.5023
20:41:02.626533 [0] Epoch: 050, Train: 0.8879, Val: 0.8784, Test: 0.7180
20:41:03.257711 [0] Epoch 00051 | Loss 0.4975
20:41:03.268507 [0] Epoch: 051, Train: 0.8887, Val: 0.8787, Test: 0.7186
20:41:03.899662 [0] Epoch 00052 | Loss 0.4927
20:41:03.911125 [0] Epoch: 052, Train: 0.8894, Val: 0.8798, Test: 0.7191
20:41:04.540876 [0] Epoch 00053 | Loss 0.4880
20:41:04.552702 [0] Epoch: 053, Train: 0.8904, Val: 0.8805, Test: 0.7196
20:41:05.181268 [0] Epoch 00054 | Loss 0.4835
20:41:05.192209 [0] Epoch: 054, Train: 0.8913, Val: 0.8814, Test: 0.7203
20:41:05.820529 [0] Epoch 00055 | Loss 0.4790
20:41:05.831553 [0] Epoch: 055, Train: 0.8917, Val: 0.8823, Test: 0.7210
20:41:06.461598 [0] Epoch 00056 | Loss 0.4748
20:41:06.473084 [0] Epoch: 056, Train: 0.8925, Val: 0.8831, Test: 0.7218
20:41:07.101920 [0] Epoch 00057 | Loss 0.4707
20:41:07.113498 [0] Epoch: 057, Train: 0.8933, Val: 0.8841, Test: 0.7228
20:41:07.744315 [0] Epoch 00058 | Loss 0.4669
20:41:07.755751 [0] Epoch: 058, Train: 0.8940, Val: 0.8845, Test: 0.7239
20:41:08.384920 [0] Epoch 00059 | Loss 0.4632
20:41:08.395580 [0] Epoch: 059, Train: 0.8946, Val: 0.8847, Test: 0.7251
20:41:09.025962 [0] Epoch 00060 | Loss 0.4597
20:41:09.037366 [0] Epoch: 060, Train: 0.8952, Val: 0.8851, Test: 0.7264
20:41:09.668017 [0] Epoch 00061 | Loss 0.4563
20:41:09.679268 [0] Epoch: 061, Train: 0.8959, Val: 0.8856, Test: 0.7276
20:41:10.309330 [0] Epoch 00062 | Loss 0.4530
20:41:10.320416 [0] Epoch: 062, Train: 0.8963, Val: 0.8863, Test: 0.7288
20:41:10.950276 [0] Epoch 00063 | Loss 0.4498
20:41:10.961361 [0] Epoch: 063, Train: 0.8968, Val: 0.8866, Test: 0.7300
20:41:11.591509 [0] Epoch 00064 | Loss 0.4468
20:41:11.602701 [0] Epoch: 064, Train: 0.8974, Val: 0.8869, Test: 0.7308
20:41:12.233378 [0] Epoch 00065 | Loss 0.4438
20:41:12.244545 [0] Epoch: 065, Train: 0.8979, Val: 0.8874, Test: 0.7316
20:41:12.876566 [0] Epoch 00066 | Loss 0.4409
20:41:12.887753 [0] Epoch: 066, Train: 0.8982, Val: 0.8878, Test: 0.7324
20:41:13.517612 [0] Epoch 00067 | Loss 0.4381
20:41:13.528849 [0] Epoch: 067, Train: 0.8986, Val: 0.8885, Test: 0.7330
20:41:14.159307 [0] Epoch 00068 | Loss 0.4353
20:41:14.171192 [0] Epoch: 068, Train: 0.8991, Val: 0.8887, Test: 0.7335
20:41:14.803609 [0] Epoch 00069 | Loss 0.4327
20:41:14.815211 [0] Epoch: 069, Train: 0.8995, Val: 0.8888, Test: 0.7339
20:41:15.445985 [0] Epoch 00070 | Loss 0.4301
20:41:15.457214 [0] Epoch: 070, Train: 0.8998, Val: 0.8891, Test: 0.7342
20:41:16.088403 [0] Epoch 00071 | Loss 0.4276
20:41:16.099571 [0] Epoch: 071, Train: 0.9001, Val: 0.8899, Test: 0.7344
20:41:16.729764 [0] Epoch 00072 | Loss 0.4252
20:41:16.740914 [0] Epoch: 072, Train: 0.9005, Val: 0.8900, Test: 0.7345
20:41:17.371243 [0] Epoch 00073 | Loss 0.4229
20:41:17.382384 [0] Epoch: 073, Train: 0.9008, Val: 0.8902, Test: 0.7346
20:41:18.012664 [0] Epoch 00074 | Loss 0.4207
20:41:18.024013 [0] Epoch: 074, Train: 0.9011, Val: 0.8906, Test: 0.7347
20:41:18.654484 [0] Epoch 00075 | Loss 0.4186
20:41:18.665953 [0] Epoch: 075, Train: 0.9014, Val: 0.8908, Test: 0.7348
20:41:19.295870 [0] Epoch 00076 | Loss 0.4165
20:41:19.307149 [0] Epoch: 076, Train: 0.9017, Val: 0.8913, Test: 0.7350
20:41:19.937418 [0] Epoch 00077 | Loss 0.4145
20:41:19.949207 [0] Epoch: 077, Train: 0.9019, Val: 0.8914, Test: 0.7351
20:41:20.579275 [0] Epoch 00078 | Loss 0.4125
20:41:20.590872 [0] Epoch: 078, Train: 0.9023, Val: 0.8915, Test: 0.7354
20:41:21.221885 [0] Epoch 00079 | Loss 0.4105
20:41:21.233411 [0] Epoch: 079, Train: 0.9026, Val: 0.8917, Test: 0.7357
20:41:21.864003 [0] Epoch 00080 | Loss 0.4086
20:41:21.874463 [0] Epoch: 080, Train: 0.9030, Val: 0.8919, Test: 0.7361
20:41:22.507127 [0] Epoch 00081 | Loss 0.4068
20:41:22.517823 [0] Epoch: 081, Train: 0.9033, Val: 0.8923, Test: 0.7366
20:41:23.148673 [0] Epoch 00082 | Loss 0.4050
20:41:23.159738 [0] Epoch: 082, Train: 0.9036, Val: 0.8927, Test: 0.7371
20:41:23.790301 [0] Epoch 00083 | Loss 0.4032
20:41:23.800650 [0] Epoch: 083, Train: 0.9038, Val: 0.8929, Test: 0.7376
20:41:24.431542 [0] Epoch 00084 | Loss 0.4015
20:41:24.442972 [0] Epoch: 084, Train: 0.9039, Val: 0.8931, Test: 0.7381
20:41:25.072540 [0] Epoch 00085 | Loss 0.3998
20:41:25.083304 [0] Epoch: 085, Train: 0.9040, Val: 0.8935, Test: 0.7386
20:41:25.712447 [0] Epoch 00086 | Loss 0.3981
20:41:25.723731 [0] Epoch: 086, Train: 0.9043, Val: 0.8937, Test: 0.7390
20:41:26.352775 [0] Epoch 00087 | Loss 0.3965
20:41:26.364364 [0] Epoch: 087, Train: 0.9045, Val: 0.8939, Test: 0.7394
20:41:26.996841 [0] Epoch 00088 | Loss 0.3949
20:41:27.008322 [0] Epoch: 088, Train: 0.9047, Val: 0.8945, Test: 0.7397
20:41:27.638031 [0] Epoch 00089 | Loss 0.3933
20:41:27.649384 [0] Epoch: 089, Train: 0.9049, Val: 0.8948, Test: 0.7399
20:41:28.278538 [0] Epoch 00090 | Loss 0.3918
20:41:28.289870 [0] Epoch: 090, Train: 0.9052, Val: 0.8949, Test: 0.7401
20:41:28.919286 [0] Epoch 00091 | Loss 0.3903
20:41:28.930544 [0] Epoch: 091, Train: 0.9055, Val: 0.8950, Test: 0.7402
20:41:29.560106 [0] Epoch 00092 | Loss 0.3888
20:41:29.571289 [0] Epoch: 092, Train: 0.9057, Val: 0.8953, Test: 0.7403
20:41:30.200741 [0] Epoch 00093 | Loss 0.3874
20:41:30.211847 [0] Epoch: 093, Train: 0.9059, Val: 0.8955, Test: 0.7404
20:41:30.841026 [0] Epoch 00094 | Loss 0.3860
20:41:30.852079 [0] Epoch: 094, Train: 0.9062, Val: 0.8959, Test: 0.7405
20:41:31.481718 [0] Epoch 00095 | Loss 0.3846
20:41:31.492766 [0] Epoch: 095, Train: 0.9064, Val: 0.8960, Test: 0.7406
20:41:32.122285 [0] Epoch 00096 | Loss 0.3833
20:41:32.133312 [0] Epoch: 096, Train: 0.9067, Val: 0.8962, Test: 0.7407
20:41:32.762740 [0] Epoch 00097 | Loss 0.3819
20:41:32.773735 [0] Epoch: 097, Train: 0.9071, Val: 0.8964, Test: 0.7408
20:41:33.403301 [0] Epoch 00098 | Loss 0.3806
20:41:33.414320 [0] Epoch: 098, Train: 0.9073, Val: 0.8967, Test: 0.7409
20:41:34.043563 [0] Epoch 00099 | Loss 0.3793
20:41:34.053952 [0] Epoch: 099, Train: 0.9076, Val: 0.8969, Test: 0.7410
20:41:34.683587 [0] Epoch 00100 | Loss 0.3781
20:41:34.694563 [0] Epoch: 100, Train: 0.9080, Val: 0.8970, Test: 0.7411
20:41:35.325009 [0] Epoch 00101 | Loss 0.3768
20:41:35.336202 [0] Epoch: 101, Train: 0.9083, Val: 0.8972, Test: 0.7413
20:41:35.964865 [0] Epoch 00102 | Loss 0.3756
20:41:35.974936 [0] Epoch: 102, Train: 0.9087, Val: 0.8972, Test: 0.7416
20:41:36.604032 [0] Epoch 00103 | Loss 0.3744
20:41:36.614882 [0] Epoch: 103, Train: 0.9089, Val: 0.8975, Test: 0.7419
20:41:37.244255 [0] Epoch 00104 | Loss 0.3732
20:41:37.255239 [0] Epoch: 104, Train: 0.9092, Val: 0.8976, Test: 0.7422
20:41:37.884827 [0] Epoch 00105 | Loss 0.3721
20:41:37.895040 [0] Epoch: 105, Train: 0.9094, Val: 0.8978, Test: 0.7426
20:41:38.524648 [0] Epoch 00106 | Loss 0.3709
20:41:38.535668 [0] Epoch: 106, Train: 0.9096, Val: 0.8979, Test: 0.7428
20:41:39.165162 [0] Epoch 00107 | Loss 0.3698
20:41:39.175558 [0] Epoch: 107, Train: 0.9098, Val: 0.8981, Test: 0.7430
20:41:39.807414 [0] Epoch 00108 | Loss 0.3687
20:41:39.819015 [0] Epoch: 108, Train: 0.9100, Val: 0.8983, Test: 0.7432
20:41:40.450988 [0] Epoch 00109 | Loss 0.3677
20:41:40.462821 [0] Epoch: 109, Train: 0.9102, Val: 0.8984, Test: 0.7434
20:41:41.093477 [0] Epoch 00110 | Loss 0.3666
20:41:41.104308 [0] Epoch: 110, Train: 0.9104, Val: 0.8986, Test: 0.7435
20:41:41.734666 [0] Epoch 00111 | Loss 0.3655
20:41:41.745459 [0] Epoch: 111, Train: 0.9106, Val: 0.8987, Test: 0.7436
20:41:42.376884 [0] Epoch 00112 | Loss 0.3645
20:41:42.388191 [0] Epoch: 112, Train: 0.9109, Val: 0.8987, Test: 0.7438
20:41:43.019784 [0] Epoch 00113 | Loss 0.3635
20:41:43.030937 [0] Epoch: 113, Train: 0.9111, Val: 0.8989, Test: 0.7439
20:41:43.661889 [0] Epoch 00114 | Loss 0.3625
20:41:43.672971 [0] Epoch: 114, Train: 0.9113, Val: 0.8990, Test: 0.7441
20:41:44.303864 [0] Epoch 00115 | Loss 0.3615
20:41:44.315185 [0] Epoch: 115, Train: 0.9115, Val: 0.8992, Test: 0.7442
20:41:44.946588 [0] Epoch 00116 | Loss 0.3605
20:41:44.958321 [0] Epoch: 116, Train: 0.9118, Val: 0.8995, Test: 0.7445
20:41:45.588994 [0] Epoch 00117 | Loss 0.3595
20:41:45.600701 [0] Epoch: 117, Train: 0.9120, Val: 0.8996, Test: 0.7447
20:41:46.231301 [0] Epoch 00118 | Loss 0.3586
20:41:46.242823 [0] Epoch: 118, Train: 0.9122, Val: 0.8999, Test: 0.7449
20:41:46.873290 [0] Epoch 00119 | Loss 0.3576
20:41:46.884673 [0] Epoch: 119, Train: 0.9123, Val: 0.8999, Test: 0.7451
20:41:47.515498 [0] Epoch 00120 | Loss 0.3567
20:41:47.526754 [0] Epoch: 120, Train: 0.9125, Val: 0.8999, Test: 0.7453
20:41:48.157635 [0] Epoch 00121 | Loss 0.3558
20:41:48.168304 [0] Epoch: 121, Train: 0.9127, Val: 0.9003, Test: 0.7454
20:41:48.799107 [0] Epoch 00122 | Loss 0.3549
20:41:48.810176 [0] Epoch: 122, Train: 0.9129, Val: 0.9005, Test: 0.7456
20:41:49.440764 [0] Epoch 00123 | Loss 0.3540
20:41:49.451699 [0] Epoch: 123, Train: 0.9131, Val: 0.9008, Test: 0.7457
20:41:50.083697 [0] Epoch 00124 | Loss 0.3531
20:41:50.095344 [0] Epoch: 124, Train: 0.9133, Val: 0.9009, Test: 0.7459
20:41:50.725883 [0] Epoch 00125 | Loss 0.3522
20:41:50.737408 [0] Epoch: 125, Train: 0.9135, Val: 0.9012, Test: 0.7460
20:41:51.368149 [0] Epoch 00126 | Loss 0.3514
20:41:51.379449 [0] Epoch: 126, Train: 0.9137, Val: 0.9012, Test: 0.7462
20:41:52.011347 [0] Epoch 00127 | Loss 0.3505
20:41:52.023159 [0] Epoch: 127, Train: 0.9139, Val: 0.9013, Test: 0.7463
20:41:52.654421 [0] Epoch 00128 | Loss 0.3497
20:41:52.665836 [0] Epoch: 128, Train: 0.9141, Val: 0.9014, Test: 0.7465
20:41:53.296982 [0] Epoch 00129 | Loss 0.3489
20:41:53.308180 [0] Epoch: 129, Train: 0.9143, Val: 0.9013, Test: 0.7466
20:41:53.938906 [0] Epoch 00130 | Loss 0.3480
20:41:53.950055 [0] Epoch: 130, Train: 0.9144, Val: 0.9013, Test: 0.7467
20:41:54.580866 [0] Epoch 00131 | Loss 0.3472
20:41:54.592450 [0] Epoch: 131, Train: 0.9145, Val: 0.9017, Test: 0.7468
20:41:55.223326 [0] Epoch 00132 | Loss 0.3464
20:41:55.235047 [0] Epoch: 132, Train: 0.9146, Val: 0.9017, Test: 0.7469
20:41:55.865889 [0] Epoch 00133 | Loss 0.3456
20:41:55.877551 [0] Epoch: 133, Train: 0.9147, Val: 0.9019, Test: 0.7470
20:41:56.508146 [0] Epoch 00134 | Loss 0.3448
20:41:56.518944 [0] Epoch: 134, Train: 0.9148, Val: 0.9022, Test: 0.7472
20:41:57.150110 [0] Epoch 00135 | Loss 0.3441
20:41:57.161295 [0] Epoch: 135, Train: 0.9150, Val: 0.9022, Test: 0.7473
20:41:57.792039 [0] Epoch 00136 | Loss 0.3433
20:41:57.802595 [0] Epoch: 136, Train: 0.9151, Val: 0.9024, Test: 0.7474
20:41:58.433630 [0] Epoch 00137 | Loss 0.3425
20:41:58.444862 [0] Epoch: 137, Train: 0.9151, Val: 0.9025, Test: 0.7475
20:41:59.076520 [0] Epoch 00138 | Loss 0.3418
20:41:59.087618 [0] Epoch: 138, Train: 0.9152, Val: 0.9026, Test: 0.7475
20:41:59.718405 [0] Epoch 00139 | Loss 0.3410
20:41:59.729865 [0] Epoch: 139, Train: 0.9154, Val: 0.9028, Test: 0.7476
20:42:00.360680 [0] Epoch 00140 | Loss 0.3403
20:42:00.372182 [0] Epoch: 140, Train: 0.9155, Val: 0.9029, Test: 0.7476
20:42:01.002492 [0] Epoch 00141 | Loss 0.3395
20:42:01.013805 [0] Epoch: 141, Train: 0.9157, Val: 0.9029, Test: 0.7477
20:42:01.644781 [0] Epoch 00142 | Loss 0.3388
20:42:01.656143 [0] Epoch: 142, Train: 0.9159, Val: 0.9032, Test: 0.7478
20:42:02.289658 [0] Epoch 00143 | Loss 0.3381
20:42:02.300443 [0] Epoch: 143, Train: 0.9160, Val: 0.9033, Test: 0.7478
20:42:02.934450 [0] Epoch 00144 | Loss 0.3374
20:42:02.944936 [0] Epoch: 144, Train: 0.9161, Val: 0.9034, Test: 0.7479
20:42:03.578726 [0] Epoch 00145 | Loss 0.3367
20:42:03.589860 [0] Epoch: 145, Train: 0.9162, Val: 0.9035, Test: 0.7479
20:42:04.222978 [0] Epoch 00146 | Loss 0.3360
20:42:04.234968 [0] Epoch: 146, Train: 0.9165, Val: 0.9036, Test: 0.7480
20:42:04.867425 [0] Epoch 00147 | Loss 0.3353
20:42:04.878564 [0] Epoch: 147, Train: 0.9166, Val: 0.9037, Test: 0.7481
20:42:05.509326 [0] Epoch 00148 | Loss 0.3346
20:42:05.520054 [0] Epoch: 148, Train: 0.9167, Val: 0.9038, Test: 0.7482
20:42:06.151175 [0] Epoch 00149 | Loss 0.3339
20:42:06.162240 [0] Epoch: 149, Train: 0.9169, Val: 0.9039, Test: 0.7482
20:42:06.793301 [0] Epoch 00150 | Loss 0.3332
20:42:06.804354 [0] Epoch: 150, Train: 0.9169, Val: 0.9038, Test: 0.7483
20:42:07.434994 [0] Epoch 00151 | Loss 0.3326
20:42:07.446243 [0] Epoch: 151, Train: 0.9170, Val: 0.9039, Test: 0.7484
20:42:08.076981 [0] Epoch 00152 | Loss 0.3319
20:42:08.087807 [0] Epoch: 152, Train: 0.9171, Val: 0.9040, Test: 0.7484
20:42:08.720236 [0] Epoch 00153 | Loss 0.3312
20:42:08.732411 [0] Epoch: 153, Train: 0.9172, Val: 0.9041, Test: 0.7485
20:42:09.364115 [0] Epoch 00154 | Loss 0.3306
20:42:09.375750 [0] Epoch: 154, Train: 0.9174, Val: 0.9041, Test: 0.7486
20:42:10.007320 [0] Epoch 00155 | Loss 0.3299
20:42:10.017987 [0] Epoch: 155, Train: 0.9175, Val: 0.9043, Test: 0.7486
20:42:10.649335 [0] Epoch 00156 | Loss 0.3293
20:42:10.660590 [0] Epoch: 156, Train: 0.9176, Val: 0.9044, Test: 0.7487
20:42:11.291951 [0] Epoch 00157 | Loss 0.3287
20:42:11.302751 [0] Epoch: 157, Train: 0.9178, Val: 0.9044, Test: 0.7488
20:42:11.934873 [0] Epoch 00158 | Loss 0.3280
20:42:11.946451 [0] Epoch: 158, Train: 0.9179, Val: 0.9044, Test: 0.7488
20:42:12.578401 [0] Epoch 00159 | Loss 0.3274
20:42:12.589692 [0] Epoch: 159, Train: 0.9180, Val: 0.9046, Test: 0.7489
20:42:13.221344 [0] Epoch 00160 | Loss 0.3268
20:42:13.233116 [0] Epoch: 160, Train: 0.9181, Val: 0.9045, Test: 0.7489
20:42:13.864776 [0] Epoch 00161 | Loss 0.3262
20:42:13.875805 [0] Epoch: 161, Train: 0.9183, Val: 0.9045, Test: 0.7490
20:42:14.506970 [0] Epoch 00162 | Loss 0.3255
20:42:14.517635 [0] Epoch: 162, Train: 0.9184, Val: 0.9047, Test: 0.7490
20:42:15.149419 [0] Epoch 00163 | Loss 0.3249
20:42:15.160513 [0] Epoch: 163, Train: 0.9186, Val: 0.9047, Test: 0.7491
20:42:15.793527 [0] Epoch 00164 | Loss 0.3243
20:42:15.805057 [0] Epoch: 164, Train: 0.9187, Val: 0.9049, Test: 0.7491
20:42:16.436702 [0] Epoch 00165 | Loss 0.3237
20:42:16.448583 [0] Epoch: 165, Train: 0.9189, Val: 0.9050, Test: 0.7492
20:42:17.080660 [0] Epoch 00166 | Loss 0.3231
20:42:17.092376 [0] Epoch: 166, Train: 0.9190, Val: 0.9052, Test: 0.7492
20:42:17.723664 [0] Epoch 00167 | Loss 0.3225
20:42:17.734960 [0] Epoch: 167, Train: 0.9191, Val: 0.9052, Test: 0.7493
20:42:18.365964 [0] Epoch 00168 | Loss 0.3220
20:42:18.377211 [0] Epoch: 168, Train: 0.9192, Val: 0.9052, Test: 0.7493
20:42:19.008011 [0] Epoch 00169 | Loss 0.3214
20:42:19.019175 [0] Epoch: 169, Train: 0.9194, Val: 0.9053, Test: 0.7494
20:42:19.649812 [0] Epoch 00170 | Loss 0.3208
20:42:19.660910 [0] Epoch: 170, Train: 0.9195, Val: 0.9052, Test: 0.7494
20:42:20.291175 [0] Epoch 00171 | Loss 0.3202
20:42:20.302473 [0] Epoch: 171, Train: 0.9196, Val: 0.9053, Test: 0.7494
20:42:20.933475 [0] Epoch 00172 | Loss 0.3197
20:42:20.945114 [0] Epoch: 172, Train: 0.9197, Val: 0.9053, Test: 0.7495
20:42:21.576413 [0] Epoch 00173 | Loss 0.3191
20:42:21.588459 [0] Epoch: 173, Train: 0.9197, Val: 0.9056, Test: 0.7495
20:42:22.219413 [0] Epoch 00174 | Loss 0.3185
20:42:22.231064 [0] Epoch: 174, Train: 0.9198, Val: 0.9056, Test: 0.7496
20:42:22.862097 [0] Epoch 00175 | Loss 0.3180
20:42:22.873506 [0] Epoch: 175, Train: 0.9199, Val: 0.9057, Test: 0.7496
20:42:23.505953 [0] Epoch 00176 | Loss 0.3174
20:42:23.517405 [0] Epoch: 176, Train: 0.9201, Val: 0.9058, Test: 0.7496
20:42:24.149000 [0] Epoch 00177 | Loss 0.3169
20:42:24.160179 [0] Epoch: 177, Train: 0.9201, Val: 0.9055, Test: 0.7496
20:42:24.790672 [0] Epoch 00178 | Loss 0.3163
20:42:24.802170 [0] Epoch: 178, Train: 0.9202, Val: 0.9057, Test: 0.7497
20:42:25.432910 [0] Epoch 00179 | Loss 0.3158
20:42:25.444583 [0] Epoch: 179, Train: 0.9203, Val: 0.9055, Test: 0.7497
20:42:26.075204 [0] Epoch 00180 | Loss 0.3152
20:42:26.086469 [0] Epoch: 180, Train: 0.9205, Val: 0.9055, Test: 0.7497
20:42:26.717584 [0] Epoch 00181 | Loss 0.3147
20:42:26.729566 [0] Epoch: 181, Train: 0.9205, Val: 0.9056, Test: 0.7497
20:42:27.361104 [0] Epoch 00182 | Loss 0.3142
20:42:27.372499 [0] Epoch: 182, Train: 0.9206, Val: 0.9058, Test: 0.7498
20:42:28.003710 [0] Epoch 00183 | Loss 0.3136
20:42:28.014948 [0] Epoch: 183, Train: 0.9207, Val: 0.9058, Test: 0.7498
20:42:28.645877 [0] Epoch 00184 | Loss 0.3131
20:42:28.656956 [0] Epoch: 184, Train: 0.9208, Val: 0.9059, Test: 0.7498
20:42:29.287630 [0] Epoch 00185 | Loss 0.3126
20:42:29.298740 [0] Epoch: 185, Train: 0.9209, Val: 0.9060, Test: 0.7498
20:42:29.929423 [0] Epoch 00186 | Loss 0.3121
20:42:29.940982 [0] Epoch: 186, Train: 0.9210, Val: 0.9062, Test: 0.7498
20:42:30.571130 [0] Epoch 00187 | Loss 0.3115
20:42:30.582824 [0] Epoch: 187, Train: 0.9211, Val: 0.9063, Test: 0.7499
20:42:31.213543 [0] Epoch 00188 | Loss 0.3110
20:42:31.225484 [0] Epoch: 188, Train: 0.9213, Val: 0.9063, Test: 0.7499
20:42:31.855371 [0] Epoch 00189 | Loss 0.3105
20:42:31.867004 [0] Epoch: 189, Train: 0.9214, Val: 0.9063, Test: 0.7499
20:42:32.498412 [0] Epoch 00190 | Loss 0.3100
20:42:32.509898 [0] Epoch: 190, Train: 0.9215, Val: 0.9064, Test: 0.7500
20:42:33.140967 [0] Epoch 00191 | Loss 0.3095
20:42:33.152248 [0] Epoch: 191, Train: 0.9216, Val: 0.9066, Test: 0.7500
20:42:33.783334 [0] Epoch 00192 | Loss 0.3090
20:42:33.794497 [0] Epoch: 192, Train: 0.9217, Val: 0.9067, Test: 0.7500
20:42:34.425460 [0] Epoch 00193 | Loss 0.3085
20:42:34.436472 [0] Epoch: 193, Train: 0.9218, Val: 0.9068, Test: 0.7501
20:42:35.067702 [0] Epoch 00194 | Loss 0.3080
20:42:35.078894 [0] Epoch: 194, Train: 0.9218, Val: 0.9067, Test: 0.7501
20:42:35.709497 [0] Epoch 00195 | Loss 0.3075
20:42:35.720644 [0] Epoch: 195, Train: 0.9220, Val: 0.9067, Test: 0.7501
20:42:36.351158 [0] Epoch 00196 | Loss 0.3070
20:42:36.363119 [0] Epoch: 196, Train: 0.9221, Val: 0.9068, Test: 0.7501
20:42:36.993928 [0] Epoch 00197 | Loss 0.3066
20:42:37.005603 [0] Epoch: 197, Train: 0.9222, Val: 0.9068, Test: 0.7501
20:42:37.636867 [0] Epoch 00198 | Loss 0.3061
20:42:37.648284 [0] Epoch: 198, Train: 0.9224, Val: 0.9069, Test: 0.7501
20:42:38.279502 [0] Epoch 00199 | Loss 0.3056
20:42:38.290733 [0] Epoch: 199, Train: 0.9224, Val: 0.9072, Test: 0.7502
20:42:38.291852 [0] 
timer summary:
  0.25s   0.25s   200 broadcast ForwardL1 0
  0.36s   0.36s   800 broadcast
103.00s 103.00s   800 spmm
  9.49s   9.49s   800 mm
  0.03s   0.03s   200 broadcast ForwardL2 0
  0.04s   0.04s   200 broadcast BackwardL2 0
  0.11s   0.11s   400 all_reduce
  0.02s   0.02s   200 broadcast BackwardL1 0
129.18s 129.18s   200 epoch
140.68s 140.68s     1 total
20:51:47.032696 [0] proc begin: <DistEnv 0/1 nccl>
20:51:57.902986 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
20:51:57.911233 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from large pool |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2396 MiB |   2396 MiB |   2396 MiB |      0 B   |
|       from large pool |   2394 MiB |   2394 MiB |   2394 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 472519 KiB | 482086 KiB | 570319 KiB |  97800 KiB |
|       from large pool | 472519 KiB | 482086 KiB | 564170 KiB |  91651 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

20:51:59.965603 [0] Epoch 00000 | Loss 4.7410
20:52:00.000423 [0] Epoch: 000, Train: 0.0291, Val: 0.0280, Test: 0.0267
20:52:00.630357 [0] Epoch 00001 | Loss 3.9229
20:52:00.641618 [0] Epoch: 001, Train: 0.0636, Val: 0.0616, Test: 0.0512
20:52:01.271039 [0] Epoch 00002 | Loss 3.2705
20:52:01.281897 [0] Epoch: 002, Train: 0.2419, Val: 0.2230, Test: 0.1489
20:52:01.912242 [0] Epoch 00003 | Loss 2.8071
20:52:01.922923 [0] Epoch: 003, Train: 0.4116, Val: 0.3910, Test: 0.2628
20:52:02.555536 [0] Epoch 00004 | Loss 2.4711
20:52:02.566016 [0] Epoch: 004, Train: 0.5207, Val: 0.5033, Test: 0.3458
20:52:03.199684 [0] Epoch 00005 | Loss 2.2229
20:52:03.210856 [0] Epoch: 005, Train: 0.5857, Val: 0.5718, Test: 0.4060
20:52:03.840586 [0] Epoch 00006 | Loss 2.0293
20:52:03.851998 [0] Epoch: 006, Train: 0.6275, Val: 0.6158, Test: 0.4482
20:52:04.481108 [0] Epoch 00007 | Loss 1.8677
20:52:04.492009 [0] Epoch: 007, Train: 0.6613, Val: 0.6507, Test: 0.4781
20:52:05.125023 [0] Epoch 00008 | Loss 1.7264
20:52:05.135881 [0] Epoch: 008, Train: 0.6909, Val: 0.6792, Test: 0.5022
20:52:05.773140 [0] Epoch 00009 | Loss 1.6003
20:52:05.784132 [0] Epoch: 009, Train: 0.7199, Val: 0.7082, Test: 0.5240
20:52:06.415250 [0] Epoch 00010 | Loss 1.4872
20:52:06.426543 [0] Epoch: 010, Train: 0.7451, Val: 0.7331, Test: 0.5442
20:52:07.058176 [0] Epoch 00011 | Loss 1.3859
20:52:07.069537 [0] Epoch: 011, Train: 0.7662, Val: 0.7539, Test: 0.5626
20:52:07.701502 [0] Epoch 00012 | Loss 1.2961
20:52:07.712833 [0] Epoch: 012, Train: 0.7828, Val: 0.7717, Test: 0.5795
20:52:08.344262 [0] Epoch 00013 | Loss 1.2172
20:52:08.355156 [0] Epoch: 013, Train: 0.7969, Val: 0.7855, Test: 0.5946
20:52:08.987161 [0] Epoch 00014 | Loss 1.1486
20:52:08.999086 [0] Epoch: 014, Train: 0.8083, Val: 0.7969, Test: 0.6085
20:52:09.630175 [0] Epoch 00015 | Loss 1.0889
20:52:09.641330 [0] Epoch: 015, Train: 0.8165, Val: 0.8067, Test: 0.6201
20:52:10.273047 [0] Epoch 00016 | Loss 1.0369
20:52:10.283471 [0] Epoch: 016, Train: 0.8232, Val: 0.8128, Test: 0.6294
20:52:10.913525 [0] Epoch 00017 | Loss 0.9914
20:52:10.924480 [0] Epoch: 017, Train: 0.8291, Val: 0.8191, Test: 0.6373
20:52:11.553556 [0] Epoch 00018 | Loss 0.9510
20:52:11.563570 [0] Epoch: 018, Train: 0.8338, Val: 0.8245, Test: 0.6437
20:52:12.193651 [0] Epoch 00019 | Loss 0.9149
20:52:12.204748 [0] Epoch: 019, Train: 0.8380, Val: 0.8289, Test: 0.6491
20:52:12.833467 [0] Epoch 00020 | Loss 0.8822
20:52:12.844464 [0] Epoch: 020, Train: 0.8418, Val: 0.8331, Test: 0.6536
20:52:13.472858 [0] Epoch 00021 | Loss 0.8523
20:52:13.483675 [0] Epoch: 021, Train: 0.8454, Val: 0.8360, Test: 0.6577
20:52:14.112579 [0] Epoch 00022 | Loss 0.8247
20:52:14.123552 [0] Epoch: 022, Train: 0.8493, Val: 0.8385, Test: 0.6616
20:52:14.752013 [0] Epoch 00023 | Loss 0.7990
20:52:14.762054 [0] Epoch: 023, Train: 0.8524, Val: 0.8415, Test: 0.6650
20:52:15.392021 [0] Epoch 00024 | Loss 0.7752
20:52:15.403076 [0] Epoch: 024, Train: 0.8556, Val: 0.8445, Test: 0.6677
20:52:16.032341 [0] Epoch 00025 | Loss 0.7531
20:52:16.042489 [0] Epoch: 025, Train: 0.8581, Val: 0.8471, Test: 0.6704
20:52:16.670745 [0] Epoch 00026 | Loss 0.7329
20:52:16.680805 [0] Epoch: 026, Train: 0.8606, Val: 0.8497, Test: 0.6726
20:52:17.309695 [0] Epoch 00027 | Loss 0.7146
20:52:17.320162 [0] Epoch: 027, Train: 0.8623, Val: 0.8519, Test: 0.6746
20:52:17.948956 [0] Epoch 00028 | Loss 0.6980
20:52:17.959862 [0] Epoch: 028, Train: 0.8638, Val: 0.8539, Test: 0.6764
20:52:18.588089 [0] Epoch 00029 | Loss 0.6829
20:52:18.598188 [0] Epoch: 029, Train: 0.8647, Val: 0.8551, Test: 0.6779
20:52:19.228039 [0] Epoch 00030 | Loss 0.6688
20:52:19.239265 [0] Epoch: 030, Train: 0.8657, Val: 0.8563, Test: 0.6795
20:52:19.867739 [0] Epoch 00031 | Loss 0.6555
20:52:19.877819 [0] Epoch: 031, Train: 0.8667, Val: 0.8579, Test: 0.6810
20:52:20.506683 [0] Epoch 00032 | Loss 0.6426
20:52:20.517618 [0] Epoch: 032, Train: 0.8682, Val: 0.8587, Test: 0.6829
20:52:21.146683 [0] Epoch 00033 | Loss 0.6303
20:52:21.157784 [0] Epoch: 033, Train: 0.8699, Val: 0.8609, Test: 0.6851
20:52:21.786303 [0] Epoch 00034 | Loss 0.6187
20:52:21.796424 [0] Epoch: 034, Train: 0.8712, Val: 0.8627, Test: 0.6883
20:52:22.425256 [0] Epoch 00035 | Loss 0.6079
20:52:22.435876 [0] Epoch: 035, Train: 0.8728, Val: 0.8634, Test: 0.6917
20:52:23.064380 [0] Epoch 00036 | Loss 0.5981
20:52:23.074663 [0] Epoch: 036, Train: 0.8741, Val: 0.8635, Test: 0.6950
20:52:23.704216 [0] Epoch 00037 | Loss 0.5891
20:52:23.714625 [0] Epoch: 037, Train: 0.8753, Val: 0.8653, Test: 0.6982
20:52:24.343679 [0] Epoch 00038 | Loss 0.5808
20:52:24.354836 [0] Epoch: 038, Train: 0.8765, Val: 0.8669, Test: 0.7014
20:52:24.984549 [0] Epoch 00039 | Loss 0.5730
20:52:24.995735 [0] Epoch: 039, Train: 0.8774, Val: 0.8677, Test: 0.7044
20:52:25.624317 [0] Epoch 00040 | Loss 0.5656
20:52:25.634524 [0] Epoch: 040, Train: 0.8785, Val: 0.8687, Test: 0.7069
20:52:26.263014 [0] Epoch 00041 | Loss 0.5583
20:52:26.274133 [0] Epoch: 041, Train: 0.8797, Val: 0.8698, Test: 0.7092
20:52:26.903255 [0] Epoch 00042 | Loss 0.5511
20:52:26.914501 [0] Epoch: 042, Train: 0.8805, Val: 0.8708, Test: 0.7110
20:52:27.543328 [0] Epoch 00043 | Loss 0.5440
20:52:27.553519 [0] Epoch: 043, Train: 0.8816, Val: 0.8714, Test: 0.7124
20:52:28.181625 [0] Epoch 00044 | Loss 0.5371
20:52:28.192752 [0] Epoch: 044, Train: 0.8827, Val: 0.8733, Test: 0.7135
20:52:28.821718 [0] Epoch 00045 | Loss 0.5305
20:52:28.832954 [0] Epoch: 045, Train: 0.8836, Val: 0.8744, Test: 0.7145
20:52:29.462884 [0] Epoch 00046 | Loss 0.5242
20:52:29.474312 [0] Epoch: 046, Train: 0.8847, Val: 0.8752, Test: 0.7156
20:52:30.103905 [0] Epoch 00047 | Loss 0.5183
20:52:30.114935 [0] Epoch: 047, Train: 0.8854, Val: 0.8760, Test: 0.7163
20:52:30.744098 [0] Epoch 00048 | Loss 0.5127
20:52:30.755349 [0] Epoch: 048, Train: 0.8861, Val: 0.8772, Test: 0.7170
20:52:31.383962 [0] Epoch 00049 | Loss 0.5074
20:52:31.394072 [0] Epoch: 049, Train: 0.8870, Val: 0.8776, Test: 0.7175
20:52:32.023123 [0] Epoch 00050 | Loss 0.5023
20:52:32.034168 [0] Epoch: 050, Train: 0.8879, Val: 0.8784, Test: 0.7180
20:52:32.663198 [0] Epoch 00051 | Loss 0.4975
20:52:32.673351 [0] Epoch: 051, Train: 0.8887, Val: 0.8787, Test: 0.7186
20:52:33.302152 [0] Epoch 00052 | Loss 0.4927
20:52:33.312452 [0] Epoch: 052, Train: 0.8894, Val: 0.8798, Test: 0.7191
20:52:33.942462 [0] Epoch 00053 | Loss 0.4880
20:52:33.952653 [0] Epoch: 053, Train: 0.8904, Val: 0.8805, Test: 0.7196
20:52:34.581462 [0] Epoch 00054 | Loss 0.4835
20:52:34.591773 [0] Epoch: 054, Train: 0.8913, Val: 0.8814, Test: 0.7203
20:52:35.221230 [0] Epoch 00055 | Loss 0.4790
20:52:35.232283 [0] Epoch: 055, Train: 0.8917, Val: 0.8823, Test: 0.7210
20:52:35.860673 [0] Epoch 00056 | Loss 0.4748
20:52:35.870958 [0] Epoch: 056, Train: 0.8925, Val: 0.8831, Test: 0.7218
20:52:36.500523 [0] Epoch 00057 | Loss 0.4707
20:52:36.511128 [0] Epoch: 057, Train: 0.8933, Val: 0.8841, Test: 0.7228
20:52:37.140609 [0] Epoch 00058 | Loss 0.4669
20:52:37.151700 [0] Epoch: 058, Train: 0.8940, Val: 0.8845, Test: 0.7239
20:52:37.780541 [0] Epoch 00059 | Loss 0.4632
20:52:37.790860 [0] Epoch: 059, Train: 0.8946, Val: 0.8847, Test: 0.7251
20:52:38.420894 [0] Epoch 00060 | Loss 0.4597
20:52:38.432367 [0] Epoch: 060, Train: 0.8952, Val: 0.8851, Test: 0.7264
20:52:39.062141 [0] Epoch 00061 | Loss 0.4563
20:52:39.073249 [0] Epoch: 061, Train: 0.8959, Val: 0.8856, Test: 0.7276
20:52:39.701822 [0] Epoch 00062 | Loss 0.4530
20:52:39.711894 [0] Epoch: 062, Train: 0.8963, Val: 0.8863, Test: 0.7288
20:52:40.340366 [0] Epoch 00063 | Loss 0.4498
20:52:40.351284 [0] Epoch: 063, Train: 0.8968, Val: 0.8866, Test: 0.7300
20:52:40.980370 [0] Epoch 00064 | Loss 0.4468
20:52:40.991356 [0] Epoch: 064, Train: 0.8974, Val: 0.8869, Test: 0.7308
20:52:41.620503 [0] Epoch 00065 | Loss 0.4438
20:52:41.631032 [0] Epoch: 065, Train: 0.8979, Val: 0.8874, Test: 0.7316
20:52:42.260512 [0] Epoch 00066 | Loss 0.4409
20:52:42.270851 [0] Epoch: 066, Train: 0.8982, Val: 0.8878, Test: 0.7324
20:52:42.899936 [0] Epoch 00067 | Loss 0.4381
20:52:42.910771 [0] Epoch: 067, Train: 0.8986, Val: 0.8885, Test: 0.7330
20:52:43.539992 [0] Epoch 00068 | Loss 0.4353
20:52:43.551015 [0] Epoch: 068, Train: 0.8991, Val: 0.8887, Test: 0.7335
20:52:44.180670 [0] Epoch 00069 | Loss 0.4327
20:52:44.191683 [0] Epoch: 069, Train: 0.8995, Val: 0.8888, Test: 0.7339
20:52:44.821519 [0] Epoch 00070 | Loss 0.4301
20:52:44.832732 [0] Epoch: 070, Train: 0.8998, Val: 0.8891, Test: 0.7342
20:52:45.461906 [0] Epoch 00071 | Loss 0.4276
20:52:45.472881 [0] Epoch: 071, Train: 0.9001, Val: 0.8899, Test: 0.7344
20:52:46.102073 [0] Epoch 00072 | Loss 0.4252
20:52:46.113021 [0] Epoch: 072, Train: 0.9005, Val: 0.8900, Test: 0.7345
20:52:46.741361 [0] Epoch 00073 | Loss 0.4229
20:52:46.751374 [0] Epoch: 073, Train: 0.9008, Val: 0.8902, Test: 0.7346
20:52:47.380694 [0] Epoch 00074 | Loss 0.4207
20:52:47.390966 [0] Epoch: 074, Train: 0.9011, Val: 0.8906, Test: 0.7347
20:52:48.020565 [0] Epoch 00075 | Loss 0.4186
20:52:48.031274 [0] Epoch: 075, Train: 0.9014, Val: 0.8908, Test: 0.7348
20:52:48.660690 [0] Epoch 00076 | Loss 0.4165
20:52:48.670797 [0] Epoch: 076, Train: 0.9017, Val: 0.8913, Test: 0.7350
20:52:49.300258 [0] Epoch 00077 | Loss 0.4145
20:52:49.311374 [0] Epoch: 077, Train: 0.9019, Val: 0.8914, Test: 0.7351
20:52:49.940441 [0] Epoch 00078 | Loss 0.4125
20:52:49.950612 [0] Epoch: 078, Train: 0.9023, Val: 0.8915, Test: 0.7354
20:52:50.579476 [0] Epoch 00079 | Loss 0.4105
20:52:50.589491 [0] Epoch: 079, Train: 0.9026, Val: 0.8917, Test: 0.7357
20:52:51.218563 [0] Epoch 00080 | Loss 0.4086
20:52:51.229474 [0] Epoch: 080, Train: 0.9030, Val: 0.8919, Test: 0.7361
20:52:51.858489 [0] Epoch 00081 | Loss 0.4068
20:52:51.868740 [0] Epoch: 081, Train: 0.9033, Val: 0.8923, Test: 0.7366
20:52:52.498854 [0] Epoch 00082 | Loss 0.4050
20:52:52.510463 [0] Epoch: 082, Train: 0.9036, Val: 0.8927, Test: 0.7371
20:52:53.139832 [0] Epoch 00083 | Loss 0.4032
20:52:53.150877 [0] Epoch: 083, Train: 0.9038, Val: 0.8929, Test: 0.7376
20:52:53.780703 [0] Epoch 00084 | Loss 0.4015
20:52:53.791609 [0] Epoch: 084, Train: 0.9039, Val: 0.8931, Test: 0.7381
20:52:54.421177 [0] Epoch 00085 | Loss 0.3998
20:52:54.432382 [0] Epoch: 085, Train: 0.9040, Val: 0.8935, Test: 0.7386
20:52:55.061806 [0] Epoch 00086 | Loss 0.3981
20:52:55.073108 [0] Epoch: 086, Train: 0.9043, Val: 0.8937, Test: 0.7390
20:52:55.702139 [0] Epoch 00087 | Loss 0.3965
20:52:55.712859 [0] Epoch: 087, Train: 0.9045, Val: 0.8939, Test: 0.7394
20:52:56.344810 [0] Epoch 00088 | Loss 0.3949
20:52:56.355779 [0] Epoch: 088, Train: 0.9047, Val: 0.8945, Test: 0.7397
20:52:56.985596 [0] Epoch 00089 | Loss 0.3933
20:52:56.997581 [0] Epoch: 089, Train: 0.9049, Val: 0.8948, Test: 0.7399
20:52:57.626831 [0] Epoch 00090 | Loss 0.3918
20:52:57.637620 [0] Epoch: 090, Train: 0.9052, Val: 0.8949, Test: 0.7401
20:52:58.266640 [0] Epoch 00091 | Loss 0.3903
20:52:58.278330 [0] Epoch: 091, Train: 0.9055, Val: 0.8950, Test: 0.7402
20:52:58.908563 [0] Epoch 00092 | Loss 0.3888
20:52:58.920075 [0] Epoch: 092, Train: 0.9057, Val: 0.8953, Test: 0.7403
20:52:59.550663 [0] Epoch 00093 | Loss 0.3874
20:52:59.562295 [0] Epoch: 093, Train: 0.9059, Val: 0.8955, Test: 0.7404
20:53:00.191711 [0] Epoch 00094 | Loss 0.3860
20:53:00.202968 [0] Epoch: 094, Train: 0.9062, Val: 0.8959, Test: 0.7405
20:53:00.832645 [0] Epoch 00095 | Loss 0.3846
20:53:00.843725 [0] Epoch: 095, Train: 0.9064, Val: 0.8960, Test: 0.7406
20:53:01.473692 [0] Epoch 00096 | Loss 0.3833
20:53:01.484839 [0] Epoch: 096, Train: 0.9067, Val: 0.8962, Test: 0.7407
20:53:02.115372 [0] Epoch 00097 | Loss 0.3819
20:53:02.125584 [0] Epoch: 097, Train: 0.9071, Val: 0.8964, Test: 0.7408
20:53:02.757123 [0] Epoch 00098 | Loss 0.3806
20:53:02.768143 [0] Epoch: 098, Train: 0.9073, Val: 0.8967, Test: 0.7409
20:53:03.398351 [0] Epoch 00099 | Loss 0.3793
20:53:03.409680 [0] Epoch: 099, Train: 0.9076, Val: 0.8969, Test: 0.7410
20:53:04.039872 [0] Epoch 00100 | Loss 0.3781
20:53:04.051319 [0] Epoch: 100, Train: 0.9080, Val: 0.8970, Test: 0.7411
20:53:04.682676 [0] Epoch 00101 | Loss 0.3768
20:53:04.694134 [0] Epoch: 101, Train: 0.9083, Val: 0.8972, Test: 0.7413
20:53:05.326018 [0] Epoch 00102 | Loss 0.3756
20:53:05.337700 [0] Epoch: 102, Train: 0.9087, Val: 0.8972, Test: 0.7416
20:53:05.970411 [0] Epoch 00103 | Loss 0.3744
20:53:05.981685 [0] Epoch: 103, Train: 0.9089, Val: 0.8975, Test: 0.7419
20:53:06.613537 [0] Epoch 00104 | Loss 0.3732
20:53:06.624807 [0] Epoch: 104, Train: 0.9092, Val: 0.8976, Test: 0.7422
20:53:07.254959 [0] Epoch 00105 | Loss 0.3721
20:53:07.265792 [0] Epoch: 105, Train: 0.9094, Val: 0.8978, Test: 0.7426
20:53:07.896463 [0] Epoch 00106 | Loss 0.3709
20:53:07.907706 [0] Epoch: 106, Train: 0.9096, Val: 0.8979, Test: 0.7428
20:53:08.538078 [0] Epoch 00107 | Loss 0.3698
20:53:08.548668 [0] Epoch: 107, Train: 0.9098, Val: 0.8981, Test: 0.7430
20:53:09.187968 [0] Epoch 00108 | Loss 0.3687
20:53:09.199452 [0] Epoch: 108, Train: 0.9100, Val: 0.8983, Test: 0.7432
20:53:09.837052 [0] Epoch 00109 | Loss 0.3677
20:53:09.848074 [0] Epoch: 109, Train: 0.9102, Val: 0.8984, Test: 0.7434
20:53:10.481312 [0] Epoch 00110 | Loss 0.3666
20:53:10.493412 [0] Epoch: 110, Train: 0.9104, Val: 0.8987, Test: 0.7435
20:53:11.127704 [0] Epoch 00111 | Loss 0.3655
20:53:11.138362 [0] Epoch: 111, Train: 0.9106, Val: 0.8987, Test: 0.7436
20:53:11.775589 [0] Epoch 00112 | Loss 0.3645
20:53:11.785992 [0] Epoch: 112, Train: 0.9109, Val: 0.8987, Test: 0.7438
20:53:12.420112 [0] Epoch 00113 | Loss 0.3635
20:53:12.431334 [0] Epoch: 113, Train: 0.9111, Val: 0.8989, Test: 0.7439
20:53:13.061729 [0] Epoch 00114 | Loss 0.3625
20:53:13.074444 [0] Epoch: 114, Train: 0.9113, Val: 0.8990, Test: 0.7441
20:53:13.754196 [0] Epoch 00115 | Loss 0.3615
20:53:13.765272 [0] Epoch: 115, Train: 0.9115, Val: 0.8992, Test: 0.7442
20:53:14.417706 [0] Epoch 00116 | Loss 0.3605
20:53:14.428607 [0] Epoch: 116, Train: 0.9118, Val: 0.8995, Test: 0.7445
20:53:15.121590 [0] Epoch 00117 | Loss 0.3595
20:53:15.132355 [0] Epoch: 117, Train: 0.9120, Val: 0.8996, Test: 0.7447
20:53:15.765716 [0] Epoch 00118 | Loss 0.3586
20:53:15.776110 [0] Epoch: 118, Train: 0.9122, Val: 0.8999, Test: 0.7449
20:53:16.414160 [0] Epoch 00119 | Loss 0.3576
20:53:16.424575 [0] Epoch: 119, Train: 0.9123, Val: 0.8999, Test: 0.7451
20:53:17.053815 [0] Epoch 00120 | Loss 0.3567
20:53:17.065272 [0] Epoch: 120, Train: 0.9125, Val: 0.8999, Test: 0.7453
20:53:17.696094 [0] Epoch 00121 | Loss 0.3558
20:53:17.707592 [0] Epoch: 121, Train: 0.9127, Val: 0.9003, Test: 0.7454
20:53:18.337107 [0] Epoch 00122 | Loss 0.3549
20:53:18.348952 [0] Epoch: 122, Train: 0.9129, Val: 0.9005, Test: 0.7456
20:53:18.979999 [0] Epoch 00123 | Loss 0.3540
20:53:18.991703 [0] Epoch: 123, Train: 0.9131, Val: 0.9008, Test: 0.7457
20:53:19.621562 [0] Epoch 00124 | Loss 0.3531
20:53:19.632244 [0] Epoch: 124, Train: 0.9133, Val: 0.9009, Test: 0.7459
20:53:20.261693 [0] Epoch 00125 | Loss 0.3522
20:53:20.273030 [0] Epoch: 125, Train: 0.9135, Val: 0.9012, Test: 0.7460
20:53:20.902473 [0] Epoch 00126 | Loss 0.3514
20:53:20.913814 [0] Epoch: 126, Train: 0.9137, Val: 0.9012, Test: 0.7462
20:53:21.543000 [0] Epoch 00127 | Loss 0.3505
20:53:21.553463 [0] Epoch: 127, Train: 0.9139, Val: 0.9013, Test: 0.7463
20:53:22.183068 [0] Epoch 00128 | Loss 0.3497
20:53:22.194119 [0] Epoch: 128, Train: 0.9141, Val: 0.9014, Test: 0.7465
20:53:22.823421 [0] Epoch 00129 | Loss 0.3489
20:53:22.833842 [0] Epoch: 129, Train: 0.9142, Val: 0.9013, Test: 0.7466
20:53:23.463689 [0] Epoch 00130 | Loss 0.3480
20:53:23.474928 [0] Epoch: 130, Train: 0.9144, Val: 0.9013, Test: 0.7467
20:53:24.105062 [0] Epoch 00131 | Loss 0.3472
20:53:24.116151 [0] Epoch: 131, Train: 0.9145, Val: 0.9017, Test: 0.7468
20:53:24.745103 [0] Epoch 00132 | Loss 0.3464
20:53:24.755385 [0] Epoch: 132, Train: 0.9146, Val: 0.9017, Test: 0.7469
20:53:25.384922 [0] Epoch 00133 | Loss 0.3456
20:53:25.395942 [0] Epoch: 133, Train: 0.9147, Val: 0.9019, Test: 0.7470
20:53:26.025040 [0] Epoch 00134 | Loss 0.3448
20:53:26.036015 [0] Epoch: 134, Train: 0.9148, Val: 0.9022, Test: 0.7472
20:53:26.665470 [0] Epoch 00135 | Loss 0.3441
20:53:26.675833 [0] Epoch: 135, Train: 0.9149, Val: 0.9022, Test: 0.7473
20:53:27.306113 [0] Epoch 00136 | Loss 0.3433
20:53:27.317140 [0] Epoch: 136, Train: 0.9151, Val: 0.9024, Test: 0.7474
20:53:27.946243 [0] Epoch 00137 | Loss 0.3425
20:53:27.956417 [0] Epoch: 137, Train: 0.9151, Val: 0.9025, Test: 0.7475
20:53:28.584994 [0] Epoch 00138 | Loss 0.3418
20:53:28.595160 [0] Epoch: 138, Train: 0.9152, Val: 0.9026, Test: 0.7475
20:53:29.223844 [0] Epoch 00139 | Loss 0.3410
20:53:29.234847 [0] Epoch: 139, Train: 0.9154, Val: 0.9028, Test: 0.7476
20:53:29.863407 [0] Epoch 00140 | Loss 0.3403
20:53:29.873589 [0] Epoch: 140, Train: 0.9155, Val: 0.9029, Test: 0.7476
20:53:30.502771 [0] Epoch 00141 | Loss 0.3395
20:53:30.513893 [0] Epoch: 141, Train: 0.9157, Val: 0.9029, Test: 0.7477
20:53:31.143818 [0] Epoch 00142 | Loss 0.3388
20:53:31.154865 [0] Epoch: 142, Train: 0.9159, Val: 0.9032, Test: 0.7478
20:53:31.784026 [0] Epoch 00143 | Loss 0.3381
20:53:31.794993 [0] Epoch: 143, Train: 0.9160, Val: 0.9033, Test: 0.7478
20:53:32.423536 [0] Epoch 00144 | Loss 0.3374
20:53:32.433685 [0] Epoch: 144, Train: 0.9161, Val: 0.9034, Test: 0.7479
20:53:33.062968 [0] Epoch 00145 | Loss 0.3367
20:53:33.074001 [0] Epoch: 145, Train: 0.9162, Val: 0.9035, Test: 0.7479
20:53:33.702846 [0] Epoch 00146 | Loss 0.3360
20:53:33.713046 [0] Epoch: 146, Train: 0.9164, Val: 0.9036, Test: 0.7480
20:53:34.342102 [0] Epoch 00147 | Loss 0.3353
20:53:34.352309 [0] Epoch: 147, Train: 0.9166, Val: 0.9037, Test: 0.7481
20:53:34.982072 [0] Epoch 00148 | Loss 0.3346
20:53:34.993198 [0] Epoch: 148, Train: 0.9167, Val: 0.9038, Test: 0.7482
20:53:35.622461 [0] Epoch 00149 | Loss 0.3339
20:53:35.632766 [0] Epoch: 149, Train: 0.9169, Val: 0.9039, Test: 0.7482
20:53:36.261849 [0] Epoch 00150 | Loss 0.3332
20:53:36.272120 [0] Epoch: 150, Train: 0.9169, Val: 0.9038, Test: 0.7483
20:53:36.901408 [0] Epoch 00151 | Loss 0.3326
20:53:36.911694 [0] Epoch: 151, Train: 0.9170, Val: 0.9039, Test: 0.7484
20:53:37.542054 [0] Epoch 00152 | Loss 0.3319
20:53:37.553169 [0] Epoch: 152, Train: 0.9171, Val: 0.9040, Test: 0.7484
20:53:38.182716 [0] Epoch 00153 | Loss 0.3312
20:53:38.193728 [0] Epoch: 153, Train: 0.9172, Val: 0.9041, Test: 0.7485
20:53:38.823993 [0] Epoch 00154 | Loss 0.3306
20:53:38.834942 [0] Epoch: 154, Train: 0.9174, Val: 0.9041, Test: 0.7486
20:53:39.464652 [0] Epoch 00155 | Loss 0.3299
20:53:39.475639 [0] Epoch: 155, Train: 0.9175, Val: 0.9043, Test: 0.7486
20:53:40.105100 [0] Epoch 00156 | Loss 0.3293
20:53:40.116137 [0] Epoch: 156, Train: 0.9176, Val: 0.9044, Test: 0.7487
20:53:40.744899 [0] Epoch 00157 | Loss 0.3287
20:53:40.755052 [0] Epoch: 157, Train: 0.9178, Val: 0.9044, Test: 0.7488
20:53:41.384390 [0] Epoch 00158 | Loss 0.3280
20:53:41.395393 [0] Epoch: 158, Train: 0.9179, Val: 0.9044, Test: 0.7488
20:53:42.024444 [0] Epoch 00159 | Loss 0.3274
20:53:42.035465 [0] Epoch: 159, Train: 0.9180, Val: 0.9046, Test: 0.7489
20:53:42.664892 [0] Epoch 00160 | Loss 0.3268
20:53:42.674990 [0] Epoch: 160, Train: 0.9181, Val: 0.9045, Test: 0.7489
20:53:43.304976 [0] Epoch 00161 | Loss 0.3262
20:53:43.316356 [0] Epoch: 161, Train: 0.9183, Val: 0.9045, Test: 0.7490
20:53:43.945808 [0] Epoch 00162 | Loss 0.3255
20:53:43.957159 [0] Epoch: 162, Train: 0.9184, Val: 0.9047, Test: 0.7490
20:53:44.586072 [0] Epoch 00163 | Loss 0.3249
20:53:44.596535 [0] Epoch: 163, Train: 0.9186, Val: 0.9047, Test: 0.7491
20:53:45.225833 [0] Epoch 00164 | Loss 0.3243
20:53:45.236926 [0] Epoch: 164, Train: 0.9188, Val: 0.9049, Test: 0.7491
20:53:45.865229 [0] Epoch 00165 | Loss 0.3237
20:53:45.876249 [0] Epoch: 165, Train: 0.9189, Val: 0.9050, Test: 0.7492
20:53:46.505074 [0] Epoch 00166 | Loss 0.3231
20:53:46.515506 [0] Epoch: 166, Train: 0.9190, Val: 0.9052, Test: 0.7492
20:53:47.145433 [0] Epoch 00167 | Loss 0.3225
20:53:47.157049 [0] Epoch: 167, Train: 0.9191, Val: 0.9052, Test: 0.7493
20:53:47.785952 [0] Epoch 00168 | Loss 0.3220
20:53:47.797366 [0] Epoch: 168, Train: 0.9192, Val: 0.9052, Test: 0.7493
20:53:48.426474 [0] Epoch 00169 | Loss 0.3214
20:53:48.437945 [0] Epoch: 169, Train: 0.9194, Val: 0.9053, Test: 0.7494
20:53:49.067939 [0] Epoch 00170 | Loss 0.3208
20:53:49.079847 [0] Epoch: 170, Train: 0.9195, Val: 0.9052, Test: 0.7494
20:53:49.709336 [0] Epoch 00171 | Loss 0.3202
20:53:49.721056 [0] Epoch: 171, Train: 0.9196, Val: 0.9053, Test: 0.7494
20:53:50.350879 [0] Epoch 00172 | Loss 0.3197
20:53:50.361974 [0] Epoch: 172, Train: 0.9197, Val: 0.9053, Test: 0.7495
20:53:50.990571 [0] Epoch 00173 | Loss 0.3191
20:53:51.001164 [0] Epoch: 173, Train: 0.9197, Val: 0.9056, Test: 0.7495
20:53:51.629724 [0] Epoch 00174 | Loss 0.3185
20:53:51.640400 [0] Epoch: 174, Train: 0.9198, Val: 0.9056, Test: 0.7496
20:53:52.268830 [0] Epoch 00175 | Loss 0.3180
20:53:52.279872 [0] Epoch: 175, Train: 0.9199, Val: 0.9057, Test: 0.7496
20:53:52.908342 [0] Epoch 00176 | Loss 0.3174
20:53:52.919095 [0] Epoch: 176, Train: 0.9201, Val: 0.9055, Test: 0.7496
20:53:53.548081 [0] Epoch 00177 | Loss 0.3169
20:53:53.559010 [0] Epoch: 177, Train: 0.9201, Val: 0.9055, Test: 0.7496
20:53:54.188207 [0] Epoch 00178 | Loss 0.3163
20:53:54.199842 [0] Epoch: 178, Train: 0.9202, Val: 0.9057, Test: 0.7497
20:53:54.828285 [0] Epoch 00179 | Loss 0.3158
20:53:54.838940 [0] Epoch: 179, Train: 0.9203, Val: 0.9055, Test: 0.7497
20:53:55.468238 [0] Epoch 00180 | Loss 0.3152
20:53:55.479833 [0] Epoch: 180, Train: 0.9205, Val: 0.9056, Test: 0.7497
20:53:56.108883 [0] Epoch 00181 | Loss 0.3147
20:53:56.120333 [0] Epoch: 181, Train: 0.9205, Val: 0.9056, Test: 0.7497
20:53:56.749827 [0] Epoch 00182 | Loss 0.3142
20:53:56.761258 [0] Epoch: 182, Train: 0.9206, Val: 0.9058, Test: 0.7498
20:53:57.390872 [0] Epoch 00183 | Loss 0.3136
20:53:57.402278 [0] Epoch: 183, Train: 0.9207, Val: 0.9058, Test: 0.7498
20:53:58.031356 [0] Epoch 00184 | Loss 0.3131
20:53:58.042136 [0] Epoch: 184, Train: 0.9208, Val: 0.9059, Test: 0.7498
20:53:58.671305 [0] Epoch 00185 | Loss 0.3126
20:53:58.682617 [0] Epoch: 185, Train: 0.9209, Val: 0.9060, Test: 0.7498
20:53:59.312637 [0] Epoch 00186 | Loss 0.3121
20:53:59.324044 [0] Epoch: 186, Train: 0.9210, Val: 0.9062, Test: 0.7498
20:53:59.954519 [0] Epoch 00187 | Loss 0.3115
20:53:59.965263 [0] Epoch: 187, Train: 0.9211, Val: 0.9063, Test: 0.7499
20:54:00.596695 [0] Epoch 00188 | Loss 0.3110
20:54:00.607713 [0] Epoch: 188, Train: 0.9213, Val: 0.9063, Test: 0.7499
20:54:01.239080 [0] Epoch 00189 | Loss 0.3105
20:54:01.250330 [0] Epoch: 189, Train: 0.9214, Val: 0.9063, Test: 0.7499
20:54:01.881451 [0] Epoch 00190 | Loss 0.3100
20:54:01.892330 [0] Epoch: 190, Train: 0.9215, Val: 0.9064, Test: 0.7500
20:54:02.526139 [0] Epoch 00191 | Loss 0.3095
20:54:02.537494 [0] Epoch: 191, Train: 0.9216, Val: 0.9066, Test: 0.7500
20:54:03.171032 [0] Epoch 00192 | Loss 0.3090
20:54:03.181770 [0] Epoch: 192, Train: 0.9217, Val: 0.9067, Test: 0.7500
20:54:03.815198 [0] Epoch 00193 | Loss 0.3085
20:54:03.826569 [0] Epoch: 193, Train: 0.9218, Val: 0.9068, Test: 0.7501
20:54:04.458974 [0] Epoch 00194 | Loss 0.3080
20:54:04.470525 [0] Epoch: 194, Train: 0.9218, Val: 0.9067, Test: 0.7501
20:54:05.102723 [0] Epoch 00195 | Loss 0.3075
20:54:05.114717 [0] Epoch: 195, Train: 0.9220, Val: 0.9067, Test: 0.7501
20:54:05.747000 [0] Epoch 00196 | Loss 0.3070
20:54:05.757992 [0] Epoch: 196, Train: 0.9221, Val: 0.9068, Test: 0.7501
20:54:06.390669 [0] Epoch 00197 | Loss 0.3066
20:54:06.401558 [0] Epoch: 197, Train: 0.9222, Val: 0.9068, Test: 0.7501
20:54:07.033187 [0] Epoch 00198 | Loss 0.3061
20:54:07.044283 [0] Epoch: 198, Train: 0.9224, Val: 0.9069, Test: 0.7501
20:54:07.675026 [0] Epoch 00199 | Loss 0.3056
20:54:07.686007 [0] Epoch: 199, Train: 0.9224, Val: 0.9072, Test: 0.7502
20:54:07.686872 [0] 
timer summary:
  0.29s   0.29s   200 broadcast ForwardL1 0
  0.40s   0.40s   800 broadcast
103.14s 103.14s   800 spmm
  9.51s   9.51s   800 mm
  0.02s   0.02s   200 broadcast ForwardL2 0
  0.03s   0.03s   200 broadcast BackwardL2 0
  0.11s   0.11s   400 all_reduce
  0.02s   0.02s   200 broadcast BackwardL1 0
129.32s 129.32s   200 epoch
140.65s 140.65s     1 total
14:35:48.187132 [0] proc begin: <DistEnv 0/1 nccl>
14:36:11.720026 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
14:36:11.728713 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from large pool |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2396 MiB |   2396 MiB |   2396 MiB |      0 B   |
|       from large pool |   2394 MiB |   2394 MiB |   2394 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 472519 KiB | 482086 KiB | 570319 KiB |  97800 KiB |
|       from large pool | 472519 KiB | 482086 KiB | 564170 KiB |  91651 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:36:13.801231 [0] Epoch 00000 | Loss 4.7410
14:36:13.828459 [0] Epoch: 000, Train: 0.0291, Val: 0.0280, Test: 0.0267
14:36:14.458085 [0] Epoch 00001 | Loss 3.9229
14:36:14.469611 [0] Epoch: 001, Train: 0.0636, Val: 0.0616, Test: 0.0512
14:36:15.098896 [0] Epoch 00002 | Loss 3.2705
14:36:15.110306 [0] Epoch: 002, Train: 0.2419, Val: 0.2230, Test: 0.1489
14:36:15.739237 [0] Epoch 00003 | Loss 2.8071
14:36:15.750111 [0] Epoch: 003, Train: 0.4116, Val: 0.3910, Test: 0.2628
14:36:16.381000 [0] Epoch 00004 | Loss 2.4711
14:36:16.392756 [0] Epoch: 004, Train: 0.5207, Val: 0.5033, Test: 0.3458
14:36:17.022485 [0] Epoch 00005 | Loss 2.2229
14:36:17.033625 [0] Epoch: 005, Train: 0.5857, Val: 0.5718, Test: 0.4060
14:36:17.662796 [0] Epoch 00006 | Loss 2.0293
14:36:17.674011 [0] Epoch: 006, Train: 0.6275, Val: 0.6158, Test: 0.4482
14:36:18.302888 [0] Epoch 00007 | Loss 1.8677
14:36:18.314018 [0] Epoch: 007, Train: 0.6613, Val: 0.6507, Test: 0.4781
14:36:18.943029 [0] Epoch 00008 | Loss 1.7264
14:36:18.953875 [0] Epoch: 008, Train: 0.6909, Val: 0.6792, Test: 0.5022
14:36:19.583486 [0] Epoch 00009 | Loss 1.6003
14:36:19.594533 [0] Epoch: 009, Train: 0.7199, Val: 0.7082, Test: 0.5240
14:36:20.224883 [0] Epoch 00010 | Loss 1.4872
14:36:20.235531 [0] Epoch: 010, Train: 0.7451, Val: 0.7331, Test: 0.5442
14:36:20.864532 [0] Epoch 00011 | Loss 1.3859
14:36:20.875504 [0] Epoch: 011, Train: 0.7662, Val: 0.7539, Test: 0.5626
14:36:21.504958 [0] Epoch 00012 | Loss 1.2961
14:36:21.516003 [0] Epoch: 012, Train: 0.7828, Val: 0.7717, Test: 0.5795
14:36:22.145956 [0] Epoch 00013 | Loss 1.2172
14:36:22.156948 [0] Epoch: 013, Train: 0.7969, Val: 0.7855, Test: 0.5946
14:36:22.786193 [0] Epoch 00014 | Loss 1.1486
14:36:22.797099 [0] Epoch: 014, Train: 0.8083, Val: 0.7969, Test: 0.6085
14:36:23.426698 [0] Epoch 00015 | Loss 1.0889
14:36:23.437775 [0] Epoch: 015, Train: 0.8165, Val: 0.8067, Test: 0.6201
14:36:24.067297 [0] Epoch 00016 | Loss 1.0369
14:36:24.078370 [0] Epoch: 016, Train: 0.8232, Val: 0.8128, Test: 0.6294
14:36:24.707256 [0] Epoch 00017 | Loss 0.9914
14:36:24.717745 [0] Epoch: 017, Train: 0.8291, Val: 0.8191, Test: 0.6373
14:36:25.346274 [0] Epoch 00018 | Loss 0.9510
14:36:25.357079 [0] Epoch: 018, Train: 0.8338, Val: 0.8245, Test: 0.6437
14:36:25.986896 [0] Epoch 00019 | Loss 0.9149
14:36:25.998128 [0] Epoch: 019, Train: 0.8380, Val: 0.8289, Test: 0.6491
14:36:26.628003 [0] Epoch 00020 | Loss 0.8822
14:36:26.639499 [0] Epoch: 020, Train: 0.8418, Val: 0.8331, Test: 0.6536
14:36:27.269186 [0] Epoch 00021 | Loss 0.8523
14:36:27.280627 [0] Epoch: 021, Train: 0.8454, Val: 0.8360, Test: 0.6577
14:36:27.912251 [0] Epoch 00022 | Loss 0.8247
14:36:27.924405 [0] Epoch: 022, Train: 0.8493, Val: 0.8385, Test: 0.6616
14:36:28.555159 [0] Epoch 00023 | Loss 0.7990
14:36:28.566456 [0] Epoch: 023, Train: 0.8524, Val: 0.8415, Test: 0.6650
14:36:29.196494 [0] Epoch 00024 | Loss 0.7752
14:36:29.207089 [0] Epoch: 024, Train: 0.8556, Val: 0.8445, Test: 0.6677
14:36:29.837848 [0] Epoch 00025 | Loss 0.7531
14:36:29.849238 [0] Epoch: 025, Train: 0.8581, Val: 0.8471, Test: 0.6704
14:36:30.479637 [0] Epoch 00026 | Loss 0.7329
14:36:30.490822 [0] Epoch: 026, Train: 0.8606, Val: 0.8497, Test: 0.6726
14:36:31.121906 [0] Epoch 00027 | Loss 0.7146
14:36:31.133176 [0] Epoch: 027, Train: 0.8623, Val: 0.8519, Test: 0.6746
14:36:31.764663 [0] Epoch 00028 | Loss 0.6980
14:36:31.776029 [0] Epoch: 028, Train: 0.8638, Val: 0.8539, Test: 0.6764
14:36:32.406247 [0] Epoch 00029 | Loss 0.6829
14:36:32.417736 [0] Epoch: 029, Train: 0.8647, Val: 0.8551, Test: 0.6779
14:36:33.047619 [0] Epoch 00030 | Loss 0.6688
14:36:33.058302 [0] Epoch: 030, Train: 0.8657, Val: 0.8563, Test: 0.6795
14:36:33.687959 [0] Epoch 00031 | Loss 0.6555
14:36:33.699490 [0] Epoch: 031, Train: 0.8667, Val: 0.8579, Test: 0.6810
14:36:34.329020 [0] Epoch 00032 | Loss 0.6426
14:36:34.340202 [0] Epoch: 032, Train: 0.8682, Val: 0.8587, Test: 0.6829
14:36:34.969570 [0] Epoch 00033 | Loss 0.6303
14:36:34.981423 [0] Epoch: 033, Train: 0.8699, Val: 0.8609, Test: 0.6851
14:36:35.611672 [0] Epoch 00034 | Loss 0.6187
14:36:35.623374 [0] Epoch: 034, Train: 0.8712, Val: 0.8627, Test: 0.6883
14:36:36.252488 [0] Epoch 00035 | Loss 0.6079
14:36:36.263910 [0] Epoch: 035, Train: 0.8728, Val: 0.8634, Test: 0.6917
14:36:36.893035 [0] Epoch 00036 | Loss 0.5981
14:36:36.904408 [0] Epoch: 036, Train: 0.8741, Val: 0.8635, Test: 0.6950
14:36:37.534009 [0] Epoch 00037 | Loss 0.5891
14:36:37.545356 [0] Epoch: 037, Train: 0.8753, Val: 0.8653, Test: 0.6982
14:36:38.174184 [0] Epoch 00038 | Loss 0.5808
14:36:38.184502 [0] Epoch: 038, Train: 0.8765, Val: 0.8669, Test: 0.7014
14:36:38.813834 [0] Epoch 00039 | Loss 0.5730
14:36:38.825260 [0] Epoch: 039, Train: 0.8774, Val: 0.8677, Test: 0.7044
14:36:39.455003 [0] Epoch 00040 | Loss 0.5656
14:36:39.466164 [0] Epoch: 040, Train: 0.8785, Val: 0.8687, Test: 0.7069
14:36:40.095769 [0] Epoch 00041 | Loss 0.5583
14:36:40.106836 [0] Epoch: 041, Train: 0.8797, Val: 0.8698, Test: 0.7092
14:36:40.736325 [0] Epoch 00042 | Loss 0.5511
14:36:40.747320 [0] Epoch: 042, Train: 0.8805, Val: 0.8708, Test: 0.7110
14:36:41.376830 [0] Epoch 00043 | Loss 0.5440
14:36:41.387867 [0] Epoch: 043, Train: 0.8817, Val: 0.8714, Test: 0.7124
14:36:42.017411 [0] Epoch 00044 | Loss 0.5371
14:36:42.027636 [0] Epoch: 044, Train: 0.8827, Val: 0.8733, Test: 0.7135
14:36:42.657066 [0] Epoch 00045 | Loss 0.5305
14:36:42.667265 [0] Epoch: 045, Train: 0.8836, Val: 0.8744, Test: 0.7145
14:36:43.296281 [0] Epoch 00046 | Loss 0.5242
14:36:43.306475 [0] Epoch: 046, Train: 0.8847, Val: 0.8752, Test: 0.7156
14:36:43.935660 [0] Epoch 00047 | Loss 0.5183
14:36:43.945799 [0] Epoch: 047, Train: 0.8854, Val: 0.8760, Test: 0.7163
14:36:44.575042 [0] Epoch 00048 | Loss 0.5127
14:36:44.586063 [0] Epoch: 048, Train: 0.8861, Val: 0.8772, Test: 0.7170
14:36:45.214869 [0] Epoch 00049 | Loss 0.5074
14:36:45.225921 [0] Epoch: 049, Train: 0.8870, Val: 0.8776, Test: 0.7175
14:36:45.856163 [0] Epoch 00050 | Loss 0.5023
14:36:45.867266 [0] Epoch: 050, Train: 0.8879, Val: 0.8784, Test: 0.7180
14:36:46.496447 [0] Epoch 00051 | Loss 0.4975
14:36:46.507614 [0] Epoch: 051, Train: 0.8887, Val: 0.8787, Test: 0.7186
14:36:47.136414 [0] Epoch 00052 | Loss 0.4927
14:36:47.147292 [0] Epoch: 052, Train: 0.8894, Val: 0.8798, Test: 0.7191
14:36:47.775442 [0] Epoch 00053 | Loss 0.4880
14:36:47.785542 [0] Epoch: 053, Train: 0.8904, Val: 0.8805, Test: 0.7196
14:36:48.414305 [0] Epoch 00054 | Loss 0.4835
14:36:48.425265 [0] Epoch: 054, Train: 0.8913, Val: 0.8814, Test: 0.7203
14:36:49.053658 [0] Epoch 00055 | Loss 0.4790
14:36:49.064584 [0] Epoch: 055, Train: 0.8917, Val: 0.8823, Test: 0.7210
14:36:49.692924 [0] Epoch 00056 | Loss 0.4748
14:36:49.703051 [0] Epoch: 056, Train: 0.8925, Val: 0.8831, Test: 0.7218
14:36:50.333507 [0] Epoch 00057 | Loss 0.4707
14:36:50.344092 [0] Epoch: 057, Train: 0.8933, Val: 0.8841, Test: 0.7228
14:36:50.974564 [0] Epoch 00058 | Loss 0.4669
14:36:50.985640 [0] Epoch: 058, Train: 0.8940, Val: 0.8845, Test: 0.7239
14:36:51.616455 [0] Epoch 00059 | Loss 0.4632
14:36:51.627374 [0] Epoch: 059, Train: 0.8946, Val: 0.8847, Test: 0.7251
14:36:52.258145 [0] Epoch 00060 | Loss 0.4597
14:36:52.269319 [0] Epoch: 060, Train: 0.8952, Val: 0.8851, Test: 0.7264
14:36:52.899563 [0] Epoch 00061 | Loss 0.4563
14:36:52.910930 [0] Epoch: 061, Train: 0.8959, Val: 0.8856, Test: 0.7276
14:36:53.541404 [0] Epoch 00062 | Loss 0.4530
14:36:53.552228 [0] Epoch: 062, Train: 0.8963, Val: 0.8863, Test: 0.7288
14:36:54.184476 [0] Epoch 00063 | Loss 0.4498
14:36:54.196355 [0] Epoch: 063, Train: 0.8968, Val: 0.8866, Test: 0.7300
14:36:54.826557 [0] Epoch 00064 | Loss 0.4468
14:36:54.837266 [0] Epoch: 064, Train: 0.8974, Val: 0.8869, Test: 0.7308
14:36:55.468206 [0] Epoch 00065 | Loss 0.4438
14:36:55.479656 [0] Epoch: 065, Train: 0.8979, Val: 0.8874, Test: 0.7316
14:36:56.109050 [0] Epoch 00066 | Loss 0.4409
14:36:56.119462 [0] Epoch: 066, Train: 0.8982, Val: 0.8878, Test: 0.7324
14:36:56.748464 [0] Epoch 00067 | Loss 0.4381
14:36:56.758857 [0] Epoch: 067, Train: 0.8986, Val: 0.8885, Test: 0.7330
14:36:57.388765 [0] Epoch 00068 | Loss 0.4353
14:36:57.400043 [0] Epoch: 068, Train: 0.8991, Val: 0.8887, Test: 0.7335
14:36:58.029974 [0] Epoch 00069 | Loss 0.4327
14:36:58.041075 [0] Epoch: 069, Train: 0.8995, Val: 0.8888, Test: 0.7339
14:36:58.671102 [0] Epoch 00070 | Loss 0.4301
14:36:58.682218 [0] Epoch: 070, Train: 0.8998, Val: 0.8891, Test: 0.7342
14:36:59.310973 [0] Epoch 00071 | Loss 0.4276
14:36:59.321105 [0] Epoch: 071, Train: 0.9001, Val: 0.8899, Test: 0.7344
14:36:59.950819 [0] Epoch 00072 | Loss 0.4252
14:36:59.961858 [0] Epoch: 072, Train: 0.9005, Val: 0.8900, Test: 0.7345
14:37:00.590839 [0] Epoch 00073 | Loss 0.4229
14:37:00.601869 [0] Epoch: 073, Train: 0.9008, Val: 0.8902, Test: 0.7346
14:37:01.230907 [0] Epoch 00074 | Loss 0.4207
14:37:01.241045 [0] Epoch: 074, Train: 0.9011, Val: 0.8906, Test: 0.7347
14:37:01.872933 [0] Epoch 00075 | Loss 0.4186
14:37:01.883371 [0] Epoch: 075, Train: 0.9014, Val: 0.8908, Test: 0.7348
14:37:02.515690 [0] Epoch 00076 | Loss 0.4165
14:37:02.526706 [0] Epoch: 076, Train: 0.9017, Val: 0.8913, Test: 0.7350
14:37:03.159772 [0] Epoch 00077 | Loss 0.4145
14:37:03.171869 [0] Epoch: 077, Train: 0.9019, Val: 0.8914, Test: 0.7351
14:37:03.803116 [0] Epoch 00078 | Loss 0.4125
14:37:03.814786 [0] Epoch: 078, Train: 0.9023, Val: 0.8915, Test: 0.7354
14:37:04.446390 [0] Epoch 00079 | Loss 0.4105
14:37:04.457818 [0] Epoch: 079, Train: 0.9026, Val: 0.8917, Test: 0.7357
14:37:05.089278 [0] Epoch 00080 | Loss 0.4086
14:37:05.100586 [0] Epoch: 080, Train: 0.9030, Val: 0.8919, Test: 0.7361
14:37:05.732064 [0] Epoch 00081 | Loss 0.4068
14:37:05.743071 [0] Epoch: 081, Train: 0.9033, Val: 0.8923, Test: 0.7366
14:37:06.375227 [0] Epoch 00082 | Loss 0.4050
14:37:06.386661 [0] Epoch: 082, Train: 0.9036, Val: 0.8927, Test: 0.7371
14:37:07.017561 [0] Epoch 00083 | Loss 0.4032
14:37:07.029301 [0] Epoch: 083, Train: 0.9038, Val: 0.8929, Test: 0.7376
14:37:07.660442 [0] Epoch 00084 | Loss 0.4015
14:37:07.672248 [0] Epoch: 084, Train: 0.9039, Val: 0.8931, Test: 0.7381
14:37:08.303378 [0] Epoch 00085 | Loss 0.3998
14:37:08.314165 [0] Epoch: 085, Train: 0.9040, Val: 0.8935, Test: 0.7386
14:37:08.944792 [0] Epoch 00086 | Loss 0.3981
14:37:08.955393 [0] Epoch: 086, Train: 0.9043, Val: 0.8937, Test: 0.7390
14:37:09.586799 [0] Epoch 00087 | Loss 0.3965
14:37:09.596979 [0] Epoch: 087, Train: 0.9045, Val: 0.8939, Test: 0.7394
14:37:10.232983 [0] Epoch 00088 | Loss 0.3949
14:37:10.244455 [0] Epoch: 088, Train: 0.9047, Val: 0.8945, Test: 0.7397
14:37:10.875934 [0] Epoch 00089 | Loss 0.3933
14:37:10.886986 [0] Epoch: 089, Train: 0.9049, Val: 0.8948, Test: 0.7399
14:37:11.518238 [0] Epoch 00090 | Loss 0.3918
14:37:11.529449 [0] Epoch: 090, Train: 0.9052, Val: 0.8949, Test: 0.7401
14:37:12.160741 [0] Epoch 00091 | Loss 0.3903
14:37:12.172425 [0] Epoch: 091, Train: 0.9055, Val: 0.8950, Test: 0.7402
14:37:12.803715 [0] Epoch 00092 | Loss 0.3888
14:37:12.814278 [0] Epoch: 092, Train: 0.9057, Val: 0.8953, Test: 0.7403
14:37:13.446565 [0] Epoch 00093 | Loss 0.3874
14:37:13.458088 [0] Epoch: 093, Train: 0.9059, Val: 0.8955, Test: 0.7404
14:37:14.088706 [0] Epoch 00094 | Loss 0.3860
14:37:14.099789 [0] Epoch: 094, Train: 0.9062, Val: 0.8959, Test: 0.7405
14:37:14.729461 [0] Epoch 00095 | Loss 0.3846
14:37:14.740469 [0] Epoch: 095, Train: 0.9064, Val: 0.8960, Test: 0.7406
14:37:15.371369 [0] Epoch 00096 | Loss 0.3833
14:37:15.382853 [0] Epoch: 096, Train: 0.9067, Val: 0.8962, Test: 0.7407
14:37:16.012808 [0] Epoch 00097 | Loss 0.3819
14:37:16.024327 [0] Epoch: 097, Train: 0.9071, Val: 0.8964, Test: 0.7408
14:37:16.654740 [0] Epoch 00098 | Loss 0.3806
14:37:16.666021 [0] Epoch: 098, Train: 0.9073, Val: 0.8967, Test: 0.7409
14:37:17.297199 [0] Epoch 00099 | Loss 0.3793
14:37:17.309160 [0] Epoch: 099, Train: 0.9076, Val: 0.8969, Test: 0.7410
14:37:17.939908 [0] Epoch 00100 | Loss 0.3781
14:37:17.951506 [0] Epoch: 100, Train: 0.9080, Val: 0.8970, Test: 0.7411
14:37:18.581308 [0] Epoch 00101 | Loss 0.3768
14:37:18.591852 [0] Epoch: 101, Train: 0.9083, Val: 0.8972, Test: 0.7413
14:37:19.221882 [0] Epoch 00102 | Loss 0.3756
14:37:19.233130 [0] Epoch: 102, Train: 0.9087, Val: 0.8972, Test: 0.7416
14:37:19.863918 [0] Epoch 00103 | Loss 0.3744
14:37:19.875065 [0] Epoch: 103, Train: 0.9089, Val: 0.8975, Test: 0.7419
14:37:20.505814 [0] Epoch 00104 | Loss 0.3732
14:37:20.516947 [0] Epoch: 104, Train: 0.9092, Val: 0.8976, Test: 0.7422
14:37:21.147460 [0] Epoch 00105 | Loss 0.3721
14:37:21.158104 [0] Epoch: 105, Train: 0.9094, Val: 0.8978, Test: 0.7426
14:37:21.788814 [0] Epoch 00106 | Loss 0.3709
14:37:21.800213 [0] Epoch: 106, Train: 0.9096, Val: 0.8979, Test: 0.7428
14:37:22.431635 [0] Epoch 00107 | Loss 0.3698
14:37:22.443200 [0] Epoch: 107, Train: 0.9098, Val: 0.8981, Test: 0.7430
14:37:23.074368 [0] Epoch 00108 | Loss 0.3687
14:37:23.085579 [0] Epoch: 108, Train: 0.9100, Val: 0.8983, Test: 0.7432
14:37:23.717115 [0] Epoch 00109 | Loss 0.3677
14:37:23.728033 [0] Epoch: 109, Train: 0.9102, Val: 0.8984, Test: 0.7434
14:37:24.359090 [0] Epoch 00110 | Loss 0.3666
14:37:24.370593 [0] Epoch: 110, Train: 0.9104, Val: 0.8986, Test: 0.7435
14:37:25.000851 [0] Epoch 00111 | Loss 0.3655
14:37:25.011393 [0] Epoch: 111, Train: 0.9106, Val: 0.8987, Test: 0.7436
14:37:25.641847 [0] Epoch 00112 | Loss 0.3645
14:37:25.652473 [0] Epoch: 112, Train: 0.9109, Val: 0.8987, Test: 0.7438
14:37:26.283387 [0] Epoch 00113 | Loss 0.3635
14:37:26.294694 [0] Epoch: 113, Train: 0.9111, Val: 0.8989, Test: 0.7439
14:37:26.924528 [0] Epoch 00114 | Loss 0.3625
14:37:26.935586 [0] Epoch: 114, Train: 0.9113, Val: 0.8990, Test: 0.7441
14:37:27.565565 [0] Epoch 00115 | Loss 0.3615
14:37:27.576543 [0] Epoch: 115, Train: 0.9115, Val: 0.8992, Test: 0.7442
14:37:28.206183 [0] Epoch 00116 | Loss 0.3605
14:37:28.217443 [0] Epoch: 116, Train: 0.9118, Val: 0.8995, Test: 0.7445
14:37:28.847983 [0] Epoch 00117 | Loss 0.3595
14:37:28.859480 [0] Epoch: 117, Train: 0.9120, Val: 0.8996, Test: 0.7447
14:37:29.491184 [0] Epoch 00118 | Loss 0.3586
14:37:29.502607 [0] Epoch: 118, Train: 0.9122, Val: 0.8999, Test: 0.7449
14:37:30.133702 [0] Epoch 00119 | Loss 0.3576
14:37:30.144586 [0] Epoch: 119, Train: 0.9123, Val: 0.8999, Test: 0.7451
14:37:30.774777 [0] Epoch 00120 | Loss 0.3567
14:37:30.785509 [0] Epoch: 120, Train: 0.9125, Val: 0.8999, Test: 0.7453
14:37:31.415867 [0] Epoch 00121 | Loss 0.3558
14:37:31.427287 [0] Epoch: 121, Train: 0.9127, Val: 0.9003, Test: 0.7454
14:37:32.056430 [0] Epoch 00122 | Loss 0.3549
14:37:32.066890 [0] Epoch: 122, Train: 0.9129, Val: 0.9005, Test: 0.7456
14:37:32.695898 [0] Epoch 00123 | Loss 0.3540
14:37:32.707177 [0] Epoch: 123, Train: 0.9131, Val: 0.9008, Test: 0.7457
14:37:33.336734 [0] Epoch 00124 | Loss 0.3531
14:37:33.347967 [0] Epoch: 124, Train: 0.9133, Val: 0.9009, Test: 0.7459
14:37:33.978423 [0] Epoch 00125 | Loss 0.3522
14:37:33.989597 [0] Epoch: 125, Train: 0.9135, Val: 0.9012, Test: 0.7460
14:37:34.619719 [0] Epoch 00126 | Loss 0.3514
14:37:34.630485 [0] Epoch: 126, Train: 0.9137, Val: 0.9012, Test: 0.7462
14:37:35.261054 [0] Epoch 00127 | Loss 0.3505
14:37:35.272095 [0] Epoch: 127, Train: 0.9139, Val: 0.9013, Test: 0.7463
14:37:35.902550 [0] Epoch 00128 | Loss 0.3497
14:37:35.913629 [0] Epoch: 128, Train: 0.9141, Val: 0.9014, Test: 0.7465
14:37:36.543813 [0] Epoch 00129 | Loss 0.3489
14:37:36.554001 [0] Epoch: 129, Train: 0.9143, Val: 0.9013, Test: 0.7466
14:37:37.182668 [0] Epoch 00130 | Loss 0.3480
14:37:37.192680 [0] Epoch: 130, Train: 0.9144, Val: 0.9013, Test: 0.7467
14:37:37.822618 [0] Epoch 00131 | Loss 0.3472
14:37:37.833799 [0] Epoch: 131, Train: 0.9145, Val: 0.9017, Test: 0.7468
14:37:38.464198 [0] Epoch 00132 | Loss 0.3464
14:37:38.475607 [0] Epoch: 132, Train: 0.9146, Val: 0.9017, Test: 0.7469
14:37:39.105734 [0] Epoch 00133 | Loss 0.3456
14:37:39.117215 [0] Epoch: 133, Train: 0.9147, Val: 0.9019, Test: 0.7470
14:37:39.745654 [0] Epoch 00134 | Loss 0.3448
14:37:39.756439 [0] Epoch: 134, Train: 0.9148, Val: 0.9022, Test: 0.7472
14:37:40.385277 [0] Epoch 00135 | Loss 0.3441
14:37:40.396677 [0] Epoch: 135, Train: 0.9150, Val: 0.9022, Test: 0.7473
14:37:41.025959 [0] Epoch 00136 | Loss 0.3433
14:37:41.037527 [0] Epoch: 136, Train: 0.9151, Val: 0.9024, Test: 0.7474
14:37:41.667186 [0] Epoch 00137 | Loss 0.3425
14:37:41.679118 [0] Epoch: 137, Train: 0.9151, Val: 0.9025, Test: 0.7475
14:37:42.308601 [0] Epoch 00138 | Loss 0.3418
14:37:42.320282 [0] Epoch: 138, Train: 0.9152, Val: 0.9026, Test: 0.7475
14:37:42.949866 [0] Epoch 00139 | Loss 0.3410
14:37:42.961373 [0] Epoch: 139, Train: 0.9154, Val: 0.9028, Test: 0.7476
14:37:43.591384 [0] Epoch 00140 | Loss 0.3403
14:37:43.602909 [0] Epoch: 140, Train: 0.9155, Val: 0.9029, Test: 0.7476
14:37:44.231429 [0] Epoch 00141 | Loss 0.3395
14:37:44.242529 [0] Epoch: 141, Train: 0.9157, Val: 0.9029, Test: 0.7477
14:37:44.871681 [0] Epoch 00142 | Loss 0.3388
14:37:44.883031 [0] Epoch: 142, Train: 0.9159, Val: 0.9032, Test: 0.7478
14:37:45.511940 [0] Epoch 00143 | Loss 0.3381
14:37:45.522305 [0] Epoch: 143, Train: 0.9160, Val: 0.9033, Test: 0.7478
14:37:46.151722 [0] Epoch 00144 | Loss 0.3374
14:37:46.162782 [0] Epoch: 144, Train: 0.9161, Val: 0.9034, Test: 0.7479
14:37:46.792889 [0] Epoch 00145 | Loss 0.3367
14:37:46.804177 [0] Epoch: 145, Train: 0.9162, Val: 0.9035, Test: 0.7479
14:37:47.433788 [0] Epoch 00146 | Loss 0.3360
14:37:47.444080 [0] Epoch: 146, Train: 0.9165, Val: 0.9036, Test: 0.7480
14:37:48.074957 [0] Epoch 00147 | Loss 0.3353
14:37:48.085472 [0] Epoch: 147, Train: 0.9166, Val: 0.9037, Test: 0.7481
14:37:48.716706 [0] Epoch 00148 | Loss 0.3346
14:37:48.727835 [0] Epoch: 148, Train: 0.9167, Val: 0.9038, Test: 0.7482
14:37:49.358189 [0] Epoch 00149 | Loss 0.3339
14:37:49.369227 [0] Epoch: 149, Train: 0.9169, Val: 0.9039, Test: 0.7482
14:37:49.999751 [0] Epoch 00150 | Loss 0.3332
14:37:50.011207 [0] Epoch: 150, Train: 0.9169, Val: 0.9038, Test: 0.7483
14:37:50.643460 [0] Epoch 00151 | Loss 0.3326
14:37:50.655371 [0] Epoch: 151, Train: 0.9170, Val: 0.9039, Test: 0.7484
14:37:51.289400 [0] Epoch 00152 | Loss 0.3319
14:37:51.300827 [0] Epoch: 152, Train: 0.9171, Val: 0.9040, Test: 0.7484
14:37:51.932649 [0] Epoch 00153 | Loss 0.3312
14:37:51.943833 [0] Epoch: 153, Train: 0.9172, Val: 0.9040, Test: 0.7485
14:37:52.575665 [0] Epoch 00154 | Loss 0.3306
14:37:52.586994 [0] Epoch: 154, Train: 0.9174, Val: 0.9041, Test: 0.7486
14:37:53.217560 [0] Epoch 00155 | Loss 0.3299
14:37:53.227786 [0] Epoch: 155, Train: 0.9175, Val: 0.9043, Test: 0.7486
14:37:53.858015 [0] Epoch 00156 | Loss 0.3293
14:37:53.869290 [0] Epoch: 156, Train: 0.9176, Val: 0.9044, Test: 0.7487
14:37:54.499820 [0] Epoch 00157 | Loss 0.3287
14:37:54.511348 [0] Epoch: 157, Train: 0.9178, Val: 0.9044, Test: 0.7488
14:37:55.143065 [0] Epoch 00158 | Loss 0.3280
14:37:55.154473 [0] Epoch: 158, Train: 0.9179, Val: 0.9044, Test: 0.7488
14:37:55.785829 [0] Epoch 00159 | Loss 0.3274
14:37:55.796801 [0] Epoch: 159, Train: 0.9180, Val: 0.9046, Test: 0.7489
14:37:56.427398 [0] Epoch 00160 | Loss 0.3268
14:37:56.438997 [0] Epoch: 160, Train: 0.9181, Val: 0.9045, Test: 0.7489
14:37:57.070733 [0] Epoch 00161 | Loss 0.3262
14:37:57.082132 [0] Epoch: 161, Train: 0.9183, Val: 0.9045, Test: 0.7490
14:37:57.713241 [0] Epoch 00162 | Loss 0.3255
14:37:57.724445 [0] Epoch: 162, Train: 0.9184, Val: 0.9047, Test: 0.7490
14:37:58.355624 [0] Epoch 00163 | Loss 0.3249
14:37:58.366521 [0] Epoch: 163, Train: 0.9186, Val: 0.9047, Test: 0.7491
14:37:58.996434 [0] Epoch 00164 | Loss 0.3243
14:37:59.006923 [0] Epoch: 164, Train: 0.9187, Val: 0.9049, Test: 0.7491
14:37:59.638664 [0] Epoch 00165 | Loss 0.3237
14:37:59.649964 [0] Epoch: 165, Train: 0.9189, Val: 0.9050, Test: 0.7492
14:38:00.281643 [0] Epoch 00166 | Loss 0.3231
14:38:00.293020 [0] Epoch: 166, Train: 0.9190, Val: 0.9052, Test: 0.7492
14:38:00.925414 [0] Epoch 00167 | Loss 0.3225
14:38:00.937106 [0] Epoch: 167, Train: 0.9191, Val: 0.9052, Test: 0.7493
14:38:01.567671 [0] Epoch 00168 | Loss 0.3220
14:38:01.578314 [0] Epoch: 168, Train: 0.9192, Val: 0.9052, Test: 0.7493
14:38:02.209683 [0] Epoch 00169 | Loss 0.3214
14:38:02.220650 [0] Epoch: 169, Train: 0.9194, Val: 0.9053, Test: 0.7494
14:38:02.853732 [0] Epoch 00170 | Loss 0.3208
14:38:02.864014 [0] Epoch: 170, Train: 0.9195, Val: 0.9052, Test: 0.7494
14:38:03.495417 [0] Epoch 00171 | Loss 0.3202
14:38:03.506826 [0] Epoch: 171, Train: 0.9196, Val: 0.9053, Test: 0.7494
14:38:04.137503 [0] Epoch 00172 | Loss 0.3197
14:38:04.148383 [0] Epoch: 172, Train: 0.9197, Val: 0.9053, Test: 0.7495
14:38:04.777924 [0] Epoch 00173 | Loss 0.3191
14:38:04.788832 [0] Epoch: 173, Train: 0.9197, Val: 0.9056, Test: 0.7495
14:38:05.419184 [0] Epoch 00174 | Loss 0.3185
14:38:05.430631 [0] Epoch: 174, Train: 0.9198, Val: 0.9056, Test: 0.7496
14:38:06.060588 [0] Epoch 00175 | Loss 0.3180
14:38:06.072296 [0] Epoch: 175, Train: 0.9199, Val: 0.9057, Test: 0.7496
14:38:06.701721 [0] Epoch 00176 | Loss 0.3174
14:38:06.713205 [0] Epoch: 176, Train: 0.9201, Val: 0.9058, Test: 0.7496
14:38:07.343157 [0] Epoch 00177 | Loss 0.3169
14:38:07.354592 [0] Epoch: 177, Train: 0.9201, Val: 0.9055, Test: 0.7496
14:38:07.985128 [0] Epoch 00178 | Loss 0.3163
14:38:07.996382 [0] Epoch: 178, Train: 0.9202, Val: 0.9057, Test: 0.7497
14:38:08.626211 [0] Epoch 00179 | Loss 0.3158
14:38:08.637470 [0] Epoch: 179, Train: 0.9203, Val: 0.9055, Test: 0.7497
14:38:09.269060 [0] Epoch 00180 | Loss 0.3152
14:38:09.280230 [0] Epoch: 180, Train: 0.9205, Val: 0.9055, Test: 0.7497
14:38:09.909989 [0] Epoch 00181 | Loss 0.3147
14:38:09.920881 [0] Epoch: 181, Train: 0.9205, Val: 0.9056, Test: 0.7497
14:38:10.550550 [0] Epoch 00182 | Loss 0.3142
14:38:10.561519 [0] Epoch: 182, Train: 0.9206, Val: 0.9058, Test: 0.7498
14:38:11.191477 [0] Epoch 00183 | Loss 0.3136
14:38:11.202511 [0] Epoch: 183, Train: 0.9207, Val: 0.9058, Test: 0.7498
14:38:11.831699 [0] Epoch 00184 | Loss 0.3131
14:38:11.841910 [0] Epoch: 184, Train: 0.9208, Val: 0.9059, Test: 0.7498
14:38:12.472215 [0] Epoch 00185 | Loss 0.3126
14:38:12.483518 [0] Epoch: 185, Train: 0.9209, Val: 0.9060, Test: 0.7498
14:38:13.114501 [0] Epoch 00186 | Loss 0.3121
14:38:13.126360 [0] Epoch: 186, Train: 0.9210, Val: 0.9062, Test: 0.7498
14:38:13.755836 [0] Epoch 00187 | Loss 0.3115
14:38:13.766889 [0] Epoch: 187, Train: 0.9211, Val: 0.9063, Test: 0.7499
14:38:14.396422 [0] Epoch 00188 | Loss 0.3110
14:38:14.407544 [0] Epoch: 188, Train: 0.9213, Val: 0.9063, Test: 0.7499
14:38:15.036878 [0] Epoch 00189 | Loss 0.3105
14:38:15.048637 [0] Epoch: 189, Train: 0.9214, Val: 0.9063, Test: 0.7499
14:38:15.678018 [0] Epoch 00190 | Loss 0.3100
14:38:15.689609 [0] Epoch: 190, Train: 0.9215, Val: 0.9064, Test: 0.7500
14:38:16.318589 [0] Epoch 00191 | Loss 0.3095
14:38:16.329847 [0] Epoch: 191, Train: 0.9216, Val: 0.9066, Test: 0.7500
14:38:16.958857 [0] Epoch 00192 | Loss 0.3090
14:38:16.970329 [0] Epoch: 192, Train: 0.9217, Val: 0.9067, Test: 0.7500
14:38:17.599919 [0] Epoch 00193 | Loss 0.3085
14:38:17.610647 [0] Epoch: 193, Train: 0.9218, Val: 0.9068, Test: 0.7501
14:38:18.239932 [0] Epoch 00194 | Loss 0.3080
14:38:18.251299 [0] Epoch: 194, Train: 0.9218, Val: 0.9067, Test: 0.7501
14:38:18.880170 [0] Epoch 00195 | Loss 0.3075
14:38:18.890630 [0] Epoch: 195, Train: 0.9220, Val: 0.9067, Test: 0.7501
14:38:19.520203 [0] Epoch 00196 | Loss 0.3070
14:38:19.531522 [0] Epoch: 196, Train: 0.9221, Val: 0.9068, Test: 0.7501
14:38:20.161236 [0] Epoch 00197 | Loss 0.3066
14:38:20.172551 [0] Epoch: 197, Train: 0.9222, Val: 0.9068, Test: 0.7501
14:38:20.803424 [0] Epoch 00198 | Loss 0.3061
14:38:20.814519 [0] Epoch: 198, Train: 0.9224, Val: 0.9069, Test: 0.7501
14:38:21.445233 [0] Epoch 00199 | Loss 0.3056
14:38:21.456272 [0] Epoch: 199, Train: 0.9224, Val: 0.9072, Test: 0.7502
14:38:21.457064 [0] 
timer summary:
  0.29s   0.29s   200 broadcast ForwardL1 0
  0.39s   0.39s   800 broadcast
102.99s 102.99s   800 spmm
  9.55s   9.55s   800 mm
  0.02s   0.02s   200 broadcast ForwardL2 0
  0.03s   0.03s   200 broadcast BackwardL2 0
  0.11s   0.11s   400 all_reduce
  0.02s   0.02s   200 broadcast BackwardL1 0
129.24s 129.24s   200 epoch
153.27s 153.27s     1 total
14:52:54.489754 [0] proc begin: <DistEnv 0/1 nccl>
14:53:06.611357 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
14:53:06.622655 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from large pool |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2396 MiB |   2396 MiB |   2396 MiB |      0 B   |
|       from large pool |   2394 MiB |   2394 MiB |   2394 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 472519 KiB | 482086 KiB | 570319 KiB |  97800 KiB |
|       from large pool | 472519 KiB | 482086 KiB | 564170 KiB |  91651 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:53:09.044839 [0] Epoch 00000 | Loss 4.7410
14:53:09.679006 [0] Epoch 00001 | Loss 3.9229
14:53:10.313161 [0] Epoch 00002 | Loss 3.2705
14:53:10.947857 [0] Epoch 00003 | Loss 2.8071
14:53:11.582101 [0] Epoch 00004 | Loss 2.4711
14:53:12.216159 [0] Epoch 00005 | Loss 2.2229
14:53:12.850859 [0] Epoch 00006 | Loss 2.0293
14:53:13.486869 [0] Epoch 00007 | Loss 1.8677
14:53:14.125697 [0] Epoch 00008 | Loss 1.7264
14:53:14.761527 [0] Epoch 00009 | Loss 1.6003
14:53:15.397053 [0] Epoch 00010 | Loss 1.4872
14:53:16.032994 [0] Epoch 00011 | Loss 1.3859
14:53:16.669050 [0] Epoch 00012 | Loss 1.2961
14:53:17.304535 [0] Epoch 00013 | Loss 1.2172
14:53:17.941264 [0] Epoch 00014 | Loss 1.1486
14:53:18.577432 [0] Epoch 00015 | Loss 1.0889
14:53:19.211956 [0] Epoch 00016 | Loss 1.0369
14:53:19.847008 [0] Epoch 00017 | Loss 0.9914
14:53:20.490656 [0] Epoch 00018 | Loss 0.9510
14:53:21.125624 [0] Epoch 00019 | Loss 0.9149
14:53:21.759757 [0] Epoch 00020 | Loss 0.8822
14:53:22.394399 [0] Epoch 00021 | Loss 0.8523
14:53:23.029210 [0] Epoch 00022 | Loss 0.8247
14:53:23.663358 [0] Epoch 00023 | Loss 0.7990
14:53:24.296550 [0] Epoch 00024 | Loss 0.7752
14:53:24.929753 [0] Epoch 00025 | Loss 0.7531
14:53:25.562614 [0] Epoch 00026 | Loss 0.7329
14:53:26.196842 [0] Epoch 00027 | Loss 0.7146
14:53:26.831560 [0] Epoch 00028 | Loss 0.6980
14:53:27.465126 [0] Epoch 00029 | Loss 0.6829
14:53:28.098624 [0] Epoch 00030 | Loss 0.6688
14:53:28.732060 [0] Epoch 00031 | Loss 0.6555
14:53:29.365468 [0] Epoch 00032 | Loss 0.6426
14:53:29.998558 [0] Epoch 00033 | Loss 0.6303
14:53:30.632349 [0] Epoch 00034 | Loss 0.6187
14:53:31.265795 [0] Epoch 00035 | Loss 0.6079
14:53:31.899316 [0] Epoch 00036 | Loss 0.5981
14:53:32.534190 [0] Epoch 00037 | Loss 0.5891
14:53:33.168676 [0] Epoch 00038 | Loss 0.5808
14:53:33.803504 [0] Epoch 00039 | Loss 0.5730
14:53:34.438037 [0] Epoch 00040 | Loss 0.5656
14:53:35.071925 [0] Epoch 00041 | Loss 0.5583
14:53:35.705559 [0] Epoch 00042 | Loss 0.5511
14:53:36.339598 [0] Epoch 00043 | Loss 0.5440
14:53:36.974462 [0] Epoch 00044 | Loss 0.5371
14:53:37.608880 [0] Epoch 00045 | Loss 0.5305
14:53:38.242878 [0] Epoch 00046 | Loss 0.5242
14:53:38.876730 [0] Epoch 00047 | Loss 0.5183
14:53:39.510242 [0] Epoch 00048 | Loss 0.5127
14:53:40.142946 [0] Epoch 00049 | Loss 0.5074
14:53:40.777303 [0] Epoch 00050 | Loss 0.5023
14:53:41.410720 [0] Epoch 00051 | Loss 0.4975
14:53:42.044211 [0] Epoch 00052 | Loss 0.4927
14:53:42.677779 [0] Epoch 00053 | Loss 0.4880
14:53:43.310510 [0] Epoch 00054 | Loss 0.4835
14:53:43.942972 [0] Epoch 00055 | Loss 0.4790
14:53:44.576148 [0] Epoch 00056 | Loss 0.4748
14:53:45.208808 [0] Epoch 00057 | Loss 0.4707
14:53:45.843221 [0] Epoch 00058 | Loss 0.4669
14:53:46.477143 [0] Epoch 00059 | Loss 0.4632
14:53:47.109928 [0] Epoch 00060 | Loss 0.4597
14:53:47.742902 [0] Epoch 00061 | Loss 0.4563
14:53:48.376366 [0] Epoch 00062 | Loss 0.4530
14:53:49.010204 [0] Epoch 00063 | Loss 0.4498
14:53:49.644092 [0] Epoch 00064 | Loss 0.4468
14:53:50.278512 [0] Epoch 00065 | Loss 0.4438
14:53:50.912447 [0] Epoch 00066 | Loss 0.4409
14:53:51.546507 [0] Epoch 00067 | Loss 0.4381
14:53:52.180188 [0] Epoch 00068 | Loss 0.4353
14:53:52.816000 [0] Epoch 00069 | Loss 0.4327
14:53:53.450664 [0] Epoch 00070 | Loss 0.4301
14:53:54.083948 [0] Epoch 00071 | Loss 0.4276
14:53:54.718142 [0] Epoch 00072 | Loss 0.4252
14:53:55.352409 [0] Epoch 00073 | Loss 0.4229
14:53:55.985983 [0] Epoch 00074 | Loss 0.4207
14:53:56.619692 [0] Epoch 00075 | Loss 0.4186
14:53:57.253973 [0] Epoch 00076 | Loss 0.4165
14:53:57.889203 [0] Epoch 00077 | Loss 0.4145
14:53:58.523880 [0] Epoch 00078 | Loss 0.4125
14:53:59.157867 [0] Epoch 00079 | Loss 0.4105
14:53:59.799835 [0] Epoch 00080 | Loss 0.4086
14:54:00.433235 [0] Epoch 00081 | Loss 0.4068
14:54:01.067513 [0] Epoch 00082 | Loss 0.4050
14:54:01.701408 [0] Epoch 00083 | Loss 0.4032
14:54:02.335957 [0] Epoch 00084 | Loss 0.4015
14:54:02.972393 [0] Epoch 00085 | Loss 0.3998
14:54:03.609250 [0] Epoch 00086 | Loss 0.3981
14:54:04.243469 [0] Epoch 00087 | Loss 0.3965
14:54:04.877827 [0] Epoch 00088 | Loss 0.3949
14:54:05.515267 [0] Epoch 00089 | Loss 0.3933
14:54:06.150110 [0] Epoch 00090 | Loss 0.3918
14:54:06.784825 [0] Epoch 00091 | Loss 0.3903
14:54:07.419915 [0] Epoch 00092 | Loss 0.3888
14:54:08.055712 [0] Epoch 00093 | Loss 0.3874
14:54:08.692347 [0] Epoch 00094 | Loss 0.3860
14:54:09.327791 [0] Epoch 00095 | Loss 0.3846
14:54:09.962785 [0] Epoch 00096 | Loss 0.3833
14:54:10.599180 [0] Epoch 00097 | Loss 0.3819
14:54:11.234830 [0] Epoch 00098 | Loss 0.3806
14:54:11.870734 [0] Epoch 00099 | Loss 0.3793
14:54:12.505698 [0] Epoch 00100 | Loss 0.3781
14:54:13.140489 [0] Epoch 00101 | Loss 0.3768
14:54:13.774839 [0] Epoch 00102 | Loss 0.3756
14:54:14.408848 [0] Epoch 00103 | Loss 0.3744
14:54:15.043507 [0] Epoch 00104 | Loss 0.3732
14:54:15.677747 [0] Epoch 00105 | Loss 0.3721
14:54:16.312385 [0] Epoch 00106 | Loss 0.3709
14:54:16.946652 [0] Epoch 00107 | Loss 0.3698
14:54:17.580544 [0] Epoch 00108 | Loss 0.3687
14:54:18.214114 [0] Epoch 00109 | Loss 0.3677
14:54:18.847819 [0] Epoch 00110 | Loss 0.3666
14:54:19.481414 [0] Epoch 00111 | Loss 0.3655
14:54:20.115054 [0] Epoch 00112 | Loss 0.3645
14:54:20.749145 [0] Epoch 00113 | Loss 0.3635
14:54:21.383342 [0] Epoch 00114 | Loss 0.3625
14:54:22.018738 [0] Epoch 00115 | Loss 0.3615
14:54:22.654071 [0] Epoch 00116 | Loss 0.3605
14:54:23.288080 [0] Epoch 00117 | Loss 0.3595
14:54:23.922633 [0] Epoch 00118 | Loss 0.3586
14:54:24.556734 [0] Epoch 00119 | Loss 0.3576
14:54:25.191314 [0] Epoch 00120 | Loss 0.3567
14:54:25.825685 [0] Epoch 00121 | Loss 0.3558
14:54:26.459988 [0] Epoch 00122 | Loss 0.3549
14:54:27.093628 [0] Epoch 00123 | Loss 0.3540
14:54:27.728320 [0] Epoch 00124 | Loss 0.3531
14:54:28.362186 [0] Epoch 00125 | Loss 0.3522
14:54:28.996925 [0] Epoch 00126 | Loss 0.3514
14:54:29.631376 [0] Epoch 00127 | Loss 0.3505
14:54:30.265449 [0] Epoch 00128 | Loss 0.3497
14:54:30.899999 [0] Epoch 00129 | Loss 0.3489
14:54:31.534471 [0] Epoch 00130 | Loss 0.3480
14:54:32.168220 [0] Epoch 00131 | Loss 0.3472
14:54:32.803125 [0] Epoch 00132 | Loss 0.3464
14:54:33.439109 [0] Epoch 00133 | Loss 0.3456
14:54:34.074392 [0] Epoch 00134 | Loss 0.3448
14:54:34.710073 [0] Epoch 00135 | Loss 0.3441
14:54:35.345254 [0] Epoch 00136 | Loss 0.3433
14:54:35.980580 [0] Epoch 00137 | Loss 0.3425
14:54:36.616797 [0] Epoch 00138 | Loss 0.3418
14:54:37.252039 [0] Epoch 00139 | Loss 0.3410
14:54:37.887155 [0] Epoch 00140 | Loss 0.3403
14:54:38.521460 [0] Epoch 00141 | Loss 0.3395
14:54:39.155376 [0] Epoch 00142 | Loss 0.3388
14:54:39.789459 [0] Epoch 00143 | Loss 0.3381
14:54:40.422847 [0] Epoch 00144 | Loss 0.3374
14:54:41.056616 [0] Epoch 00145 | Loss 0.3367
14:54:41.691883 [0] Epoch 00146 | Loss 0.3360
14:54:42.325126 [0] Epoch 00147 | Loss 0.3353
14:54:42.958810 [0] Epoch 00148 | Loss 0.3346
14:54:43.593183 [0] Epoch 00149 | Loss 0.3339
14:54:44.227490 [0] Epoch 00150 | Loss 0.3332
14:54:44.860961 [0] Epoch 00151 | Loss 0.3326
14:54:45.495279 [0] Epoch 00152 | Loss 0.3319
14:54:46.129247 [0] Epoch 00153 | Loss 0.3312
14:54:46.762896 [0] Epoch 00154 | Loss 0.3306
14:54:47.397436 [0] Epoch 00155 | Loss 0.3299
14:54:48.032042 [0] Epoch 00156 | Loss 0.3293
14:54:48.665947 [0] Epoch 00157 | Loss 0.3287
14:54:49.299998 [0] Epoch 00158 | Loss 0.3280
14:54:49.933749 [0] Epoch 00159 | Loss 0.3274
14:54:50.567024 [0] Epoch 00160 | Loss 0.3268
14:54:51.201016 [0] Epoch 00161 | Loss 0.3262
14:54:51.835440 [0] Epoch 00162 | Loss 0.3255
14:54:52.470224 [0] Epoch 00163 | Loss 0.3249
14:54:53.104645 [0] Epoch 00164 | Loss 0.3243
14:54:53.739281 [0] Epoch 00165 | Loss 0.3237
14:54:54.373947 [0] Epoch 00166 | Loss 0.3231
14:54:55.008060 [0] Epoch 00167 | Loss 0.3225
14:54:55.642172 [0] Epoch 00168 | Loss 0.3220
14:54:56.277326 [0] Epoch 00169 | Loss 0.3214
14:54:56.911945 [0] Epoch 00170 | Loss 0.3208
14:54:57.547226 [0] Epoch 00171 | Loss 0.3202
14:54:58.181889 [0] Epoch 00172 | Loss 0.3197
14:54:58.816583 [0] Epoch 00173 | Loss 0.3191
14:54:59.451162 [0] Epoch 00174 | Loss 0.3185
14:55:00.085500 [0] Epoch 00175 | Loss 0.3180
14:55:00.720049 [0] Epoch 00176 | Loss 0.3174
14:55:01.354716 [0] Epoch 00177 | Loss 0.3169
14:55:01.990765 [0] Epoch 00178 | Loss 0.3163
14:55:02.627665 [0] Epoch 00179 | Loss 0.3158
14:55:03.264449 [0] Epoch 00180 | Loss 0.3152
14:55:03.900184 [0] Epoch 00181 | Loss 0.3147
14:55:04.534671 [0] Epoch 00182 | Loss 0.3142
14:55:05.169148 [0] Epoch 00183 | Loss 0.3136
14:55:05.802665 [0] Epoch 00184 | Loss 0.3131
14:55:06.436061 [0] Epoch 00185 | Loss 0.3126
14:55:07.070430 [0] Epoch 00186 | Loss 0.3121
14:55:07.705657 [0] Epoch 00187 | Loss 0.3115
14:55:08.339894 [0] Epoch 00188 | Loss 0.3110
14:55:08.975087 [0] Epoch 00189 | Loss 0.3105
14:55:09.610070 [0] Epoch 00190 | Loss 0.3100
14:55:10.244520 [0] Epoch 00191 | Loss 0.3095
14:55:10.879110 [0] Epoch 00192 | Loss 0.3090
14:55:11.514034 [0] Epoch 00193 | Loss 0.3085
14:55:12.148022 [0] Epoch 00194 | Loss 0.3080
14:55:12.782776 [0] Epoch 00195 | Loss 0.3075
14:55:13.416330 [0] Epoch 00196 | Loss 0.3070
14:55:14.050454 [0] Epoch 00197 | Loss 0.3066
14:55:14.684453 [0] Epoch 00198 | Loss 0.3061
14:55:15.318502 [0] Epoch 00199 | Loss 0.3056
14:55:15.323853 [0] 
timer summary:
  0.33s   0.33s   200 broadcast ForwardL1 0
  0.43s   0.43s   800 broadcast
103.00s 103.00s   800 spmm
  9.53s   9.53s   800 mm
  0.02s   0.02s   200 broadcast ForwardL2 0
  0.03s   0.03s   200 broadcast BackwardL2 0
  0.10s   0.10s   400 all_reduce
  0.02s   0.02s   200 broadcast BackwardL1 0
128.02s 128.02s   200 epoch
140.83s 140.83s     1 total
14:55:42.410706 [0] proc begin: <DistEnv 0/1 nccl>
14:55:52.344532 [0] graph loaded <COO Graph: ogbn-products, |V|: 2449029, |E|: 123718280, masks: 196615,39323,2213091><Local: 0, |V|: 2449029, |E|: 126167053>
14:55:52.352612 [0] graph loaded
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from large pool |   1932 MiB |   1951 MiB |   2925 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from large pool |   1931 MiB |   1950 MiB |   2924 MiB |    992 MiB |
|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2396 MiB |   2396 MiB |   2396 MiB |      0 B   |
|       from large pool |   2394 MiB |   2394 MiB |   2394 MiB |      0 B   |
|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory | 472519 KiB | 482086 KiB | 570319 KiB |  97800 KiB |
|       from large pool | 472519 KiB | 482086 KiB | 564170 KiB |  91651 KiB |
|       from small pool |      0 KiB |   2047 KiB |   6148 KiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Allocations           |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |       8    |      12    |      22    |      14    |
|       from large pool |       8    |       9    |      13    |       5    |
|       from small pool |       0    |       3    |       9    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       6    |       6    |       6    |       0    |
|       from large pool |       5    |       5    |       5    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       5    |       9    |       5    |
|       from large pool |       4    |       4    |       6    |       2    |
|       from small pool |       0    |       1    |       3    |       3    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

14:55:54.376087 [0] Epoch 00000 | Loss 4.7410
14:55:55.009747 [0] Epoch 00001 | Loss 3.9229
14:55:55.643046 [0] Epoch 00002 | Loss 3.2705
14:55:56.276518 [0] Epoch 00003 | Loss 2.8071
14:55:56.910775 [0] Epoch 00004 | Loss 2.4711
14:55:57.544352 [0] Epoch 00005 | Loss 2.2229
14:55:58.178154 [0] Epoch 00006 | Loss 2.0293
14:55:58.811403 [0] Epoch 00007 | Loss 1.8677
14:55:59.444075 [0] Epoch 00008 | Loss 1.7264
14:56:00.076386 [0] Epoch 00009 | Loss 1.6003
14:56:00.708603 [0] Epoch 00010 | Loss 1.4872
14:56:01.342611 [0] Epoch 00011 | Loss 1.3859
14:56:01.978889 [0] Epoch 00012 | Loss 1.2961
14:56:02.615433 [0] Epoch 00013 | Loss 1.2172
14:56:03.249175 [0] Epoch 00014 | Loss 1.1486
14:56:03.883633 [0] Epoch 00015 | Loss 1.0889
14:56:04.517888 [0] Epoch 00016 | Loss 1.0369
14:56:05.150551 [0] Epoch 00017 | Loss 0.9914
14:56:05.783704 [0] Epoch 00018 | Loss 0.9510
14:56:06.416432 [0] Epoch 00019 | Loss 0.9149
14:56:07.051993 [0] Epoch 00020 | Loss 0.8822
14:56:07.686548 [0] Epoch 00021 | Loss 0.8523
14:56:08.319607 [0] Epoch 00022 | Loss 0.8247
14:56:08.952381 [0] Epoch 00023 | Loss 0.7990
14:56:09.586267 [0] Epoch 00024 | Loss 0.7752
14:56:10.220982 [0] Epoch 00025 | Loss 0.7531
14:56:10.854200 [0] Epoch 00026 | Loss 0.7329
14:56:11.487870 [0] Epoch 00027 | Loss 0.7146
14:56:12.121104 [0] Epoch 00028 | Loss 0.6980
14:56:12.754825 [0] Epoch 00029 | Loss 0.6829
14:56:13.388804 [0] Epoch 00030 | Loss 0.6688
14:56:14.022105 [0] Epoch 00031 | Loss 0.6555
14:56:14.655234 [0] Epoch 00032 | Loss 0.6426
14:56:15.288388 [0] Epoch 00033 | Loss 0.6303
14:56:15.921249 [0] Epoch 00034 | Loss 0.6187
14:56:16.555512 [0] Epoch 00035 | Loss 0.6079
14:56:17.189023 [0] Epoch 00036 | Loss 0.5981
14:56:17.822855 [0] Epoch 00037 | Loss 0.5891
14:56:18.456218 [0] Epoch 00038 | Loss 0.5808
14:56:19.089157 [0] Epoch 00039 | Loss 0.5730
14:56:19.722198 [0] Epoch 00040 | Loss 0.5656
14:56:20.355333 [0] Epoch 00041 | Loss 0.5583
14:56:20.988319 [0] Epoch 00042 | Loss 0.5511
14:56:21.621776 [0] Epoch 00043 | Loss 0.5440
14:56:22.255597 [0] Epoch 00044 | Loss 0.5371
14:56:22.890213 [0] Epoch 00045 | Loss 0.5305
14:56:23.524047 [0] Epoch 00046 | Loss 0.5242
14:56:24.157948 [0] Epoch 00047 | Loss 0.5183
14:56:24.791822 [0] Epoch 00048 | Loss 0.5127
14:56:25.425426 [0] Epoch 00049 | Loss 0.5074
14:56:26.058684 [0] Epoch 00050 | Loss 0.5023
14:56:26.692693 [0] Epoch 00051 | Loss 0.4975
14:56:27.326050 [0] Epoch 00052 | Loss 0.4927
14:56:27.961398 [0] Epoch 00053 | Loss 0.4880
14:56:28.594820 [0] Epoch 00054 | Loss 0.4835
14:56:29.228784 [0] Epoch 00055 | Loss 0.4790
14:56:29.862789 [0] Epoch 00056 | Loss 0.4748
14:56:30.495872 [0] Epoch 00057 | Loss 0.4707
14:56:31.129446 [0] Epoch 00058 | Loss 0.4669
14:56:31.762457 [0] Epoch 00059 | Loss 0.4632
14:56:32.395856 [0] Epoch 00060 | Loss 0.4597
14:56:33.029291 [0] Epoch 00061 | Loss 0.4563
14:56:33.663438 [0] Epoch 00062 | Loss 0.4530
14:56:34.296984 [0] Epoch 00063 | Loss 0.4498
14:56:34.931002 [0] Epoch 00064 | Loss 0.4468
14:56:35.564991 [0] Epoch 00065 | Loss 0.4438
14:56:36.198981 [0] Epoch 00066 | Loss 0.4409
14:56:36.831940 [0] Epoch 00067 | Loss 0.4381
14:56:37.465608 [0] Epoch 00068 | Loss 0.4353
14:56:38.099588 [0] Epoch 00069 | Loss 0.4327
14:56:38.732842 [0] Epoch 00070 | Loss 0.4301
14:56:39.367272 [0] Epoch 00071 | Loss 0.4276
14:56:40.001951 [0] Epoch 00072 | Loss 0.4252
14:56:40.635625 [0] Epoch 00073 | Loss 0.4229
14:56:41.268848 [0] Epoch 00074 | Loss 0.4207
14:56:41.902647 [0] Epoch 00075 | Loss 0.4186
14:56:42.536449 [0] Epoch 00076 | Loss 0.4165
14:56:43.170950 [0] Epoch 00077 | Loss 0.4145
14:56:43.804069 [0] Epoch 00078 | Loss 0.4125
14:56:44.437804 [0] Epoch 00079 | Loss 0.4105
14:56:45.071800 [0] Epoch 00080 | Loss 0.4086
14:56:45.705678 [0] Epoch 00081 | Loss 0.4068
14:56:46.339230 [0] Epoch 00082 | Loss 0.4050
14:56:46.972718 [0] Epoch 00083 | Loss 0.4032
14:56:47.606591 [0] Epoch 00084 | Loss 0.4015
14:56:48.240950 [0] Epoch 00085 | Loss 0.3998
14:56:48.875230 [0] Epoch 00086 | Loss 0.3981
14:56:49.510677 [0] Epoch 00087 | Loss 0.3965
14:56:50.143940 [0] Epoch 00088 | Loss 0.3949
14:56:50.779345 [0] Epoch 00089 | Loss 0.3933
14:56:51.413037 [0] Epoch 00090 | Loss 0.3918
14:56:52.047454 [0] Epoch 00091 | Loss 0.3903
14:56:52.682126 [0] Epoch 00092 | Loss 0.3888
14:56:53.316314 [0] Epoch 00093 | Loss 0.3874
14:56:53.950843 [0] Epoch 00094 | Loss 0.3860
14:56:54.585097 [0] Epoch 00095 | Loss 0.3846
14:56:55.218907 [0] Epoch 00096 | Loss 0.3833
14:56:55.852761 [0] Epoch 00097 | Loss 0.3819
14:56:56.486387 [0] Epoch 00098 | Loss 0.3806
14:56:57.119372 [0] Epoch 00099 | Loss 0.3793
14:56:57.752916 [0] Epoch 00100 | Loss 0.3781
14:56:58.386883 [0] Epoch 00101 | Loss 0.3768
14:56:59.020156 [0] Epoch 00102 | Loss 0.3756
14:56:59.654496 [0] Epoch 00103 | Loss 0.3744
14:57:00.288876 [0] Epoch 00104 | Loss 0.3732
14:57:00.923553 [0] Epoch 00105 | Loss 0.3721
14:57:01.557596 [0] Epoch 00106 | Loss 0.3709
14:57:02.195032 [0] Epoch 00107 | Loss 0.3698
14:57:02.831660 [0] Epoch 00108 | Loss 0.3687
14:57:03.469400 [0] Epoch 00109 | Loss 0.3677
14:57:04.103716 [0] Epoch 00110 | Loss 0.3666
14:57:04.739295 [0] Epoch 00111 | Loss 0.3655
14:57:05.374425 [0] Epoch 00112 | Loss 0.3645
14:57:06.008982 [0] Epoch 00113 | Loss 0.3635
14:57:06.643668 [0] Epoch 00114 | Loss 0.3625
14:57:07.278267 [0] Epoch 00115 | Loss 0.3615
14:57:07.913839 [0] Epoch 00116 | Loss 0.3605
14:57:08.548182 [0] Epoch 00117 | Loss 0.3595
14:57:09.181756 [0] Epoch 00118 | Loss 0.3586
14:57:09.816351 [0] Epoch 00119 | Loss 0.3576
14:57:10.451915 [0] Epoch 00120 | Loss 0.3567
14:57:11.086789 [0] Epoch 00121 | Loss 0.3558
14:57:11.721474 [0] Epoch 00122 | Loss 0.3549
14:57:12.355701 [0] Epoch 00123 | Loss 0.3540
14:57:12.990664 [0] Epoch 00124 | Loss 0.3531
14:57:13.625641 [0] Epoch 00125 | Loss 0.3522
14:57:14.259928 [0] Epoch 00126 | Loss 0.3514
14:57:14.894310 [0] Epoch 00127 | Loss 0.3505
14:57:15.529557 [0] Epoch 00128 | Loss 0.3497
14:57:16.164328 [0] Epoch 00129 | Loss 0.3489
14:57:16.798717 [0] Epoch 00130 | Loss 0.3480
14:57:17.433687 [0] Epoch 00131 | Loss 0.3472
14:57:18.068326 [0] Epoch 00132 | Loss 0.3464
14:57:18.703319 [0] Epoch 00133 | Loss 0.3456
14:57:19.337741 [0] Epoch 00134 | Loss 0.3448
14:57:19.971775 [0] Epoch 00135 | Loss 0.3441
14:57:20.606327 [0] Epoch 00136 | Loss 0.3433
14:57:21.240328 [0] Epoch 00137 | Loss 0.3425
14:57:21.874739 [0] Epoch 00138 | Loss 0.3418
14:57:22.509537 [0] Epoch 00139 | Loss 0.3410
14:57:23.144336 [0] Epoch 00140 | Loss 0.3403
14:57:23.779186 [0] Epoch 00141 | Loss 0.3395
14:57:24.413230 [0] Epoch 00142 | Loss 0.3388
14:57:25.047801 [0] Epoch 00143 | Loss 0.3381
14:57:25.683283 [0] Epoch 00144 | Loss 0.3374
14:57:26.317721 [0] Epoch 00145 | Loss 0.3367
14:57:26.951882 [0] Epoch 00146 | Loss 0.3360
14:57:27.586476 [0] Epoch 00147 | Loss 0.3353
14:57:28.220374 [0] Epoch 00148 | Loss 0.3346
14:57:28.853762 [0] Epoch 00149 | Loss 0.3339
14:57:29.487866 [0] Epoch 00150 | Loss 0.3332
14:57:30.121353 [0] Epoch 00151 | Loss 0.3326
14:57:30.755551 [0] Epoch 00152 | Loss 0.3319
14:57:31.389012 [0] Epoch 00153 | Loss 0.3312
14:57:32.022792 [0] Epoch 00154 | Loss 0.3306
14:57:32.657053 [0] Epoch 00155 | Loss 0.3299
14:57:33.291222 [0] Epoch 00156 | Loss 0.3293
14:57:33.925123 [0] Epoch 00157 | Loss 0.3287
14:57:34.558764 [0] Epoch 00158 | Loss 0.3280
14:57:35.192479 [0] Epoch 00159 | Loss 0.3274
14:57:35.825847 [0] Epoch 00160 | Loss 0.3268
14:57:36.460128 [0] Epoch 00161 | Loss 0.3262
14:57:37.093888 [0] Epoch 00162 | Loss 0.3255
14:57:37.727774 [0] Epoch 00163 | Loss 0.3249
14:57:38.361824 [0] Epoch 00164 | Loss 0.3243
14:57:38.995904 [0] Epoch 00165 | Loss 0.3237
14:57:39.630218 [0] Epoch 00166 | Loss 0.3231
14:57:40.263820 [0] Epoch 00167 | Loss 0.3225
14:57:40.897201 [0] Epoch 00168 | Loss 0.3220
14:57:41.531623 [0] Epoch 00169 | Loss 0.3214
14:57:42.165319 [0] Epoch 00170 | Loss 0.3208
14:57:42.798781 [0] Epoch 00171 | Loss 0.3202
14:57:43.434147 [0] Epoch 00172 | Loss 0.3197
14:57:44.068894 [0] Epoch 00173 | Loss 0.3191
14:57:44.703118 [0] Epoch 00174 | Loss 0.3185
14:57:45.337554 [0] Epoch 00175 | Loss 0.3180
14:57:45.971885 [0] Epoch 00176 | Loss 0.3174
14:57:46.607998 [0] Epoch 00177 | Loss 0.3169
14:57:47.243774 [0] Epoch 00178 | Loss 0.3163
14:57:47.879135 [0] Epoch 00179 | Loss 0.3158
14:57:48.513513 [0] Epoch 00180 | Loss 0.3152
14:57:49.148249 [0] Epoch 00181 | Loss 0.3147
14:57:49.782841 [0] Epoch 00182 | Loss 0.3142
14:57:50.417385 [0] Epoch 00183 | Loss 0.3136
14:57:51.052351 [0] Epoch 00184 | Loss 0.3131
14:57:51.687178 [0] Epoch 00185 | Loss 0.3126
14:57:52.322026 [0] Epoch 00186 | Loss 0.3121
14:57:52.957888 [0] Epoch 00187 | Loss 0.3115
14:57:53.593200 [0] Epoch 00188 | Loss 0.3110
14:57:54.228147 [0] Epoch 00189 | Loss 0.3105
14:57:54.863050 [0] Epoch 00190 | Loss 0.3100
14:57:55.498493 [0] Epoch 00191 | Loss 0.3095
14:57:56.132859 [0] Epoch 00192 | Loss 0.3090
14:57:56.767621 [0] Epoch 00193 | Loss 0.3085
14:57:57.402261 [0] Epoch 00194 | Loss 0.3080
14:57:58.037537 [0] Epoch 00195 | Loss 0.3075
14:57:58.672383 [0] Epoch 00196 | Loss 0.3070
14:57:59.306419 [0] Epoch 00197 | Loss 0.3066
14:57:59.941938 [0] Epoch 00198 | Loss 0.3061
14:58:00.577199 [0] Epoch 00199 | Loss 0.3056
14:58:00.582663 [0] 
timer summary:
  0.25s   0.25s   200 broadcast ForwardL1 0
  0.33s   0.33s   800 broadcast
102.98s 102.98s   800 spmm
  9.53s   9.53s   800 mm
  0.02s   0.02s   200 broadcast ForwardL2 0
  0.02s   0.02s   200 broadcast BackwardL2 0
  0.10s   0.10s   400 all_reduce
  0.01s   0.01s   200 broadcast BackwardL1 0
127.73s 127.73s   200 epoch
138.17s 138.17s     1 total
